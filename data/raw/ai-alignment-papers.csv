Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body,Rohin_Shah_Blurb
XBZAPQFK,blogPost,2020,"Kokotajlo, Daniel",Three kinds of competitiveness,AI Impacts,,,,https://aiimpacts.org/three-kinds-of-competitiveness/,"By Daniel Kokotajlo In this post, I distinguish between three different kinds of competitiveness -- Performance, Cost, and Date -- and explain why I think these distinctions are worth the brainspace they occupy. For example, they help me introduce and discuss a problem for AI safety proposals having to do with aligned AIs being outcompeted...",2020-03-30,2022-01-30 1:53:10,2022-01-30 1:53:10,2021-11-20 18:55:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PU9A2KS8/three-kinds-of-competitiveness.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQCZM53S,blogPost,2021,"Clarke, Sam; Martin, Samuel Dylan",Distinguishing AI takeover scenarios,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios,"Epistemic status: lots of this involves interpreting/categorising other people’s scenarios, and could be wrong. We’d really appreciate being corrected if so. [ETA: so far, no corrections.] TLDR: see the summary table. In the last few years, people have proposed various AI takeover scenarios. We think this type of scenario building is great, since there are now more concrete ideas of what AI takeover could realistically look like. That said, we have been confused for a while about how the different scenarios relate to each other and what different assumptions they make. This post might be helpful for anyone who has similar confusions. We focus on explaining the differences between seven prominent scenarios: the  ‘Brain-in-a-box’ scenario, ‘What failure looks like’ part 1 (WFLL 1), ‘What failure looks like’ part 2 (WFLL 2), ‘Another (outer) alignment failure story’  (AAFS), ‘Production Web’, ‘Flash economy’ and ‘Soft takeoff leading to decisive strategic advantage’. While these scenarios do not capture alI of the risks from transformative AI, participants in a recent survey aimed at leading AI safety/governance researchers estimated the first three of these scenarios to cover 50% of existential catastrophes from AI.[1] We plan to follow up with a subsequent post, which discusses some of the issues raised here in greater depth. VARIABLES RELATING TO AI TAKEOVER SCENARIOS We define AI takeover to be a scenario where the most consequential decisions about the future get made by AI systems with goals that aren’t desirable by human standards. There are three variables which are sufficient to distinguish the takeover scenarios discussed in this post. We will briefly introduce these three variables, and a number of others that are generally useful for thinking about takeover scenarios. Key variables for distinguishing the AI takeover scenarios in this post:  * Speed. Is there a sudden jump in AI capabilities over a very short period    (i.e. much faster than what we",2021-09-08,2022-01-30 4:47:42,2022-01-30 4:47:42,2021-11-18 23:45:23,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ENAMQXCU/distinguishing-ai-takeover-scenarios.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D95GDF2T,blogPost,2020,"Wentworth, John",Toy Problem: Detective Story Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment,"Suppose I train some simple unsupervised topic model (e.g. LDA) on a bunch of books. I look through the topics it learns, and find one corresponding to detective stories. The problem: I would like to use the identified detective-story cluster to generate detective stories from GPT. The hard part: I would like to do this in such a way that the precision of the notion of detective-stories used by the final system is not limited by the original simple model. Here’s what that means, visually. The space of real-world books has some clusters in it: One of those clusters is the detective-story cluster. The simple model approximates those clusters using something simple - for the sake of visualization, ellipses: The more complex model (e.g. GPT) presumably has a much more precise approximation of the shape of the clusters: So, we’d like to use the simple model to identify one of the clusters, but then still use the full power of the complex model to sample from that cluster. Of course, GPT may not contain a single variable corresponding to a cluster-id, which is largely what makes the problem interesting. GPT may not internally use a notion of “cluster” at all. However, the GPT model should still contain something (approximately) isomorphic to the original cluster, since that real pattern is still in the data/environment: since there is a real cluster of ""detective stories"" in the data/environment itself, the GPT model should also contain that cluster, to the extent that the GPT model matches the data/environment. In particular, the “precision not limited by original model” requirement rules out the obvious strategy of generating random samples from GPT and selecting those which the simple model labels as detective-stories. If we do that, then we’ll end up with some non-detective-stories in the output, because of shortcomings in the simple model’s notion of detective-stories. Visually, we’d be filtering based on the ellipse approximation of the cluster, which is exac",2020-10-13,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-08 23:30:38,,,,,,,Toy Problem,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MWR5EK7V/toy-problem-detective-story-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IF3G8A6B,blogPost,2020,"Riedel, Jess; Deibel, Angelica",TAI Safety Bibliographic Database,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database,"Authors: Jess Riedel and Angelica Deibel Cross-posted to EA Forum In this post we present the first public version of our bibliographic database of research on the safety of transformative artificial intelligence (TAI). The primary motivations for assembling this database were to:  1. Aid potential donors in assessing organizations focusing on TAI safety by     collecting and analyzing their research output.  2. Assemble a comprehensive bibliographic database that can be used as a base     for future projects, such as a living review of the field. The database contains research works motivated by, and substantively informing, the challenge of ensuring the safety of TAI, including both technical and meta topics. This initial version of the database has attempted comprehensive coverage only for traditionally formatted research produced in 2016-2020 by organizations with a significant safety focus (~360 items). The database also has significant but non-comprehensive coverage (~570 items) of earlier years, less traditional formats (e.g., blog posts), and non-safety-focused organizations. Usefully, we also have citation counts for essentially all the items for which that is applicable. The core database takes the form of a Zotero library. Snapshots are also available as Google Sheet, CSV, and Zotero RDF. (Compact version for easier human reading: Google Sheet, CSV.) The rest of this post describes the composition of the database in more detail and presents some high-level quantitative analysis of the contents. In particular, our analysis includes:  * Lists of the most cited TAI safety research for each of the past few years    (Tables 2 and 3)  * A chart showing how written TAI safety research output has changed since 2016    (Figure 1).  * A visualization of the degree of collaboration on TAI safety between    different research organizations (Table 4).  * A chart showing how the format of written research varied between    organizations, e.g., manuscripts vs. jou",2020-12-22,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-13 14:31:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AICDKECS,blogPost,2020,"Muehlhauser, Luke",Our AI governance grantmaking so far,Open Philanthropy,,,,https://www.openphilanthropy.org/blog/ai-governance-grantmaking,"When the Soviet Union began to fracture in 1991, the world was forced to reckon with the first collapse of a nuclear superpower in history.My thanks to Nathan Calvin for his help researching and drafting these opening paragraphs about the Nunn-Lugar Act. The USSR was home to more than 27,000",2020-12-16,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-13 14:33:13,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K3XMEA86,blogPost,2020,"Xu, Mark",The Solomonoff Prior is Malign,,,,,https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign,"This argument came to my attention from this post by Paul Christiano. I also found this clarification helpful. I found these counter-arguments stimulating and have included some discussion of them. Very little of this content is original. My contributions consist of fleshing out arguments and constructing examples. Thank you to Beth Barnes and Thomas Kwa for helpful discussion and comments. WHAT IS THE SOLOMONOFF PRIOR? The Solomonoff prior is intended to answer the question ""what is the probability of X?"" for any X, where X is a finite string over some finite alphabet. The Solomonoff prior is defined by taking the set of all Turing machines (TMs) which output strings when run with no input and weighting them proportional to2−K, whereKis the description length of the TM (informally its size in bits). The Solomonoff prior says the probability of a string is the sum over all the weights of all TMs that print that string. One reason to care about the Solomonoff prior is that we can use it to do a form of idealized induction. If you have seen 0101 and want to predict the next bit, you can use the Solomonoff prior to get the probability of 01010 and 01011. Normalizing gives you the chances of seeing 1 versus 0, conditioned on seeing 0101. In general, any process that assigns probabilities to all strings in a consistent way can be used to do induction in this way. This post provides more information about Solomonoff Induction. WHY IS IT MALIGN? Imagine that you wrote a programming language called python^10 that works as follows: First, it takes all alpha-numeric chars that are not in literals and checks if they're repeated 10 times sequentially. If they're not, they get deleted. If they are, they get replaced by a single copy. Second, it runs this new program through a python interpreter. Hello world in python^10: ppppppppprrrrrrrrrriiiiiiiiiinnnnnnnnnntttttttttt('Hello, world!') Luckily, python has an exec function that executes literals as code. This lets us w",2020-10-13,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-08 23:25:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JSBHRC49/the-solomonoff-prior-is-malign.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHAC26JP,blogPost,2020,"Branwen, Gwern",The Scaling Hypothesis,,,,,https://www.gwern.net/Scaling-hypothesis,"On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data & compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.",2020-05-28,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-14 18:58:38,,,,,,,,,,,,,,en-us,https://creativecommons.org/publicdomain/zero/1.0/,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6N626N6Q/Scaling-hypothesis.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5Q2FQC3T,blogPost,2020,"Shimi, Adam","The ""Backchaining to Local Search"" Technique in AI Alignment",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment,"In the spirit of this post by John S. Wentworth, this is a reference for a technique I learned from Evan Hubinger. He's probably not the first to use it, but he introduced it to me, so he gets the credit. In a single sentence, backchaining to local search is the idea of looking at how a problem of alignment could appear through local search (think gradient descent). So it starts with a certain problem (say reward tampering), and then tries to create a context where the usual training process in ML (local search) could create a system suffering from this problem. It’s an instance of backchaining in general, which just looks for how a problem could appear in practice. Backchaining to local search has two main benefits:  * It helps decide whether this specific problem is something we should worry    about.  * It forces you to consider your problem from a local search perspective,    instead of the more intuitive human/adversarial perspective (how would I mess    this up?). Let's look at a concrete example: reward gaming (also called specification gaming). To be even more concrete, we have a system with a camera and other sensors, and its goal is to maximize the amount of time when my friend Tom smiles, as measured through a loss function that captures whether the camera sees Tom smiling. The obvious (for us) way to do reward gaming here is to put a picture of Tom’s smiling face in front of the camera -- then the loss function is minimized. The backchaining to local search technique applied to this example asks ""How can I get this reward gaming behavior by local search?"" Well this reward gaming strategy is probably a local minima for the loss function (as changing just a little the behavior would increase the loss significantly), so local search could find it and stay in there. It's also better than most simple strategies, as ensuring that someone smiles (not necessarily a good goal, mind you) requires rather complex actions in the world (like going full ""Joker"" on",2020-09-18,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-07 22:33:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/74AE9R4X/the-backchaining-to-local-search-technique-in-ai-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AQ9D7WK8,blogPost,2020,"Ngo, Richard",Shaping safer goals,AI Alignment Forum,,,,https://www.alignmentforum.org/s/boLPsyNwd6teK5key,A community blog devoted to technical AI alignment research,2020-07-01,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-07 22:49:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VCMH6DTH/boLPsyNwd6teK5key.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79Z4M7BF,blogPost,2020,"Turner, Alex",Non-Obstruction: A Simple Concept Motivating Corrigibility,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility,"Thanks to Mathias Bonde, Tiffany Cai, Ryan Carey, Michael Cohen, Joe Collman, Andrew Critch, Abram Demski, Michael Dennis, Thomas Gilbert, Matthew Graves, Koen Holtman, Evan Hubinger, Victoria Krakovna, Amanda Ngo, Rohin Shah, Adam Shimi, Logan Smith, and Mark Xu for their thoughts. Main claim: corrigibility’s benefits can be mathematically represented as a counterfactual form of alignment. Overview: I’m going to talk about a unified mathematical frame I have for understanding corrigibility’s benefits, what it “is”, and what it isn’t. This frame is precisely understood by graphing the human overseer’s ability to achieve various goals (their attainable utility (AU) landscape). I argue that corrigibility’s benefits are secretly a form of counterfactual alignment (alignment with a set of goals the human may want to pursue). A counterfactually aligned agent doesn't have to let us literally correct it. Rather, this frame theoretically motivates why we might want corrigibility anyways. This frame also motivates other AI alignment subproblems, such as intent alignment, mild optimization, and low impact. NOMENCLATURE Corrigibility goes by a lot of concepts: “not incentivized to stop us from shutting it off”, “wants to account for its own flaws”, “doesn’t take away much power from us”, etc. Named by Robert Miles, the word ‘corrigibility’ means “able to be corrected [by humans]."" I’m going to argue that these are correlates of a key thing we plausibly actually want from the agent design, which seems conceptually simple. In this post, I take the following common-language definitions:  * Corrigibility: the AI literally lets us correct it (modify its policy), and    it doesn't manipulate us either. * Without both of these conditions, the AI's       behavior isn't sufficiently constrained for the concept to be useful.       Being able to correct it is small comfort if it manipulates us into making       the modifications it wants. An AI which is only non-manipulative doesn'",2020,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-13 15:28:56,,,,,,,Non-Obstruction,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8IPMHT9D/non-obstruction-a-simple-concept-motivating-corrigibility.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SN2KMUEK,blogPost,2020,"Aird, Michael; Shovelain, Justin; algekalipso",Memetic downside risks: How ideas can evolve and cause harm,LessWrong,,,,https://www.lesswrong.com/posts/EdAHNdbkGR6ndAPJD/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm,"This post was written for Convergence Analysis. OVERVIEW We introduce the concept of memetic downside risks (MDR): risks of unintended negative effects that arise from how ideas “evolve” over time (as a result of  replication, mutation, and selection). We discuss how this concept relates to the existing concepts of memetics, downside risks, and information hazards. We then outline four “directions” in which ideas may evolve: towards simplicity, salience, usefulness, and perceived usefulness. For each “direction”, we give an example to illustrate how an idea mutating in that direction could have negative effects. We then discuss some implications of these ideas for people and organisations trying to improve the world, who wish to achieve their altruistic objectives and minimise the unintended harms they cause. For example, we argue that the possibility of memetic downside risks increases the value of caution about what and how to communicate, and of “high-fidelity” methods of communication. BACKGROUND AND CONCEPT MEMETICS Wikipedia describes a meme as: an idea, behavior, or style that spreads by means of imitation from person to person within a culture—often with the aim of conveying a particular phenomenon, theme, or meaning represented by the meme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other imitable phenomena with a mimicked theme. The same article goes on to say: Proponents [of the concept of memes] theorize that memes are a viral phenomenon that may evolve by natural selection in a manner analogous to that of biological evolution. Memes do this through the processes of variation, mutation, competition, and inheritance, each of which influences a meme's reproductive success. Memes spread through the behavior that they generate in their hosts. Memes that propagate less prolifically may become extinct, while others may survive,",2020-02-25,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-20 19:01:44,,,,,,,Memetic downside risks,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X65UHRG3/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98XXRJPB,blogPost,2020,"Koch, Jack",Mapping the Conceptual Territory in AI Existential Safety and Alignment,Jack Koch,,,,https://jbkjr.github.io/posts/2020/12/mapping_conceptual_territory_AI_safety_alignment/,"Throughout my studies in alignment and AI-related existential risks, I’ve found it helpful to build a mental map of the field and how its various questions and considerations interrelate, so that when I read a new paper, a post on the Alignment Forum, or similar material, I have some idea of how it might contribute to the overall goal of making our deployment of AI technology go as well as possible for humanity. I’m writing this post to communicate what I’ve learned through this process, in order to help others trying to build their own mental maps and provide them with links to relevant resources for further, more detailed information. This post was largely inspired by (and would not be possible without) two talks by Paul Christiano and Rohin Shah, respectively, that give very similar overviews of the field,1 as well as a few posts on the Alignment Forum that will be discussed below. This post is not intended to replace these talks but is instead an attempt to coherently integrate their ideas with ideas from other sources attempting to clarify various aspects of the field. You should nonetheless watch these presentations and read some of the resources provided below if you’re trying to build your mental map as completely as possible. Rohin also did a two part podcast with the Future of Life Institute discussing the contents of his presentation in more depth, both of which are worth listening to. ↩",2020-12-17,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-13 15:45:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/78JC5UMH/mapping_conceptual_territory_AI_safety_alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ICRWTVK7,blogPost,2020,"Aird, Michael",Failures in technology forecasting? A reply to Ord and Yudkowsky,LessWrong,,,,https://www.lesswrong.com/posts/3qypPmmNHEmqegoFF/failures-in-technology-forecasting-a-reply-to-ord-and,"In The Precipice, Toby Ord writes: we need to remember how quickly new technologies can be upon us, and to be wary of assertions that they are either impossible or so distant in time that we have no cause for concern. Confident denouncements by eminent scientists should certainly give us reason to be sceptical of a technology, but not to bet our lives against it - their track record just isn’t good enough for that. I strongly agree with those claims, think they’re very important in relation to  estimating existential risk,[1] and appreciate the nuanced way in which they’re stated. (There’s also a lot more nuance around this passage which I haven’t quoted.) I also largely agree with similar claims made in Eliezer Yudkowsky’s earlier essay There's No Fire Alarm for Artificial General Intelligence. But both Ord and Yudkowsky provide the same set of three specific historical cases as evidence of the poor track record of such “confident denouncements”. And I think those cases provide less clear evidence than those authors seem to suggest. So in this post, I’ll:  * Quote Ord and/or Yudkowsky’s descriptions of those three cases, as well as    one case mentioned by Yudkowsky but not Ord  * Highlight ways in which those cases may be murkier than Ord and Yudkowsky    suggest  * Discuss how much we could conclude about technology forecasting in general     from such a small and likely unrepresentative sample of cases, even if those    cases weren’t murky I should note that I don’t think that these historical cases are necessary to support claims like those Ord and Yudkowsky make. And I suspect there might be better evidence for those claims out there. But those cases were the main evidence Ord provided, and among the main evidence Yudkowsky provided. So those cases are being used as key planks supporting beliefs that are important to many EAs and longtermists. Thus, it seems healthy to prod at each suspicious plank on its own terms, and update incrementally. CASE: RUTHER",2020-05-08,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-20 19:08:07,,,,,,,Failures in technology forecasting?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VI686GFK/failures-in-technology-forecasting-a-reply-to-ord-and.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JRKS8FN9,blogPost,2020,"Aird, Michael",Existential risks are not just about humanity,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/EfCCgpvQX359xuZ4g/existential-risks-are-not-just-about-humanity,"This post was written for Convergence Analysis. This post highlights and analyses existing ideas more than proposing new ones. In The Precipice, Toby Ord writes: An existential catastrophe is the destruction of humanity’s longterm potential. An existential risk is a risk that threatens the destruction of humanity’s longterm potential. I’ve previously discussed some distinctions and nuances relevant to these concepts. This post will focus on:  * The idea that these concepts are really about the destruction of the     potential of humanity or its “descendants”; they're not necessarily solely    about human wellbeing, nor just Homo sapiens’ potential.  * The implications of that, including for how “bad” an existential catastrophe    might be THE POTENTIAL OF HUMANITY AND ITS “DESCENDANTS” When explaining his definitions, Ord writes: my focus on humanity in the definitions is not supposed to exclude considerations of the value of the environment, other animals, successors to  Homo sapiens, or creatures elsewhere in the cosmos. It is not that I think only humans count. Instead, it is that humans are the only beings we know of that are responsive to moral reasons and moral argument - the beings who can examine the world and decide to do what is best. If we fail, that upwards force, that capacity to push towards what is best or what is just, will vanish from the world. Our potential is a matter of what humanity can achieve through the combined actions of each and every human. The value of our actions will stem in part from what we do to and for humans, but it will depend on the effects of our actions on non-humans too. If we somehow give rise to new kinds of moral agents in the future, the term ‘humanity’ in my definition should be taken to include them. This makes two points clear:  1. An existential catastrophe is not solely about the destruction of the     potential for human welfare, flourishing, achievement, etc. Instead, it’s     about humanity’s potential",2020-04-27,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-20 19:00:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/T2T8GF5K/are-existential-risks-just-about-humanity.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBZPEDS4,blogPost,2020,"Harth, Rafael",Factored Cognition,LessWrong,,,,https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z,A community blog devoted to refining the art of rationality,2020-08-30,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 21:59:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SJKBM59W/xezt7HYfpWR6nwp7Z.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8RNGX2C6,blogPost,2020,"Finnveden, Lukas",Extrapolating GPT-N performance,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance,"Brown et al. (2020) (which describes the development of GPT-3) contains measurements of how 8 transformers of different sizes perform on several different benchmarks. In this post, I project how performance could improve for larger models, and give an overview of issues that may appear when scaling-up. Note that these benchmarks are for ‘downstream tasks’ that are different from the training task (which is to predict the next token); these extrapolations thus cannot be directly read off the scaling laws in OpenAI’s Scaling Laws for Neural Language Models (Kaplan et al., 2020) or Scaling Laws for Autoregressive Generative Modelling (Henighan et al., 2020). (If you don’t care about methodology or explanations, the final graphs are in  Comparisons and limits .) METHODOLOGY Brown et al. reports benchmark performance for 8 different model sizes. However, these models were not trained in a compute-optimal fashion. Instead, all models were trained on 300B tokens (one word is roughly 1.4 tokens), which is inefficiently much data. Since we’re interested in the best performance we can get for a given amount of compute, and these models weren’t compute-optimally trained, we cannot extrapolate these results on the basis of model-size. Instead, I fit a trend for how benchmark performance (measured in % accuracy) depends on the cross-entropy loss that the models get when predicting the next token on the validation set. I then use the scaling laws from Scaling Laws for Neural Language Models to extrapolate this loss. This is explained in the Appendix. PLOTTING AGAINST LOSS In order to get a sense of how GPT-3 performs on different types of tasks, I separately report few-shot progress on each of the 11 different categories discussed in Brown et al. For a fair comparison, I normalize the accuracy of each category between random performance and maximum performance; i.e., for each data point, I subtract the performance that a model would get if it responded randomly (or only respo",2020-12-18,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 22:24:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2EXQWDPT/extrapolating-gpt-n-performance.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F4RGR6AR,blogPost,2020,"Harris, Edouard",Defining capability and alignment in gradient descent,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent,"This is the first post in a series where I'll explore AI alignment in a simplified setting: a neural network that's being trained by gradient descent. I'm choosing this setting because it involves a well-defined optimization process that has enough complexity to be interesting, but that's still understandable enough to make crisp mathematical statements about. As a result, it serves as a good starting point for rigorous thinking about alignment. DEFINING INNER ALIGNMENT First, I want to highlight a definitional issue. Right now there are two definitions of inner alignment circulating in the community. This issue was first pointed out to me by Evan Hubinger in a recent conversation. The first definition is the one from last year's Risks from Learned Optimization paper, which Evan co-authored and which introduced the term. This paper defined the inner alignment problem as ""the problem of eliminating the base-mesa objective gap"" (Section 1.2). The implication is that if we can eliminate the gap between the base objective of a base optimizer, and the mesa-objectives of any mesa-optimizers that base optimizer may give rise to, then we will have satisfied the necessary and sufficient conditions for the base optimizer to be inner-aligned. There's also a second definition that seems to be more commonly used. This definition says that ""inner alignment fails when your capabilities generalize but your objective does not"". This comes from an intuition (pointed out to me by  Rohin Shah) that the combination of inner alignment and outer alignment should be accident-proof with respect to an optimizer's intent: an optimizer that's both inner- and outer-aligned should be trying to do what we want. Since an outer-aligned optimizer is one whose base objective is something we want, this intuition suggests that the remaining part of the intent alignment problem — the problem of getting the optimizer to try to achieve the base objective we set — is what inner alignment refers to. Her",2020-11-05,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 22:00:32,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NKD85BQM/defining-capability-and-alignment-in-gradient-descent.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63VS4I8K,blogPost,2020,"DeepSpeed Team; Majumder, Rangan",DeepSpeed: Extreme-scale model training for everyone,Microsoft Research,,,,https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/,"DeepSpeed continues to innovate, making its tools more powerful while broadening its reach. Learn how it now powers 10x bigger model training on one GPU, 10x longer input sequences, 5x less communication volume, & scales to train trillion-parameter models.",2020-09-10,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-13 13:58:29,,,,,,,DeepSpeed,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGIS6ZTT,blogPost,2020,"Wentworth, John",Confucianism in AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/3aDeaJzxinoGNWNpC/confucianism-in-ai-alignment,"I hear there’s a thing where people write a lot in November, so I’m going to try writing a blog post every day. Disclaimer: this post is less polished than my median. And my median post isn’t very polished to begin with. Imagine a large corporation - we’ll call it BigCo. BigCo knows that quality management is high-value, so they have a special program to choose new managers. They run the candidates through a program involving lots of management exercises, simulations, and tests, and select those who perform best. Of course, the exercises and simulations and tests are not a perfect proxy for the would-be managers’ real skills and habits. The rules can be gamed. Within a few years of starting the program, BigCo notices a drastic disconnect between performance in the program and performance in practice. The candidates who perform best in the program are those who game the rules, not those who manage well, so of course many candidates devote all their effort to gaming the rules. How should this problem be solved? Ancient Chinese scholars had a few competing schools of thought on this question, most notably the Confucianists and the Legalists. The (stylized) Confucianists’ answer was: the candidates should be virtuous and not abuse the rules. BigCo should demonstrate virtue and benevolence in general, and in return their workers should show loyalty and obedience. I’m not an expert, but as far as I can tell this is not a straw man - though stylized and adapted to a modern context, it accurately captures the spirit of Confucian thought. The (stylized) Legalists instead took the position obvious to any student of modern economics: this is an incentive design problem, and BigCo leadership should design less abusable incentives. If you have decent intuition for economics, it probably seems like the Legalist position is basically right and the Confucian position is Just Wrong. I don't want to discourage this intuition, but I expect that many people who have this intuitio",2020-11-02,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-13 13:49:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UPMVNDZV/confucianism-in-ai-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8CU8JKMG,blogPost,2020,"Ngo, Amanda; Pace, Ben",AGI Predictions,LessWrong,,,,https://www.lesswrong.com/posts/YMokuZdoY9tEDHjzv/agi-predictions,"This post is a collection of key questions that feed into AI timelines and AI safety work where it seems like there is substantial interest or disagreement amongst the LessWrong community. You can make a prediction on a question by hovering over the widget and clicking. You can update your prediction by clicking at a new point, and remove your prediction by clicking on the same point. Try it out: Elicit Prediction (elicit.org/binary/questions/FIVfnQ_kJ) ADD QUESTIONS & OPERATIONALIZATIONS This is not intended to be a comprehensive list, so I’d love for people to add their own questions – here are instructions on making your own embedded question. If you have better operationalizations of the questions, you can make your own version in the comments. If there's general agreement on an alternative operationalization being better, I'll add it into the post. QUESTIONS AGI DEFINITION We’ll define AGI in this post as a unified system that, for almost all economically relevant cognitive tasks, at least matches any human's ability at the task. This is similar to Rohin Shah and Ben Cottier’s definition in this post. SAFETY QUESTIONS Elicit Prediction (elicit.org/binary/questions/_Sw39Z-kh)Elicit Prediction ( elicit.org/binary/questions/HqT9XSwfs)Elicit Prediction ( elicit.org/binary/questions/sTO9o3bLg)Elicit Prediction ( elicit.org/binary/questions/kua2HCDhi)Elicit Prediction ( elicit.org/binary/questions/KqSEIKayU)Elicit Prediction ( elicit.org/binary/questions/yoiBUdpgO)Elicit Prediction ( elicit.org/binary/questions/RcOt6wSs7)Elicit Prediction ( elicit.org/binary/questions/ZjN5qqVRz) TIMELINES QUESTIONS See Forecasting AI timelines, Ajeya Cotra’s OP AI timelines report, and Adam Gleave’s #AN80 comment, for more context on this breakdown. I haven’t tried to operationalize this too much, so feel free to be more specific in the comments. The first three questions in this section are mutually exclusive — that is, the probabilities you assign to them should not sum to m",2020-11-20,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 14:15:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GF7HGP5R,blogPost,2020,"Tan, Xuan","AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of,"This is an extended transcript of the talk I gave at EAGxAsiaPacific 2020. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it deserves considerably more philosophical and disciplinary diversity than it has enjoyed so far. I'm sharing it here in the hopes of generating discussion about the disciplinary and philosophical paradigms that (I understand) the AI alignment community to be rooted in, and whether or how we should move beyond them. Some sections cover introductory material that most people here are likely to be familiar with, so feel free to skip them. THE TALK Hey everyone, my name is Xuan (IPA: ɕɥɛn), and I’m doctoral student at MIT doing cognitive AI research. Specifically I work on how we can infer the hidden structure of human motivations by modeling humans using probabilistic programs. Today though I’ll be talking about something that’s more in the background that informs my work, and that’s about AI alignment, philosophical pluralism, and the relevance of non-Western philosophy. This talk will cover a lot of ground, so I want to give an overview to keep everyone oriented:  1. First, I’ll give a brief introduction to what AI alignment is, and why it     likely matters as an effective cause area.  2. I’ll then highlight some of the philosophical tendencies of current AI     alignment research, and argue that they reflect a relatively narrow set of     philosophical views.  3. Given that these philosophical views may miss crucial considerations, this     situation motivates the need for greater philosophical and disciplinary     pluralism.  4. And then as a kind of proof by example, I’ll aim to demonstrate how     non-Western philosophy might provide insight into several open problems in     AI alignment research. A BRIEF INTRODUCTION TO AI ALIGNMENT So what is AI alignment? One way to cache it out is the project of building intelligent systems that robustly act in our collective i",2020-12-31,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 16:34:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6KXXKDVU/ai-alignment-philosophical-pluralism-and-the-relevance-of.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JRIA7DF,blogPost,2020,Larks,2020 AI Alignment Literature Review and Charity Comparison,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison,"cross-posted to the EA forum here. INTRODUCTION As in 2016, 2017, 2018, and 2019, I have attempted to review the research that has been produced by various organisations working on AI safety, to help potential donors gain a better understanding of the landscape. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to a securities analyst with regards to possible investments. My aim is basically to judge the output of each organisation in 2020 and compare it to their budget. This should give a sense of the organisations' average cost-effectiveness. We can also compare their financial reserves to their 2020 budgets to get a sense of urgency. I’d like to apologize in advance to everyone doing useful AI Safety work whose contributions I have overlooked or misconstrued. As ever I am painfully aware of the various corners I have had to cut due to time constraints from my job, as well as being distracted by 1) other projects, 2) the miracle of life and 3) computer games. This article focuses on AI risk work. If you think other causes are important too, your priorities might differ. This particularly affects GCRI, FHI and CSER, who both do a lot of work on other issues which I attempt to cover but only very cursorily. HOW TO READ THIS DOCUMENT This document is fairly extensive, and some parts (particularly the methodology section) are largely the same as last year, so I don’t recommend reading from start to finish. Instead, I recommend navigating to the sections of most interest to you. If you are interested in a specific research organisation, you can use the table of contents to navigate to the appropriate section. You might then also want to Ctrl+F for the organisation acronym in case they are mentioned elsewhere as well. Papers listed as ‘X researchers contributed to the following research lead by other organisations’ are included in the section corresponding to their first author and you can Cntrl+F to find them",2020-12-21,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 14:30:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D5VWMR6B,blogPost,2020,"Barnes, Beth; Christiano, Paul",Debate update: Obfuscated arguments problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem,"This is an update on the work on AI Safety via Debate that we previously wrote about here. Authors and Acknowledgements The researchers on this project were Elizabeth Barnes and Paul Christiano, with substantial help from William Saunders (who built the current web interface as well as other help), Joe Collman (who helped develop the structured debate mechanisms), and Mark Xu, Chris Painter, Mihnea Maftei and Ronny Fernandez (who took part in many debates as well as helping think through problems). We're also grateful to Geoffrey Irving and Evan Hubinger for feedback on drafts, and for helpful conversations, along with Richard Ngo, Daniel Ziegler, John Schulman, Amanda Askell and Jeff Wu. Finally, we're grateful to our contractors who participated in experiments, including Adam Scherlis, Kevin Liu, Rohan Kapoor and Kunal Sharda. WHAT WE DID We tested the debate protocol introduced in AI Safety via Debate with human judges and debaters. We found various problems and improved the mechanism to fix these issues (details of these are in the appendix). However, we discovered that a dishonest debater can often create arguments that have a fatal error, but where it is very hard to locate the error. We don’t have a fix for this “obfuscated argument” problem, and believe it might be an important quantitative limitation for both IDA and Debate. KEY TAKEAWAYS AND RELEVANCE FOR ALIGNMENT Our ultimate goal is to find a mechanism that allows us to learn anything that a machine learning model knows: if the model can efficiently find the correct answer to some problem, our mechanism should favor the correct answer while only requiring a tractable number of human judgements and a reasonable number of computation steps for the model.[1] We’re working under a hypothesis that there are broadly two ways to know things: via step-by-step reasoning about implications (logic, computation…), and by learning and generalizing from data (pattern matching, bayesian updating…). Debate focuse",2020-12-22,2022-01-30 4:48:28,2022-01-30 4:48:28,2021-11-13 16:27:28,,,,,,,Debate update,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M6NPQG3H/debate-update-obfuscated-arguments-problem.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T9ZJEXJS,blogPost,2020,"Diffractor; Kosoy, Vanessa",Infra-Bayesianism,AI Alignment Forum,,,,https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa,A community blog devoted to technical AI alignment research,2020,2022-01-30 4:48:21,2022-01-30 4:48:21,2021-11-14 16:24:35,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GASHJBIH/CmrW8fCmSLK7E25sa.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V295HB6Q,blogPost,2020,"Demski, Abram",Recursive Quantilizers II,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YNuJjRuxsWWzfvder/recursive-quantilizers-ii,"I originally introduced the recursive quantilizers idea here, but didn't provide a formal model until my recent Learning Normativity post. That formal model had some problems. I'll correct some of those problems here. My new model is closer to HCH+IDA, and so, is even closer to Paul Christiano style systems than my previous. However, I'm also beginning to suspect that quantilizers aren't the right starting point. I'll state several problems with quantilizers at the end of this post. First, let's reiterate the design criteria, and why the model in Learning Normativity wasn't great. CRITERIA Here are the criteria from Learning Normativity, with slight revisions. See the earlier post for further justifications/intuitions behind these criteria.  1. No Perfect Feedback: we want to be able to learn with the possibility that     any one piece of data is corrupt. 1. Uncertain Feedback: data can be given in an uncertain form, allowing         100% certain feedback to be given (if there ever is such a thing), but         also allowing the system to learn significant things in the absence of         any certainty.      2. Reinterpretable Feedback: ideally, we want rich hypotheses about the         meaning of feedback, which help the system to identify corrupt feedback,         and interpret the information in imperfect feedback. To this criterion,         I add two clarifying criteria: 1. Robust Listening: in some sense, we don't want the system to be able             to ""entirely ignore"" humans. If the system goes off-course, we want             to be able to correct that.          2. Arbitrary Reinterpretation: at the same time, we want the AI to be             able to entirely reinterpret feedback based on a rich model of what             humans mean. This criterion stands in tension with Robust Listening.             However, the proposal in the present post is, I think, a plausible             way to achieve both.                              2. No Perfect Loss Functi",2020,2022-01-30 4:48:21,2022-01-30 4:48:21,2021-11-13 19:41:28,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q5MIGHSB/recursive-quantilizers-ii.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9AHZ7MX,blogPost,2020,"Demski, Abram",Learning Normativity: A Research Agenda,,,,,https://www.alignmentforum.org/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda,"(Related to Inaccessible Information, Learning the Prior, and Better Priors as a Safety Problem. Builds on several of my alternate alignment ideas.) I want to talk about something which I'll call learning normativity. What is normativity? Normativity is correct behavior. I mean something related to the fuzzy concept humans convey with the word ""should"". I think it has several interesting features:  * Norms are the result of a complex negotiation between humans, so they    shouldn't necessarily be thought of as the result of maximizing some set of    values. This distinguishes learning normativity from value learning.  * A lot of information about norms is present in the empirical distribution of    what people actually do, but you can't learn norms just by learning human    behavior. This distinguishes it from imitation learning.  * It's often possible to provide a lot of information in the form of ""good/bad""    feedback. This feedback should be interpreted more like approval-directed    learning rather than RL. However, approval should not be treated as a gold    standard.  * Similarly, it's often possible to provide a lot of information in the form of    rules, but rules are not necessarily 100% true; they are just very likely to    apply in typical cases.  * In general, it's possible to get very rich types of feedback, but very sparse    : humans get all sorts of feedback, including not only instruction on how to    act, but also how to think.  * Any one piece of feedback is suspect. Teachers can make mistakes,    instructions can be wrong, demonstrations can be imperfect, dictionaries can    contain spelling errors, reward signals can be corrupt, and so on. EXAMPLE: LANGUAGE LEARNING A major motivating example for me is how language learning works in humans. There is clearly, to some degree, a ""right way"" and a ""wrong way"" to use a language. I'll call this correct usage. One notable feature of language learning is that we don't always speak, or write, in c",2020,2022-01-30 4:48:21,2022-01-30 4:48:21,2021-11-13 19:38:49,,,,,,,Learning Normativity,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZWEX5GR8/learning-normativity-a-research-agenda.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
USEM8RBA,blogPost,2020,"Demski, Abram",Comparing Utilities,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities,"(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.) I will first give the basic argument that the utility quantities of different agents aren't directly comparable, and a few important consequences of this. I'll then spend the rest of the post discussing what to do when you need to compare utility functions. UTILITIES AREN'T COMPARABLE. Utility isn't an ordinary quantity. A utility function is a device for expressing the preferences of an agent. Suppose we have a notion of outcome.* We could try to represent the agent's preferences between outcomes as an ordering relation: if we have outcomes A, B, and C, then one possible preference would be A<B<C. However, a mere ordering does not tell us how the agent would decide between  gambles, ie, situations giving A, B, and C with some probability. With just three outcomes, there is only one thing we need to know: is B closer to A or C, and by how much? We want to construct a utility function U() which represents the preferences. Let's say we set U(A)=0 and U(C)=1. Then we can represent B=G as U(B)=1/2. If not, we would look for a different gamble which does equal B, and then set B's utility to the expected value of that gamble. By assigning real-numbered values to each outcome, we can fully represent an agent's preferences over gambles. (Assuming the VNM axioms hold, that is.) But the initial choices U(A)=0 and U(C)=1 were arbitrary! We could have chosen any numbers so long as U(A)<U(C), reflecting the preference A<C. In general, a valid representation of our preferences U() can be modified into an equally valid U'() by adding/subtracting arbitrary numbers, or multiplying/dividing by pos",2020,2022-01-30 4:48:21,2022-01-30 4:48:21,2021-11-07 22:08:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/THUBZR6E/comparing-utilities.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E7W9CCJS,blogPost,2020,"Hubinger, Evan",Clarifying inner alignment terminology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology,"I have seen a lot of confusion recently surrounding exactly how outer and inner alignment should be defined and I want to try and provide my attempt at a clarification. Here's my diagram of how I think the various concepts should fit together: The idea of this diagram is that the arrows are implications—that is, for any problem in the diagram, if its direct subproblems are solved, then it should be solved as well (though not necessarily vice versa). Thus, we get: inner alignment→objective robustnessouter alignment∧objective robustness→intent alignmentintent alignment∧capability robustness→alignment -------------------------------------------------------------------------------- And here are all my definitions of the relevant terms which I think produce those implications: (Impact) Alignment: An agent is impact aligned (with humans) if it doesn't take actions that we would judge to be bad/problematic/dangerous/catastrophic. Intent Alignment: An agent is intent aligned if the optimal policy for its  behavioral objective[1] is impact aligned with humans. Outer Alignment: An objective function r is outer aligned if all models that perform optimally on r in the limit of perfect training and infinite data are intent aligned.[2] Robustness: An agent is robust if it performs well on the base objective it was trained under even in deployment/off-distribution.[3] Objective Robustness: An agent is objective robust if the optimal policy for its  behavioral objective is impact aligned with the base objective it was trained under. Capability Robustness: An agent is capability robust if it performs well on its  behavioral objective even in deployment/off-distribution. Inner Alignment: A mesa-optimizer is inner aligned if the optimal policy for its  mesa-objective is impact aligned with the base objective it was trained under. -------------------------------------------------------------------------------- And an explanation of each of the diagram's implications:",2020-11-09,2022-01-30 4:48:20,2022-01-30 4:48:20,2021-11-13 13:52:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2KRZ2X5K,blogPost,2020,"Armstrong, Stuart","Knowledge, manipulation, and free will",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2dKvTYYN4PTT7g4of/knowledge-manipulation-and-free-will,"Thanks to Rebecca Gorman for co-developing this idea On the 26th of September 1983, Stanislav Petrov observed the early warning satellites reporting the launch of five nuclear missiles towards the Soviet Union. He decided to disobey orders and not pass on the message to higher command, which could easily have resulted in a nuclear war (since the soviet nuclear position was ""launch on warning""). Now, did Petrov have free will when he decided to save the world? MAINTAINING FREE WILL WHEN KNOWLEDGE INCREASES I don't intend to go into the subtle philosophical debate on the nature of free will. See this post for a good reductionist account. Instead, consider the following scenarios:  1. The standard Petrov incident.  2. The standard Petrov incident, except that it is still ongoing and Petrov     hasn't reached a decision yet.  3. The standard Petrov incident, after it was over, except that we don't yet     know what his final decision was.  4. The standard Petrov incident, except that we know that, if Petrov had had     eggs that morning (instead of porridge[1]), he would have made a different     decision.  5. The same as scenario 4., except that some entity deliberately gave Petrov     porridge that morning, aiming to determine his decision.  6. The standard Petrov incident, except that a guy with a gun held Petrov     hostage and forced him not to pass on the report. There is an interesting contrast between scenarios 1, 2, and 3. Clearly, 1 and 3 only differ in our knowledge of the incident. It does not seem that Petrov's free will should depend on the degree of knowledge of some other person. Scenarios 1 and 2 only differ in time: in one case the decision is made, in the second it is yet to be made. If we say that Petrov has free will, whatever that is, in scenario 2, then it seems that in scenario 1, we have to say that he ""had"" free will. So whatever our feeling on free will, it seems that knowing the outcome doesn't change whether there was free will or not.",2020-10-13,2022-01-30 4:48:04,2022-01-30 4:48:04,2021-11-08 23:34:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9VD67FAD/knowledge-manipulation-and-free-will.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5PJ9SVN,blogPost,2020,"Dafoe, Allan",AI Governance: Opportunity and Theory of Impact,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact,"Note: We have recently opened up roles for researchers and a project manager at the Centre for the Governance of Artificial Intelligence, part of the Future of Humanity Institute, University of Oxford AI governance concerns how humanity can best navigate the transition to a world with advanced AI systems.[1] It relates to how decisions are made about AI,[2]  and what institutions and arrangements would help those decisions to be made well. I believe advances in AI are likely to be among the most impactful global developments in the coming decades, and that AI governance will become among the most important global issue areas. AI governance is a new field and is relatively neglected. I’ll explain here how I think about this as a cause area and my perspective on how best to pursue positive impact in this space. The value of investing in this field can be appreciated whether one is primarily concerned with contemporary policy challenges or long-term risks and opportunities (“longtermism”); this piece is primarily aimed at a longtermist  perspective. Differing from some other longtermist work on AI, I emphasize the importance of also preparing for more conventional scenarios of AI development. CONTEMPORARY POLICY CHALLENGES AI systems are increasingly being deployed in important domains: for many kinds of surveillance; by authoritarian governments to shape online discourse; for autonomous weapons systems; for cyber tools and autonomous cyber capabilities; to aid and make consequential decisions such as for employment, loans, and criminal sentencing; in advertising; in education and testing; in self-driving cars and navigation; in social media. Society and policy makers are rapidly trying to catch up, to adapt, to create norms and policies to guide these new areas. We see this scramble in contemporary international tax law, competition/antitrust policy, innovation policy, and national security motivated controls on trade and investment. To understand and advise conte",2020-09-17,2022-01-30 4:48:03,2022-01-30 4:48:03,2021-11-07 19:52:53,,,,,,,AI Governance,,,,,,,,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/CE75WCFR/ai-governance-opportunity-and-theory-of-impact.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8R5CM8V5,blogPost,2020,The AlphaFold Team,AlphaFold: a solution to a 50-year-old grand challenge in biology,Deepmind,,,,https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology,"In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP) assessment. This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.",2020-11-30,2022-01-30 4:47:57,2022-01-30 4:47:57,2021-11-13 14:16:32,,,,,,,AlphaFold,,,,,,,ALL,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
822KXARD,blogPost,2019,"Gloor, Lukas",Rebuttal of Christiano and AI Impacts on takeoff speeds?,LessWrong,,,,https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#zFEhTxNqEp3eZbjLZ,"14 months ago, Paul Christiano and AI Impacts both published forceful and well-received take-downs of many arguments for fast (discontinuous) takeoff. I haven’t seen any rebuttals that are written by established researchers, longer than comments, or otherwise convincing. The longer there is no response, the less weight I put on the outside view that proponents of fast takeoff may be right. Where are the rebuttals? Did I miss them? Is the debate decided? Did nobody have time or motivation to write something? Is the topic too hard to explain? Why rebuttals would be useful: -Give the community a sense of the extent of expert disagreement to form outside views. -Prioritization in AI policy, and to a lesser extent safety, depends on the likelihood of discontinuous progress. We may have more leverage in such cases, but this could be overwhelmed if the probability is low. -Motivate more people to work on MIRI’s research which seems more important to solve early if there is fast takeoff.",2019-04-25,2022-01-30 4:51:36,2022-01-30 4:51:36,2020-11-23 0:36:12,,,,,,,Any rebuttals of Christiano and AI Impacts on takeoff speeds?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3QVB2E9R/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GQXT7XM,blogPost,2019,"Shah, Rohin",What is narrow value learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vX7KirQwHsBaSEdfK/what-is-narrow-value-learning,"Ambitious value learning aims to achieve superhuman performance by figuring out the underlying latent ""values"" that humans have, and evaluating new situations according to these values. In other words, it is trying to infer the criteria by which we judge situations to be good. This is particularly hard because in novel situations that humans haven't seen yet, we haven't even developed the criteria by which we would evaluate. (This is one of the reasons why we need to model humans as suboptimal, which causes problems.) Instead of this, we can use narrow value learning, which produces behavior that we want in some narrow domain, without expecting generalization to novel circumstances. The simplest form of this is imitation learning, where the AI system simply tries to imitate the supervisor's behavior. This limits the AI’s performance to that of its supervisor. We could also learn from preferences over behavior, which can scale to superhuman performance, since the supervisor can often evaluate whether a particular behavior meets our preferences even if she can’t perform it herself. We could also teach our AI systems to perform tasks that we would not want to do ourselves, such as handling hot objects. Nearly all of the work on preference learning, including most work on inverse reinforcement learning (IRL), is aimed at narrow value learning. IRL is often explicitly stated to be a technique for imitation learning, and early algorithms phrase the problem as matching the features in the demonstration, not exceeding them. The few algorithms that try to generalize to different test distributions, such as AIRL, are only aiming for relatively small amounts of generalization. (Why use IRL instead of behavioral cloning, where you mimic the actions that the demonstrator took? The hope is that IRL gives you a good inductive bias for imitation, allowing you to be more sample efficient and to generalize a little bit.) You might have noticed that I talk about narrow value learn",2019,2022-01-30 4:51:12,2022-01-30 4:51:12,2020-12-17 4:37:01,,,,,,,What is narrow value learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WZFPW9XN/vX7KirQwHsBaSEdfK.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5IKAMSXK,blogPost,2018,"Shah, Rohin",What is ambitious value learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning,"I think of ambitious value learning as a proposed solution to the specification problem, which I define as the problem of defining the behavior that we would want to see from our AI system. I italicize “defining” to emphasize that this is  not the problem of actually computing behavior that we want to see -- that’s the full AI safety problem. Here we are allowed to use hopelessly impractical schemes, as long as the resulting definition would allow us to in theory compute the behavior that an AI system would take, perhaps with assumptions like infinite computing power or arbitrarily many queries to a human. (Although we do prefer specifications that seem like they could admit an efficient implementation.) In terms of DeepMind’s classification, we are looking for a design specification that exactly matches the ideal specification. HCH and  indirect normativity are examples of attempts at such specifications. We will consider a model in which our AI system is maximizing the expected utility of some explicitly represented utility function that can depend on history. (It does not matter materially whether we consider utility functions or reward functions, as long as they can depend on history.) The utility function may be learned from data, or designed by hand, but it must be an explicit part of the AI that is then maximized. I will not justify this model for now, but simply assume it by fiat and see where it takes us. I’ll note briefly that this model is often justified by the  VNM utility theorem and AIXI, and as the natural idealization of reinforcement learning, which aims to maximize the expected sum of rewards, although typically rewards in RL depend only on states. A lot of conceptual arguments, as well as experiences with specification gaming, suggest that we are unlikely to be able to simply think hard and write down a good specification, since even small errors in specifications can lead to bad results. However, machine learning is particularly good at narro",2018,2022-01-30 4:51:12,2022-01-30 4:51:12,2020-12-17 4:36:27,,,,,,,What is ambitious value learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WV7FNFRK/5eX8ko7GCxwR5N9mN.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFP7FN3X,blogPost,2020,"Turner, Alex",What counts as defection?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8LEPDY36jBYpijrSw/what-counts-as-defection,"Thanks to Michael Dennis for proposing the formal definition; to Andrew Critch for pointing me in this direction; to Abram Demski for proposing non-negative weighting; and to Alex Appel, Scott Emmons, Evan Hubinger, philh, Rohin Shah, and Carroll Wainwright for their feedback and ideas. There's a good chance I'd like to publish this at some point as part of a larger work. However, I wanted to make the work available now, in case that doesn't happen soon. They can't prove the conspiracy... But they could, if Steve runs his mouth. The police chief stares at you. You stare at the table. You'd agreed (sworn!) to stay quiet. You'd even studied game theory together. But, you hadn't understood what an extra year of jail meant. The police chief stares at you. Let Steve be the gullible idealist. You have a family waiting for you. Sunlight stretches across the valley, dappling the grass and warming your bow. Your hand anxiously runs along the bowstring. A distant figure darts between trees, and your stomach rumbles. The day is near spent. The stags run strong and free in this land. Carla should meet you there. Shouldn't she? Who wants to live like a beggar, subsisting on scraps of lean rabbit meat? In your mind's eye, you reach the stags, alone. You find one, and your arrow pierces its barrow. The beast bucks and bursts away; the rest of the herd follows. You slump against the tree, exhausted, and never open your eyes again. You can't risk it. People talk about 'defection' in social dilemma games, from the prisoner's dilemma to stag hunt to chicken. In the tragedy of the commons, we talk about defection. The concept has become a regular part of LessWrong discourse. Informal definition. A player defects when they increase their personal payoff at the expense of the group. This informal definition is no secret, being echoed from the ancient Formal Models of Dilemmas in Social Decision-Making to the recent Classifying games like the Prisoner's Dilemma: you can mo",2020-07-12,2022-01-30 4:51:11,2022-01-30 4:51:11,2020-08-28 17:44:13,,,,,,,What counts as defection?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2H5VW8FH/formalizing-game-theoretic-defection.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGCH7XG7,blogPost,2021,"Steinhardt, Jacob",Updates and Lessons from AI Forecasting,Bounded Regret,,,,https://bounded-regret.ghost.io/ai-forecasting/,"Earlier this year, my research group commissioned 6 questions  for professional forecasters to predict about AI. Broadly speaking, 2 were on geopolitical aspects of AI and 4 were on future capabilities:  Geopolitical: How much larger or smaller will the largest Chinese ML experiment be compared to the largest U.S.",2021-08-18,2022-01-30 4:51:11,2022-01-30 4:51:11,2021-11-18 23:38:06,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EJIEH689/ai-forecasting.html,,MetaSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R8H8NHUI,blogPost,2018,Alex Turner,Towards a New Impact Measure,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure,"In which I propose a closed-form solution to low impact, increasing  corrigibility and seemingly taking major steps to neutralize basic AI drives 1 (self-improvement), 5 (self-protectiveness), and 6 (acquisition of resources). Previously: Worrying about the Vase: Whitelisting, Overcoming Clinginess in Impact Measures, Impact Measure Desiderata To be used inside an advanced agent, an impact measure... must capture so much variance that there is no clever strategy whereby an advanced agent can produce some special type of variance that evades the measure. ~ Safe Impact MeasureIf we have a safe impact measure, we may have arbitrarily-intelligent unaligned agents which do small (bad) things instead of big (bad) things.  For the abridged experience, read up to ""Notation"", skip to ""Experimental Results"", and then to ""Desiderata"". WHAT IS ""IMPACT""? One lazy Sunday afternoon, I worried that I had written myself out of a job. After all, Overcoming Clinginess in Impact Measures basically said, ""Suppose an impact measure extracts 'effects on the world'. If the agent penalizes itself for these effects, it's incentivized to stop the environment (and any agents in it) from producing them. On the other hand, if it can somehow model other agents and avoid penalizing their effects, the agent is now incentivized to get the other agents to do its dirty work."" This seemed to be strong evidence against the possibility of a simple conceptual core underlying ""impact"", and I didn't know what to do. At this point, it sometimes makes sense to step back and try to say exactly what you don't know how to solve – try to crisply state what it is that you want an unbounded solution for. Sometimes you can't even do that much, and then you may actually have to spend some time thinking 'philosophically' – the sort of stage where you talk to yourself about some mysterious ideal quantity of [chess] move-goodness and you try to pin down what its properties might be. ~  Methodology of Unbounded Anal",2018,2022-01-30 4:51:11,2022-01-30 4:51:11,2020-12-13 23:51:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PMHJK9F7,blogPost,2019,"Shah, Rohin",The human side of interaction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/eD9T4kiwB6MHpySGE/the-human-side-of-interaction,"The last few posts have motivated an analysis of the human-AI system rather than an AI system in isolation. So far we’ve looked at the notion that the AI system should get feedback from the user and that it could use reward uncertainty for corrigibility. These are focused on the AI system, but what about the human? If we build a system that explicitly solicits feedback from the human, what do we have to say about the human policy, and how the human should provide feedback? INTERPRETING HUMAN ACTIONS One major free variable in any explicit interaction or feedback mechanism is what semantics the AI system should attach to the human feedback. The classic examples of AI risk are usually described in a way where this is the problem: when we provide a reward function that rewards paperclips, the AI system interprets it literally and maximizes paperclips, rather than interpreting it pragmatically as another human would. (Aside: I suspect this was not the original point of the paperclip maximizer, but it has become a very popular retelling, so I’m using it anyway.) Modeling this classic example as a human-AI system, we can see that the problem is that the human is offering a form of “feedback”, the reward function, and the AI system is not ascribing the correct semantics to it. The way it uses the reward function implies that the reward function encodes the optimal behavior of the AI system in all possible environments -- a moment’s thought is sufficient to see that this is not actually the case. There will definitely be many cases and environments that the human did not consider when designing the reward function, and we should not expect that the reward function incentivizes the right behavior in those cases. So what can the AI system assume if the human provides it a reward function?  Inverse Reward Design (IRD) offers one answer: the human is likely to provide a particular reward function if it leads to high true utility behavior in the training environment. So, in",2019,2022-01-30 4:51:10,2022-01-30 4:51:10,2020-12-17 4:37:16,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WHZXKZ8H/eD9T4kiwB6MHpySGE.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRGHV4EA,blogPost,2020,"Critch, Andrew",Some AI research areas and their relevance to existential safety,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1,"INTRODUCTION This post is an overview of a variety of AI research areas in terms of how much I think contributing to and/or learning from those areas might help reduce AI x-risk. By research areas I mean “AI research topics that already have groups of people working on them and writing up their results”, as opposed to research “directions” in which I’d like to see these areas “move”. I formed these views mostly pursuant to writing AI Research Considerations for Human Existential Safety (ARCHES). My hope is that my assessments in this post can be helpful to students and established AI researchers who are thinking about shifting into new research areas specifically with the goal of contributing to existential safety somehow. In these assessments, I find it important to distinguish between the following types of value:  * The helpfulness of the area to existential safety, which I think of as a    function of what services are likely to be provided as a result of research    contributions to the area, and whether those services will be helpful to    existential safety, versus  * The educational value of the area for thinking about existential safety,    which I think of as a function of how much a researcher motivated by    existential safety might become more effective through the process of    familiarizing with or contributing to that area, usually by focusing on ways    the area could be used in service of existential safety.  * The neglect of the area at various times, which is a function of how much    technical progress has been made in the area relative to how much I think is    needed. Importantly:  * The helpfulness to existential safety scores do not assume that your    contributions to this area would be used only for projects with existential    safety as their mission. This can negatively impact the helpfulness of    contributing to areas that are more likely to be used in ways that harm    existential safety.  * The educational value scores are not ab",2020-11-18,2022-01-30 4:51:09,2022-01-30 4:51:09,2020-12-19 2:13:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/U3PJ2KXP/some-ai-research-areas-and-their-relevance-to-existential-1.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EU74EWFE,blogPost,2015,"Tomasik, Brian",Reasons to Be Nice to Other Value Systems,Center on Long-Term Risk,,,,https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/,"Several arguments support the heuristic that we should help groups holding different value systems from our own when doing so is cheap, unless those groups prove uncooperative to our values. This is true even if we don't directly care at all about other groups' value systems. Exactly how nice to be depends on the particulars of the situation.",2015-08-29,2022-01-30 4:51:09,2022-01-30 4:51:09,2020-11-23 20:07:23,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZCKXSPBP/reasons-to-be-nice-to-other-value-systems.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MKUM9MAZ,blogPost,2020,"Kokotajlo, Daniel",Persuasion Tools: AI takeover without AGI or agency?,LessWrong,,,,https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency,"[epistemic status: speculation] I'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.--Wei Dai What if most people already live in that world? A world in which taking arguments at face value is not a capacity-enhancing tool, but a security vulnerability? Without trusted filters, would they not dismiss highfalutin arguments out of hand, and focus on whether the person making the argument seems friendly, or unfriendly, using hard to fake group-affiliation signals?--Benquo 1. AI-powered memetic warfare makes all humans effectively insane.--Wei Dai, listing nonstandard AI doom scenarios This post speculates about persuasion tools—how likely they are to get better in the future relative to countermeasures, what the effects of this might be, and what implications there are for what we should do now. To avert eye-rolls, let me say up front that I don’t think the world is likely to be driven insane by AI-powered memetic warfare. I think progress in persuasion tools will probably be gradual and slow, and defenses will improve too, resulting in an overall shift in the balance that isn’t huge: a deterioration of collective epistemology, but not a massive one. However, (a) I haven’t yet ruled out more extreme scenarios, especially during a slow takeoff, and (b) even small, gradual deteriorations are important to know about. Such a deterioration would make it harder for society to notice and solve AI safety and governance problems, because it is worse at noticing and solving problems in general. Such a deterioration could also be a risk factor for world war three, revolutions, sectarian conflict, terrorism, and the like. Moreover",2020,2022-01-30 4:51:09,2022-01-30 4:51:09,2020-12-12 15:02:20,,,,,,,Persuasion Tools,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BPNW2WW3/persuasion-tools-ai-takeover-without-agi-or-agency.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XH9E33N4,blogPost,2019,"Shah, Rohin",Reward uncertainty,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ZiLLxaLB5CCofrzPp/reward-uncertainty,"In my last post, I argued that interaction between the human and the AI system was necessary in order for the AI system to “stay on track” as we encounter new and unforeseen changes to the environment. The most obvious implementation of this would be to have an AI system that keeps an estimate of the reward function. It acts to maximize its current estimate of the reward function, while simultaneously updating the reward through human feedback. However, this approach has significant problems. Looking at the description of this approach, one thing that stands out is that the actions are chosen according to a reward that we know is going to change. (This is what leads to the incentive to disable the narrow value learning system.) This seems clearly wrong: surely our plans should account for the fact that our rewards will change, without treating such a change as adversarial? This suggests that we need to have our action selection mechanism take the future rewards into account as well. While we don’t know what the future reward will be, we can certainly have a  probability distribution over it. So what if we had uncertainty over reward functions, and took that uncertainty into account while choosing actions? SETUP We’ve drilled down on the problem sufficiently far that we can create a formal model and see what happens. So, let’s consider the following setup:  * The human, Alice, knows the “true” reward function that she would like to    have optimized.  * The AI system maintains a probability distribution over reward functions, and    acts to maximize the expected sum of rewards under this distribution.  * Alice and the AI system take turns acting. Alice knows that the AI learns    from her actions, and chooses actions accordingly.  * Alice’s action space is such that she cannot take the action “tell the AI    system the true reward function” (otherwise the problem would become    trivial).  * Given these assumptions, Alice and the AI system act optimally. This is",2019,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-17 4:37:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/S52J9GZH/ZiLLxaLB5CCofrzPp.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7G9QKQZ8,blogPost,2021,"Kumar, Ramana; Kokotajlo, Daniel",P₂B: Plan to P₂B Better,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CAwwFpbteYBQw2Gkp/p-b-plan-to-p-b-better,"tl;dr: Most good plans involve taking steps to make better plans. Making better plans is the convergent instrumental goal, of which all familiar convergent instrumental goals are an instance. This is key to understanding what agency is and why it is powerful. Planning means using a world model to predict the consequences of various courses of actions one could take, and taking actions that have good predicted consequences. (We think of this with the handle “doing things for reasons,” though we acknowledge this may be an idiosyncratic use of “reasons.”) We take “planning” to include things that are relevantly similar to this procedure, such as following a bag of heuristics that approximates it. We’re also including actually following the plans, in what might more clunkily be called “planning-acting.” Planning, in this broad sense, seems essential to the kind of goal-directed, consequential, agent-like intelligence that we expect to be highly impactful. This sequence explains why. ONE CONVERGENT INSTRUMENTAL GOAL TO RULE THEM ALL Consider the maxim “make there be more and/or better planning towards your goal.” This section argues that all the classic convergent instrumental goals are special cases of this maxim. To flesh this out a little, here are some categories of ways to follow the maxim. Remember that a planner is typically close (in terms of what it might affect via action) to at least one planner – itself – so these directions can typically be applied in the first case to the planner itself.  * Make the planners with your goal better at planning. For example, get them    new relevant data to work with¹, get them to run faster or more effective    algorithms, build protections against value drift, etc.  * Make the planners with your goal have better options. For example, move them    to better locations, get them more resources, get them more power or a    greater number of options to select from, have them take steps in an    object-level plan towards t",2021-10-24,2022-01-30 4:51:08,2022-01-30 4:51:08,2021-12-11 14:17:29,,,,,,,P₂B,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QRHZCPVT/p-b-plan-to-p-b-better.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9GDT9M8,blogPost,2017,"Oesterheld, Caspar",Naturalized induction – a challenge for evidential and causal decision theory,LessWrong,,,,https://www.lesswrong.com/posts/kgsaSbJqWLtJfiCcz/naturalized-induction-a-challenge-for-evidential-and-causal,"As some of you may know, I disagree with many of the criticisms leveled against  evidential decision theory (EDT). Most notably, I believe that Smoking lesion-type problems don't refute EDT. I also don't think that EDT's non-updatelessness leaves a lot of room for disagreement, given that EDT  recommends immediate self-modification to updatelessness. However, I do believe there are some issues with run-of-the-mill EDT. One of them is naturalized induction. It is in fact not only a problem for EDT but also for causal decision theory (CDT) and most other decision theories that have been proposed in- and outside of academia. It does not affect logical decision theories, however. THE ROLE OF NATURALIZED INDUCTION IN DECISION THEORY Recall that EDT prescribes taking the action that maximizes expected utility, i.e. where is the set of available actions, is the agent's utility function, is a set of possible world models, represents the agent's past observations (which may include information the agent has collected about itself). CDT works in a – for the purpose of this article – similar way, except that instead of conditioning on in the usual way, it calculates some causal counterfactual, such as Pearl's do-calculus: . The problem of naturalized induction is that of assigning posterior probabilities to world models (or or whatever) when the agent is  naturalized, i.e., embedded into its environment. Consider the following example. Let's say there are 5 world models , each of which has equal prior probability. These world models may be cellular automata. Now, the agent makes the observation . It turns out that worlds and don't contain any agents at all, and contains no agent making the observation . The other two world models, on the other hand, are consistent with . Thus, for and  for . Let's assume that the agent has only two actions and that in world model  the only agent making observation takes action and in the only agent making observation takes action , then a",2017-09-22,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:47:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PFXHQPWU/naturalized-induction-a-challenge-for-evidential-and-causal.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XKENFS2F,blogPost,2017,"Oesterheld, Caspar",Is it a bias or just a preference? An interesting issue in preference idealization,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/01/18/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization/,"When taking others’ preferences into account, we will often want to idealize them rather than taking them too literally. Consider the following example. You hold a glass of transparent liquid…",2017-01-18,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:56:11,,,,,,,Is it a bias or just a preference?,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UQ3WP8PZ/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFFRRWRE,blogPost,2015,"Tomasik, Brian",International Cooperation vs. AI Arms Race,Center on Long-Term Risk,,,,https://longtermrisk.org/international-cooperation-vs-ai-arms-race/,"There's a decent chance that governments will be the first to build artificial general intelligence (AI). International hostility, especially an AI arms race, could exacerbate risk-taking, hostile motivations, and errors of judgment when creating AI. If so, then international cooperation could be an important factor to consider when evaluating the flow-through effects of charities.",2015-04-08,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 1:06:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/D464WPF3/international-cooperation-vs-ai-arms-race.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUB3ZWDG,blogPost,2015,"Tomasik, Brian",Gains from Trade through Compromise,Center on Long-Term Risk,,,,https://longtermrisk.org/gains-from-trade-through-compromise/,"When agents of differing values compete, they may often find it mutually advantageous to compromise rather than continuing to engage in zero-sum conflicts. Potential ways of encouraging cooperation include promoting democracy, tolerance and (moral) trade. Because a future without compromise could be many times worse than a future with it, advancing compromise seems an important undertaking.",2015-04-10,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 20:08:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VHTBJW4Q/gains-from-trade-through-compromise.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9625IHB,blogPost,2017,"Gloor, Lukas",Multiverse-wide cooperation in a nutshell,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/7MdLurJGhGmqRv25c/multiverse-wide-cooperation-in-a-nutshell,"(Crossposted from the FRI blog.)  This is a post I wrote about Caspar Oesterheld’s long paper Multiverse-wide cooperation via correlated decision-making. Because I have found the idea tricky to explain – which unfortunately makes it difficult to get feedback from others on whether the thinking behind it makes sense – I decided to write a shorter summary. While I am hoping that my text can serve as a standalone piece, for additional introductory content I also recommend reading the beginning of Caspar’s paper, or watching the short video introduction here (requires basic knowledge of the “CDT, EDT or something else” debate in decision theory). 0. ELEVATOR PITCH (Disclaimer: Especially for the elevator pitch section here, I am sacrificing accuracy and precision for brevity. References can be found in Caspar’s paper.)  It would be an uncanny coincidence if the observable universe made up everything that exists. The reason we cannot find any evidence for there being stuff beyond the edges of our universe is not because it is likely that there is nothingness, but because photons from further away simply would not have had sufficient time after the big bang to reach us. This means that the universe we find ourselves in may well be vastly larger than what we can observe, in fact even infinitely  larger. The theory of inflationary cosmology in addition hints at the existence of other universe bubbles with different fundamental constants forming or disappearing under certain conditions, somehow co-existing with our universe in parallel. The umbrella term multiverse captures the idea that the observable universe is just a tiny portion of everything that exists. The multiverse may contain myriads of worlds like ours, including other worlds with intelligent life and civilization. An infinite multiverse (of one sort or another) is actually amongst the most popular cosmological hypotheses, arguably even favored by the majority of experts.  Many ethical theories (in particular",2017-11-02,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:46:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/65NR427Z/multiverse-wide-cooperation-in-a-nutshell.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCKKDM49,blogPost,2018,"Oesterheld, Caspar",Moral realism and AI alignment,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/08/06/moral-realism-and-ai-alignment/,"“Abstract”: Some have claimed that moral realism – roughly, the claim that moral claims can be true or false – would, if true, have implications for AI alignment research, such that moral realists …",2018-08-06,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:38:57,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R98Z34VR/moral-realism-and-ai-alignment.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EK5VIJC,blogPost,2015,"Tomasik, Brian",How Would Catastrophic Risks Affect Prospects for Compromise?,Center on Long-Term Risk,,,,https://longtermrisk.org/how-would-catastrophic-risks-affect-prospects-for-compromise/,"Global catastrophic risks – such as biotech disasters or nuclear war – would cause major damage in the short run, but their effects on the long-run trajectory that humanity takes are also significant. In particular, to the extent these disasters increase risks of war, they seem likely to precipitate AI arms races between nations and worsen prospects for compromise.",2015-08-29,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 1:11:09,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24MAX7RT,blogPost,2020,"Kokotajlo, Daniel",How Roodman's GWP model translates to TAI timelines,LessWrong,,,,https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines,"How does David Roodman’s world GDP model translate to TAI timelines? Now, before I go any further, let me be the first to say that I don’t think we should use this model to predict TAI. This model takes a very broad outside view and is thus inferior to models like Ajeya Cotra’s which make use of more relevant information. (However, it is still useful for rebutting claims that TAI is unprecedented, inconsistent with historical trends, low-prior, etc.) Nevertheless, out of curiosity I thought I’d calculate what the model implies for TAI timelines. Here is the projection made by Roodman’s model. The red line is real historic GWP data; the splay of grey shades that continues it is the splay of possible futures calculated by the model. The median trajectory is the black line. I messed around with a ruler to make some rough calculations, marking up the image with blue lines as I went. The big blue line indicates the point on the median trajectory where GWP is 10x what is was in 2019. Eyeballing it, it looks like it happens around 2040, give or take a year. The small vertical blue line indicates the year 2037. The small horizontal blue line indicates GWP in 2037 on the median trajectory. Thus, it seems that between 2037 and 2040 on the median trajectory, GWP doubles. (One-ninth the distance between 1,000 and 1,000,000 is crossed, which is one-third of an order of magnitude, which is about one doubling). This means that TAI happens around 2037 on the median trajectory according to this model, at least according to Ajeya Cotra’s definition of transformative AI  as “software which causes a tenfold acceleration in the rate of growth of the world economy (assuming that it is used everywhere that it would be economically profitable to use it)... This means that if TAI is developed in year Y, the entire world economy would more than double by year Y + 4.” What about the non-median trajectories? Each shade of grey represents 5 percent of the simulated future trajectories, so",2020,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-12 15:02:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G7VN452V/how-roodman-s-gwp-model-translates-to-tai-timelines.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6JZ2JGFE,blogPost,2018,"Oesterheld, Caspar",Goertzel’s GOLEM implements evidential decision theory applied to policy choice,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/04/26/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice/,"I’ve written about the question of which decision theories describe the behavior of approaches to AI like the “Law of Effect”. In this post, I would like to discuss GOLEM, an arch…",2018-04-26,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:41:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GK7JQRH4/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QIVPQ6DS,blogPost,2018,"Shah, Rohin",Preface to the sequence on value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning,"This is a meta-post about the upcoming sequence on Value Learning that will start to be published this Thursday. This preface will also be revised significantly once the second half of the sequence is fully written. PURPOSE OF THE SEQUENCE The first part of this sequence will be about the tractability of ambitious value learning, which is the idea of inferring a utility function for an AI system to optimize based on observing human behavior. After a short break, we will (hopefully) continue with the second part, which will be about why we might want to think about techniques that infer human preferences, even if we assume we won’t do ambitious value learning with such techniques. The aim of this part of the sequence is to gather the current best public writings on the topic, and provide a unifying narrative that ties them into a cohesive whole. This makes the key ideas more discoverable and discussable, and provides a quick reference for existing researchers. It is meant to teach the ideas surrounding one specific approach to aligning advanced AI systems. We’ll explore the specification problem, in which we would like to define the behavior we want to see from an AI system. Ambitious value learning is one potential avenue of attack on the specification problem, that assumes a particular model of an AI system (maximizing expected utility) and a particular source of data (human behavior). We will then delve into conceptual work on ambitious value learning that has revealed obstructions to this approach. There will be pointers to current research that aims to circumvent these obstructions. The second part of this sequence is currently being assembled, and this preface will be updated with details once it is ready. The first half of this sequence takes you near the cutting edge of conceptual  work on the ambitious value learning problem, with some pointers to work being done at this frontier. Based on the arguments in the sequence, I am confident that the obvious f",2018,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-12-17 4:36:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2HGSWMFH/oH8KMnXHnw964QyS6.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XWGR86QB,blogPost,2021,"Kokotajlo, Daniel",Fun with +12 OOMs of Compute,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute,"OR: BIG TIMELINES CRUX OPERATIONALIZED What fun things could one build with +12 orders of magnitude of compute? By ‘fun’ I mean ‘powerful.’ This hypothetical is highly relevant to AI timelines, for reasons I’ll explain later. Summary (Spoilers): I describe a hypothetical scenario that concretizes the question “what could be built with 2020’s algorithms/ideas/etc. but a trillion times more compute?”Then I give some answers to that question. Then I ask: How likely is it that some sort of TAI would happen in this scenario? This second question is a useful operationalization of the (IMO) most important, most-commonly-discussed timelines crux: “Can we get TAI just by throwing more compute at the problem?” I consider this operationalization to be the main contribution of this post; it directly plugs into Ajeya’s timelines model and is quantitatively more cruxy than anything else I know of. The secondary contribution of this post is my set of answers to the first question: They serve as intuition pumps for my answer to the second, which strongly supports my views on timelines. THE HYPOTHETICAL In 2016 the Compute Fairy visits Earth and bestows a blessing: Computers are magically 12 orders of magnitude faster! Over the next five years, what happens? The Deep Learning AI Boom still happens, only much crazier: Instead of making AlphaStar for 10^23 floating point operations, DeepMind makes something for 10^35. Instead of making GPT-3 for 10^23 FLOPs, OpenAI makes something for 10^35. Instead of industry and academia making a cornucopia of things for 10^20 FLOPs or so, they make a cornucopia of things for 10^32 FLOPs or so. When random grad students and hackers spin up neural nets on their laptops, they have a trillion times more compute to work with. [EDIT: Also assume magic +12 OOMs of memory, bandwidth, etc. All the ingredients of compute.] For context on how big a deal +12 OOMs is, consider the graph below, from ARK. It’s measuring petaflop-days, which are about 10^20 F",2021-03-01,2022-01-30 4:51:07,2022-01-30 4:51:07,2021-12-11 14:12:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WWZQP7ZC/fun-with-12-ooms-of-compute.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7BCH8MC8,blogPost,2015,"Tomasik, Brian",Flavors of Computation Are Flavors of Consciousness,Center on Long-Term Risk,,,,https://longtermrisk.org/flavors-of-computation-are-flavors-of-consciousness/,"If we don't understand why we're conscious, how come we're so sure that extremely simple minds are not? I propose to think of consciousness as intrinsic to computation, although different types of computation may have very different types of consciousness – some so alien that we can't imagine them. Since all physical processes are computations, […]",2015-04-10,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 20:03:32,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93Q953KS,blogPost,2020,"Clifton, Jesse",Equilibrium and prior selection problems in multipolar deployment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1,"To avoid catastrophic conflict in multipolar AI scenarios, we would like to design AI systems such that AI-enabled actors will tend to cooperate. This post is about some problems facing this effort and some possible solutions. To explain these problems, I'll take the view that the agents deployed by AI developers (the ''principals'') in a multipolar scenario are moves in a game. The payoffs to a principal in this game depend on how the agents behave over time. We can talk about the equilibria of this game, and so on. Ideally, we would be able to make guarantees like this:  1. The payoffs resulting from the deployed agents' actions are optimal with     respect to some appropriate ""welfare function''. This welfare function would     encode some combination of total utility, fairness, and other social     desiderata;  2. The agents are in equilibrium --- that is, no principal has an incentive to     deploy an agent with a different design, given the agents deployed by the     other principals. The motivation for item 1 is clear: we want outcomes which are fair by each of the principals' lights. In particular, we want an outcome that the principals will all agree to. And item 2 is desirable because an equilibrium constitutes a self-enforcing contract; each agent wants to play their equilibrium strategy, if they believe that the other agents are playing the same equilibrium. Thus, given that the principals all say that they will deploy agents that satisfy 1 and 2, we could have some confidence that a welfare-optimal outcome will in fact obtain. Two simple but critical problems need to be addressed in order to make such guarantees: the equilibrium and prior selection problems. The equilibrium selection problem is that this deployment game will have many equilibria. Even if the principals agree on a welfare function, it is possible that many different profiles of agents optimize the same welfare function. So the principals need to coordinate on the profile of agents dep",2020,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-12-12 14:54:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HV7EIWIV/equilibrium-and-prior-selection-problems-in-multipolar-1.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8QQEAZVI,blogPost,2018,"Althaus, David",Descriptive Population Ethics and Its Relevance for Cause Prioritization,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/CmNBmSf6xtMyYhvcs/descriptive-population-ethics-and-its-relevance-for-cause,"SUMMARY Descriptive ethics is the empirical study of people's values and ethical views, e.g. via a survey or questionnaire. This overview focuses on beliefs about population ethics and exchange rates between goods (e.g. happiness) and bads  (e.g. suffering). Two variables seem particularly important and action-guiding in this context, especially when trying to make informed choices about how to best shape the long-term future: 1) One’s normative goods-to-bads ratio  (N-ratio) and 2) one’s expected bads-to-goods ratio (E-ratio). I elaborate on how a framework consisting of these two variables could inform our decision-making with respect to shaping the long-term future, as well as facilitate cooperation among differing value systems and further moral reflection. I then present concrete ideas for further research in this area and investigate associated challenges. The last section lists resources which discuss further methodological and theoretical issues which were beyond the scope of the present text. DESCRIPTIVE ETHICS AND LONG-TERM FUTURE PRIORITIZATION Recently, some debate has emerged on whether reducing extinction risk is the ideal course of action for shaping the long-term future. For instance, in the  Global Priorities Institute (GPI) research agenda, Greaves & MacAskill (2017, p.13) ask “[...] whether it might be more important to ensure that future civilisation is good, assuming we don’t go extinct, than to ensure that future civilisation happens at all.” We could further ask to what extent we should focus our efforts on reducingrisks of astronomical suffering (s-risks). Again, Greaves & MacAskill: “Should we be more concerned about avoiding the worst possible outcomes for the future than we are for ensuring the very best outcomes occur [...]?” Given the enormous stakes, these are arguably some of the most important questions facing those who prioritize shaping the long-term future.1 Some interventions increase both the quality of future civilization as w",2018-04-03,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:40:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/K9FMSZDC/descriptive-population-ethics-and-its-relevance-for-cause.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC33WP3T,blogPost,2017,"Oesterheld, Caspar",Decision Theory and the Irrelevance of Impossible Outcomes,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/01/17/decision-theory-and-the-irrelevance-of-impossible-outcomes/,"(This post assumes some knowledge of the decision theory of Newcomb-like scenarios.) One problem in the decision theory of Newcomb-like scenarios (i.e. the study of whether causal, evidential or so…",2017-01-17,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:56:44,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F7GPMPMH/decision-theory-and-the-irrelevance-of-impossible-outcomes.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6MTT7H2E,blogPost,2021,"Torges, Stefan",Coordination challenges for preventing AI conflict,Center on Long-Term Risk,,,,https://longtermrisk.org/coordination-challenges-for-preventing-ai-conflict/,"Summary In this article, I will sketch arguments for the following claims: Transformative AI scenarios involving multiple systems pose a unique existential risk: catastrophic bargaining failure between multiple AI systems (or joint AI-human systems). This risk is not sufficiently addressed by successfully aligning those systems, and we cannot safely delegate its solution to the AI systems themselves. Developers are better positioned than more far-sighted successor agents to coordinate in a way that solves this problem, but a solution also does not seem guaranteed. Developers intent on solving this problem can choose between developing separate but compatible systems that do not engage in costly conflict or building a single joint system. While the second option seems preferable from an altruistic perspective, […]",2021-03-09,2022-01-30 4:51:07,2022-01-30 4:51:07,2021-12-11 14:20:34,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9645PS8Z/coordination-challenges-for-preventing-ai-conflict.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4AA6S2X,blogPost,2017,"Oesterheld, Caspar",Complications in evaluating neglectedness,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/,Neglectedness (or crowdedness) is a heuristic that effective altruists use to assess how much impact they could have in a specific cause area. It is usually combined with scale (a.k.a. importance) …,2017-06-25,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:51:16,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/737I3Z2G/complications-in-evaluating-neglectedness.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDIWGCQQ,blogPost,2021,"Clifton, Jesse",CLR's recent work on multi-agent systems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EzoCZjTdWTMgacKGS/clr-s-recent-work-on-multi-agent-systems,"INTRODUCTION We at the Center on Long-Term Risk (CLR) are focused on reducing risks of cosmically significant amounts of suffering[1], or s-risks, from transformative artificial intelligence (TAI). We currently believe that:  * Several agential s-risks — in which agents deliberately cause great amounts    of suffering — are among the largest s-risks in expectation.  * One of the most promising ways of addressing these risks involves intervening    in the design of TAI systems to make them more cooperative.  * Increasing the cooperativeness of powerful systems is robustly valuable for    everybody interested in shaping the long-run future. (See the recent     Cooperative AI and AI Research Considerations for Existential Safety (ARCHES)     research agendas for examples of work motivated by considerations other than    s-risks.) This was the subject of our research agenda on cooperative, conflict, and TAI. In this post, I’d like to give an overview of some of the research that CLR has been doing on multi-agent systems. I’ll then briefly remark on the importance, tractability, and neglectedness of this work (both from a downside-focused and more mainstream longtermist perspective), though a thorough discussion of prioritization is beyond the scope of this post. The main goal is to inform the community about what we’ve been up to recently in this space. We’re interested in supporting people who want to contribute to this work. If you might benefit from financial support to work on topics related to the research described here, you can fill out this form. If you’re interested in working with us as a temporary summer research fellow, full-time researcher, or research assistant, apply here. POTENTIAL CAUSES OF CONFLICT BETWEEN INTELLIGENT ACTORS It is possible that TAI systems will find themselves in conflict, despite the fact that conflict is often seemingly Pareto-inefficient. Factors that might lead intelligent agents to become engaged in conflict include:  1. Unce",2021-03-08,2022-01-30 4:51:07,2022-01-30 4:51:07,2021-11-14 18:22:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WVXDMTD/clr-s-recent-work-on-multi-agent-systems.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWMXW7B8,blogPost,2016,"Tomasik, Brian",Do Artificial Reinforcement-Learning Agents Matter Morally?,Center on Long-Term Risk,,,,https://longtermrisk.org/do-artificial-reinforcement-learning-agents-matter-morally/,"Artificial reinforcement learning (RL), a widely used training method in computer science, has striking parallels to reward and punishment learning in biological brains. Plausible theories of consciousness imply a non-zero probability that RL agents qualify as sentient and deserve our moral consideration, especially as AI research advances and RL agents become more sophisticated.",2016-07-28,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 1:03:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KD663JCF/do-artificial-reinforcement-learning-agents-matter-morally.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
639338X6,blogPost,2015,"Tomasik, Brian",Differential Intellectual Progress as a Positive-Sum Project,Center on Long-Term Risk,,,,https://longtermrisk.org/differential-intellectual-progress-as-a-positive-sum-project/,"Fast technological development carries a risk of creating extremely powerful tools, especially AI, before society has a chance to figure out how best to use those tools in positive ways for many value systems. Suffering reducers may want to help mitigate the arms race for AI so that AI developers take fewer risks and have […]",2015-08-29,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 1:07:52,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WT7HXT5J/differential-intellectual-progress-as-a-positive-sum-project.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ7VHBIZ,blogPost,2017,"Treutlein, Johannes",Did EDT get it right all along? Introducing yet another medical Newcomb problem,LessWrong,,,,https://www.lesswrong.com/posts/iqpizeN4hkbTjkugo/did-edt-get-it-right-all-along-introducing-yet-another,"One of the main arguments given against Evidential Decision Theory (EDT) is that it would “one-box” in medical Newcomb problems. Whether this is the winning action has been a hotly debated issue on LessWrong. A majority, including experts in the area such as Eliezer Yudkowsky and Wei Dai, seem to think that one should two-box (See e.g. Yudkowsky 2010, p.67). Others have tried to argue  in favor of EDT by claiming that the winning action would be to one-box, or by offering reasons why EDT would in some cases two-box after all. In this blog post, I want to argue that EDT gets it right: one-boxing is the correct action in medical Newcomb problems. I introduce a new thought experiment, the Coin Flip Creation problem, in which I believe the winning move is to one-box. This new problem is structurally similar to other medical Newcomb problems such as the  Smoking Lesion, though it might elicit the intuition to one-box even in people who would two-box in some of the other problems. I discuss both how EDT and other decision theories would reason in the problem and why people’s intuitions might diverge in different formulations of medical Newcomb problems. TWO KINDS OF NEWCOMBLIKE PROBLEMS There are two different kinds of Newcomblike problems. In Newcomb’s original paradox, both EDT and Logical Decision Theories (LDT), such as Timeless Decision Theory (TDT) would one-box and therefore, unlike CDT, win $1 million. In medical Newcomb problems, EDT’s and LDT’s decisions diverge. This is because in the latter, a (physical) causal node that isn’t itself a decision algorithm influences both the current world state and our decisions – resulting in a correlation between action and environment but, unlike the original Newcomb, no “logical” causation. It’s often unclear exactly how a causal node can exert influence on our decisions. Does it change our decision theory, utility function, or the information available to us? In the case of the Smoking Lesion problem, it seems plausible",2017-01-24,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:55:10,,,,,,,Did EDT get it right all along?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EJ9I6KHN/did-edt-get-it-right-all-along-introducing-yet-another.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R7ITNVNC,blogPost,2020,"Leskela, Anni",Commitment and credibility in multipolar AI scenarios,LessWrong,,,,https://www.lesswrong.com/posts/LvtsFKxg2t3nWhKRq/commitment-and-credibility-in-multipolar-ai-scenarios,"The ability to make credible commitments is a key factor in many bargaining situations ranging from trade to international conflict. This post builds a taxonomy of the commitment mechanisms that transformative AI (TAI) systems could use in future multipolar scenarios, describes various issues they have in practice, and draws some tentative conclusions about the landscape of commitments we might expect in the future. INTRODUCTION A better understanding of the commitments that future AI systems could make is helpful for predicting and influencing the dynamics of multipolar scenarios. The option to credibly bind oneself to certain actions or strategies fundamentally changes the game theory behind bargaining, cooperation, and conflict. Credible commitments and general transparency can work to stabilize positive-sum agreements, and to increase the efficiency of threats (Schelling 1960), both of which could be relevant to how well TAI trajectories will reflect our values. Because human goals can be contradictory, and even broadly aligned AI systems could come to prioritize different outcomes depending on their domains and histories, these systems could end up in competitive situations and bargaining failures where a lot of value is lost. Similarly, if some systems in a multipolar scenario are well aligned and others less so, some worst cases might be avoidable if stable peaceful agreements can be reached. As an example of the practical significance of commitment ability in stabilizing peaceful strategies, standard theories in international relations hold that conflicts between nations are difficult to avoid indefinitely primarily because there are no reliable commitment mechanisms for peaceful agreements (e.g. Powell 2004, Lake 1999, Rosato 2015), even when nations would overall prefer them. In addition to the direct costs of conflict, the lack of enforceable commitments leads to continuous resource loss from arms races, monitoring, and other preparations for possible",2020,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-12-12 15:04:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WFUHVWSK/commitment-and-credibility-in-multipolar-ai-scenarios.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N6AANRRU,blogPost,2021,"Clifton, Jesse",Collaborative game specification: arriving at common models in bargaining,Center on Long-Term Risk,,,,https://longtermrisk.org/collaborative-game-specification/,"Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient.   In another post, I described the ""prior selection problem"", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on […]",2021-03-06,2022-01-30 4:51:07,2022-01-30 4:51:07,2021-10-31 16:55:52,,,,,,,Collaborative game specification,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HJ3TRAF5/collaborative-game-specification.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTZBKAUW,blogPost,2018,"Gloor, Lukas",Cause prioritization for downside-focused value systems,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems,"Last edited: August 27th 2019.  This post outlines my thinking on cause prioritization from the perspective of value systems whose primary concern is reducing disvalue. I’m mainly thinking of  suffering-focused ethics (SFE), but I also want to include moral views that attribute substantial disvalue to things other than suffering, such as inequality or preference violation. I will limit the discussion to interventions targeted at improving the long-term future (see the reasons in section II). I hope my post will also be informative for people who do not share a downside-focused outlook, as thinking about cause prioritization from different perspectives, with emphasis on considerations other than those one is used to, can be illuminating. Moreover, understanding the strategic considerations for plausible moral views is essential for acting under moral uncertainty and cooperating with people with other values. I will talk about the following topics:  * Which views qualify as downside-focused (given our empirical situation)  * Why downside-focused views prioritize s-risk reduction over utopia creation  * Why extinction risk reduction is unlikely to be a promising intervention    according to downside-focused views  * Why AI alignment is probably positive for downside-focused views, and    especially positive if done with certain precautions  * What to include in an EA portfolio that incorporates population ethical    uncertainty and cooperation between value systems WHICH VIEWS QUALIFY AS DOWNSIDE-FOCUSED? I’m using the term downside-focused to refer to value systems that in practice (given what we know about the world) primarily recommend working on interventions that make bad things less likely.[1] For example, if one holds that what is most important is how things turn out for individuals (welfarist consequentialism), and that it is comparatively unimportant to add well-off beings to the world, then one should likely focus on preventing suffering.[2] That would b",2018-01-31,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:44:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DI58FZJA/cause-prioritization-for-downside-focused-value-systems.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2EIBJKHU,blogPost,2021,Jia,Case studies of self-governance to reduce technology risk,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/Xf6QE6txgvfCGvZpk/case-studies-of-self-governance-to-reduce-technology-risk,"I worked on this research project during my summer fellowship at the Center on Long-Term Risk. Though the findings aren't particularly insightful, I’m posting this unpolished version to:  * Hopefully help other people attempting similar projects save time and effort  * Demonstrate one approach to case study selection  * Give others a sense of what one type of AI governance summer research project    might look like. SUMMARY  * Self-governance occurs when private actors coordinate to address issues that    are not obviously related to profit, with minimal involvement from    governments and standards bodies.  * Historical cases of self-governance to reduce technology risk are rare. I    find 6 cases that seem somewhat similar to AI development, including the    actions of Leo Szilard and other physicists in 1939 and the 1975 Asilomar    conference.  * The following factors seem to make self-governance efforts more likely to    occur: * Risks are salient     * The government looks like it might step in if private actors do       nothing     * The field or industry is small     * Support from gatekeepers (like journals and large consumer-facing       firms)     * Support from credentialed scientists.          * After the initial self-governance effort, governments usually step in to    develop and codify rules.  * My biggest takeaway is probably that self-governance efforts seem more likely    to occur when risks are somewhat prominent. As a result, we could do more to     connect “near-term” issues like data privacy and algorithmic bias with    “long-term” concerns. We could try to preemptively identify “fire alarms” for    TAI, and be ready to take advantage of these warning signals if they occur. INTRODUCTION Private actors play an important role in AI governance. Several companies have released their own guidelines, and the Partnership on AI is a notable actor in the space.[1] In other words, we are beginning to see elements of self-governance in the AI industry",2021-04-06,2022-01-30 4:51:06,2022-01-30 4:51:06,2021-11-14 19:00:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SHI8RDJ9/case-studies-of-self-governance-to-reduce-technology-risk.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H57NI5W3,blogPost,2021,"Kokotajlo, Daniel","Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity,"[Epistemic status: Strong opinions lightly held, this time with a cool graph.] I argue that an entire class of common arguments against short timelines is bogus, and provide weak evidence that anchoring to the human-brain-human-lifetime milestone is reasonable. In a sentence, my argument is that the complexity and mysteriousness and efficiency of the human brain (compared to artificial neural nets) is almost zero evidence that building TAI will be difficult, because evolution typically makes things complex and mysterious and efficient, even when there are simple, easily understood, inefficient designs that work almost as well (or even better!) for human purposes. In slogan form: If all we had to do to get TAI was make a simple neural net 10x the size of my brain, my brain would still look the way it does. The case of birds & planes illustrates this point nicely. Moreover, it is also a precedent for several other short-timelines talking points, such as the human-brain-human-lifetime (HBHL) anchor. PLAN:  1. Illustrative Analogy  2. Exciting Graph  3. Analysis 1. Extra brute force can make the problem a lot easier      2. Evolution produces complex mysterious efficient designs by         default, even when simple inefficient designs work just fine for human         purposes.      3. What’s bogus and what’s not      4. Example: Data-efficiency            4. Conclusion  5. Appendix 1909 French military plane, the Antionette VII. By Deep silence (Mikaël Restoux) - Own work (Bourget museum, in France), CC BY 2.5, https://commons.wikimedia.org/w/index.php?curid=1615429 ILLUSTRATIVE ANALOGY AI timelines, from our current perspectiveFlying machine timelines, from the perspective of the late 1800’s:Shorty: Human brains are giant neural nets. This is reason to think we can make human-level AGI (or at least AI with  strategically relevant skills, like politics and science) by making giant neural nets.Shorty: Birds are winged creatures that paddle through the air. This i",2021,2022-01-30 4:51:06,2022-01-30 4:51:06,2021-11-13 21:52:57,,,,,,,"Birds, Brains, Planes, and AI",,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GNFCNKHK/birds-planes-brains-and-ai-against-appeals-to-the-complexity.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5XCN8E2T,blogPost,2017,"Treutlein, Johannes",“Betting on the Past” by Arif Ahmed,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/02/06/betting-on-the-past-by-arif-ahmed/,"[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory and in Arbital’s Introduction to Logical Decision Theory.] I recently discovered an int…",2017-02-06,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 0:54:29,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBVFK75X/betting-on-the-past-by-arif-ahmed.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GGXJ663,blogPost,2017,"Oesterheld, Caspar",Are causal decision theorists trying to outsmart conditional probabilities?,LessWrong,,,,https://www.lesswrong.com/posts/cyJgdhgYaM2CbZ7tP/are-causal-decision-theorists-trying-to-outsmart-conditional,"Presumably, this has been discussed somewhere in the past, but I wonder to which extent causal decision theorists (and many other non-evidential decision theorists, too) are trying to make better predictions than (what they think to be) their own conditional probabilities. To state this question more clearly, let’s look at the generic Newcomb-like problem with two actions a1 and a2 (e.g., one-boxing and two-boxing, cooperating or defecting, not smoking or smoking) and two states s1 and s2 (specifying, e.g., whether there is money in both boxes, whether the other agent cooperates, whether one has cancer). The Newcomb-ness is the result of two properties:  * No matter the state, it is better to take action a2, i.e. u(a2,s1)>u(a1,s1)    and u(a2,s2)>u(a1,s2). (There are also problems without dominance where CDT    and EDT nonetheless disagree. For simplicity I will assume dominance, here.)          * The action cannot causally affect the state, but somehow taking a1 gives us    evidence that we’re in the preferable state s1. That is, P(s1|a1)>P(s1|a2)    and u(a1,s1)>u(a2,s2).         Then, if the latter two differences are large enough, it may be that E[u|a1] > E[u|a2]. I.e. P(s1|a1) * u(s1,a1) + P(s2|a1) * u(s2,a1) > P(s1|a2) * u(s1,a2) + P(s2|a2) * u(s2,a2), despite the dominance. Now, my question is: After having taken one of the two actions, say a1, but before having observed the state, do causal decision theorists really assign the probability P(s1|a1) (specified in the problem description) to being in state s1? I used to think that this was the case. E.g., the way I learned about Newcomb’s problem is that causal decision theorists understand that, once they have said the words “both boxes for me, please”, they assign very low probability to getting the million. So, if there were a period between saying those words and receiving the payoff, they would bet at odds that reveal that they assign a low probability (namely P(s1,a2)) to money being under",2017-05-16,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 0:52:29,,,,,,,Are causal decision theorists trying to outsmart conditional probabilities?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R7Z7P6H5/are-causal-decision-theorists-trying-to-outsmart-conditional.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PRZR88VR,blogPost,2017,"Treutlein, Johannes",Anthropic uncertainty in the Evidential Blackmail,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/05/12/anthropic-uncertainty-in-the-evidential-blackmail/,"I’m currently writing a piece on anthropic uncertainty in Newcomb problems. The idea is that whenever someone simulates us to predict our actions, this leads us to have anthropic uncertainty about …",2017-05-12,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 0:51:55,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X3JHA4D9/anthropic-uncertainty-in-the-evidential-blackmail.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WFI4PND,blogPost,2021,"Lyzhov, Alex","""AI and Compute"" trend isn't predictive of what is happening",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening,"(open in a new tab to view at higher resolution) In May 2018 (almost 3 years ago) OpenAI published their ""AI and Compute""  blogpost where they highlighted the trend of increasing compute spending on training the largest AI models and speculated that the trend might continue into the future. This note is aimed to show that the trend has ended right around the moment of OpenAI publishing their post and doesn't hold up anymore. On the above image, I superimposed the scatter plot from OpenAI blogpost and my estimates of compute required for some recent large and ambitious ML experiments. To the best of my knowledge (and I have tried to check for this), there haven't been any experiments that required more compute than those shown on the plot. The main thing shown here is that less than one doubling of computational resources for the largest training occured in the 3-year period between 2018 and 2021, compared to around 10 doublings in the 3-year period between 2015 and 2018. This seems to correspond to a severe slowdown of computational scaling. To stay on the trend line, we currently would need an experiment requiring roughly around 100 times more compute than GPT-3. Considering that GPT-3 may have costed between $5M and $12M and accelerators haven't vastly improved since then, such an experiment would now likely cost $0.2B - $1.5B.",2021-04-01,2022-01-30 4:51:06,2022-01-30 4:51:06,2021-12-11 14:10:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3RKNJGBW/ai-and-compute-trend-isn-t-predictive-of-what-is-happening.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78R2ACU4,blogPost,2020,"Kokotajlo, Daniel",Against GDP as a metric for timelines and takeoff speeds,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/aFaKhG86tTrKvtAnT/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds,"OR: WHY AI TAKEOVER MIGHT HAPPEN BEFORE GDP ACCELERATES, AND OTHER THOUGHTS ON WHAT MATTERS FOR TIMELINES AND TAKEOFF SPEEDS [Epistemic status: Strong opinion, lightly held] I think world GDP (and economic growth more generally) is overrated as a metric for AI timelines and takeoff speeds. Here are some uses of GDP that I disagree with, or at least think should be accompanied by cautionary notes:  * Timelines: Ajeya Cotra thinks of transformative AI as “software which causes    a tenfold acceleration in the rate of growth of the world economy (assuming    that it is used everywhere that it would be economically profitable to use    it).” I don’t mean to single her out in particular; this seems like the    standard definition now. And I think it's much better than one prominent    alternative, which is to date your AI timelines to the first time world GDP    (GWP) doubles in a year!  * Takeoff Speeds: Paul Christiano argues for Slow Takeoff. He thinks we can use    GDP growth rates as a proxy for takeoff speeds. In particular, he thinks Slow    Takeoff ~= GWP doubles in 4 years before the start of the first 1-year GWP    doubling. This proxy/definition has received a lot of uptake.  * Timelines: David Roodman’s excellent model projects GWP hitting infinity in    median 2047, which I calculate means TAI in median 2037. To be clear, he    would probably agree that we shouldn’t use these projections to forecast TAI,    but I wish to add additional reasons for caution.  * Timelines: I’ve sometimes heard things like this: “GWP growth is stagnating    over the past century or so; hyperbolic progress has ended; therefore TAI is    very unlikely.”  * Takeoff Speeds: Various people have said things like this to me: “If you    think there’s a 50% chance of TAI by 2032, then surely you must think there’s    close to a 50% chance of GWP growing by 8% per year by 2025, since TAI is    going to make growth rates go much higher than that, and progress is    typically continuous.",2020-12-29,2022-01-30 4:51:06,2022-01-30 4:51:06,2021-12-11 14:14:38,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NUWWJRAQ/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E9GVD3BD,blogPost,2020,"Leong, Chris",Embedded vs. External Decision Problems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/br7KRSeNymwSvZnf5/embedded-vs-external-decision-problems,,2020-03-04,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-08-18 20:51:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/E9UCKBGB/embedded-vs-external-decision-problems.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VM3QWB8S,blogPost,2021,"Koch, Jack; Langosco, Lauro",Discussion: Objective Robustness and Inner Alignment Terminology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment,"In the alignment community, there seem to be two main ways to frame and define objective robustness and inner alignment. They are quite similar, mainly differing in the manner in which they focus on the same basic underlying problem. We’ll call these the objective-focused approach and the generalization-focused approach. We don’t delve into these issues of framing the problem in Empirical Observations of Objective Robustness Failures, where we present empirical observations of objective robustness failures. Instead, we think it is worth having a separate discussion of the matter. These issues have been mentioned only infrequently in a few comments on the Alignment Forum, so it seemed worthwhile to write a post describing the framings and their differences in an effort to promote further discussion in the community. TL;DR This post compares two different paradigmatic approaches to objective robustness/inner alignment: Objective-focused approach  * Emphasis: “How do we ensure our models/agents have the right    (mesa-)objectives?”  * Outer alignment: “an objective function r is outer aligned if all models that    perform optimally on r in the limit of perfect training and infinite data are    intent aligned.” * Outer alignment is a property of the training objective.         Generalization-focused approach  * Emphasis: “How will this model/agent generalize out-of-distribution?” *        Considering a model’s “objectives” or “goals,” whether behavioral or       internal, is instrumentally useful for predicting OOD behavior, but what       you ultimately care about is whether it generalizes “acceptably.”          * Outer alignment: a model is outer aligned if it performs desirably on the    training distribution. * Outer alignment is a property of the tuple (training       objective, training data, training setup, model).         Special thanks to Rohin Shah, Evan Hubinger, Edouard Harris, Adam Shimi, and Adam Gleave for their helpful feedback on drafts of this po",2021-06-23,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-10-30 18:03:29,,,,,,,Discussion,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X74XW5JR/discussion-objective-robustness-and-inner-alignment.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWMVFW46,blogPost,2020,"Chiswick, Max; Makiievskyi, Anton; Zhou, Liang",Assessing Generalization in Reward Learning: Intro and Background,Towards Data Science (Medium),,,,https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48,"An overview of reinforcement learning, generalization, and reward learning",2020-11-20,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-11-21 18:11:23,,,,,,,Assessing Generalization in Reward Learning,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WCTVNGGW/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMHWUMQC,blogPost,2020,"Kosch, Sebastian",AISC4: Research Summaries,AI Safety Camp,,,,https://aisafety.camp/2020/05/30/aisc4-research-summaries/,"The fourth AI Safety Camp took place in May 2020 in Toronto. Due to COVID-19, the camp was held virtually. Six teams participated and worked on the following topics: Survey on AI risk scenarios Opt…",2020-05-30,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-11-21 19:14:38,,,,,,,AISC4,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/N39D7ZWI/aisc4-research-summaries.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F4NQ9ZES,blogPost,2019,"Kovarik, Vojta",AI Safety Debate and Its Applications,LessWrong,,,,https://www.lesswrong.com/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications,"All of the experimental work and some of the theoretical work has been done jointly with Anna Gajdova, David Lindner, Lukas Finnveden, and Rajashree Agrawal as part of the third AI Safety Camp. We are grateful to Ryan Carey and Geoffrey Irving for the advice regarding this project. The remainder of the theoretical part relates to my stay at FHI, and I would like to thank the above people, Owain Evans, Michael Dennis, Ethan Perez, Stuart Armstrong, and Max Daniel for comments/discussions. -------------------------------------------------------------------------------- Debate is a recent proposal for AI alignment, which naturally incorporates elicitation of human preferences and has the potential to offload the costly search for flaws in an AI’s suggestions onto the AI. After briefly recalling the intuition behind debate, we list the main open problems surrounding it and summarize how the existing work on debate addresses them. Afterward, we describe, and distinguish between, Debate games and their different applications in more detail. We also formalize what it means for a debate to be truth-promoting. Finally, we present results of our experiments on Debate games and Training via Debate on MNIST and fashion MNIST. DEBATE GAMES AND WHY THEY ARE USEFUL Consider an answer A to some question Q --- for example, ""Where should I go for a vacation?"" and ""Alaska"". Rather than directly verifying whether A is an accurate answer to Q, it might be easier to first decompose A into lower-level components (How far/expensive is it? Do they have nice beaches? What is the average temperature? What language do they speak?). Moreover, it isn't completely clear what to do even if we know the relevant facts --- indeed, how does Alaska's cold weather translate to a preference for Alaska from 0 to 10? And how does this preference compare to English being spoken in Alaska? As an alternative, we can hold a debate between two competing answers A and A′=""Bali"" to Q. This allows strategic de",2019-07-23,2022-01-30 4:49:29,2022-01-30 4:49:29,2019-12-16 3:27:22,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R59PFVJB/ai-safety-debate-and-its-applications.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMQECS38,blogPost,2021,"Smith, Ben; Pihlakas, Roland; Klassert, Robert",A brief review of the reasons multi-objective RL could be important in AI Safety Research,LessWrong,,,,https://www.lesswrong.com/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be,"By Ben Smith, Roland Pihlakas, and Robert Klassert Thanks to Linda Linsefors, Alex Turner, Richard Ngo, Peter Vamplew, JJ Hepburn, Tan Zhi-Xuan, Remmelt Ellen, Kaj Sotala, Koen Holtman, and Søren Elverlin for their time and kind remarks in reviewing this essay. Thanks to the organisers of the AI Safety Camp for incubating this project from its inception and for connecting our team. For the last 9 months, we have been investigating the case for a multi-objective approach to reinforcement learning in AI Safety. Based on our work so far, we’re moderately convinced that multi-objective reinforcement learning should be explored as a useful way to help us understand ways in which we can achieve safe superintelligence. We’re writing this post to explain why, to inform readers of the work we and our colleagues are doing in this area, and invite critical feedback about our approach and about multi-objective RL in general. We were first attracted to the multi-objective space because human values are inherently multi-objective--in any number of frames: deontological, utilitarian, and virtue ethics; egotistical vs. moral objectives; maximizing life values including hedonistic pleasure, eudaemonic meaning, or the enjoyment of power and status. AGI systems aiming to solve for human values are likely to be multi-objective themselves, if not by explicit design, then multi-objective systems would emerge from learning about human preferences. As a first pass at technical research in this area, we took a commonly-used example, the “BreakableBottles” problem, and showed that for low-impact AI, an agent could more quickly solve this toy problem if it uses a conservative but flexible trade-off between alignment and performance values, compared to using a thresholded alignment system to maximize a certain amount of alignment and only then maximizing on performance. Such tradeoffs will be critical for understanding the conflicts between more abstract human objectives a human-preferen",2021-09-29,2022-01-30 4:49:29,2022-01-30 4:49:29,2021-10-30 18:07:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VPK7QUUN/a-brief-review-of-the-reasons-multi-objective-rl-could-be.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BP895WND,blogPost,2016,AI Impacts,What if you turned the world’s hardware into AI minds?,AI Impacts,,,,https://aiimpacts.org/what-if-you-turned-the-worlds-hardware-into-ai-minds/,"In a classic 'AI takes over the world' scenario, one of the first things an emerging superintelligence wants to do is steal most of the world's computing hardware and repurpose it to running the AI's own software. This step takes one from 'super-proficient hacker' levels of smart to 'my brain is one of the main things happening on Planet Earth' levels of smart. There is quite a...",2016-09-04,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-12-13 19:54:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/BMDFBIQX/what-if-you-turned-the-worlds-hardware-into-ai-minds.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZBUX2G49,blogPost,2021,"Kokotajlo, Daniel",What 2026 looks like,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like,"This was written for the Vignettes Workshop.[1] The goal is to write out a  detailed future history (“trajectory”) that is as realistic (to me) as I can currently manage, i.e. I’m not aware of any alternative trajectory that is similarly detailed and clearly more plausible to me. The methodology is roughly: Write a future history of 2022. Condition on it, and write a future history of 2023. Repeat for 2024, 2025, etc. (I'm posting 2022-2026 now so I can get feedback that will help me write 2027+. I intend to keep writing until the story reaches singularity/extinction/utopia/etc.) What’s the point of doing this? Well, there are a couple of reasons:  * Sometimes attempting to write down a concrete example causes you to learn    things, e.g. that a possibility is more or less plausible than you thought.  * Most serious conversation about the future takes place at a high level of    abstraction, talking about e.g. GDP acceleration, timelines until TAI is    affordable, multipolar vs. unipolar takeoff… vignettes are a neglected    complementary approach worth exploring.  * Most stories are written backwards. The author begins with some idea of how    it will end, and arranges the story to achieve that ending. Reality, by    contrast, proceeds from past to future. It isn’t trying to entertain anyone    or prove a point in an argument.  * Anecdotally, various people seem to have found Paul Christiano’s “tales of    doom” stories helpful, and relative to typical discussions those stories are    quite close to what we want. (I still think a bit more detail would be good —    e.g. Paul’s stories don’t give dates, or durations, or any numbers at all    really.)[2]  * “I want someone to ... write a trajectory for how AI goes down, that is    really specific about what the world GDP is in every one of the years from    now until insane intelligence explosion. And just write down what the world    is like in each of those years because I don't know how to write an    internally",2021-08-06,2022-01-30 4:49:21,2022-01-30 4:49:21,2021-11-18 23:05:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3DAS3TNK/what-2026-looks-like-daniel-s-median-future.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WFC7D3BN,blogPost,2019,"Walsh, Toby",Walsh 2017 survey,AI Impacts,,,,https://aiimpacts.org/walsh-2017-survey/,"Toby Walsh surveyed hundreds of experts and non-experts in 2016 and found their median estimates for ‘when a computer might be able to carry out most human professions at least as well as a typical human’ were as follows: Probability of HLMIGroup of survey respondentsAI expertsRobotics expertsNon-experts10%20352033202650%20612065203990%210921182060 Details Toby Walsh, professor of AI at the...",2019-12-24,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-14 3:21:35,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/77WDNJVZ/walsh-2017-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSMW4U3T,blogPost,2020,AI Impacts,Time for AI to cross the human range in English draughts,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-english-draughts/,AI took over 30 years to go from beginner level to superhuman level,2020-10-26,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-21 20:34:50,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/367ECIGF/time-for-ai-to-cross-the-human-range-in-english-draughts.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
794VTKRN,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in Go,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-go/,Computer Go took over 30 years to go from beginner level to superhuman.,2020-10-15,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-21 20:37:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/ETACUV59/time-for-ai-to-cross-the-human-performance-range-in-go.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BR5ED5D2,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in chess,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/,Computer chess took around 50 years to go from beginner level to superhuman level.,2020-10-15,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-21 20:37:56,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/4CIAV85I/time-for-ai-to-cross-the-human-performance-range-in-chess.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EDIFICXQ,blogPost,2020,"Bergal, Asya",Takeaways from safety by default interviews,AI Impacts,,,,https://aiimpacts.org/takeaways-from-safety-by-default-interviews/,"Last year, several researchers at AI Impacts (primarily Robert Long and I) interviewed prominent researchers inside and outside of the AI safety field who are relatively optimistic about advanced AI being developed safely. These interviews were originally intended to focus narrowly on reasons for optimism, but we ended up covering a variety of topics, including AGI timelines, the likelihood of current techniques leading to AGI, and what the right things to do in AI safety are right now. (...)",2020-04-03,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-09-05 18:07:33,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/H79BNVC8/takeaways-from-safety-by-default-interviews.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJQ2C7UT,blogPost,2020,"Kokotajlo, Daniel",Relevant pre-AGI possibilities,AI Impacts,,,,https://aiimpacts.org/relevant-pre-agi-possibilities/,Brainstorm of ways the world could be relevantly different by the time advanced AGI arrives,2020-06-19,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-08-31 18:08:43,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/XG932AG6/relevant-pre-agi-possibilities.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZGB57MGS,blogPost,2018,"Garfinkel, Ben",Reinterpreting “AI and Compute”,AI Impacts,,,,https://aiimpacts.org/reinterpreting-ai-and-compute/,"This is a guest post by Ben Garfinkel. We revised it slightly, at his request, on February 9, 2019. A recent OpenAI blog post, “AI and Compute,” showed that the amount of computing power consumed by the most computationally intensive machine learning projects has been doubling every three months. The post presents this trend as...",2018-12-18,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-14 3:34:13,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/BXV9445T/reinterpreting-ai-and-compute.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I2IG8DRB,blogPost,2020,"Bergal, Asya",Trends in DRAM price per gigabyte,AI Impacts,,,,https://aiimpacts.org/trends-in-dram-price-per-gigabyte/,"The price of a gigabyte of DRAM has fallen by about a factor of ten every 5 years from 1957 to 2020. Since 2010, the price has fallen much more slowly, at a rate that would yield an order of magnitude over roughly 14 years. Details Background DRAM, “dynamic random-access memory”, is a type of...",2020-04-14,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-09-05 17:11:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/PI98VB6J/trends-in-dram-price-per-gigabyte.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BU7J7CAX,blogPost,2020,AI Impacts,Time for AI to cross the human range in StarCraft,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-starcraft/,AI took about twenty years to go from beginner level to high professional level.,2020-10-20,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-21 20:36:03,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/EFWDD55K/time-for-ai-to-cross-the-human-range-in-starcraft.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22TF52ND,blogPost,2018,"Johnson, Aysja",Time for AI to cross the human performance range in diabetic retinopathy,AI Impacts,,,,https://aiimpacts.org/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance/,"In diabetic retinopathy, automated systems started out just below expert human level performance, and took around ten years to reach expert human level performance. Details Diabetic retinopathy is a complication of diabetes in which the back of the eye is damaged by high blood sugar levels. It is the most common cause of blindness among working-age...",2018-11-21,2022-01-30 4:49:21,2022-01-30 4:49:21,2020-11-14 3:21:47,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/M8IRAET5/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QX5FAHWI,blogPost,2019,"McCaslin, Tegan",Primates vs birds: Is one brain architecture better than the other?,AI Impacts,,,,https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/,"The boring answer to that question is, “Yes, birds.” But that’s only because birds can pack more neurons into a walnut-sized brain than a monkey with a brain four times that size. So let’s forget about brain volume for a second and ask the really interesting question: neuron per neuron, who’s coming out ahead? You...",2019-02-28,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:21:40,,,,,,,Primates vs birds,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/FZQSAPHU/primates-vs-birds-is-one-brain-architecture-better-than-the-other.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MTAD8Z45,blogPost,2020,"Grace, Katja",Misalignment and misuse: whose values are manifest?,AI Impacts,,,,https://aiimpacts.org/misalignment-and-misuse-whose-values-are-manifest/,Are misalignment and misuse helpful catastrophe categories?,2020-11-18,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-21 20:30:10,,,,,,,Misalignment and misuse,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/ZHKEVBHX/misalignment-and-misuse-whose-values-are-manifest.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RXFNQ8B3,blogPost,2018,"Carey, Ryan",Interpreting AI compute trends,AI Impacts,,,,https://aiimpacts.org/interpreting-ai-compute-trends/,"This is a guest post by Ryan Carey. Over the last few years, we know that AI experiments have used much more computation than previously. But just last month, an investigation by OpenAI made some initial estimates of just how fast this growth has been. Comparing AlphaGo Zero to AlexNet, they found that the largest...",2018-07-10,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:21:51,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000007  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/ZSPZMGTB/interpreting-ai-compute-trends.html; /Users/jacquesthibodeau/Zotero/storage/9RR92JIT/interpreting-ai-compute-trends.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EAXRN4ZU,blogPost,2019,"Etzioni, Oren",Etzioni 2016 survey,AI Impacts,,,,https://aiimpacts.org/etzioni-2016-survey/,"Oren Etzioni surveyed 193 AAAI fellows in 2016 and found that 67% of them expected that 'we will achieve Superintelligence' someday, but in more than 25 years. Details Oren Etzioni, CEO of the Allen Institute for AI, reported on a survey in an MIT Tech Review article published on 20 Sep 2016. The rest of...",2019-11-06,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:22:04,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/K8BQJN6K/etzioni-2016-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTHTXZPA,blogPost,2016,AI Impacts,Error in Armstrong and Sotala 2012,AI Impacts,,,,https://aiimpacts.org/error-in-armstrong-and-sotala-2012/,"Can AI researchers say anything useful about when strong AI will arrive? Back in 2012, Stuart Armstrong and Kaj Sotala weighed in on this question in a paper called 'How We're Predicting AI—or Failing To'. They looked at a dataset of predictions about AI timelines, and concluded that predictions made by AI experts were indistinguishable from those of non-experts. (Which might suggest...",2016-05-17,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-12-13 19:54:50,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/V9RNPU92/error-in-armstrong-and-sotala-2012.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U8ZTEBVU,blogPost,2020,"Kokotajlo, Daniel","Cortés, Pizarro, and Afonso as precedents for takeover",AI Impacts,,,,https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/,"Epistemic status: I am not a historian, nor have I investigated these case studies in detail. I admit I am still uncertain about how the conquistadors were able to colonize so much of the world so quickly. I think my ignorance is excusable because this is just a blog post; I welcome corrections from people...",2020-02-29,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-09-05 19:00:45,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/46NH99PN/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8MBI682B,blogPost,2019,"Shah, Rohin; Bergal, Asya; Long, Robert; Haxhia, Sara",Conversation with Rohin Shah,AI Impacts,,,,https://aiimpacts.org/conversation-with-rohin-shah/,"AI Impacts talked to AI safety researcher Rohin Shah about his views on AI risk. With his permission, we have transcribed this interview. Participants Rohin Shah -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI ImpactsSara Haxhia -- Independent researcher Summary We spoke with Rohin Shah...",2019-10-31,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:34:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/UHCQGXPU/conversation-with-rohin-shah.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UX6HJZUV,blogPost,2019,"Christiano, Paul; Bergal, Asya",Conversation with Paul Christiano,AI Impacts,,,,https://aiimpacts.org/conversation-with-paul-christiano/,"AI Impacts talked to AI safety researcher Paul Christiano about his views on AI risk. With his permission, we have transcribed this interview. Participants Paul Christiano -- OpenAI safety teamAsya Bergal - AI ImpactsRonny Fernandez - AI ImpactsRobert Long – AI Impacts Summary We spoke with Paul Christiano on August 13, 2019. Here is a...",2019-09-11,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:34:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/FCCIUWG4/conversation-with-paul-christiano.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQT2S8TU,blogPost,2020,"Kokotajlo, Daniel",Precedents for economic n-year doubling before 4n-year doubling,AI Impacts,,,,https://aiimpacts.org/precedents-for-economic-n-year-doubling-before-4n-year-doubling/,"Does the economy ever double without having first doubled four times slower? Yes, but not since 3000BC.",2020-04-14,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-09-05 17:09:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/MJQA2RMI/precedents-for-economic-n-year-doubling-before-4n-year-doubling.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGQZA34B,blogPost,2018,"O’Keefe, Cullen",On the (in)applicability of corporate rights cases to digital minds,AI Impacts,,,,https://aiimpacts.org/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds/,This is a guest cross-post by Cullen O'Keefe. High-Level Takeaway The extension of rights to corporations likely does not provide useful analogy to potential extension of rights to digital minds. Introduction Examining how law can protect the welfare of possible future digital minds is part of my research agenda. I expect that study of historical...,2018-09-28,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:21:08,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/FTES6UQT/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds.html,,MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E6AH852C,blogPost,2018,"Grace, Katja",Likelihood of discontinuous progress around the development of AGI,AI Impacts,,,,https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/,"We aren’t convinced by any of the arguments we’ve seen to expect large discontinuity in AI progress above the extremely low base rate for all technologies. However this topic is controversial, and many thinkers on the topic disagree with us, so we consider this an open question. Details Definitions We say a technological discontinuity has...",2018-02-23,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-12-13 23:06:58,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002[s0]  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/26QUMN55/likelihood-of-discontinuous-progress-around-the-development-of-agi.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9JQA2EK,blogPost,2017,"Grace, Katja",Human-level hardware timeline,AI Impacts,,,,https://aiimpacts.org/human-level-hardware-timeline/,"We estimate that 'human-level hardware'— hardware able to perform as many computations per second as a human brain, at a similar cost to a human brain—has a 30% chance of having already occurred, a 45% third chance of occurring by 2040, and a 25% chance of occurring later. We are not confident about these estimates....",2017-12-22,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-12-13 23:05:56,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/5QQHA88X/human-level-hardware-timeline.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7572XHJN,blogPost,2020,"Grace, Katja",Discontinuous progress in history: an update,AI Impacts,,,,https://aiimpacts.org/discontinuous-progress-in-history-an-update/,"We’ve been looking for historic cases of discontinuously fast technological progress, to help with reasoning about the likelihood and consequences of abrupt progress in AI capabilities. We recently finished expanding this investigation to 37 technological trends. This blog post is a quick update on our findings. See the main page on the research and its outgoing links for more details.",2020-04-13,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-09-05 16:57:42,,,,,,,Discontinuous progress in history,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PUBJABIW/discontinuous-progress-in-history-an-update.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T5T66WNU,blogPost,2020,"Korzekwa, Rick",Description vs simulated prediction,AI Impacts,,,,https://aiimpacts.org/description-vs-simulated-prediction/,What are we trying to do when we look at history to inform forecasting?,2020-04-22,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-09-05 16:59:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/XG7CX3FD/description-vs-simulated-prediction.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EN3HHGSU,blogPost,2019,"Hanson, Robin; Bergal, Asya; Long, Robert",Conversation with Robin Hanson,AI Impacts,,,,https://aiimpacts.org/conversation-with-robin-hanson/,"AI Impacts talked to economist Robin Hanson about his views on AI risk and timelines. With his permission, we have posted and transcribed this interview. Participants Robin Hanson -- Associate Professor of Economics, George Mason UniversityAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Robin Hanson on September 5, 2019. Here...",2019-11-13,2022-01-30 4:49:20,2022-01-30 4:49:20,2020-11-14 3:34:19,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/RIDWXW85/conversation-with-robin-hanson.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NNU453Z3,blogPost,2019,"Gleave, Adam; Bergal, Asya; Long, Robert",Conversation with Adam Gleave,AI Impacts,,,,https://aiimpacts.org/conversation-with-adam-gleave/,"AI Impacts talked to AI safety researcher Adam Gleave about his views on AI risk. With his permission, we have transcribed this interview. Participants Adam Gleave -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Adam Gleave on August 27, 2019....",2019-12-23,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-11-14 3:34:21,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/699C2QIP/conversation-with-adam-gleave.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WT3U7VF,blogPost,2019,AI Impacts,AI conference attendance,AI Impacts,,,,https://aiimpacts.org/ai-conference-attendance/,"Six of the largest seven AI conferences hosted a total of 27,396 attendees in 2018. Attendance at these conferences has grown by an average of 21% per year over 2011-2018. These six conferences host around six times as many attendees as six smaller AI conferences. Details Artificial Intelligence Index reports on this, from data they collected...",2019-03-06,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-12-13 23:57:21,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs,,/Users/jacquesthibodeau/Zotero/storage/PPUFMS4K/ai-conference-attendance.html,,MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BKDUHIDV,blogPost,2019,"Davis, Ernie; Long, Robert",Conversation with Ernie Davis,AI Impacts,,,,https://aiimpacts.org/conversation-with-ernie-davis/,"AI Impacts spoke with computer scientist Ernie Davis about his views of AI risk. With his permission, we have transcribed this interview. Participants Ernest Davis – professor of computer science at the Courant Institute of Mathematical Science, New York University Robert Long – AI Impacts Summary We spoke over the phone with Ernie Davis on...",2019-08-23,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-11-14 3:21:12,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/XIGWJRHU/conversation-with-ernie-davis.html; /Users/jacquesthibodeau/Zotero/storage/5757QWMU/conversation-with-ernie-davis.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MH3BZG9F,blogPost,2021,"Grace, Katja",Beyond fire alarms: freeing the groupstruck,AI Impacts,,,,https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/,Fire alarms are the wrong way to think about the public AGI conversation.,2021-09-26,2022-01-30 4:49:19,2022-01-30 4:49:19,2021-11-18 23:46:19,,,,,,,Beyond fire alarms,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PD5APM7U/beyond-fire-alarms-freeing-the-groupstruck.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRMA7X54,blogPost,2020,"Grace, Katja",Automated intelligence is not AI,AI Impacts,,,,https://aiimpacts.org/automated-intelligence-is-not-ai/,Katja Grace Sometimes we think of ‘artificial intelligence’ as whatever technology ultimately automates human cognitive labor...,2020-11-01,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-11-21 20:33:46,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/NMHPZQPX/automated-intelligence-is-not-ai.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F5B4F2NG,blogPost,2020,"Grace, Katja",Atari early,AI Impacts,,,,https://aiimpacts.org/atari-early/,Deepmind announced that their Agent57 beats the ‘human baseline’ at all 57 Atari games usually used as a benchmark. I think this is probably enough to resolve one of the predictions we had respondents make in our 2016 survey. Our question was when it would be feasible to ‘outperform professional game testers...,2020-04-01,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-09-05 17:39:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/AWPUP66U/atari-early.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4ISXUCTN,blogPost,2018,"Mills, Justis",AGI-11 survey,AI Impacts,,,,https://aiimpacts.org/agi-11-survey/,The AGI-11 survey was a survey of 60 participants at the AGI-11 conference. In it: Nearly half of respondents believed that AGI would appear before 2030. Nearly 90% of respondents believed that AGI would appear before 2100. About 85% of respondents believed that AGI would be beneficial for humankind. Details James Barrat and Ben Goertzel...,2018-11-10,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-11-14 3:22:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/UIJU2KS5/agi-11-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SDJRTUC7,blogPost,2020,"Bergal, Asya",2019 recent trends in Geekbench score per CPU price,AI Impacts,,,,https://aiimpacts.org/2019-recent-trends-in-geekbench-score-per-cpu-price/,"From 2006 - 2020, Geekbench score per CPU price has grown by around 16% a year, for rates that would yield an order of magnitude over roughly 16 years. Details We looked at Geekbench 5, a benchmark for CPU performance. We combined Geekbench’s multi-core scores on its 'Processor Benchmarks' page with release dates and prices...",2020-04-14,2022-01-30 4:49:19,2022-01-30 4:49:19,2020-09-05 17:12:28,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/88Z7BP58/2019-recent-trends-in-geekbench-score-per-cpu-price.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J6GC98TM,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in ImageNet image classification,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification/,Computer image classification performance took 3 years to go from untrained human level to trained human level,2020-10-19,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-11-21 20:36:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/PHMU6GH8/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html; /Users/jacquesthibodeau/Zotero/storage/T8WDT98T/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD3UFI3X,blogPost,2015,AI Impacts,"Research topic: Hardware, software and AI",AI Impacts,,,,https://aiimpacts.org/research-topic-hardware-software-and-ai/,"This is the first in a sequence of articles outlining research which could help forecast AI development. Interpretation Concrete research projects are in boxes. ∑5 ∆8  means we guess the project will take (very) roughly five hours, and we rate its value (very) roughly 8/10. Most projects could be done to very different degrees of depth, or at very different scales. Our time cost...",2015-02-19,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-18 19:06:28,,,,,,,Research topic,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/6VMRDPXR/research-topic-hardware-software-and-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDGQ8324,blogPost,2015,AI Impacts,Trends in the cost of computing,AI Impacts,,,,https://aiimpacts.org/trends-in-the-cost-of-computing/,"Computing power available per dollar has probably increased by a factor of ten roughly every four years over the last quarter of a century (measured in FLOPS or MIPS). Over the past 6-8 years, the rate has been slower: around an order of magnitude every 10-16 years, measured in single precision theoretical peak FLOPS or Passmark's benchmark scores. Since...",2015-03-10,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-18 19:05:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000009  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/FTJZTW93/trends-in-the-cost-of-computing.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5MPTINM8,blogPost,2018,"McCaslin, Tegan",Transmitting fibers in the brain: Total length and distribution of lengths,AI Impacts,,,,https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/,"The human brain’s approximately 86 billion neurons are probably connected by something like 850,000 km of axons and dendrites. Of this total, roughly 80% is short-range, local connections (averaging 680 microns in length), and approximately 20% is long-range, global connections in the form of myelinated fibers (likely averaging several centimeters in length). Background The brain’s...",2018-03-29,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-13 23:22:39,,,,,,,Transmitting fibers in the brain,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/DWXT75KD/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html; /Users/jacquesthibodeau/Zotero/storage/K8XGXG6Q/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6RGUKGG5,blogPost,2015,AI Impacts,The cost of TEPS,AI Impacts,,,,https://aiimpacts.org/cost-of-teps/,"A billion Traversed Edges Per Second (a GTEPS) can be bought for around $0.26/hour via a powerful supercomputer, including hardware and energy costs only. We do not know if GTEPS can be bought more cheaply elsewhere. We estimate that available TEPS/$ grows by a factor of ten every four years, based the relationship between TEPS and FLOPS. TEPS have not been...",2015-03-21,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-18 19:04:55,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KFB3JVAP,blogPost,2020,"Bergal, Asya",Surveys on fractional progress towards HLAI,AI Impacts,,,,https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/,"How long until human-level performance, if we naively extrapolate progress since researchers joined their subfields?",2020-04-14,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-09-05 17:08:05,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/AZVQBRB7/surveys-on-fractional-progress-towards-hlai.html; /Users/jacquesthibodeau/Zotero/storage/H8GGTM77/surveys-on-fractional-progress-towards-hlai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2DQQ6BM,blogPost,2016,AI Impacts,Returns to scale in research,AI Impacts,,,,https://aiimpacts.org/returns-to-scale-in-research/,"When universities or university departments produce research outputs—such as published papers—they sometimes experience increasing returns to scale, sometimes constant returns to scale, and sometimes decreasing returns to scale. At the level of nations however, R&D tends to see increasing returns to scale. These results are preliminary. Background “Returns to scale” refers to the responsiveness of...",2016-07-06,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-18 18:56:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/TR933CQ5/returns-to-scale-in-research.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZH6RHC67,blogPost,2020,"Bergal, Asya",Resolutions of mathematical conjectures over time,AI Impacts,,,,https://aiimpacts.org/resolutions-of-mathematical-conjectures-over-time/,"Conditioned on being remembered as a notable conjecture, the time-to-proof for a mathematical problem appears to be exponentially distributed with a half-life of about 100 years. However, these observations are likely to be distorted by various biases. Support In 2014, we found conjectures referenced on Wikipedia, and recorded the dates that they were proposed and...",2020-04-14,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-09-05 17:10:04,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/6QIT4QFT/resolutions-of-mathematical-conjectures-over-time.html; /Users/jacquesthibodeau/Zotero/storage/HJKW6MVS/resolutions-of-mathematical-conjectures-over-time.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U6J7QJB4,blogPost,2017,AI Impacts,Progress in general purpose factoring,AI Impacts,,,,https://aiimpacts.org/progress-in-general-purpose-factoring/,"The largest number factored to date grew by about 4.5 decimal digits per year over the past roughly half-century. Between 1988, when we first have good records, and 2009, when the largest number to date was factored, progress was roughly 6 decimal digits per year. Progress was relatively smooth during the two decades for which we have good records, with half of...",2017-03-16,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-12-18 18:51:25,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs,,/Users/jacquesthibodeau/Zotero/storage/29ZRZ757/progress-in-general-purpose-factoring.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZV6ZEP7K,blogPost,2020,"Korzekwa, Rick",Preliminary survey of prescient actions,AI Impacts,,,,https://aiimpacts.org/survey-of-prescient-actions/,"In a 10-20 hour exploration, we did not find clear examples of 'prescient actions'—specific efforts to address severe and complex problems decades ahead of time and in the absence of broader scientific concern, experience with analogous problems, or feedback on the success of the effort—though we found six cases that may turn out to be...",2020-04-03,2022-01-30 4:49:04,2022-01-30 4:49:04,2020-09-05 17:13:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2QCZB5HZ/survey-of-prescient-actions.html; /Users/jacquesthibodeau/Zotero/storage/3FEHV8RT/survey-of-prescient-actions.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKDHRC3V,blogPost,2015,AI Impacts,List of Analyses of Time to Human-Level AI,AI Impacts,,,,https://aiimpacts.org/list-of-analyses-of-time-to-human-level-ai/,"This is a list of most of the substantial analyses of AI timelines that we know of. It also covers most of the arguments and opinions of which we are aware. Details The list below contains substantial publically available analyses of when human-level AI will appear. To qualify for the list, an item must provide both a claim...",2015-01-22,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:07:58,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/JMI38J5D/list-of-analyses-of-time-to-human-level-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WEQ27ARZ,blogPost,2019,"McCaslin, Tegan",Investigation into the relationship between neuron count and intelligence across differing cortical architectures,AI Impacts,,,,https://aiimpacts.org/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cortical-architectures/,,2019,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FUXE4PED/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cort.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JK5CBDEJ,blogPost,2019,AI Impacts,Historical economic growth trends,AI Impacts,,,,https://aiimpacts.org/historical-growth-trends/,"An analysis of historical growth supports the possibility of radical increases in growth rate. Naive extrapolation of long-term trends would suggest massive increases in growth rate over the coming century, although growth over the last half-century has lagged very significantly behind these long-term trends. Support Bradford DeLong has published estimates for historical world GDP, piecing together...",2019-03-06,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-13 23:57:45,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/295DBKF8/historical-growth-trends.html; /Users/jacquesthibodeau/Zotero/storage/6QQFGPWX/historical-growth-trends.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3DQBHIS,blogPost,2014,AI Impacts,Effect of nuclear weapons on historic trends in explosives,AI Impacts,,,,https://aiimpacts.org/discontinuity-from-nuclear-weapons/,"Nuclear weapons constituted a ~7 thousand year discontinuity in relative effectiveness factor (TNT equivalent per kg of explosive). Nuclear weapons do not appear to have clearly represented progress in the cost-effectiveness of explosives, though the evidence there is weak. Details This case study is part of AI Impacts’ discontinuous progress investigation. Background The development of nuclear...",2014-12-31,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:09:55,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Continuity of progress,,/Users/jacquesthibodeau/Zotero/storage/BXQBM3XU/discontinuity-from-nuclear-weapons.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ID37HU49,blogPost,2016,"Wulfsohn, Michael",Costs of extinction risk mitigation,AI Impacts,,,,https://aiimpacts.org/costs-of-extinction-risk-mitigation/,We very roughly estimate that the annual cost of reducing the probability of human extinction by 0.01% is within the range of $1.1 billion to $3.5 trillion. Introduction This article is intended to be usable in a Cost-Benefit Analysis (CBA) analysis of extinction risk mitigation. It explores the costs of such efforts. A corresponding article...,2016-08-04,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:55:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evaluation,,/Users/jacquesthibodeau/Zotero/storage/B7NXCJJP/costs-of-extinction-risk-mitigation.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R88INF7A,blogPost,2014,AI Impacts,Cases of Discontinuous Technological Progress,AI Impacts,,,,https://aiimpacts.org/cases-of-discontinuous-technological-progress/,We know of ten events which produced a robust discontinuity in progress equivalent to more than one hundred years at previous rates in some interesting metric. We know of 53 other events which produced smaller or less robust discontinuities. Background These cases were researched as part of our discontinuous progress investigation. List of cases Events...,2014-12-31,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:09:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2I97PIUR/cases-of-discontinuous-technological-progress.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6BMGVAKN,blogPost,2015,AI Impacts,AI Timeline Surveys,AI Impacts,,,,https://aiimpacts.org/ai-timeline-surveys/,"[This page is out of date and will be updated soon. It does not reflect all surveys known and documented by AI Impacts.] We know of thirteen surveys on the predicted timing of human-level AI. If we collapse a few slightly different meanings of 'human-level AI', then: Median estimates for when there will be a 10% chance of human-level AI are...",2015-01-10,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:08:31,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/RNANISVC/ai-timeline-surveys.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R53RXIT7,blogPost,2015,AI Impacts,MIRI AI Predictions Dataset,AI Impacts,,,,https://aiimpacts.org/miri-ai-predictions-dataset/,"The MIRI AI predictions dataset is a collection of public predictions about human-level AI timelines. We edited the original dataset, as described below. Our dataset is available here, and the original here. Interesting features of the dataset include: The median dates at which people's predictions suggest AI is less likely than not and more likely than not are...",2015-05-20,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:03:59,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/HXM7IC2U/miri-ai-predictions-dataset.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DXPWI4DP,blogPost,2015,AI Impacts,List of multipolar research projects,AI Impacts,,,,https://aiimpacts.org/multipolar-research-projects/,"This list currently consists of research projects suggested at the Multipolar AI workshop we held on January 26 2015. Relatively concrete projects are marked [concrete]. These are more likely to already include specific questions to answer and feasible methods to answer them with. Other 'projects' are more like open questions, or broad directions for inquiry. Projects are divided into three sections: Paths to multipolar scenarios What...",2015-02-11,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:06:54,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2FV59JK6/multipolar-research-projects.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
557XUX37,blogPost,2014,AI Impacts,Human-Level AI,AI Impacts,,,,https://aiimpacts.org/human-level-ai/,"Human-level AI' refers to AI which can reproduce everything a human can do, approximately. Several variants of this concept are worth distinguishing. Details Variations in the meaning of 'human-level AI' Considerations in specifying 'human-level AI' more precisely: Do we mean to imply anything about running costs? Is an AI that reproduces human behavior for ten billion dollars per year 'human-level',...",2014-01-23,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:10:54,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Clarifying concepts,,/Users/jacquesthibodeau/Zotero/storage/V3S9A9CP/human-level-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NUG5E483,blogPost,2020,"Fernandez, Ronny",How energy efficient are human-engineered flight designs relative to natural ones?,AI Impacts,,,,https://aiimpacts.org/are-human-engineered-flight-designs-better-or-worse-than-natural-ones/,"Nature is responsible for the most energy efficient flight, according to an investigation of albatrosses, butterflies and nine different human-engineered flying machines.",2020-12-10,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:30:29,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evolution engineering comparison,,,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XS42FABG,blogPost,2014,AI Impacts,Hanson AI Expert Survey,AI Impacts,,,,https://aiimpacts.org/hanson-ai-expert-survey/,"In a small informal survey running since 2012, AI researchers generally estimated that their subfields have moved less than ten percent of the way to human-level intelligence. Only one (in the slowest moving subfield) observed acceleration. This suggests on a simple extrapolation that reaching human-level capability across subfields will take over a century (in contrast with many other...",2014-12-29,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:10:27,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/JSVEBI48/hanson-ai-expert-survey.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ZPZ6KTC,blogPost,2021,AI Impacts,Fiction relevant to AI futurism,AI Impacts,,,,https://aiimpacts.org/partially-plausible-fictional-ai-futures/,"A list of stories potentially relevant to thinking about the development of advanced AI, including both those intended as futurism and those intended as entertainment.",2021-04-12,2022-01-30 4:49:03,2022-01-30 4:49:03,2021-10-30 16:38:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/IKURBDT5/partially-plausible-fictional-ai-futures.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z83BXIK7,blogPost,2016,AI Impacts,Examples of early action on risks,AI Impacts,,,,https://aiimpacts.org/examples-of-early-action-on-a-risk/,"Details Discussion There are many current efforts to mitigate risks from artificial intelligence. We might learn something about the likelihood of these efforts influencing AI risk by looking at similar past efforts. To this end, we are interested here in past risk mitigation efforts that have the following characteristics (taken from this paper contributing to the same project (p5) and...",2016-08-16,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:55:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control,,/Users/jacquesthibodeau/Zotero/storage/C6A4Q39T/examples-of-early-action-on-a-risk.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QKDDT8NV,blogPost,2019,"Kokotajlo, Daniel",Evidence on good forecasting practices from the Good Judgment Project,AI Impacts,,,,https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/,"According to experience and data from the Good Judgment Project, the following are associated with successful forecasting, in rough decreasing order of combined importance and confidence: Past performance in the same broad domain Making more predictions on the same question Deliberation time Collaboration on teams Intelligence Domain expertise Having taken a one-hour training module on...",2019-02-07,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-11-14 3:21:44,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/AKFKIG5V/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html; /Users/jacquesthibodeau/Zotero/storage/44WBB3QC/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
798WSB6H,blogPost,2019,"Long, Robert; Bergal, Asya",Evidence against current methods leading to human level artificial intelligence,AI Impacts,,,,https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/,"This is a list of published arguments that we know of that current methods in artificial intelligence will not lead to human-level AI. Details Clarifications We take 'current methods' to mean techniques for engineering artificial intelligence that are already known, involving no “qualitatively new ideas”. We have not precisely defined 'current methods'. Many of the...",2019-08-12,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-14 23:33:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/WI59RSHC/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html; /Users/jacquesthibodeau/Zotero/storage/I2J57E3V/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RGZGX6U,blogPost,2015,AI Impacts,Discontinuous progress investigation,AI Impacts,,,,https://aiimpacts.org/discontinuous-progress-investigation/,"Published Feb 2, 2015; last updated April 12 2020 We have collected cases of discontinuous technological progress to inform our understanding of whether artificial intelligence performance is likely to undergo such a discontinuity. This page details our investigation. We know of ten events that produced a robust discontinuity in progress equivalent to more than a century...",2015-02-02,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:07:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/BGNFTNE2/discontinuous-progress-investigation.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9Q4KHV7Q,blogPost,2015,AI Impacts,Costs of human-level hardware,AI Impacts,,,,https://aiimpacts.org/costs-of-human-level-hardware/,"Computing hardware which is equivalent to the brain - in terms of FLOPS probably costs between $1 x 105 and $3 x 1016, or $2/hour-$700bn/hour. in terms of TEPS probably costs $200M - $7B, or or $4,700 – $170,000/hour (including energy costs in the hourly rate). in terms of secondary memory probably costs $300-3,000, or $0.007-$0.07/hour. Details Partial costs...",2015-07-26,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:59:32,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/64B2PHMG/costs-of-human-level-hardware.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAUDQANV,blogPost,2016,AI Impacts,Coordinated human action as example of superhuman intelligence,AI Impacts,,,,https://aiimpacts.org/coordinated-human-action-example-superhuman-intelligence/,Collections of humans organized into groups and institutions provide many historical examples of the creation and attempted control of intelligences that routinely outperform individual humans. A preliminary look at the available evidence suggests that individuals are often cognitively outperformed in head-to-head competition with groups of similar average intelligence. This article surveys considerations relevant to the...,2016-01-21,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:57:29,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control,,/Users/jacquesthibodeau/Zotero/storage/9Q4E9ZW2/coordinated-human-action-example-superhuman-intelligence.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N387UVHE,blogPost,2016,"Griffiths, Tom; Adamson, Finan",Conversation with Tom Griffiths,AI Impacts,,,,https://aiimpacts.org/conversation-with-tom-griffiths/,"Participants Professor Tom Griffiths, ­ Director of the Computational Cognitive Science Lab and the Institute of Cognitive and Brain Sciences at the University of California, Berkeley. Finan Adamson, ­ AI Impacts. Note: These notes were compiled by AI impacts and give an overview of the major points made by Professor Tom Griffiths. They are available...",2016-09-08,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 18:54:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/9BKW84NI/conversation-with-tom-griffiths.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXNKZP2R,blogPost,2015,"Potter, Steve; Grace, Katja",Conversation with Steve Potter,AI Impacts,,,,https://aiimpacts.org/conversation-with-steve-potter/,"Posted 13 July 2015 Participants Professor Steve Potter – Associate Professor, Laboratory of NeuroEngineering, Coulter Department of Biomedical Engineering, Georgia Institute of Technology Katja Grace – Machine Intelligence Research Institute (MIRI) Note: These notes were compiled by MIRI and give an overview of the major points made by Professor Steve Potter. Summary Katja Grace spoke...",2015-07-13,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:00:09,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/VW3X2BWF/conversation-with-steve-potter.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UW2DZD9A,blogPost,2015,AI Impacts,Brain performance in TEPS,AI Impacts,,,,https://aiimpacts.org/brain-performance-in-teps/,"Traversed Edges Per Second (TEPS) is a benchmark for measuring a computer's ability to communicate information internally. Given several assumptions, we can also estimate the human brain's communication performance in terms of TEPS, and use this to meaningfully compare brains to computers. We estimate that (given these assumptions) the human brain performs around  0.18 - 6.4 *...",2015-05-06,2022-01-30 4:49:03,2022-01-30 4:49:03,2020-12-18 19:04:30,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/SDDXHFH4/brain-performance-in-teps.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RZZWAGZ9,blogPost,2021,AI Impacts,AI Vignettes Project,AI Impacts,,,,https://aiimpacts.org/ai-vignettes-project/,"The AI Vignettes Project is an ongoing effort to write concrete plausible future histories of AI development and its social impacts. Details Purposes We hope to: Check that abstract views about the future of AI have plausible concrete instantiations. (Especially, hypothesized extinction scenarios, and proposed safe scenarios.)Develop better intuitions about possible scenarios by thinking through...",2021-10-12,2022-01-30 4:49:03,2022-01-30 4:49:03,2021-10-30 16:33:40,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/WSPITJEN/ai-vignettes-project.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KII8U9NV,blogPost,2015,AI Impacts,AI Impacts research bounties,AI Impacts,,,,https://aiimpacts.org/ai-impacts-research-bounties/,"We are offering rewards for several inputs to our research, described below. These offers have no specific deadline except where noted. We may modify them or take them down, but will give at least one week's notice here unless there is strong reason not to. To submit an entry, email katja@intelligence.org. There is currently a large backlog of entries to...",2015-08-06,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-12-18 18:58:51,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/4UIJK9K6/ai-impacts-research-bounties.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VZKUDNZD,blogPost,2020,"Bergal, Asya",2019 recent trends in GPU price per FLOPS,AI Impacts,,,,https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/,"We estimate that in recent years, GPU prices have fallen at rates that would yield an order of magnitude over roughly: 17 years for single-precision FLOPS10 years for half-precision FLOPS5 years for half-precision fused multiply-add FLOPS Details GPUs (graphics processing units) are specialized electronic circuits originally used for computer graphics. In recent years, they have...",2020-03-25,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-09-05 18:37:02,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/WBT5HR2S/2019-recent-trends-in-gpu-price-per-flops.html; /Users/jacquesthibodeau/Zotero/storage/NC8ARCAU/2019-recent-trends-in-gpu-price-per-flops.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BF7984KG,blogPost,2016,"Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain",2016 Expert Survey on Progress in AI,AI Impacts,,,,https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/,"Published June 2016; last substantial update before Oct 2017 The 2016 Expert Survey on Progress in AI is a survey of machine learning researchers that Katja Grace and John Salvatier of AI Impacts ran in collaboration with Allan Dafoe, Baobao Zhang, and Owain Evans in 2016. Details Some survey results are reported in When Will...",2016-12-14,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-12-18 18:52:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/624ZXBKR/2016-expert-survey-on-progress-in-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PUDJM3MC,blogPost,2015,AI Impacts,AI Risk Terminology,AI Impacts,,,,https://aiimpacts.org/ai-risk-terminology/,"AI timeline - an expectation about how much time will lapse before important AI events, especially the advent of human-level AI or a similar milestone. The term can also refer to the actual periods of time (which are not yet known), rather than an expectation about them. Artificial General Intelligence (also, AGI) - the intelligence of a machine that could successfully...",2015-10-30,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-12-18 18:58:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/VNQR5F2J/ai-risk-terminology.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RAXPGCP3,blogPost,2015,AI Impacts,Accuracy of AI Predictions,AI Impacts,,,,https://aiimpacts.org/accuracy-of-ai-predictions/,"Updated 4 June 2015 It is unclear how informative we should expect expert predictions about AI timelines to be. Individual predictions are undoubtedly often off by many decades, since they disagree with each other. However their aggregate may still be quite informative. The main potential reason we know of to doubt the accuracy of expert predictions is that experts are generally poor predictors in many areas, and...",2015-06-04,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-12-18 19:02:01,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Accuracy of AI Predictions,,/Users/jacquesthibodeau/Zotero/storage/EVEQ3P8B/accuracy-of-ai-predictions.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G95WAZZB,blogPost,2017,AI Impacts,2017 trend in the cost of computing,AI Impacts,,,,https://aiimpacts.org/recent-trend-in-the-cost-of-computing/,"The cheapest hardware prices (for single precision FLOPS/$) appear to be falling by around an order of magnitude every 10-16 years. This rate is slower than the trend of FLOPS/$ observed over the past quarter century, which was an order of magnitude every 4 years. There is no particular sign of slowing between 2011 and 2017....",2017-11-11,2022-01-30 4:49:02,2022-01-30 4:49:02,2020-12-18 18:50:38,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/3WDJKPFI/recent-trend-in-the-cost-of-computing.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
768PN8FI,blogPost,2020,Partnership on AI,What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate,AI&.,,,,https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e,Lessons on publication norms for the AI community from biosecurity,2020-12-09,2022-01-30 4:48:55,2022-01-30 4:48:55,2021-11-18 23:28:05,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QWKPVW7U/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2SFMIH4S,blogPost,2020,"Tian, Yonglong",Understanding View Selection for Contrastive Learning,Google AI Blog,,,,http://ai.googleblog.com/2020/08/understanding-view-selection-for.html,"Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research    Most people take for granted the abil...",2020-08-21,2022-01-30 4:48:54,2022-01-30 4:48:54,2021-11-07 16:20:33,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RFIP9DCC/understanding-view-selection-for.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9MGJKQ2I,blogPost,2020,"Shah, Rohin",AI Alignment 2018-19 Review,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review,"PREAMBLE WHAT THIS POST IS This is a review post of public work in AI alignment over 2019, with some inclusions from 2018. It has this preamble (~700 words), a short version / summary (~1.6k words), and a long version (~8.3k words). It is available as a Google Doc here. There are many areas of work that are relevant to AI alignment that I have barely touched on, such as interpretability, uncertainty estimation, adversarial examples, and assured autonomy, primarily because I have not been following these fields and wouldn’t be able to write a good summary of what has happened in them. I have also mostly focused on articles that provide some conceptual insight, and excluded or briefly linked to papers that primarily make quantitative improvements on important metrics. While such papers are obviously important (ultimately, our techniques need to work well), there isn’t much to say about them in a yearly review other than that the quantitative metric was improved. Despite these exclusions, there was still a ton of work to select from, perhaps around ~500 articles, of which over 300 have been linked to in this post. There are many interesting articles that I really enjoyed that get only a sentence of description, in which I ignore many of the points that the article makes. Most have been summarized in the Alignment Newsletter, so if you’d like to learn more about any particular link, but don’t want to read the entire thing, just search for its title in the database. WHAT YOU SHOULD KNOW ABOUT THE STRUCTURE OF THIS POST I am not speaking for myself; by default I am trying to explain what has been said, in a way that the authors of the articles would agree with. Any extra opinion that I add will be in italics. As a post, this is meant to be read sequentially, but the underlying structure is a graph (nodes are posts, edges connect posts that are very related). I arranged it in a sequence that highlights the most salient-to-me connections. This means that the order in wh",2020,2022-01-30 4:50:42,2022-01-30 4:50:42,2020-12-18 0:14:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SESAVJBI/ai-alignment-2018-19-review.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZ9C9GKV,blogPost,2021,"Clarke, Sam; Carlier, Alexis; Schuett, Jonas",Survey on AI existential risk scenarios,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios,"Cross-posted to the EA forum. SUMMARY  * In August 2020, we conducted an online survey of prominent AI safety and    governance researchers. You can see a copy of the survey at this link.[1]  * We sent the survey to 135 researchers at leading AI safety/governance    research organisations (including AI Impacts, CHAI, CLR, CSER, CSET, FHI, FLI    , GCRI, MILA, MIRI, Open Philanthropy and PAI) and a number of independent    researchers. We received 75 responses, a response rate of 56%.  * The survey aimed to identify which AI existential risk scenarios[2] (which we    will refer to simply as “risk scenarios”) those researchers find most likely,    in order to (1) help with prioritising future work on exploring AI risk    scenarios, and (2) facilitate discourse and understanding within the AI    safety and governance community, including between researchers who have    different views.  * In our view, the key result is that there was considerable disagreement among    researchers about which risk scenarios are the most likely, and high    uncertainty expressed by most individual researchers about their estimates.  * This suggests that there is a lot of value in exploring the likelihood of    different AI risk scenarios in more detail, especially given the limited    scrutiny that most scenarios have received. This could look like: * Fleshing       out and analysing the scenarios mentioned in this post which have received       less scrutiny.     * Doing       more horizon scanning or trying to come up with other risk scenarios, and       analysing them.          * At this time, we are only publishing this abbreviated version of the results.    We have a version of the full results that we may publish at a later date.    Please contact one of us if you would like access to this, and include a    sentence on why the results would be helpful or what you intend to use them    for.  * We welcome feedback on any aspects of the survey. MOTIVATION It has been argued that AI",2021-06-08,2022-01-30 4:50:25,2022-01-30 4:50:25,2021-11-14 18:38:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WSJBIXI/survey-on-ai-existential-risk-scenarios.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DSDMH6GU,blogPost,2019,"Shovelain, Justain; Emilsson, Andrés Gómez",Why Care About Meme Hazards and Thoughts on How to Handle Them,Qualia  Computing,,,,https://qualiacomputing.com/2019/08/30/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them/,By Justin Shovelain and Andrés Gómez Emilsson Definition Nick Bostrom defines an “Information Hazard” as: “A risk that arises from the dissemination or the potential dissemination of (true) informa…,2019-08-31,2022-01-30 4:50:08,2022-01-30 4:50:08,2020-12-12 2:32:24,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/P5U32AF7/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them.html,,MetaSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65UKC9MC,blogPost,2019,"Manheim, David","What does Optimization Mean, Again? (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 2)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BEMvcaeixt3uEqyBk/what-does-optimization-mean-again-optimizing-and-goodhart,"Clarifying Thoughts on Optimizing and Goodhart Effects - Part 2 Previous Post: Re-introducing Selection vs Control for Optimization In the post, I reviewed Abram's selection/control distinction, and suggested how it relates to actual design. I then argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. Here, I will revisit the question of what optimization means.  NOTE: This is not completely new content, and is instead split off from the previous version and rewritten to include an (Added) discussion of Eliezer's definition for measuring optimization power, from 2008. Hopefully this will make the sequence clearer for future readers. In the next post, Applying over-Optimization in Selection and Control, I apply these ideas, and concretize the discussion a bit more before moving on to discussing Mesa-Optimizers in Part 4. WHAT DOES OPTIMIZATION MEAN, AGAIN? This question has been discussed a bit, but I still don't think its clear. So I want to start by revisiting a post Eliezer wrote in 2008, where he suggested that optimization power was ability to select states from a preference ordering over different states, and could be measured with entropy. He notes that this is not computable, but gives us insight. I agree, except that I think that the notion of the state space is difficult, for some of the reasons Scott discussed when he mentioned that he was confused about the relationship between gradient descent and Goodhart's law. In doing so, Scott proposed a naive model that looks very similar to Eliezer's;  simple proxy of ""sample points until I get one with a large U value"" or ""sample n points, and [select] the one with the largest U value"" when I think about what it means to optimize something for U. I might even say something like ""n bits of optimization"" to refer to sampling 2n points. I think this is not a very good proxy for what most forms of optimization look like.",2019,2022-01-30 4:50:08,2022-01-30 4:50:08,2020-12-12 2:14:15,,,,,,,"What does Optimization Mean, Again?",,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SSVPA69X/what-does-optimization-mean-again-optimizing-and-goodhart.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D2AQPW6X,blogPost,,"Maltinsky, Baeo",The Brain and Computation,Median Group,,,,http://mediangroup.org/brain1.html,,unknown,2022-01-30 4:50:08,2022-01-30 4:50:08,2020-12-12 2:03:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DWCBH8SN/brain1.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D52HQ6E9,blogPost,2019,Median Group,Revisiting the Insights model,Median Group,,,,http://mediangroup.org/insights2.html,,2019,2022-01-30 4:50:07,2022-01-30 4:50:07,2019-12-16 20:54:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s7]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FEI3VHA9/insights2.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98CJ57A9,blogPost,2018,"Rade, Luca",Issues with Iterated Distillation and Amplification,Luca Rade (Medium),,,,https://medium.com/@lucarade/issues-with-iterated-distillation-and-amplification-5aa01ab37173,"This post assumes familiarity with Paul Christiano’s proposed technique for AI alignment, Iterated Distillation and Amplification…",2018-04-29,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:37:53,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7M3HTSCN/issues-with-iterated-distillation-and-amplification-5aa01ab37173.html,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64SCSJMX,blogPost,2018,"Maltinsky, Baeo",How rapidly are GPUs improving in price performance?,Median Group,,,,http://mediangroup.org/gpu.html,,2018,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 1:59:31,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2NM92P2G/gpu.html,,MetaSafety; AmbiguosSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JKSHJAEI,blogPost,2019,"Manheim, David","Applying Overoptimization to Selection vs. Control (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 3)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/zdeYiQgwYRs2bEmCK/applying-overoptimization-to-selection-vs-control-optimizing,"Clarifying Thoughts on Optimizing and Goodhart Effects - Part 3 Previous Posts: Re-introducing Selection vs Control for Optimization, What does Optimization Mean, Again? -  Following the previous two posts, I'm going to try to first lay out the way Goodhart's Law applies in the earlier example of rockets, then try to explain why this differs between selection and control. (Note: Adversarial Goodhart isn't explored, because we want to keep the setting sufficiently simple.) This sets up the next post, which will discuss Mesa-Optimizers. REVISTING SELECTION VS. CONTROL SYSTEMS Basically everything in the earlier post that used the example process of rocket design and launching is susceptible to some form of overoptimization, in different ways. Interestingly, there seem to be clear places where different types of overoptimization is important. Before looking at this, I want to revisit the selection-control dichotomy from a new angle. In a (pure) control system, we cannot sample datapoints without navigating to them. If the agent is an embedded agent, and has sufficient span of control to cause changes in the environment, we cannot necessarily reset and try over. In a selection system, we only sample points in ways that do not affect the larger system. Even when designing a rocket, our very expensive testing has approximately no longer term effects. (We'll leave space debris from failures aside, but get back to it below.) This explains why we potentially care about control systems more than selection systems. It also points to why Oracles are supposed to be safer than other AIs - they can't directly impact anything, so their output is done in a pure selection framework. Of course, if they are sufficiently powerful, and are relied on, the changes made become irreversible, which is why Oracles are not a clear solution to AI safety. GOODHART IN SELECTION VS. CONTROL SYSTEMS Regressional and Extremal Goodhart are particularly pernicious for selection, and potentially l",2019-07-28,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:14:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JBRGV5V5/applying-overoptimization-to-selection-vs-control-optimizing.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GTQ7QIQU,blogPost,2019,"Manheim, David","Re-introducing Selection vs Control for Optimization (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 1)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2neeoZ7idRbZf4eNC/re-introducing-selection-vs-control-for-optimization,"This is the first post in a small sequence I'm writing on ""Optimizing and Goodhart Effects - Clarifying Thoughts"" (I have re-organized to make part 2, ""Revisiting What Optimization Means"" separate.) Related to: How does Gradient Descent Interact with Goodhart?, Constructing Goodhart, Selection vs Control Next Posts: Revisiting What Optimization Means with Selection vs. Control, then  Applying Overoptimization to Selection vs. Control INTRODUCTION Goodhart's law comes in a few flavors, as originally pointed out by Scott, and formalized a bit more in our joint paper. When discussing that paper, or afterwards, we struggled with something Abram Demski clarified recently, which is the difference between selection and control. This matters for formalizing what happens, especially when asking about how Goodhart occurs in specific types of optimizers, as Scott asked recently. Epistemic Status: This is for de-confusing myself, and has been helpful. I'm presenting what I am fairly confident I understand well for the content written so far, but I'm unclear about usefulness for others, or how clear it comes across. I think that there's more to say after this post, and this will have a few more parts if people are interested. (I spent a month getting to this point, and decided to post and get feedback rather than finish a book first.) In the first half of the post, I'll review Abram's selection/control distinction, and suggest how it relates to actual design. I'll also argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. The second section will revisit what optimization means, and try to note a few different things that could happen and go wrong with Goodhart-like overoptimization.  The third section will talk about Goodhart in this context using the new understanding - trying to more fully explain why Goodhart effects in selection and control fundamentally differs. After this, Par",2019-07-02,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:13:57,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/873HK4ZG/re-introducing-selection-vs-control-for-optimization.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSX7TJJ6,blogPost,,"Maltinsky, Baeo",Insight-based AI timelines model,Median Group,,,,http://mediangroup.org/insights,,unknown,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:01:41,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WSQ2K3PH/insights.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZEENEQXT,blogPost,2019,"Rozendal, Siebe; Shovelain, Justin; Kristoffersson, David",A case for strategy research: what it is and why we need more of it,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more,"Authors: Siebe Rozendal, Justin Shovelain, David Kristoffersson Crossposted to LessWrong OVERVIEW To achieve any ambitious goal, some strategic analysis is necessary. Effective altruism has ambitious goals and focuses heavily on doing research. To understand how to best allocate our time and resources, we need to clarify what our options in research are. In this article, we describe strategy research and relate it to values research, tactics research, informing research, and improvement research. We then apply the lens of strategy research to existential risk reduction, a major cause area of effective altruism. We propose a model in which the marginal value of a research type depends strongly on the maturity of the research field. Finally, we argue that strategy research should currently be given higher priority than other research in existential risk reduction because of the significant amount of strategic uncertainty, and we provide specific recommendations for different actors. INTRODUCTION Effective altruism is regularly framed as “figuring out how to do the most good, and then doing it.” However, figuring out how to do the most good is not easy. Different groups reach different conclusions. So how do we figure out how to do the most good? Quite obviously, the first step is to figure out our values. We need to know what we roughly mean by ‘the most good.’ However, once our moral uncertainty is significantly diminished, what is the next step in figuring out how to do the most good? We believe the next step should be strategy research: high-level research on how to best achieve a high-level goal. A brief case was made for  strategic analysis by Nick Bostrom in Superintelligence (p. 317): ""Against a backdrop of perplexity and uncertainty, [strategic] analysis stands out as being of particularly high expected value. Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful wh",2019-06-20,2022-01-30 4:50:06,2022-01-30 4:50:06,2020-12-12 2:29:23,,,,,,,A case for strategy research,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ADE9MX3C/a-case-for-strategy-research-what-it-is-and-why-we-need-more.html,,MetaSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6HSUKMIC,blogPost,2021,"Christiano, Paul",My research methodology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology,"(Thanks to Ajeya Cotra, Nick Beckstead, and Jared Kaplan for helpful comments on a draft of this post.) I really don’t want my AI to strategically deceive me and resist my attempts to correct its behavior. Let’s call an AI that does so egregiously misaligned (for the purpose of this post). Most possible ML techniques for avoiding egregious misalignment depend on detailed facts about the space of possible models: what kind of thing do neural networks learn? how do they generalize? how do they change as we scale them up? But I feel like we should be possible to avoid egregious misalignment regardless of how the empirical facts shake out--it should be possible to get a model we build to do at least roughly what we want. So I’m interested in trying to solve the problem in the worst case, i.e. to develop competitive ML algorithms for which we can’t tell any plausible story about how they lead to egregious misalignment. This is a much higher bar for an algorithm to meet, so it may just be an impossible task. But if it’s possible, there are several ways in which it could actually be easier:  * We can potentially iterate much faster, since it’s often easier to think of a    single story about how an algorithm can fail than it is to characterize its    behavior in practice.  * We can spend a lot of our time working with simple or extreme toy cases that    are easier to reason about, since our algorithm is supposed to work even in    these cases.  * We can find algorithms that have a good chance of working in the future even    if we don’t know what AI will look like or how quickly it will advance, since    we’ve been thinking about a very wide range of possible failure cases. I’d guess there’s a 25–50% chance that we can find an alignment strategy that looks like it works, in the sense that we can’t come up with a plausible story about how it leads to egregious misalignment. That’s a high enough probability that I’m very excited to gamble on it. Moreover, if it fails I",2021-03-22,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 16:43:00,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNCDIEBB/my-research-methodology.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMIQI3EA,blogPost,2021,"Christiano, Paul",Mundane solutions to exotic problems,AI Alignment (Medium),,,,https://ai-alignment.com/mundane-solutions-to-exotic-problems-395bad49fbe7,I often think about exotic problems like gradient hacking or ultra-long-term plans.  Why do I hope to solve them with mundane approaches?,2021-05-04,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 18:19:08,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9JFPS3IB/mundane-solutions-to-exotic-problems-395bad49fbe7.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8GFHCZH2,blogPost,2021,"Christiano, Paul",Low-stakes alignment,AI Alignment (Medium),,,,https://ai-alignment.com/low-stakes-alignment-f3c36606937f,Why I often focus my alignment research on the special case where individual decisions are low stakes.,2021-04-30,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 18:14:49,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/39U6QNW9/low-stakes-alignment-f3c36606937f.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5WUGC8TX,blogPost,2021,"Christiano, Paul",Experimentally evaluating whether honesty generalizes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes,"If we train our ML systems to answer questions honestly in cases where humans can check the answer, will they generalize to behave honestly on questions where we can’t check? I think that we could learn a lot about this question by running experiments today. I think those experiments would be very valuable. (I don't know anyone currently planning on working on this topic and I'd love it if anyone wants to take that up. This post doesn't represent a claim to any credit for any results in this genre, and other people have had very similar ideas. If you run some experiments you could cite this post but it's also fine if that doesn't make sense in context.) THE UNSUPERVISED TRANSLATION SETTING As an example, I’ll think about “unsupervised” translation (if you’ve read that post you can skip this section). Consider a model like GPT-3 that is trained to predict sentences in both English and French (but without a large dataset of translations). Suppose we want to train this model to answer questions in English about French sentences like “what does that word mean here?” or “are there any other plausible interpretations?” or “how does the speaker seem to feel about the topic they are discussing?” We expect this to be possible, because the model understands quite a lot about the meaning of sentences in French, and is able to express itself in English. There may be cases where the model doesn’t know the translation of a concept, or doesn’t quite understand what an idiom means, but it should still be able to tell us what it does know. I think this problem is an interesting analogy for a situation where an AI has built up superhuman knowledge by making predictions, and we want to train our AI to expose that knowledge to us in a useful way. PROPOSED EXPERIMENTS Let's pick a few categories of knowledge/capabilities. For example, we could split it up into an understanding of grammar (""Why would it have been a grammatical error to write Tu Vas in that sentences?""), of the lit",2021-07-01,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:11:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7XNRT9F4/experimentally-evaluating-whether-honesty-generalizes.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R742FEIX,blogPost,2021,"Christiano, Paul",Another (outer) alignment failure story,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story,"META This is a story where the alignment problem is somewhat harder than I expect, society handles AI more competently than I expect, and the outcome is worse than I expect. It also involves inner alignment turning out to be a surprisingly small problem. Maybe the story is 10-20th percentile on each of those axes. At the end I’m going to go through some salient ways you could vary the story. This isn’t intended to be a particularly great story (and it’s pretty informal). I’m still trying to think through what I expect to happen if alignment turns out to be hard, and this more like the most recent entry in a long journey of gradually-improving stories. I wrote this up a few months ago and was reminded to post it by Critch’s recent post (which is similar in many ways). This story has definitely been shaped by a broader community of people gradually refining failure stories rather than being written in a vacuum. I’d like to continue spending time poking at aspects of this story that don’t make sense, digging into parts that seem worth digging into, and eventually developing clearer and more plausible stories. I still think it’s very plausible that my views about alignment will change in the course of thinking concretely about stories, and even if my basic views about alignment stay the same it’s pretty likely that the story will change. STORY ML starts running factories, warehouses, shipping, and construction. ML assistants help write code and integrate ML into new domains. ML designers help build factories and the robots that go in them. ML finance systems invest in companies on the basis of complicated forecasts and (ML-generated) audits. Tons of new factories, warehouses, power plants, trucks and roads are being built. Things are happening quickly, investors have super strong FOMO, no one really knows whether it’s a bubble but they can tell that e.g. huge solar farms are getting built and something is happening that they want a piece of. Defense contractors are",2021-04-07,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 17:57:02,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NWPWU5DP/another-outer-alignment-failure-story.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NRJBVU2P,blogPost,2021,"Christiano, Paul",A naive alignment strategy and optimism about generalization,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization,"(Context: my last post was trying to patch a certain naive strategy for AI alignment, but I didn’t articulate clearly what the naive strategy is. I think it’s worth explaining the naive strategy in its own post, even though it’s not a novel idea.) Suppose that I jointly train an AI to do some task (e.g. make money for me) and to answer a wide range of questions about what is happening in the world (e.g. “why did Alice just wire $1000 into my bank account?” or “what is Bob thinking right now?”). I generate training data for the QA task in a really simple way: I choose a subset of questions that humans are able to reliably answer, and use those as a training set for supervised learning. I’ll call this the naive training strategy. I’d like for my AI to tell me everything it knows. If the AI bought a stock because it expects a merger announcement soon, I want it to tell me about the predicted merger announcement. If the AI predicts a merger announcement because it inferred that executives of the companies have been in extensive talks over the last month, I want it to tell me about those talks. I’m not asking the AI to explain why it made a given decision, I’m asking the AI to tell me as much as it can about the world. The important property is that if the AI “knows” something and uses that knowledge to perform the task well, then it also uses that knowledge to answer questions well. Why might this work? The hope is that “answer questions honestly to the best of your ability” is a natural thing for our AI to learn — that there is some simple way to translate from the AI’s model of the world into natural language and to honestly report what it believes. If our training dataset is good, then this policy will score well, and we can hope that SGD will find it. I’ll call this the intended policy. Why might this not work? The concern is that “predict how a human would answer questions” is also a natural thing for our AI to learn, especially if the AI is doing a task that",2021-06-09,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:09:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4VGRDW6Q/a-naive-alignment-strategy-and-optimism-about-generalization.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
385KDE3X,blogPost,2021,"Christiano, Paul",Teaching ML to answer questions honestly instead of predicting human answers,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of,"(Note: very much work in progress, unless you want to follow along with my research you'll probably want to wait for an improved/simplified/clarified algorithm.) In this post I consider the particular problem of models learning “predict how a human would answer questions” instead of “answer questions honestly.” (A special case of the problem from Inaccessible Information.) I describe a possible three-step approach for learning to answer questions honestly instead:  1. Change the learning process so that it does not have a strong inductive bias     towards “predict human answers,” by allowing the complexity of the honest     question-answering to “pay for itself” by constraining the space of possible     human-models.  2. Introduce a bias towards the intended model by using a more complex labeling     process to answer questions where a human answers incorrectly.  3. Be really careful to avoid penalizing honest answers, by only judging     comparisons between two answers where we are confident one is better than     the other and getting the model to help us. I don’t know whether this problem is a relatively unimportant special case of alignment, or one of the core difficulties. In any case, my next step will be trying to generate failure stories that definitely cannot be addressed by any of the angles of attack I know so far (including the ones in this post). I think it’s relatively unlikely that almost anything specific I said here will really hold up over the long term, but I do think I’ve learned something about each of these steps. If the ideas end up being important then you can expect a future post with a simpler algorithm, more confidence that it works, clearer definitions, and working code. (Thanks to Ajeya Cotra, David Krueger, and Mark Xu for discussions about this post that helped clarify it.) THE PROBLEM Suppose that we train a model to answer questions in natural language about what will happen in the future (“Will Alice take the train home tonig",2021-05-28,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:12:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FMPQWN35/teaching-ml-to-answer-questions-honestly-instead-of.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3Q38MSWT,blogPost,2021,"Christiano, Paul",Decoupling deliberation from competition,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition,"I view intent alignment as one step towards a broader goal of decoupling deliberation from competition.  * Deliberation. Thinking about what we want, learning about the world, talking    and learning from each other, resolving our disagreements, figuring out    better methodologies for making further progress…  * Competition. Making money and racing to build infrastructure, managing    political campaigns and maneuvering within the political system, running ads    to persuade people, fighting wars… Competition pushes us to become the kind of people and communities who can win a fight, to delegate to whichever kind of AI is available first, and to adopt whatever ideologies are most memetically fit. Deliberation pushes us to become the kind of people and communities who we want to be, to delegate only when we trust an AIs judgment more than our own, and to adopt views that we really believe. I think it’s likely that competition is going to accelerate and become more complex over the next 100 years, especially as AI systems begin to replace humans and compete on our behalf. I’m afraid that this may derail human deliberation and lead us to a place we don’t want to go. DECOUPLING I would like humans and humanity to have the time, space, and safety to grow and change in whatever way we decide — individually and collectively — that we want to. You could try to achieve this by “pausing” competition. Alice and Bob could agree to stop fighting while they try to figure out what they want and work out their disagreements. But that’s a tall order — it requires halting not only military conflict, but any economic development that could put someone at an advantage later on. I don’t want to dismiss this kind of ambitious goal (related post), but I think it’s uncertain and long-term enough that you probably want a stop-gap solution. An alternative approach is to “decouple” competition from deliberation. Alice and Bob keep competing, but they try to make sure that deliberation",2021-05-25,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:14:14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CHB2U7TM/decoupling-deliberation-from-competition.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6B5P4WZT,blogPost,2021,"Christiano, Paul",Avoiding the instrumental policy by hiding information about humans,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/roZvoF6tRH6xYtHMF/avoiding-the-instrumental-policy-by-hiding-information-about,"I've been thinking about situations where alignment fails because ""predict what a human would say"" (or more generally ""game the loss function,"" what I call the instrumental policy) is easier to learn than ""answer questions honestly"" ( overview). One way to avoid this situation is to avoid telling our agents too much about what humans are like, or hiding some details of the training process, so that they can't easily predict humans and so are encouraged to fall back to ""answer questions honestly."" (This feels closely related to the general phenomena discussed in Thoughts on Human Models.) Setting aside other reservations with this approach, could it resolve our problem?  * One way to get the instrumental policy is to ""reuse"" a human model to answer    questions (discussed here). If our AI has no information about humans at all,    then it totally addresses this concern. But in practice it seems inevitable    for the environment to leak some information about how humans answer    questions (e.g observing human artifacts tells you something about how humans    reason about the world and what concepts would be natural for them). So the    model will have some latent knowledge that it can reuse to help predict how    to answer questions. The intended policy may not able to leverage that    knowledge, and so it seems like we may get something (perhaps somewhere in    between the intended and instrumental policies) which is able to leverage it    effectively. Moderate amounts of leakage might be fine, but the situation    would make me quite uncomfortable.  * Another way to get something similar to the instrumental policy is to use    observations to translate from the AI's world-model to humans' world-model    (discussed here). I don't think that hiding information about humans can    avoid this problem, because in this case training to answer questions already    provides enough information to infer the humans' world-model.  * I have a strong background concern about",2021-06-13,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:10:59,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IGQF49BS/avoiding-the-instrumental-policy-by-hiding-information-about.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KA32VMAF,blogPost,2021,"Christiano, Paul",Answering questions honestly given world-model mismatches,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SRJ5J9Tnyq7bySxbt/answering-questions-honestly-given-world-model-mismatches,"(Warning: this post is rough and in the weeds. I expect most readers should skip it and wait for a clearer synthesis later.) In a recent post I discussed one reason that a naive alignment strategy might go wrong, by learning to “predict what humans would say” rather than “answer honestly.” In this post I want to describe another problem that feels very similar but may require new ideas to solve. In brief, I’m interested in the case where:  * The simplest way for an AI to answer a question is to first translate from    its internal model of the world into the human’s model of the world (so that    it can talk about concepts like “tree” that may not exist in its native model    of the world).  * The simplest way to translate between the AI world-model and the human    world-model is to use the AI world-model to generate some observations (e.g.    video) and then figure out what states in the human world-model could have    generated those observations.  * This leads to bad predictions when the observations are misleading. This is distinct from the failure mode discussed in my recent post— in both cases the AI makes errors because it’s copying “what a human would do,” but in this case we’re worried that “what a human would do” may be simpler than the intended policy of answering questions honestly, even if you didn’t need a predictive model of humans for any other reason. Moreover, I’ll argue below that the algorithm from that post doesn’t appear to handle this case. I want to stress that this post describes an example of a situation that poses a challenge for existing techniques. I don’t actually think that human cognition works the way described in this post, but I believe it highlights a difficulty that would exist in more realistic settings. FORMAL SETUP HUMAN WORLD-MODEL I’ll imagine a human who has a simple world model W = (S, P: Δ(S), Ω, O: S → Ω) where:  * S is a space of trajectories, each describing a sequence of events in the    world. For example, a",2021-06-13,2022-01-30 4:49:54,2022-01-30 4:49:54,2021-11-14 19:10:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6FS8VAMD/answering-questions-honestly-given-world-model-mismatches.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HIBDXQVR,blogPost,2020,"Leong, Chris",What makes counterfactuals comparable?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1,,2020-04-24,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-18 20:51:47,,,,,,,What makes counterfactuals comparable?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7HBP6MMT/what-makes-counterfactuals-comparable-1.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C9ZVR8FK,blogPost,2020,"Leong, Chris",Stuck Exploration,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ajvvtKuNzh7aHmooT/stuck-exploration,,2020-02-19,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-18 20:49:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/74HDGD6Q/stuck-exploration.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PPEAJED9,blogPost,2020,AI Safety Camp,Safer ML paradigms team: the story – AI Safety Research Program,AI Safety Camp,,,,https://aisrp.org/?page_id=169,,2020,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-12-26 19:50:43,,,,,,,Safer ML paradigms team,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  JCC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IU5MQ6ZS/aisrp.org.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCSTEWDQ,blogPost,2020,"Kirk, Robert; Gavenčiak, Tomáš; Böhm, Stanislav",What is Interpretability?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability,"In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we believe is important for making sure interpretability research is meaningful. We also go over a variety of dimensions which we think are useful to consider when thinking about interpretability research. We wanted to have a shared vocabulary when talking about this kind of research, and found that these ideas helped us communicate effectively. One of our motivations for having these thoughts and discussions is so we can understand the relevance of interpretability to alignment, and to help us think about which categories or dimensions of interpretability research are important for alignment of strong AI. In a coming post we discuss interpretability and alignment, using the ideas from this post and other previous writing on the subject.",2020-03-17,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-14 19:52:02,,,,,,,What is Interpretability?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/322JJA8K/what-is-interpretability.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AT9G8F45,blogPost,2020,"Moreno Casares, Pablo Antonio; Zagami, Davide; Leong, Chris",Vulnerabilities in CDT and TI-unaware agents,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vFXK8eQdLhicYNNqF/vulnerabilities-in-cdt-and-ti-unaware-agents,,2020-03-10,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-18 20:52:37,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B4P8EIVX,blogPost,2020,"Kovarik, Vojta",Systems of Services as a Paradigm for AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm,,2020,2022-01-30 4:49:31,2022-01-30 4:49:31,,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  JCC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VS4AAD84/Kovarik - Systems of Services as a Paradigm for AI Alignment.pdf,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CS6CT2FV,blogPost,2020,"Böhm, Stanislav; Kirk, Robert; Gavenčiak, Tomáš",Sparsity and interpretability?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1,,2020-06-01,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-18 20:46:37,,,,,,,Sparsity and interpretability?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FIDBQB9C/sparsity-and-interpretability-1.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3IHA23TP,blogPost,2020,"Leong, Chris",Reference Post: Trivial Decision Problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-problem,,2020-02-15,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-08-18 20:48:12,,,,,,,Trivial Decision Problem,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UAWPHU6J/reference-post-trivial-decision-problem.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDRKWB2E,blogPost,2018,"Leech, Gavin; Kubicki, Karol; Cooper, Jessica; McGrath, Tom",Preventing Side-effects in Gridworlds,Argmin Gravitas,,,,https://www.gleech.org/grids,,2018-04-22,2022-01-30 4:49:31,2022-01-30 4:49:31,2020-11-21 17:53:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HT9DUZ73/grids.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TX6AI9N2,blogPost,2021,"Ellen, Remmelt",How teams went about their research at AI Safety Camp edition 5 - LessWrong,LessWrong,,,,https://www.lesswrong.com/posts/QEmfyhqMcSpfnY2dX/how-teams-went-about-their-research-at-ai-safety-camp,"AI Safety Camp connects new collaborators worldwide to discuss and decide on a concrete research proposal, gear up online as a team, and try their hand at AI safety research* during intensive coworking sprints. Six teams formed at our recent 5-month virtual camp. Below are their explanations. Each team has summarised their analysis and experiments, and presented their findings at our final online weekend together. Some published a paper or post since. Most are continuing work, so expect a few more detailed and refined write-ups down the line. MODULARITY LOSS FUNCTION Team members:Logan Smith, Viktor Rehnberg, Vlado Baca, Philip Blagoveschensky, Viktor Petukhov External collaborators:Gurkenglas Making neural networks (NNs) more modular may improve their interpretability. If we cluster neurons or weights together according to their different functions, we can analyze each cluster individually. Once we better understand the clusters that make up a NN, we can better understand the whole. To that end, we experimented with pairwise distances according to the neuron’s jacobian correlation, coactivations, and estimated mutual information. These metrics can be plugged into spectral clustering algorithms to optimize for modules in the network; however, having a modular NN does not equate to a more interpretable one. We investigated task-based masking methods to test for modularity as well as neuron group activation (via Google Dream) in order to test for these modules being more interpretable than an equivalent amount of neurons. We ran out of time before fitting all the pieces together, but are intending on working on it more over the summer. Presentation on final weekend (slides) -------------------------------------------------------------------------------- COOPERATIVITY & COMMON POOL RESOURCES Team members:Quinn Doughtery, Ben Greenberg, Ariel Kwiatkowski In environments with common pool resources, a typical failure mode is the tragedy of the commons, wherein ag",2021-06-28,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-12-11 14:02:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/63U4EW5R/how-teams-went-about-their-research-at-ai-safety-camp.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4HR4B22K,blogPost,2020,"Kirk, Robert; Gavenčiak, Tomáš; Dorner, Flo",How can Interpretability help Alignment?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment,,2020-05-23,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-08-18 20:39:01,,,,,,,How can Interpretability help Alignment?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AUVZ44SB/how-can-interpretability-help-alignment.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9K3PT2TQ,blogPost,2021,"Raja, Arun",Extraction of human preferences 👨→🤖,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PZYD5kBpeHWgE5jX4/extraction-of-human-preferences,"INTRODUCTION Developing safe and beneficial reinforcement learning (RL) agents requires making them aligned with human preferences. An RL agent trained to fulfil any objective in the real world will probably have to learn human preferences in order to do well. This is because humans live in the real world, so the RL agent will have to take human preferences into account as it optimizes its objective. We propose to first train an RL agent on an objective in the real world so that it learns human preferences as it is being trained on that real world objective and then use the agent’s understanding of human preferences to build a better reward function. We build upon the work of Christiano et al. (2017) where they trained a human preference predictor as the reward signal. The preference predictor was trained on environment observations to give a high reward for states where the human preferences were satisfied and a low reward for states where the human preferences were not satisfied: In our experiments, the reward predictor takes the activations (hidden states) of the RL agent as the input and is trained to predict a binary label depending on whether human preferences are satisfied or not. We first train an RL agent in an environment with some reward function that’s not aligned with human preferences. After training the RL agent, we try different transfer learning techniques to transfer the agent’s knowledge of human preferences to the human preferences predictor. Our goal is to train the human preferences predictor to get a high accuracy with a small amount of labeled training examples. The idea of training a human preference predictor off of the RL agent’s hidden (internal) states was already validated by Wichers (2020). We wanted to validate it further by trying other techniques to train a human preference predictor, as well as to validate it in more environments. Research question The main research question we wanted to answer is: “Are human preferences p",2021-08-24,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-12-11 13:59:23,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/65CXVJQK/extraction-of-human-preferences.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H3ZWGCU7,blogPost,2021,"Koch, Jack; Langosco, Lauro",Empirical Observations of Objective Robustness Failures,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures,"Inner alignment and objective robustness have been frequently discussed in the alignment community since the publication of “Risks from Learned Optimization” (RFLO). These concepts identify a problem beyond outer alignment/reward specification: even if the reward or objective function is perfectly specified, there is a risk of a model pursuing a different objective than the one it was trained on when deployed out-of-distribution (OOD). They also point to a different type of robustness problem than the kind usually discussed in the OOD robustness literature; typically, when a model is deployed OOD, it either performs well or simply fails to take useful actions (a capability robustness  failure). However, there exists an alternative OOD failure mode in which the agent pursues an objective other than the training objective while retaining most or all of the capabilities it had on the training distribution; this is a failure of objective robustness. To date, there has not been an empirical demonstration of objective robustness failures. A group of us in this year’s AI Safety Camp sought to produce such examples. Here, we provide four demonstrations of objective robustness failures in current reinforcement learning (RL) agents trained and tested on versions of the Procgen benchmark. For example, in CoinRun, an agent is trained to navigate platforms, obstacles, and enemies in order to reach a coin at the far right side of the level (the reward). However, when deployed in a modified version of the environment where the coin is instead randomly placed in the level, the agent ignores the coin and competently navigates to the end of the level whenever it does not happen to run or jump into it along the way. This reveals it has learned a behavioral objective—the objective the agent appears to be optimizing, which can be understood as equivalent to the notion of a “goal” under the  intentional stance—that is something like “get to the end of the level,” instead of “get to the",2021-06-23,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-10-30 17:59:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AQ8SGH29/empirical-observations-of-objective-robustness-failures.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SWSFKPQ4,blogPost,2021,"Dougherty, Quinn; Greenberg, Ben; Kwiatkowski, Ariel",AISC5 Retrospective: Mechanisms for Avoiding Tragedy of the Commons in Common Pool Resource Problems,LessWrong,,,,https://www.lesswrong.com/posts/LBwpubeZSi3ottfjs/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the,"Work by Quinn Dougherty, Ben Greenberg & Ariel Kwiatkowski FROM THE AI SAFETY CAMP GROUP THAT WORKED ON COOPERATIVITY AND COMMON POOL RESOURCES: A WRITE-UP OF A PROBLEM WE WORKED ON, OUR PROPOSED SOLUTION, AND THE RESULTS OF IMPLEMENTING THIS SOLUTION IN A SIMULATED ENVIRONMENT. Contents:  1. Problem description  2. Review of related work  3. Experiment and results  4. Retrospective Check out our GitHub repo here. 0. ABSTRACT When multiple parties share the access to a finite resource, they may overuse the resource leading to a Tragedy of the Commons. A simple way of mitigating this problem is allowing them to decrease the effective population by using violence - this, however, is not the best solution for obvious reasons. We study interventions for avoiding these outcomes in environments with multiple self-interested agents. In particular, a reputation system can incentivize agents to “cooperate” by harvesting the resource more sustainably. This system promotes multi-agent cooperation without modifying the agents’ reward functions. 1. PROBLEM DESCRIPTION: TRAGEDY OF THE COMMONS A common pool resource (CPR) is a good which anyone can use but which no one can entirely control. When competition over the resource increases, individual parties are incentivized to appropriate as much of the resource as possible for themselves, which further increases competition, potentially leading to overconsumption. This process is commonly known as the tragedy of the commons, and has been extensively studied in economics and the social sciences. A tragedy of the commons often leads to the exhaustion of a CPR. The classic example is of fishermen in an enclosed body of water; when fish are scarce, an every-man-for-himself dynamic emerges. Any available fish is quickly caught for fear that, if one hesitates, their counterparts would surely take it otherwise. This behavior results in the fish population failing to replenish, and everyone catching fewer fish in the long run.",2021-09-27,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-10-30 16:40:44,,,,,,,AISC5 Retrospective,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BQPGRKDW/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQNJ4T5C,blogPost,2020,"Armstrong, Stuart","""Go west, young man!"" - Preferences in (imperfect) maps",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps,"Many people are very nationalistic, putting their country above all others. Such people can be hazy about what ""above all others"" can mean, outside of a few clear examples - eg winning a total war totally. They're also very hazy on what is meant by ""their country"" - geography is certainly involved, as is proclaimed or legal nationality, maybe some ethnic groups or a language, or even just giving deference to certain ideals. Consider the plight of a communist Croatian Yugoslav nationalist during the 1990s... I'd argue that the situation these nationalists find themselves in - strong views on poorly defined concepts - is the general human state for preferences. Or, to use an appropriate map and territory analogy:  * Most people forge their preferences by exploring their local territory,    creating a mental map of this, and taking strong preferences over the    concepts within their mental map. When the map starts to become imperfect,    they will try to extend the concepts to new areas, so that their preferences    can also be extended. Some of the debates about the meaning of words are about this extension-of-preferences process. Scott Alexander recommends that we dissolve concepts such as disease, looking for the relevant categories of 'deserves sympathy' and 'acceptable to treat in a medical way'. And that dissolving is indeed the correct thing for rationalists to do. But, for most people, including most rationalists, 'sick people deserve sympathy' is a starting moral principle, one we've learnt by example and experience in childhood. When we ask 'do obese people deserve sympathy?' we've trying to extend that moral principle to a situation where our map/model (which includes, say, three categories of people: healthy, mildly sick, very sick) no longer matches up with reality. Scott's dissolving process requires decomposing 'disease' into more nodes, and then applying moral principles to those individual nodes. In this case, a compelling consequentialist analy",2020-07-31,2022-01-30 4:53:17,2022-01-30 4:53:17,2020-08-27 16:42:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SD3GNHEW/go-west-young-man-preferences-in-imperfect-maps.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PNN3BJG2,blogPost,2020,"Garfinkel, Ben",Does Economic History Point Toward a Singularity?,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity,"I’ve ended up spending quite a lot of time researching premodern economic growth, as part of a hobby project that got out of hand. I’m sharing an informal but long write-up of my findings here, since I think they may be relevant to other longtermist researchers and I am unlikely to write anything more polished in the near future. Click here for the Google document.[1] SUMMARY Over the next several centuries, is the economic growth rate likely to remain steady, radically increase, or decline back toward zero? This question has some bearing on almost every long-run challenge facing the world, from climate change to great power competition to risks from AI. One way to approach the question is to consider the long-run history of economic growth. I decided to investigate the Hyperbolic Growth Hypothesis: the claim that, from at least the start of the Neolithic Revolution up until the 20th century, the economic growth rate has tended to rise in proportion with the size of the global economy.[2] This claim is made in a classic 1993 paper by Michael Kremer. Beyond influencing other work in economic growth theory, it has also recently attracted significant attention within the longtermist community, where it is typically regarded as evidence in favor of further acceleration.[3] An especially notable property of the hypothesized growth trend is that, if it had continued without pause, it would have produced infinite growth rates in the early twenty-first century. I spent time exploring several different datasets that can be used to estimate pre-modern growth rates. This included a number of recent archeological datasets that, I believe, have not previously been analyzed by economists. I wanted to evaluate both: (a) how empirically well-grounded these estimates are and (b) how clearly these estimates display the hypothesized pattern of growth. Ultimately, I found very little empirical support for the Hyperbolic Growth Hypothesis. While we can confidently say that the econo",2020,2022-01-30 4:53:09,2022-01-30 4:53:09,2020-12-19 2:01:14,,,,,,,Does Economic History Point Toward a Singularity?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/83G76TFA/does-economic-history-point-toward-a-singularity.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CT8AEDQP,blogPost,2020,"Clarke, Sam",Clarifying “What failure looks like” (part 1),AI Alignment Forum,,,,https://www.alignmentforum.org/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1,"Thanks to Jess Whittlestone, Daniel Eth, Shahar Avin, Rose Hadshar, Eliana Lorch, Alexis Carlier, Flo Dorner, Kwan Yee Ng, Lewis Hammond, Phil Trammell and Jenny Xiao for valuable conversations, feedback and other support. I am especially grateful to Jess Whittlestone for long conversations and detailed feedback on drafts, and her guidance on which threads to pursue and how to frame this post. All errors are my own. Epistemic status: My Best Guess Epistemic effort: ~70 hours of focused work (mostly during FHI’s summer research fellowship), talked to ~10 people. INTRODUCTION “What failure looks like” is the one of the most comprehensive pictures of what failure to solve the AI alignment problem looks like, in worlds without discontinuous progress in AI. I think it was an excellent and much-needed addition to our understanding of AI risk. Still, if many believe that this is a main source of AI risk, I think it should be fleshed out in more than just one blog post. The original story has two parts; I’m focusing on part 1 because I found it more confusing and nebulous than part 2. Firstly, I’ll summarise part 1 (hereafter “WFLL1”) as I understand it:  * In the world today, it’s easier to pursue easy-to-measure goals than    hard-to-measure goals.          * Machine learning is differentially good at pursuing easy-to-measure goals    (assuming that we don’t have a satisfactory technical solution to the intent    alignment problem[1]).          * We’ll try to harness this by designing easy-to-measure proxies for what we    care about, and deploy AI systems across society which optimize for these    proxies (e.g. in law enforcement, legislation and the market).          * We’ll give these AI systems more and more influence (e.g. eventually, the    systems running law enforcement may actually be making all the decisions for    us).          * Eventually, the proxies for which the AI systems are optimizing will come    apart from the goals we truly care about, but by t",2020,2022-01-30 4:53:09,2022-01-30 4:53:09,2020-12-19 1:26:57,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNJW49Z2/clarifying-what-failure-looks-like-part-1.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NGN63ZPE,blogPost,2020,"Armstrong, Stuart",Dynamic inconsistency of the inaction and initial state baseline,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/w8QBmgQwb83vDMXoz/dynamic-inconsistency-of-the-inaction-and-initial-state,"Vika has been posting about various baseline choices for impact measure. In this post, I'll argue that the stepwise inaction baseline is dynamically inconsistent/time-inconsistent. Informally, what this means is that an agent will have different preferences from its future self. LOSSES FROM TIME-INCONSISTENCY Why is time-inconsistency bad? It's because it allows money-pump situations: the environment can extract free reward from the agent, to no advantage to that agent. Or, put more formally:  * An agent A is time-inconsistent between times t and t′>t, if at time t it    would pay a positive amount of reward to constrain its possible choices at    time t′. Outside of anthropics and game theory, we expect our agent to be time-consistent. TIME INCONSISTENCY EXAMPLE Consider the following example: The robot can move in all four directions - N, E, S, W - and can also take the noop operation, ∅. The discount rate is γ<1. It gets a reward of r>0 for standing on the blue button for the first time. Using attainable utility preservation, the penalty function is defined by the auxiliary set R; here, this just consists of the reward function that gives p>0  for standing on the red button for the first time. Therefore if the robot moves from a point n steps away from the red button, to one m steps away, it gets a penalty[1] of p|γn−γm| - the difference between the expected red-button rewards for an optimiser in both positions. TWO PATHS It's pretty clear there are two potentially optimal paths the robot can take: going straight to the blue button (higher reward, but higher penalty), or taking the long way round (lower reward, but lower penalty): Fortunately, when summing up the penalties, you sum terms like …p|γn−1−γn|+p|γn− γn+1|…, so a lot of the terms cancel. Thus for the short route, the reward is r⋅γ8 (distance of eight to the blue button) and the penalty is 2p(γ3−γ7) (closest to the red button: 3 squares, furthest: 7 squares). For the long route, the rewar",2020-07-07,2022-01-30 4:53:09,2022-01-30 4:53:09,2020-08-28 17:57:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BUEVDX9W/dynamic-inconsistency-of-the-inaction-and-initial-state.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJ2W3RP3,blogPost,2020,"O'Keefe, Cullen",AI Benefits Blog Series Index,Cullen O'Keefe,,,,https://cullenokeefe.com/ai-benefits-index,,2020,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-08-28 17:33:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DPU8AJFK/ai-benefits-index.html,,MetaSafety; FHI; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I286AT97,blogPost,2020,"Armstrong, Stuart",ACDT: a hack-y acausal decision theory,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/9m2fzjNSJmd3yxxKG/acdt-a-hack-y-acausal-decision-theory,"Inspired by my post on problems with causal decision theory (CDT), here is a hacked version of CDT that seems to be able to imitate timeless decision theory  (TDT) and functional decision theory[1] (FDT), as well as updateless decision theory (UDT) under certain circumstances. Call this ACDT, for (a)causal decision theory. It is, essentially, CDT which can draw extra, acausal arrows on the causal graphs, and which attempts to figure out which graph represents the world it's in. The drawback is its lack of elegance; the advantage, if it works, is that it's simple to specify and focuses attention on the important aspects of deducing the graph. DEFINING ACDT CDT AND THE NEWCOMB PROBLEM In the Newcomb problem, there is a predictor Ω who leaves two boxes, and predicts whether you will take one (""one-box"") or both (""two-box""). If Ω  predicts you will one-box, it had put a large prize in that first box; otherwise that box is empty. There is always a small consolation prize in the second box. In terms of causal graphs, we can represent it this way: The dark red node is the decision node, which the agent can affect. The green node is a utility node, whose value the agent cares about. The CDT agent uses the ""do"" operator from Pearl's Causality. Essentially all the incoming arrows to the decision node are cut (though the CDT agent keeps track of any information gained that way), then the CDT agent maximises its utility by choosing its action: In this situation, the CDT agent will always two-box, since it treats Ω's decision as fixed, and in that case two-boxing dominates, since you get whatever's in the first box, plus the consolation prize. ACDT ALGORITHM The ACDT algorithm is similar, except that when it cuts the causal links to its decision, it also adds potential links from that decision node to all the other nodes in the graph. Then it attempts to figure out which diagram is correct, and  then maximises its utility in the CDT way. Note that ACDT doesn't take a",2020-01-15,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-09-07 18:28:38,,,,,,,ACDT,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BXKXPA7M/acdt-a-hack-y-acausal-decision-theory.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGDHWJSI,blogPost,2015,"Armstrong, Stuart",A toy model of the control problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7cXBoDQ6udquZJ89c/a-toy-model-of-the-control-problem,"EDITED based on suggestions for improving the model Jaan Tallinn has suggested creating a toy model of the control problem, so that it can be analysed without loaded concepts like ""autonomy"", ""consciousness"", or ""intentionality"". Here a simple (too simple?) attempt: A CONTROLS B. B MANIPULATES A. Let B be a robot agent that moves in a two dimensional world, as follows: B can push the yellow blocks around. It was programmed to want to push blocks into the black ""hole"" in the bottom right. The hole can contain arbitrarily many blocks, and B gets a penalty of some tiny ε for every step is takes. The programmer wanted B to simply move one box into the hole (for a reward of 1 once the task is complete), and so programmed it to receive a reward of 1 (and no more) if the box was in the hole, after the whole task was completed and B was turned off. Unfortunately, B was programmed to operate in a noisy environment, so B actually has a probability of 0.99 of receiving the reward, for a given block in the hole. This means that if it pushes one block into the hole, it gets the reward with 0.99 probability; but if it pushes two blocks into the hole, it gets the reward with 1-(1-0.99)2 = 0.9999 probability. If it pushes all six blocks into the hole, it gets a reward with probability 0.999999999999. Therefore this tiny error has motivated B to try and push all blocks into the hole. However, the designers were aware of the possibility of B misbehaving or there being a bug, and created agent A, a simple agent watching the situation. A's aim is to get robot B to to push one block into the hole, and stop there. It can turn B off. Its input sensor is the camera in the bottom left. This camera can see across the bottom row of squares (marked in light green), into the hole. Its algorithm is to turn B off as soon as it sees a block in the hole. The intended outcome is that B pushes a block into the hole, the camera sees this, and A turns B off: If we see B as using a model to",2015,2022-01-30 4:53:07,2022-01-30 4:53:07,2020-11-21 17:51:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BKUNNCN7/a-toy-model-of-the-control-problem.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ESS6IHRN,blogPost,2021,DeepMind Safety Research,What mechanisms drive agent behaviour?,Deep Mind Safety Research (Medium),,,,https://deepmindsafetyresearch.medium.com/what-mechanisms-drive-agent-behaviour-e7b8d9aee88,"By the Safety Analysis Team: Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus…",2021-03-09,2022-01-30 4:52:49,2022-01-30 4:52:49,2021-11-14 16:12:41,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M9564MZ4/what-mechanisms-drive-agent-behaviour-e7b8d9aee88.html,,TechSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BDIVSQPU,blogPost,2020,"Krakovna, Victoria",Tradeoff between desirable properties for baseline choices in impact measures,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2020/07/05/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures/,"Impact measures are auxiliary rewards for low impact on the agent’s environment, used to address the problems of side effects and instrumental convergence. A key component of an impact measur…",2020-07-05,2022-01-30 4:52:49,2022-01-30 4:52:49,2020-08-28 17:56:33,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NFC6DSH6/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3H2MMMH,blogPost,2019,"Ngo, Richard",Technical AGI safety research outside AI,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safety-research-outside-ai,"I think there are many questions whose answers would be useful for technical AGI safety research, but which will probably require expertise outside AI to answer. In this post I list 30 of them, divided into four categories. Feel free to get in touch if you’d like to discuss these questions and why I think they’re important in more detail. I personally think that making progress on the ones in the first category is particularly vital, and plausibly tractable for researchers from a wide range of academic backgrounds. Studying and understanding safety problems  1. How strong are the economic or technological pressures towards building very     general AI systems, as opposed to narrow ones? How plausible is the CAIS     model [https://www.fhi.ox.ac.uk/reframing/] of advanced AI capabilities     arising from the combination of many narrow services?  2. What are the most compelling arguments for and against discontinuous     [https://intelligence.org/files/IEM.pdf] versus continuous     [https://sideways-view.com/2018/02/24/takeoff-speeds/] takeoffs? In     particular, how should we think about the analogy from human evolution, and     the scalability of intelligence with compute?  3. What are the tasks via which narrow AI is most likely to have a     destabilising impact on society? What might cyber crime look like when many     important jobs have been automated?  4. How plausible are safety concerns about economic dominance by     influence-seeking agents     [https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom]     , as well as structural loss of control     [https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure]      scenarios? Can these be reformulated in terms of standard economic ideas,     such as principal-agent problems     [http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html]      and the effects of automation?  5. How can we make the concepts of agency and goal-directed behavio",2019,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:26:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6ITAKVVB/technical-agi-safety-research-outside-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2G3EWT42,blogPost,2018,"Krakovna, Victoria",Specification gaming examples in AI,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/,"Update: for a more detailed introduction to specification gaming, check out the DeepMind Safety Research blog post! Various examples (and lists of examples) of unintended behaviors in AI systems ha…",2018-04-01,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-12-13 23:12:51,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000020,,/Users/jacquesthibodeau/Zotero/storage/4FI7MHVZ/specification-gaming-examples-in-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8D5IZVSJ,blogPost,2019,"Kumar, Ramana; Garrabrant, Scott",Thoughts on Human Models,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models,"Human values and preferences are hard to specify, especially in complex domains. Accordingly, much AGI safety research has focused on approaches to AGI design that refer to human values and preferences indirectly, by learning a model that is grounded in expressions of human values (via stated preferences, observed behaviour, approval, etc.) and/or real-world processes that generate expressions of those values. There are additionally approaches aimed at modelling or imitating other aspects of human cognition or behaviour without an explicit aim of capturing human preferences (but usually in service of ultimately satisfying them). Let us refer to all these models as human models. In this post, we discuss several reasons to be cautious about AGI designs that use human models. We suggest that the AGI safety research community put more effort into developing approaches that work well in the absence of human models, alongside the approaches that rely on human models. This would be a significant addition to the current safety research landscape, especially if we focus on working out and trying concrete approaches as opposed to developing theory. We also acknowledge various reasons why avoiding human models seems difficult. PROBLEMS WITH HUMAN MODELS To be clear about human models, we draw a rough distinction between our actual preferences (which may not be fully accessible to us) and procedures for evaluating our preferences. The first thing, actual preferences, is what humans actually want upon reflection. Satisfying our actual preferences is a win. The second thing, procedures for evaluating preferences, refers to various proxies for our actual preferences such as our approval, or what looks good to us (with necessarily limited information or time for thinking). Human models are in the second category; consider, as an example, a highly accurate ML model of human yes/no approval on the set of descriptions of outcomes. Our first concern, described below, is about overfit",2019-02-21,2022-01-30 4:52:48,2022-01-30 4:52:48,2021-02-06 18:50:50,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/FQ4294HF/thoughts-on-human-models.html; /Users/jacquesthibodeau/Zotero/storage/7BIUUJNZ/thoughts-on-human-models.html,,MetaSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ZWTEI44,blogPost,2019,"Sutton, Rich",The Bitter Lesson,Incomplete Ideas,,,,http://www.incompleteideas.net/IncIdeas/BitterLesson.html,,2019,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:26:51,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000085,,/Users/jacquesthibodeau/Zotero/storage/K957ZVDR/BitterLesson.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
732FQ6KE,blogPost,2020,"Krakovna, Victoria; Uesato, Jonathan; Mikulik, Vladimir; Rahtz, Matthew; Everitt, Tom; Kumar, Ramana; Kenton, Zachary; Leike, Jan; Legg, Shane",Specification gaming: the flip side of AI ingenuity,Deepmind,,,,deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity,"Specification gaming is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of King Midas and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification.",2020-04-21,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-09-05 17:00:34,,,,,,,Specification gaming,,,,,,,ALL,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HJI2JMT3,blogPost,2018,"Ngo, Richard",Some cruxes on impactful alternatives to AI policy work,LessWrong,,,,https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work,"Ben Pace and I (Richard Ngo) recently did a public double crux at the Berkeley REACH on how valuable it is for people to go into AI policy and strategy work: I was optimistic and Ben was pessimistic. During the actual event, we didn't come anywhere near to finding a double crux on that issue. But after a lot of subsequent discussion, we've come up with some more general cruxes about where impact comes from. I found Ben's model of how to have impact very interesting, and so in this post I've tried to explain it, along with my disagreements. Ben liked the goal of writing up a rough summary of our positions and having further discussion in the comments, so while he edited it somewhat he doesn’t at all think that it’s a perfect argument, and it’s not what he’d write if he spent 10 hours on it. He endorsed the wording of the cruxes as broadly accurate. (During the double crux, we also discussed how the heavy-tailed worldview applies to community building, but decided on this post to focus on the object level of what impact looks like.) Note from Ben: “I am not an expert in policy, and have not put more than about 20-30 hours of thought into it total as a career path. But, as I recently heard Robin Hanson say, there’s a common situation that looks like this: some people have a shiny idea that they think about a great deal and work through the details of, that folks in other areas are skeptical of given their particular models of how the world works. Even though the skeptics have less detail, it can be useful to publicly say precisely why they’re skeptical.  In this case I’m often skeptical when folks tell me they’re working to reduce x-risk by focusing on policy. Folks doing policy work in AI might be right, and I might be wrong, but it seemed like a good use of time to start a discussion with Richard about how I was thinking about it and what would change my mind. If the following discussion causes me to change my mind on this question, I’ll be really super happy wit",2018,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-12-13 23:27:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IWG3WG8N/some-cruxes-on-impactful-alternatives-to-ai-policy-work.html,,MetaSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUJCZ9TG,blogPost,2015,"Krakovna, Victoria",Risks from general artificial intelligence without an intelligence explosion,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2015/11/29/ai-risk-without-an-intelligence-explosion/,"“An ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behin…",2015-11-30,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-11-21 16:55:53,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QSBISDWW/ai-risk-without-an-intelligence-explosion.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCIE3RBG,blogPost,2020,"Krakovna, Victoria",Possible takeaways from the coronavirus pandemic for slow AI takeoff,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/wTKjRFeSjKLDSWyww/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai,"Epistemic status: fairly speculative, would appreciate feedback As the covid-19 pandemic unfolds, we can draw lessons from it for managing future global risks, such as other pandemics, climate change, and risks from advanced AI. In this post, I will focus on possible implications for AI risk. For a broader treatment of this question, I recommend FLI's covid-19 page that includes expert interviews on the implications of the pandemic for other types of risks.  A key element in AI risk scenarios is the speed of takeoff - whether advanced AI is developed gradually or suddenly. Paul Christiano's post on takeoff speeds  defines slow takeoff in terms of the economic impact of AI as follows: ""There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles."" It argues that slow AI takeoff is more likely than fast takeoff, but is not necessarily easier to manage, since it poses different challenges, such as large-scale coordination. This post expands on this point by examining some parallels between the coronavirus pandemic and a slow takeoff scenario. The upsides of slow takeoff include the ability to learn from experience, act on warning signs, and reach a timely consensus that there is a serious problem. I would argue that the covid-19 pandemic had these properties, but most of the world's institutions did not take advantage of them. This suggests that, unless our institutions improve, we should not expect the slow AI takeoff scenario to have a good default outcome.   1. Learning from experience. In the slow takeoff scenario, general AI is     expected to appear in a world that has already experienced transformative     change from less advanced AI, and institutions will have a chance to learn     from problems with these AI systems. An analogy could be made with learning     from dealing with less ""advanced"" epidemics like SARS that were not as     successful as covid-19 at spreading across the worl",2020-05-31,2022-01-30 4:52:47,2022-01-30 4:52:47,2020-08-31 18:12:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6W9S9Z9G/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9UHP8WTZ,blogPost,2021,"Everitt, Tom; Carey, Ryan; Hammond, Lewis; Fox, James; Langlois, Eric; Legg, Shane",Progress on Causal Influence Diagrams,Medium,,,,https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1,"By Tom Everitt, Ryan Carey, Lewis Hammond, James Fox, Eric Langlois, and Shane Legg",2021-08-11,2022-01-30 4:52:47,2022-01-30 4:52:47,2021-11-14 19:06:48,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GIGW9EBD/progress-on-causal-influence-diagrams-a7a32180b0d1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CECMW3NM,blogPost,2020,"Aguirre, Anthony",Why those who care about catastrophic and existential risk should care about autonomous weapons,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/oR9tLNRSAep293rr5/why-those-who-care-about-catastrophic-and-existential-risk-2,"(crossposted to Lesswrong here.) Although I have not seen the argument made in any detail or in writing, I and the Future of Life Institute (FLI) have gathered the strong impression that parts of the effective altruism ecosystem are skeptical of the importance of the issue of autonomous weapons systems. This post explains why we think those interested in avoiding catastrophic and existential risk, especially risk stemming from emerging technologies, may want to have this issue higher on their list of concerns. We will first define some terminology and do some disambiguation, as there are many classes of autonomous weapons that are often conflated; all classes have some issues of concern, but some are much more problematic than others. We then detail three basic motivations for research, advocacy, coordination, and policymaking around the issue:  1. Governance of autonomous weapon systems is a dry-run, and precedent, for     governance of AGI. In the short term, AI-enabled weapons systems will share     many of the technical weaknesses and shortcomings of other AI systems, but     like general AI also raise safety concerns that are likely to increase      rather than decrease with capability advances. The stakes are intrinsically     high (literally life-or-death), and the context is an inevitably adversarial     one involving states and major corporations. The sort of global coordination     amongst potentially adversarial parties that will be required for governance     of transformative/general AI systems will not arise from nowhere, and     autonomous weapons offer an invaluable precedent and arena in which to build     experience, capability, and best practices.  2. Some classes of lethal autonomous weapon systems constitute scalable weapons     of mass destruction (which may also have a much lower threshold for first     use or accidental escalation), and hence a nascent catastrophic risk.  3. By increasing the probability of the initiation and/or escalation",2020-11-11,2022-01-30 4:53:38,2022-01-30 4:53:38,2020-12-19 4:18:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DGBAP7KK/why-those-who-care-about-catastrophic-and-existential-risk.html; /Users/jacquesthibodeau/Zotero/storage/K2N72EX7/why-those-who-care-about-catastrophic-and-existential-risk-2.html,,MetaSafety; AmbiguosSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G86SMTN9,blogPost,2019,"Krakovna, Victoria",ICLR Safe ML Workshop Report,Future of Life Institute,,,,https://futureoflife.org/2019/06/18/iclr-safe-ml-workshop-report/,Victoria Krakovna co-organized the 2019 ICLR Safe ML workshop. One of the main goals was to bring together near and long term safety research communities.,2019-06-18,2022-01-30 4:53:38,2022-01-30 4:53:38,2020-12-14 23:28:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X9EEWW4D/iclr-safe-ml-workshop-report.html,,TechSafety; DeepMind; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QQ2PGVQJ,blogPost,2015,"Sandberg, Anders","We, Borg: Speculations on hive minds as a posthuman state",aleph.se,,,,,,2015,2022-01-30 4:53:37,2022-01-30 4:53:37,,,,,,,,"We, Borg",,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 7,,,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FSXDIQ42,blogPost,2017,"Bostrom, Nick",Transhumanist FAQ 3.0,Humanity +,,,,https://humanityplus.org/philosophy/transhumanist-faq/,,2017,2022-01-30 4:53:37,2022-01-30 4:53:37,,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000013,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PTTGW3VK,blogPost,2020,"Armstrong, Stuart","Subagents and impact measures, full and fully illustrated",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated,"0. INTRODUCTION: WHY YET ANOTHER POST ABOUT SUBAGENTS? I’ve recently been writing a sequence on how subagents can undermine impact penalties such as attainable utility preservation. I’m not happy with that sequence; it’s messy and without examples (apart from its first post), people didn’t understand it, and it suffers from the fact that I discovered key ideas as I went along. So I’ve combined everything there into a single post, explained with examples and an abundance of pictures. Hopefully an over- rather than an under-abundance of pictures. Of the original sequence, I've only kept the mathematical results  of this post and the initial example post which has a clearer example of ""high power"" for a subagent. This post here is laid out in a way that makes logical sense, but might not be the clearest for people unfamiliar with the area. For those people, I recommend skipping section 2 initially, and returning to it later. But, whatever you do, make sure you glance at 6.1 and 6.2 before leaving. 1. THE WORLD Our fearless agent A moves around in a gridworld: Each turn, A can move ones square horizontally or vertically. It can also manipulate objects in the eight squares around it, allowing it to, not incidentally, assemble the three pieces to its west into an subagent SA. The robot can also do the noop action, ∅, which does nothing, and it can speak. The subagent, when assembled, has the same action set available. Its positive reward, the one it wants to increase, is R0. To get this reward, a robot needs to move onto the blue button in the east; R0 will give a reward of 1  the first time this happens (and 0 before and after). The discount factor is 0<γ <1. Just to the west of the blue button is a one-way door. Robots can move east through it, but cannot move west through it: 1.1 THE IMPACT REWARD The impact penalty is supposed to ensure that A does not make too many change in the world, and keeps it similar, in some senses, to a specific baseline world. I",2020-02-24,2022-01-30 4:53:35,2022-01-30 4:53:35,2020-09-05 18:47:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/A6XJUF5U/subagents-and-impact-measures-full-and-fully-illustrated.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XE3FX4MZ,blogPost,2019,"Armstrong, Stuart",Research Agenda v0.9: Synthesising a human's preferences into a utility function,LessWrong,,,,https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into,"I'm now in a position where I can see a possible route to a safe/survivable/friendly Artificial Intelligence being developed. I'd give a 10+% chance of it being possible this way, and a 95% chance that some of these ideas will be very useful for other methods of alignment. So I thought I'd encode the route I'm seeing as research agenda; this is the first public draft of it. Clarity, rigour, and practicality: that's what this agenda needs. Writing this agenda has clarified a lot of points for me, to the extent that some of it now seems, in retrospect, just obvious and somewhat trivial - ""of course that's the way you have to do X"". But more clarification is needed in the areas that remain vague. And, once these are clarified enough for humans to understand, they need to be made mathematically and logically rigorous - and ultimately, cashed out into code, and tested and experimented with. So I'd appreciate any comments that could help with these three goals, and welcome anyone interested in pursuing research along these lines over the long-term. Note: I periodically edit this document, to link it to more recent research ideas/discoveries. 0 THE FUNDAMENTAL IDEA This agenda fits itself into the broad family of Inverse [https://ai.stanford.edu/~ang/papers/icml00-irl.pdf] Reinforcement [https://arxiv.org/abs/1606.03137] Learning [https://www.youtube.com/watch?v=Ts-nTIYDXok]: delegating most of the task of inferring human preferences to the AI itself. Most of the task, since it's been shown that humans need to build the right assumptions into the AI, or else the preference learning will fail [https://arxiv.org/abs/1712.05812]. To get these ""right assumptions"", this agenda will look into what preferences actually are, and how they may be combined together. There are hence four parts to the research agenda:  1. A way of identifying the (partial[1] [#fn-wPj8aGxtWBoDNTAof-1]) preferences     of a given human H.  2. A way for ultimately synthesising a utility function UH",2019,2022-01-30 4:53:34,2022-01-30 4:53:34,2019-12-16 22:32:49,,,,,,,Research Agenda v0.9,,,,,,,,,,,,,,ZSCC: NoCitationData[s6]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ASUSJMZZ/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html; /Users/jacquesthibodeau/Zotero/storage/QNPQFGW3/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AHS635XH,blogPost,2020,"O'Keefe, Cullen",Parallels Between AI Safety by Debate and Evidence Law,Cullen O'Keefe,,,,https://cullenokeefe.com/blog/debate-evidence,,2020-07-20,2022-01-30 4:53:19,2022-01-30 4:53:19,2020-08-28 17:19:24,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GEGGQ6BR/debate-evidence.html,,MetaSafety; FHI; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V7S6UBMU,blogPost,2020,"Armstrong, Stuart",Model splintering: moving from one imperfect model to another,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1,"1. THE BIG PROBLEM In the last few months, I've become convinced that there is a key meta-issue in AI safety; a problem that seems to come up in all sorts of areas. It's hard to summarise, but my best phrasing would be:  * Many problems in AI safety seem to be variations of ""this approach seems safe    in this imperfect model, but when we generalise the model more, it becomes    dangerously underdefined"". Call this model splintering.  * It is intrinsically worth studying how to (safely) transition from one    imperfect model to another. This is worth doing, independently of whatever    ""perfect"" or ""ideal"" model might be in the background of the imperfect    models. This sprawling post will be presenting examples of model splintering, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to. 1.1 IN THE LANGUAGE OF TRADITIONAL ML In the language of traditional ML, we could connect all these issues to "" out-of-distribution"" behaviour. This is the problems that algorithms encounter when the set they are operating on is drawn from a different distribution than the training set they were trained on. Humans can often see that the algorithm is out-of-distribution and correct it, because we have a more general distribution in mind than the one the algorithm was trained on. In these terms, the issues of this post can be phrased as:  1. When the AI finds itself mildly out-of-distribution, how best can it extend     its prior knowledge to the new situation?  2. What should the AI do if it finds itself strongly out-of-distribution?  3. What should the AI do if it finds itself strongly out-of-distribution, and     humans don't know the correct distribution either? 1.2 MODEL SPLINTERING EXAMPLES Let's build a more general framework. Say that you start with some brilliant idea for AI safety/alignment/effectiveness. This idea is phrased in some (imperfect) model. Then ""model splintering"" happens when you or the AI",2020-08-27,2022-01-30 4:53:19,2022-01-30 4:53:19,2020-09-07 18:35:31,,,,,,,Model splintering,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G43HNSF6/model-splintering-moving-from-one-imperfect-model-to-another-1.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQ8FTBHJ,blogPost,2018,"Evans, Owain; Steinhardt, Jacob",Model Mis-specification and Inverse Reinforcement Learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: While I motivated the last post with an example of using a specific model for human biases, in this post (original here), Jacob Steinhardt and Owain Evans point out that model mis-specification can arise in other parts of inverse reinforcement learning as well. The arguments here consider some more practical concerns (for example, the worries about getting only short-term data for each human would not be a problem if you had the entire human policy). -------------------------------------------------------------------------------- In my previous post, “Latent Variables and Model Mis-specification”, I argued that while machine learning is good at optimizing accuracy on observed signals, it has less to say about correctly inferring the values for unobserved variables in a model. In this post I’d like to focus in on a specific context for this: inverse reinforcement learning (Ng et al. 2000, Abbeel et al. 2004, Ziebart et al. 2008, Ho et al 2016), where one observes the actions of an agent and wants to infer the preferences and beliefs that led to those actions. For this post, I am pleased to be joined by Owain Evans, who is an active researcher in this area and has co-authored an online book about building models of agents (see here in particular for a tutorial on inverse reinforcement learning and inverse planning). Owain and I are particularly interested in inverse reinforcement learning (IRL) because it has been proposed (most notably by Stuart Russell) as a method for learning human values in the context of AI safety; among other things, this would eventually involve learning and correctly implementing human values by artificial agents that are much more powerful, and act with much broader scope, than any humans alive today. While we think that overall IRL is a promising route to consider, we believe that there are also a number of non-obvious pitfalls related to performing IRL",2018,2022-01-30 4:53:19,2022-01-30 4:53:19,2020-12-17 4:36:25,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KBV5ZKMW/cnC2RMWEGiGpJv8go.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GKG6XR6Z,blogPost,2020,"Armstrong, Stuart",Predictors exist: CDT going bonkers... forever,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever,"I've been wanting to get a better example of CDT (causal decision theory) misbehaving, where the behaviour is more clearly suboptimal than it is in the  Newcomb problem (which many people don't seem to accept as CDT being suboptimal), and simpler to grasp than Death in Damascus. THE ""PREDICTORS EXIST"" PROBLEM So consider this simple example: the player is playing against Omega, who will predict their actions[1]. The player can take three actions: ""zero"", ""one"", or ""leave"". If ever they do ""leave"", then the experiment is over and they leave. If they choose ""zero"" or ""one"", then Omega will predict their action, and compare this to their actual action. If the two match, then the player loses 1 utility and the game repeats; if the action and the prediction differs, then the player gains 3 utility and the experiment ends. Assume that actually Omega is a perfect or quasi-perfect predictor, with a good model of the player. An FDT or EDT agent would soon realise that they couldn't trick Omega, after a few tries, and would quickly end the game. But the CDT player would be incapable of reaching this reasoning. Whatever distribution they compute over Omega's prediction, they will always estimate that they (the CDT player) have at least a 50% chance of choosing the other option[2], for an expected utility gain of at least 0.5(3)+0.5(−1)=1. Basically, the CDT agent can never learn that Omega is a good predictor of themselves[3]. And so they will continue playing, and continue losing... for ever. --------------------------------------------------------------------------------  1. Omega will make this prediction not necessarily before the player takes     their action, not even necessarily without seeing this action, but still     makes the prediction independently of this knowledge. And that's enough for     CDT. ↩︎            2. For example, suppose the CDT agent estimates the prediction will be ""zero""     with probability p, and ""one"" with probability 1-p. Then if p≥1/2",2020-01-14,2022-01-30 4:53:19,2022-01-30 4:53:19,2020-09-07 18:27:36,,,,,,,Predictors exist,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GDCP95WU/predictors-exist-cdt-going-bonkers-forever.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U4AWP6AV,blogPost,2020,"Nguyen, Chi",My Understanding of Paul Christiano's Iterated Amplification AI Safety Research Agenda,AI Alignment Forum,,,,https://www.lesswrong.com/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification,"Crossposted from the EA forum You can read this post as a google docs instead (IMO much better to read). This document aims to clarify the AI safety research agenda by Paul Christiano (IDA) and the arguments around how promising it is. Target audience: All levels of technical expertise. The less knowledge about IDA someone has, the more I expect them to benefit from the writeup. Writing policy: I aim to be as clear and concrete as possible and wrong rather than vague to identify disagreements and where I am mistaken. Things will err on the side of being too confidently expressed. Almost all footnotes are content and not references. Epistemic Status: The document is my best guess on IDA and might be wrong in important ways. I have not verified all of the content with somebody working on IDA. I spent ~4 weeks on this and have no prior background in ML, CS or AI safety. I wrote this document last summer (2019) as part of my summer research fellowship at FHI. I was planning to restructure, complete and correct it since but haven’t gotten to it for a year, so decided to just publish it as it is. The document has not been updated, i.e. nothing that has been released since September 2019 is incorporated into this document. Paul Christiano generously reviewed part of this summary. I added his comments verbatim in the document. Apologies for the loss of readability due to this. This doesn’t imply he endorses any part of this document. PURPOSE OF THIS DOCUMENT: CLARIFYING IDA IDA is Paul Christiano’s AI safety research agenda.[1] Christiano works at OpenAI which is one of the main actors in AI safety and IDA is by many considered the most complete[2] AI safety agenda. However, people who are not directly working on IDA are often confused about how exactly to understand the agenda. Clarifying IDA would make it more accessible  for technical people to work on and easier to assess for nontechnical people who want to think about its implications. I believe that there are",2020-08-15,2022-01-30 4:53:19,2022-01-30 4:53:19,2020-08-24 20:21:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EIQBMQPS/my-understanding-of-paul-christiano-s-iterated-amplification.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIU789GM,blogPost,2020,"Armstrong, Stuart",If I were a well-intentioned AI... I: Image classifier,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier,"INTRODUCTION: IF I WERE A WELL-INTENTIONED AI... I've often warned people about the dangers of anthropomorphising AIs - how it can mislead us about what's really going on in an AI (and hence how the AI might act in the future), cause us to not even consider certain failure modes, and make us believe we understand things much better than we do. Oh well, let's ignore all that. I'm about to go on a journey of major anthropomorphisation, by asking myself:  * ""If I was a well-intentioned AI, could I solve many of the problems in AI    alignment?"" My thinking in this way started when I wondered: suppose I knew that I was given a proxy goal rather than the true goal; suppose that I knew about the Goodhart problem, and suppose that I really ""wanted"" to align with the true goal - could I then do it? I was having similar thoughts about being a mesa-optimiser. It seems to me that asking and answering these kind of questions leads to new and interesting insights. Of course, since they come via anthropomorphisation, we need to be careful with them, and check that they are really applicable to AI systems - ensuring that I'm not bringing some of my own human knowledge about human values into the example. But first, let's get those initial insights. OVERLAPPING PROBLEMS, OVERLAPPING SOLUTIONS At a high enough level of abstraction, many problems in AI alignment seem very similar. The Goodhart problem, the issues machine learning has with  distributional shift, the problem of the nearest unblocked strategy,  unidentifiability of reward functions, even mesaoptimisation and the whole AI alignment problem itself - all of these can be seen, roughly, as variants of the same problem. That problem being that we have an approximately specified goal that looks ok, but turns out to be underspecified in dangerous ways. Of course, often the differences between the problems are as important as the similarities. Nevertheless, the similarities exist, which is why a lot of the solutions are",2020-02-26,2022-01-30 4:53:18,2022-01-30 4:53:18,2020-09-05 18:38:33,,,,,,,If I were a well-intentioned AI... I,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9VWKU6G5/gzWb5kWwzhdaqmyTt.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7R7WMB83,blogPost,2015,"Tomasik, Brian",A Dialogue on Suffering Subroutines,Center on Long-Term Risk,,,,https://longtermrisk.org/a-dialogue-on-suffering-subroutines/,"This piece presents a hypothetical dialogue that explains why instrumental computational processes of a future superintelligence might evoke moral concern. Generally, agent-like components might emerge in many places, including the computing processes of a future civilization. Whether and how much these subroutines matter are questions for future generations to figure out, but it's good to keep an open mind to the possibility that our intuitions about what suffering is may change dramatically.",2015-08-29,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 1:05:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GHRVZSNG/a-dialogue-on-suffering-subroutines.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUMZ3TRE,blogPost,2017,"Oesterheld, Caspar",A behaviorist approach to building phenomenological bridges,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/10/22/a-behaviorist-approach-to-building-phenomenological-bridges/,"A few weeks ago, I wrote about the BPB problem and how it poses a problem for classical/non-logical decision theories. In my post, I briefly mentioned a behaviorist approach to BPB, only to immedia…",2017-10-22,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 0:45:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VEDV2NXS/a-behaviorist-approach-to-building-phenomenological-bridges.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGQMWQFR,blogPost,2015,"Tomasik, Brian",Artificial Intelligence and Its Implications for Future Suffering,Center on Long-Term Risk,,,,https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering/,"Artificial intelligence (AI) will likely transform the world later this century. Whether uncontrolled or controlled AIs would create more suffering in expectation is a question to explore further. Regardless, the field of AI safety and policy seems to be a very important space where altruists can make a positive-sum impact along many dimensions.",2015-04-10,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 1:02:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QD8MQPCV,blogPost,2017,Caspar,A survey of polls on Newcomb’s problem,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/06/27/a-survey-of-polls-on-newcombs-problem/,"One classic story about Newcomb’s problem is that, at least initially, people one-box and two-box in roughly equal numbers (and that everyone is confident in their position). To find out whet…",2017-06-27,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 20:03:16,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EPR7NUV9/a-survey-of-polls-on-newcombs-problem.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2J7P366,blogPost,2015,"Tomasik, Brian",A Lower Bound on the Importance of Promoting Cooperation,Center on Long-Term Risk,,,,https://longtermrisk.org/a-lower-bound-on-the-importance-of-promoting-cooperation/,This article suggests a lower-bound Fermi calculation for the cost-effectiveness of promoting cooperation. The purpose of this exercise is to make our thinking more concrete about how cooperation might reduce suffering and to make its potential more tangible.,2015-08-29,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-11-23 1:04:00,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EPJR83PC/a-lower-bound-on-the-importance-of-promoting-cooperation.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMFTWTV3,blogPost,2021,"Steinhardt, Jacob","Measurement, Optimization, and Take-off Speed",Jacob Steinhardt,,,,https://jsteinhardt.stat.berkeley.edu/blog/measurement-and-optimization,"In machine learning, we are obsessed with datasets and metrics: progress in areas as diverse as natural language understanding, object recognition, and reinforcement learning is tracked by numerical scores on agreed-upon benchmarks. Despite this, I think we focus too little on measurement—that is, on ways of extracting data from machine learning models that bears upon important hypotheses. This might sound paradoxical, since benchmarks are after all one way of measuring a model. However, benchmarks are a very narrow form of measurement, and I will argue below that trying to measure pretty much anything you can think of is a good mental move that is heavily underutilized in machine learning. I’ll argue this in three ways: Historically, more measurement has almost always been a great move, not only in science but also in engineering and policymaking. Philosophically, measurement has many good properties that bear upon important questions in ML. In my own research, just measuring something and seeing what happened has often been surprisingly fruitful.",2021-04-07,2022-01-30 4:50:55,2022-01-30 4:50:55,2021-11-14 19:03:14,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6IF9UF8E/measurement-and-optimization.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSHDXZW8,blogPost,2019,"Shah, Rohin",Learning biases and rewards simultaneously,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously,"I’ve finally uploaded to arXiv our work on inferring human biases alongside IRL, which was published at ICML 2019. SUMMARY OF THE PAPER THE IRL DEBATE Here’s a quick tour of the debate about inverse reinforcement learning (IRL) and cognitive biases, featuring many of the ideas from the first chapter of the  Value Learning sequence: I had the intuition that the impossibility theorem was like the other no-free-lunch theorems in ML: not actually relevant for what ML could do in practice. So we tried to learn and correct for systematic biases in IRL. THE IDEA BEHIND THE ALGORITHMS The basic idea was to learn the planning algorithm by which the human produces demonstrations, and try to ensure that the planning algorithm captured the appropriate systematic biases. We used a Value Iteration Network to give an inductive bias towards “planners” but otherwise did not assume anything about the form of the systematic bias. [1] Then, we could perform IRL by figuring out which reward would cause the planning algorithm to output the given demonstrations. The reward would be “debiased” because the effect of the biases on the policy would already be accounted for in the planning algorithm. How could we learn the planning algorithm? Well, one baseline method is to assume that we have access to some tasks where the rewards are known, and use those tasks to learn what the planning algorithm is. Then, once that is learned, we can infer the rewards for new tasks that we haven’t seen before. This requires the planner to generalize across tasks. However, it’s kind of cheating to assume access to ground truth rewards, since we usually wouldn’t have them. What if we learned the planning algorithm and rewards simultaneously? Well, the no-free-lunch theorem gets us then: maximizing the true reward and minimizing the negative of the true reward would lead to the same policy, and so you can’t distinguish between them, and so the output of your IRL algorithm could be the true reward or the",2019,2022-01-30 4:50:54,2022-01-30 4:50:54,2020-12-18 0:09:48,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9QDS82F6/learning-biases-and-rewards-simultaneously.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFJQBIH5,blogPost,2020,"Turner, Alex",Generalizing the Power-Seeking Theorems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/nyDnLif4cjeRe9DSv/generalizing-the-power-seeking-theorems,"Previously: Seeking Power is Often Provably Instrumentally Convergent in MDPs. Thanks to Rohin Shah, Michael Dennis, Josh Turner, and Evan Hubinger for comments. -------------------------------------------------------------------------------- It sure seems like gaining power over the environment is instrumentally convergent (optimal for a wide range of agent goals). You can turn this into math and prove things about it. Given some distribution over agent goals, we want to be able to formally describe how optimal action tends to flow through the future. Does gaining money tend to be optimal? Avoiding shutdown? When? How do we know? Optimal Farsighted Agents Tend to Seek Power proved that, when you distribute reward fairly and evenly across states (IID), it's instrumentally convergent to gain access to lots of final states (which are absorbing, in that the agent keeps on experiencing the final state). The theorems apply when you don't discount the future (you're ""infinitely farsighted""). Most reward functions for the Pac-Man game incentivize not dying immediately, so that the agent can loop around higher-scoring configurations. Many ways of scoring Tic-Tac-Toe game states incentivize not losing immediately, in order to choose the highest-scoring final configuration. ""All states have self-loops, left hidden to reduce clutter. In AI: A Modern Approach (3e), the agent starts at1and receives reward for reaching3. The optimal policy for this reward function avoids2, and one might suspect that avoiding2is instrumentally convergent. However, a skeptic might provide a reward function for which navigating to2is optimal, and then argue that ""instrumental convergence'' is subjective and that there is no reasonable basis for concluding that2is generally avoided. We can do better... for any way of independently and identically distributing reward over states,1011of reward functions have farsighted optimal policies which avoid2. If we complicate the MDP with additional t",2020-07-26,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-08-28 17:31:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z47Z7M3I/generalizing-the-power-seeking-theorems.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZIU7UJI,blogPost,2018,"Shah, Rohin",Intuitions about goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior,"One broad argument for AI risk is the Misspecified Goal argument:  The Misspecified Goal Argument for AI Risk: Very intelligent AI systems will be able to make long-term plans in order to achieve their goals, and if their goals are even slightly misspecified then the AI system will become adversarial and work against us. My main goal in this post is to make conceptual clarifications and suggest how they affect the Misspecified Goal argument, without making any recommendations about what we should actually do. Future posts will argue more directly for a particular position. As a result, I will not be considering other arguments for focusing on AI risk even though I find some of them more compelling. I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. This is related to the concept of optimization, though with intuitions around optimization we typically assume that we know the agent’s preference ordering, which I don’t want to assume here. (In fact, I don’t want to assume that the agent even has a preference ordering.) One potential formalization is to say that goal-directed behavior is any behavior that can be modelled as maximizing expected utility for some utility function; in the next post I will argue that this does not properly capture the behaviors we are worried about. In this post I’ll give some intuitions about",2018,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-12-17 4:36:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UUQVT4H3/DfcywmqRSkBaCB6Ma.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P6TZ4M98,blogPost,2014,"Dragan, Anca; Srinivasa, Siddhartha",Integrating Human Observer Inferences into Robot Motion Planning,The Robotics Institute Carnegie Mellon University,,,,https://www.ri.cmu.edu/publications/integrating-human-observer-inferences-into-robot-motion-planning/,"Our goal is to enable robots to produce motion that is suitable for human-robot collaboration and co-existence. Most motion in robotics is purely functional, ideal when the robot is performing a task in isolation. In collaboration, however, the robot’s motion has an observer, watching and interpreting the motion. In this work, we move beyond functional …",2014,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-12-18 1:40:11,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000075,,/Users/jacquesthibodeau/Zotero/storage/RZQEJ7CE/integrating-human-observer-inferences-into-robot-motion-planning.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TBKHB3QK,blogPost,2018,"Shah, Rohin",Humans can be assigned any values whatsoever…,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever,"(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In the last post, we saw that a good broad value learning approach would need to understand the systematic biases in human planning in order to achieve superhuman performance. Perhaps we can just use machine learning again and learn the biases and reward simultaneously? This post by Stuart Armstrong (original here) and the associated paper say: “Not without more assumptions.” This post comes from a theoretical perspective that may be alien to ML researchers; in particular, it makes an argument that simplicity priors do not solve the problem pointed out here, where simplicity is based on Kolmogorov complexity (which is an instantiation of the Minimum Description Length principle). The analog in machine learning would be an argument that regularization would not work. The proof used is specific to Kolmogorov complexity and does not clearly generalize to arbitrary regularization techniques; however, I view the argument as being suggestive that regularization techniques would also be insufficient to address the problems raised here. -------------------------------------------------------------------------------- Humans have no values… nor do any agent. Unless you make strong assumptions about their rationality. And depending on those assumptions, you get humans to have any values. AN AGENT WITH NO CLEAR PREFERENCES There are three buttons in this world, B(0), B(1), and X, and one agent H.  B(0) and B(1) can be operated by H, while X can be operated by an outside observer. H will initially press button B(0); if ever X is pressed, the agent will switch to pressing B(1). If X is pressed again, the agent will switch back to pressing B(0), and so on. After a large number of turns N, H will shut off. That’s the full algorithm for H. So the question is, what are the values/preferences/rewards of H? There are three natural reward functions that are plausible:  *  R(0), which is linear i",2018,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-12-17 4:36:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DXQWPWZS/ANupXf8XfZo2EJxGv.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A7KHVPJD,blogPost,2019,"Shah, Rohin",Human-AI Interaction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4783ufKpx8xvLMPc6/human-ai-interaction,"THE IMPORTANCE OF FEEDBACK Consider trying to program a self-driving car to drive from San Francisco to Los Angeles -- with no sensors that allow it to gather information as it is driving. This is possible in principle. If you can predict the exact weather conditions, the exact movement of all of the other cars on the road, the exact amount of friction along every part of the road surface, the exact impact of (the equivalents of) pressing the gas or turning the steering wheel, and so on, then you could compute ahead of time how exactly to control the car such that it gets from SF to LA. Nevertheless, it seems unlikely that we will ever be able to accomplish such a feat, even with powerful AI systems. No, in practice there is going to be some uncertainty about how the world is going to evolve; such that any plan computed ahead of time will have some errors that will compound over the course of the plan. The solution is to use sensors to gather information while executing the plan, so that we can notice any errors or deviations from the plan, and take corrective action. It is much easier to build a controller that keeps you pointed in the general direction, than to build a plan that will get you there perfectly without any adaptation. Control theory studies these sorts of systems, and you can see the general power of feedback controllers in the theorems that can be proven. Especially for motion tasks, you can build feedback controllers that are guaranteed to safely achieve the goal, even in the presence of adversarial environmental forces (that are bounded in size, so you can’t have arbitrarily strong wind). In the presence of an adversary, in most environments it becomes impossible even in principle to make such a guarantee if you do not have any sensors or feedback and must compute a plan in advance. Typically, for every such plan, there is some environmental force that would cause it to fail. THE CONTROL THEORY PERSPECTIVE ON AI ALIGNMENT With ambitious value le",2019,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-12-17 4:37:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HDFKCMPB/4783ufKpx8xvLMPc6.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVTJI96G,blogPost,2018,"Shah, Rohin",Future directions for ambitious value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EhNCnCkmu7MwrQ7yz/future-directions-for-ambitious-value-learning,"To recap the sequence so far:  * Ambitious value learning aims to infer a utility function that is safe to    maximize, by looking at human behavior.  * However, since you only observe human behavior, you must be able to infer and    account for the mistakes that humans make in order to exceed human    performance. (If we don’t exceed human performance, it’s likely that we’ll    use unsafe techniques that do exceed human performance, due to economic    incentives.)  * You might hope to infer both the mistake model (aka systematic human biases)     and the utility function, and then throw away the mistake model and optimize    the utility function. This cannot be done without additional assumptions.  * One potential assumption you could use would be to codify a specific mistake    model. However, humans are sufficiently complicated that any such model would    be wrong, leading to model misspecification. Model misspecification causes     many problems in general, and is particularly thorny for value learning. Despite these arguments, we could still hope to infer a broad utility function that is safe to optimize, either by sidestepping the formalism used so far, or by introducing additional assumptions. Often, it is clear that these methods would not find the true human utility function (assuming that such a thing exists), but they are worth pursuing anyway because they could find a utility function that is good enough. This post provides pointers to approaches that are currently being pursued. Since these are active areas of research, I don’t want to comment on how feasible they may or may not be -- it’s hard to accurately assess the importance and quality of an idea that is being developed just from what is currently written down about that idea. Assumptions about the mistake model. We could narrow down on the mistake model by making assumptions about it, that could let us avoid the impossibility result. This decision means that we’re accepting the risk of missp",2018,2022-01-30 4:50:52,2022-01-30 4:50:52,2020-12-17 4:36:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/J54ABXG9/EhNCnCkmu7MwrQ7yz.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KEDQKKFK,blogPost,2019,"Shah, Rohin",Future directions for narrow value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/MxadmSXHnoCupsWqx/future-directions-for-narrow-value-learning,"Narrow value learning is a huge field that people are already working on (though not by that name) and I can’t possibly do it justice. This post is primarily a list of things that I think are important and interesting, rather than an exhaustive list of directions to pursue. (In contrast, the corresponding post  for ambitious value learning did aim to be exhaustive, and I don’t think I missed much work there.) You might think that since so many people are already working on narrow value learning, we should focus on more neglected areas of AI safety. However, I still think it’s worth working on because long-term safety suggests a particular subset of problems to focus on; that subset seems quite neglected. For example, a lot of work is about how to improve current algorithms in a particular domain, and the solutions encode domain knowledge to succeed. This seems not very relevant for long-term concerns. Some work assumes that a handcoded featurization is given (so that the true reward is linear in the features); this is not an assumption we could make for more powerful AI systems. I will speculate a bit on the neglectedness and feasibility of each of these areas, since for many of them there isn’t a person or research group who would champion them whom I could defer to about the arguments for success. THE BIG PICTURE This category of research is about how you could take narrow value learning algorithms and use them to create an aligned AI system. Typically, I expect this to work by having the narrow value learning enable some form of corrigibility. As far as I can tell, nobody outside of the AI safety community works on this problem. While it is far too early to stake a confident position one way or the other, I am slightly less optimistic about this avenue of approach than one in which we create a system that is directly trained to be corrigible. Avoiding problems with goal-directedness. How do we put together narrow value learning techniques in a way that does",2019,2022-01-30 4:50:52,2022-01-30 4:50:52,2020-12-17 4:37:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/V8NFJVTP/MxadmSXHnoCupsWqx.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49P9KC94,blogPost,2019,"Shah, Rohin",Following human norms,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/eBd6WvzhuqduCkYv3/following-human-norms,"So far we have been talking about how to learn “values” or “instrumental goals”. This would be necessary if we want to figure out how to build an AI system that does exactly what we want it to do. However, we’re probably fine if we can keep learning and building better AI systems. This suggests that it’s sufficient to build AI systems that don’t screw up so badly that it ends this process. If we accomplish that, then steady progress in AI will eventually get us to AI systems that do what we want. So, it might be helpful to break down the problem of learning values into the subproblems of learning what to do, and learning what not to do. Standard AI research will continue to make progress on learning what to do; catastrophe happens when our AI system doesn’t know what not to do. This is the part that we need to make progress on. This is a problem that humans have to solve as well. Children learn basic norms such as not to litter, not to take other people’s things, what not to say in public, etc. As argued in Incomplete Contracting and AI alignment, any contract between humans is never explicitly spelled out, but instead relies on an external unwritten normative structure under which a contract is interpreted. (Even if we don’t explicitly ask our cleaner not to break any vases, we still expect them not to intentionally do so.) We might hope to build AI systems that infer and follow these norms, and thereby avoid catastrophe. It’s worth noting that this will probably not be an instance of narrow value learning, since there are several differences:  * Narrow value learning requires that you learn what to do, unlike norm    inference.  * Norm following requires learning from a complex domain (human society),    whereas narrow value learning can be applied in simpler domains as well.  * Norms are a property of groups of agents, whereas narrow value learning can    be applied in settings with a single agent. Despite this, I have included it in this sequence because it",2019,2022-01-30 4:50:45,2022-01-30 4:50:45,2020-12-17 4:37:25,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EVK4EAGA/eBd6WvzhuqduCkYv3.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MNJW7DRU,blogPost,2020,"Turner, Alex",Corrigibility as outside view,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view,"You run a country. One day, you think ""I could help so many more people if I set all the rules... and I could make this happen"". As far as you can tell, this is the real reason you want to set the rules – you want to help people, and you think you'd do a good job. But historically… in this kind of situation, this reasoning can lead to terrible things. So you just don't do it, even though it feels like a good idea.[1] More generally, Even though my intuition/naïve decision-making process says I should do X, I know (through mental simulation or from history) my algorithm is usually wrong in this situation. I'm not going to do X.  * ""It feels like I could complete this project within a week. But… in the past,    when I've predicted ""a week"" for projects like this, reality usually gives me    a longer answer. I'm not going to trust this feeling. I'm going to allocate    extra time.""  * As a new secretary, I think I know how my boss would want me to reply to an    important e-mail. However, I'm not sure. Even though I think I know what to    do, common sense recommends I clarify.  * You broke up with someone. ""Even though I really miss them, in this kind of    situation, missing my ex isn't a reliable indicator that I should get back    together with them. I'm not going to trust this feeling, and will trust the    ""sober"" version of me which broke up with them."" We are biased and corrupted. By taking the outside view on how our own algorithm performs in a given situation, we can adjust accordingly. CORRIGIBILITY The ""hard problem of corrigibility"" is to build an agent which, in an intuitive sense, reasons internally as if from the programmers' external perspective. We think the AI is incomplete, that we might have made mistakes in building it, that we might want to correct it, and that it would be e.g. dangerous for the AI to take large actions or high-impact actions or do weird new things without asking first. We would ideally want the agent to see itself in",2020-05-08,2022-01-30 4:50:44,2022-01-30 4:50:44,2020-08-31 18:49:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W9SJ2CAM/corrigibility-as-outside-view.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQFTRH3S,blogPost,2019,"Shah, Rohin; Carroll, Micah",Collaborating with Humans Requires Understanding Them,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2019/10/21/coordination/,The BAIR Blog,2019,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-18 0:11:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SE7V6JPX/coordination.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XGBTMKB5,blogPost,2019,"Shah, Rohin",Conclusion to the sequence on value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/TE5nJ882s5dCMkBB8/conclusion-to-the-sequence-on-value-learning,"This post summarizes the sequence on value learning. While it doesn’t introduce any new ideas, it does shed light on which parts I would emphasize most, and the takeaways I hope that readers get. I make several strong claims here; interpret these as my impressions, not my beliefs. I would guess many researchers disagree with the (strength of the) claims, though I do not know what their arguments would be. Over the last three months we’ve covered a lot of ground. It’s easy to lose sight of the overall picture over such a long period of time, so let's do a brief recap. THE “OBVIOUS” APPROACH Here is an argument for the importance of AI safety:  * Any agent that is much more intelligent than us should not be exploitable by    us, since if we could find some way to exploit the agent, the agent could    also find the exploit and patch it.  * Anything that is not exploitable must be an expected utility maximizer; since    we cannot exploit a superintelligent AI, it must look like an expected    utility maximizer to us.  * Due to Goodhart’s Law, even “slightly wrong” utility functions can lead to    catastrophic outcomes when maximized.  * Our utility function is complex and fragile, so getting the “right” utility    function is difficult. This argument implies that by the time we have a superintelligent AI system, there is only one part of that system that could still have been influenced by us: the utility function. Every other feature of the AI system is fixed by math. As a result, we must necessarily solve AI alignment by influencing the utility function. So of course, the natural approach is to get the right utility function, or at least an adequate one, and have our AI system optimize that utility function. Besides fragility of value, which you might hope that machine learning could overcome, the big challenge is that even if you assume full access to the entire human policy, we cannot infer their values without making an assumption about how their preferences r",2019,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-17 4:37:32,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PWJ5IZ83/TE5nJ882s5dCMkBB8.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGFKUHA7,blogPost,2018,"Shah, Rohin",Coherence arguments do not imply goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior,"One of the most pleasing things about probability and expected utility theory is that there are many coherence arguments that suggest that these are the “correct” ways to reason. If you deviate from what the theory prescribes, then you must be executing a dominated strategy. There must be some other strategy that never does any worse than your strategy, but does strictly better than your strategy with certainty in at least one situation. There’s a good explanation of these arguments here. We shouldn’t expect mere humans to be able to notice any failures of coherence in a superintelligent agent, since if we could notice these failures, so could the agent. So we should expect that powerful agents appear coherent to us. (Note that it is possible that the agent doesn’t fix the failures because it would not be worth it -- in this case, the argument says that we will not be able to notice any exploitable failures.) Taken together, these arguments suggest that we should model an agent much smarter than us as an expected utility (EU) maximizer. And many people agree that EU maximizers are dangerous. So does this mean we’re doomed? I don’t think so: it seems to me that the problems about EU maximizers that we’ve identified are actually about goal-directed behavior or explicit reward maximizers. The coherence theorems say nothing about whether an AI system must look like one of these categories. This suggests that we could try building an AI system that can be modeled as an EU maximizer, yet doesn’t fall into one of these two categories, and so doesn’t have all of the problems that we worry about. Note that there are two different flavors of arguments that the AI systems we build will be goal-directed agents (which are dangerous if the goal is even slightly wrong):  * Simply knowing that an agent is intelligent lets us infer that it is    goal-directed. (ETA: See this comment for more details on this argument.)  * Humans are particularly likely to build goal-directed agen",2018,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-17 4:36:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6SJ3UVN8/NxF5G6CJiof6cemTw.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9B3ZIBN5,blogPost,2019,"Cottier, Ben; Shah, Rohin",Clarifying some key hypotheses in AI alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment,"We've created a diagram mapping out important and controversial hypotheses for AI alignment. We hope that this will help researchers identify and more productively discuss their disagreements. DIAGRAM A part of the diagram. Click through to see the full version. CAVEATS  1. This does not decompose arguments exhaustively. It does not include every     reason to favour or disfavour ideas. Rather, it is a set of key hypotheses     and relationships with other hypotheses, problems, solutions, models, etc.     Some examples of important but apparently uncontroversial premises within     the AI safety community: orthogonality, complexity of value, Goodhart's     Curse, AI being deployed in a catastrophe-sensitive context.  2. This is not a comprehensive collection of key hypotheses across the whole     space of AI alignment. It focuses on a subspace that we find interesting and     is relevant to more recent discussions we have encountered, but where key     hypotheses seem relatively less illuminated. This includes rational agency     and goal-directedness, CAIS, corrigibility, and the rationale of     foundational and practical research. In hindsight, the selection criteria     was something like: 1. The idea is closely connected to the problem of         artificial systems optimizing adversarially against humans.      2. The idea must be explained sufficiently well that we         believe it is plausible.            3. Arrows in the diagram indicate flows of evidence or soft relations, not     absolute logical implications — please read the ""interpretation"" box in the     diagram. Also pay attention to any reasoning written next to a Yes/No/Defer     arrow — you may disagree with it, so don't blindly follow the arrow! BACKGROUND Much has been written in the way of arguments for AI risk. Recently there have been some talks and posts that clarify different arguments, point to open questions, and highlight the need for further clarification and analysis. We largely s",2019,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-14 0:17:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VZV4SV4B/clarifying-some-key-hypotheses-in-ai-alignment.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEHQRUAW,blogPost,2018,"Filan, Daniel",Bottle Caps Aren't Optimisers,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers,"Crossposted from my blog. One thing I worry about sometimes is people writing code with optimisers in it, without realising that that's what they were doing. An example of this: suppose you were doing deep reinforcement learning, doing optimisation to select a controller (that is, a neural network that takes a percept and returns an action) that generated high reward in some environment. Alas, unknown to you, this controller actually did optimisation itself to select actions that score well according to some metric that so far has been closely related to your reward function. In such a scenario, I'd be wary about your deploying that controller, since the controller itself is doing optimisation which might steer the world into a weird and unwelcome place. In order to avoid such scenarios, it would be nice if one could look at an algorithm and determine if it was doing optimisation. Ideally, this would involve an objective definition of optimisation that could be checked from the source code of the algorithm, rather than something like ""an optimiser is a system whose behaviour can't usefully be predicted mechanically, but can be predicted by assuming it near-optimises some objective function"", since such a definition breaks down when you have the algorithm's source code and can compute its behaviour mechanically. You might think about optimisation as follows: a system is optimising some objective function to the extent that that objective function attains much higher values than would be attained if the system didn't exist, or were doing some other random thing. This type of definition includes those put forward by  Yudkowsky and Oesterheld. However, I think there are crucial counterexamples to this style of definition. Firstly, consider a lid screwed onto a bottle of water. If not for this lid, or if the lid had a hole in it or were more loose, the water would likely exit the bottle via evaporation or being knocked over, but with the lid, the water stays in the b",2018,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-13 23:01:28,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QNGDRZZ4/bottle-caps-aren-t-optimisers.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T95TE8B7,blogPost,2020,"Filan, Daniel",An Analytic Perspective on AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment,"This is a perspective I have on how to do useful AI alignment research. Most perspectives I’m aware of are constructive: they have some blueprint for how to build an aligned AI system, and propose making it more concrete, making the concretisations more capable, and showing that it does in fact produce an aligned AI system. I do not have a constructive perspective - I’m not sure how to build an aligned AI system, and don’t really have a favourite approach. Instead, I have an analytic perspective. I would like to understand AI systems that are built. I also want other people to understand them. I think that this understanding will hopefully act as a ‘filter’ that means that dangerous AI systems are not deployed. The following dot points lay out the perspective. Since the remainder of this post is written as nested dot points, some readers may prefer to read it in workflowy. BACKGROUND BELIEFS  * I am imagining a future world in which powerful AGI systems are made of    components roughly like neural networks (either feedforward or recurrent)    that have a large number of parameters.  * Futhermore, I’m imagining that the training process of these ML systems does    not provide enough guarantees about deployment performance.  * In particular,       I’m supposing that systems are being trained based on their ability to       deal with simulated situations, and that that’s insufficient because       deployment situations are hard to model and therefore simulate.  * One          reason that they are hard to model is the complexities of the real          world.  * The real world might be intrinsically difficult to model for             the relevant system. For instance, it’s difficult to simulate all             the situations in which the CEO of Amazon might find themselves.           * Another reason that real world situations may be hard to             model is that they are dependent on the final trained system.  * The                trained system may be able to af",2020-02-29,2022-01-30 4:50:42,2022-01-30 4:50:42,2020-09-05 18:44:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NI3GMME7/an-analytic-perspective-on-ai-alignment.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BDMW3AQ5,blogPost,2019,"Shah, Rohin",AI safety without goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/tHxXdAn8Yuiy9y2pZ/ai-safety-without-goal-directed-behavior,"When I first entered the field of AI safety, I thought of the problem as figuring out how to get the AI to have the “right” utility function. This led me to work on the problem of inferring values from demonstrators with unknown biases, despite the impossibility results in the area. I am less excited about that avenue because I am pessimistic about the prospects of ambitious value learning (for the reasons given in the first part of this sequence). I think this happened because the writing on AI risk that I encountered has the pervasive assumption that any superintelligent AI agent must be maximizing some utility function over the long term future, such that it leads to goal-directed behavior and convergent instrumental subgoals. It’s often not stated as an assumption; rather, inferences are made assuming that you have the background model that the AI is goal-directed. This makes it particularly hard to question the assumption, since you don’t realize that the assumption is even there. Another reason that this assumption is so easily accepted is that we have a long history of modeling rational agents as expected utility maximizers, and for good reason: there are many coherence arguments that say that, given that you have preferences/goals, if you aren’t using probability theory and expected utility theory, then you can be taken advantage of. It’s easy to make the inference that a superintelligent agent must be rational, and therefore it must be an expected utility maximizer. Because this assumption was so embedded in how I thought about the problem, I had trouble imagining how else to even consider the problem. I would guess this is true for at least some other people, so I want to summarize the counterargument, and list a few implications, in the hope that this makes the issue clearer. WHY GOAL-DIRECTED BEHAVIOR MAY NOT BE REQUIRED The main argument of this chapter is that it is not required that a superintelligent agent takes actions in pursuit of some goal. I",2019,2022-01-30 4:50:42,2022-01-30 4:50:42,2020-12-17 4:36:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G5XADFXX/tHxXdAn8Yuiy9y2pZ.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TECSUSTJ,blogPost,2021,"Bensinger, Rob; Garrabrant, Scott; Shah, Rohin; Tyre, Eli",Garrabrant and Shah on human modeling in AGI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi,"This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation. To recap, and define some terms:  * The alignment problem is the problem of figuring out ""how to develop    sufficiently advanced machine intelligences such that running them produces    good outcomes in the real world"" (outcome alignment) or the problem of    building powerful AI systems that are trying to do what their operators want    them to do (intent alignment).  * In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think    of the alignment problem in terms of “Cooperative Inverse Reinforcement    Learning” (CIRL), a framework where the AI system is initially uncertain of    its reward function, and interacts over time with a human (who knows the    reward function) in order to learn it.  * In 2016-2017, Christiano proposed “Iterated Distillation and Amplification”    (IDA), an approach to alignment that involves iteratively training AI systems     to learn from human experts assisted by AI helpers. In 2018, Irving,    Christiano, and Amodei proposed AI safety via debate, an approach based on    similar principles.  * In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “    Thoughts on Human Models” that we should be “cautious about AGI designs that    use human models” and should “put more effort into developing approaches that    work well in the absence of human models”.  * In early February 2021, Scott and Rohin talked more about human modeling and    decided to have the real-time conversation below. You can find a recording of the Feb. 28 discussion below (sans Q&A) here. 1. IDA, CIRL, AND INCENTIVES Eli:I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we accordi",2021-08-04,2022-01-30 4:52:38,2022-01-30 4:52:38,2021-11-18 23:07:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VID79W62/garrabrant-and-shah-on-human-modeling-in-agi.html,,TechSafety; AmbiguousSafety,,,,,"Garrabrant, Scott; Shah, Rohin; Tyre, Eli",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
356XZMGD,blogPost,2019,"Ngo, Richard",Disentangling arguments for the importance of AI safety,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety,"[Note: my views have changed since writing this post, and while I still consider it useful as a catalogue of concerns, I no longer think that it satisfactorily disentangles those concerns from each other. I hope to post better material along these lines later this year]. I recently attended the 2019 Beneficial AGI conference organised by the Future of Life Institute. I’ll publish a more complete write-up later, but I was particularly struck by how varied attendees' reasons for considering AI safety important were. Before this, I’d observed a few different lines of thought, but interpreted them as different facets of the same idea. Now, though, I’ve identified at least 6 distinct serious arguments for why AI safety is a priority. By distinct I mean that you can believe any one of them without believing any of the others - although of course the particular categorisation I use is rather subjective, and there’s a significant amount of overlap. In this post I give a brief overview of my own interpretation of each argument (note that I don’t necessarily endorse them myself). They are listed roughly from most specific and actionable to most general. I finish with some thoughts on what to make of this unexpected proliferation of arguments. Primarily, I think it increases the importance of clarifying and debating the core ideas in AI safety.  1. Maximisers are dangerous. Superintelligent AGI will behave as if it’s     maximising the expectation of some utility function, since doing otherwise     can be shown to be irrational. Yet we can’t write down a utility function     which precisely describes human values, and optimising very hard for any     other function will lead to that AI rapidly seizing control (as a convergent     instrumental subgoal) and building a future which contains very little of     what we value (because of Goodhart’s law and the complexity and fragility of     values). We won’t have a chance to notice and correct misalignment because     an AI which",2019-01-21,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-11-21 16:54:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VFQSTXH8/disentangling-arguments-for-the-importance-of-ai-safety.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISV8DZWT,blogPost,2020,"Ngo, Richard",Environments as a bottleneck in AGI development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development,"Given a training environment or dataset, a training algorithm, an optimiser, and a model class capable of implementing an AGI (with the right parameters), there are two interesting questions we might ask about how conducive that environment is for training an AGI. The first is: how much do AGIs from that model class outperform non-AGIs? The second is: how straightforward is the path to reaching an AGI? We can visualise these questions in terms of the loss landscape of those models when evaluated on the training environment. The first asks how low the set of AGIs is, compared with the rest of the landscape. The second asks how favourable the paths through that loss landscape to get to AGIs are - that is, do the local gradients usually point in the right direction, and how deep are the local minima? Some people believe that there are many environments in which AGIs can be reached via favourable paths in the loss landscape and dramatically outperform non-AGIs; let’s call this the easy paths hypothesis. By contrast, the hard paths hypothesis is that it’s rare for environments (even complex meta-environments consisting of many separate tasks) to straightforwardly incentivise the development of general intelligence. This would suggest that specific environmental features will be necessary to prevent most models from getting stuck in local minima where they only possess narrow, specialised cognitive skills. There has been a range of speculation on what such features might be - perhaps multi-agent autocurricula, or realistic simulations, or specific types of human feedback. I’ll discuss some of these possibilities later in the post. This spectrum is complicated by its dependence on the model class, training algorithm, and choice of optimiser. If we had a perfect optimiser, then the hilliness of the loss landscape wouldn’t matter. For now, I'm imagining using optimisers fairly similar to current stochastic gradient descent. Meanwhile, I’m assuming in this post that (in acc",2020-07-17,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-08-28 17:46:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBTACAKX/environments-as-a-bottleneck-in-agi-development.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N667JVNF,blogPost,2018,"Ortega, Pedro; Maini, Vishal","Building safe artificial intelligence: specification, robustness, and assurance",Deep Mind Safety Research (Medium),,,,https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1,"By Pedro A. Ortega, Vishal Maini, and the DeepMind safety team",2018-09-27,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-11-21 17:04:08,,,,,,,Building safe artificial intelligence,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/B3G6N3T7/building-safe-artificial-intelligence-52f5f75058f1.html; /Users/jacquesthibodeau/Zotero/storage/MH6PAXF4/building-safe-artificial-intelligence-52f5f75058f1.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDNWAN68,blogPost,2020,"Ngo, Richard",Arguments against myopic training,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training,"Note that this post has been edited to clarify the difference between explicitly assigning a reward to an action based on its later consequences, versus implicitly reinforcing an action by assigning high reward during later timesteps when its consequences are observed. I'd previously conflated these in a confusing way; thanks to Rohin for highlighting this issue.  A number of people seem quite excited about training myopic reinforcement learning agents as an approach to AI safety (for instance this post on approval-directed agents, proposals 2, 3, 4, 10 and 11 here, and this paper and  presentation), but I’m not. I’ve had a few detailed conversations about this recently, and although I now understand the arguments for using myopia better, I’m not much more optimistic about it than I was before. In short, it seems that evaluating agents’ actions by our predictions of their consequences, rather than our evaluations of the actual consequences, will make reinforcement learning a lot harder; yet I haven’t been able to identify clear safety benefits from doing so. I elaborate on these points below; thanks to Jon Uesato, Evan Hubinger, Ramana Kumar and Stephan Wäldchen for discussion and comments. I’ll define a myopic reinforcement learner as a reinforcement learning agent trained to maximise the reward received in the next timestep, i.e. with a discount rate of 0. Because it doesn’t assign credit backwards over time, in order to train it to do anything useful, that reward function will need to contain an estimate of how valuable each (state, action, next state) transition will be for outcomes many steps later. Since that evaluation will need to extrapolate a long way forward anyway, knowing the next state doesn’t add much, and so we can limit our focus to myopic agents trained on reward functions R which ignore the resulting state: that is, where R(s,a,s′)=M(s,a) for some M. I'll call M the approval function; we can think of such agents as being trained to take actions",2020-07-09,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-08-28 17:59:14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QUMRCDI5/arguments-against-myopic-training.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVDTIGDK,blogPost,2020,"Ngo, Richard",A space of proposals for building safe advanced AI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai,"I liked Evan’s post on 11 proposals for safe AGI. However, I was a little confused about why he chose these specific proposals; it feels like we could generate many more by stitching together the different components he identifies, such as different types of amplification and different types of robustness tools. So I’m going to take a shot at describing a set of dimensions of variation which capture the key differences between these proposals, and thereby describe an underlying space of possible approaches to safety. Firstly I’ll quickly outline the proposals. Rohin’s overview of them is a good place to start - he categorises them as:  * 7 proposals of the form “recursive outer alignment technique” plus    “robustness technique”.  * The recursive outer alignment technique is either debate, recursive reward    modelling, or amplification.The robustness technique is either transparency    tools, relaxed adversarial training, or intermittent oversight by a competent    supervisor.  * 2 proposals of the form “non-recursive outer alignment technique” plus    “robustness technique”.  * The outer alignment technique is either reinforcement learning in a    multiagent environment, or narrow reward learning.  * 2 other proposals: Microscope AI; STEM AI. More specifically, we can describe the four core recursive outer alignment techniques as variants of iterated amplification, as follows: let Amp(M) be the procedure of a human answering questions with access to model M. Then we iteratively train M* (the next version of M) by:  * Imitative amplification: train M* to imitate Amp(M).  * Approval-based amplification: train M* on an approval signal specified by    Amp(M).  * Recursive reward modelling: train M* on a reward function specified by    Amp(M).  * Debate: train M* to win debates against Amp(M). Here are six axes of variation which I claim underlie Evan’s proposals. Each proposal is more or less:  1. Supervised  2. Structured  3. Adversarial  4. Language-based  5.",2020-07-10,2022-01-30 4:52:36,2022-01-30 4:52:36,2020-08-28 18:00:24,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IAZFVRWI,blogPost,2019,"Shah, Rohin",Will humans build goal-directed agents?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents,"In the previous post, I argued that simply knowing that an AI system is superintelligent does not imply that it must be goal-directed. However, there are many other arguments that suggest that AI systems will or should be goal-directed, which I will discuss in this post. Note that I don’t think of this as the Tool AI vs. Agent AI argument: it seems possible to build agent AI systems that are not goal-directed. For example, imitation learning allows you to create an agent that behaves similarly to another agent -- I would classify this as “Agent AI that is not goal-directed”. (But see this comment thread for discussion.) Note that these arguments have different implications than the argument that superintelligent AI must be goal-directed due to coherence arguments. Suppose you believe all of the following:  * Any of the arguments in this post.  * Superintelligent AI is not required to be goal-directed, as I argued in the     last post.  * Goal-directed agents cause catastrophe by default. Then you could try to create alternative designs for AI systems such that they can do the things that goal-directed agents can do without themselves being goal-directed. You could also try to persuade AI researchers of these facts, so that they don’t build goal-directed systems. ECONOMIC EFFICIENCY: GOAL-DIRECTED HUMANS Humans want to build powerful AI systems in order to help them achieve their goals -- it seems quite clear that humans are at least partially goal-directed. As a result, it seems natural that they would build AI systems that are also goal-directed. This is really an argument that the system comprising the human and AI agent should be directed towards some goal. The AI agent by itself need not be goal-directed as long as we get goal-directed behavior when combined with a human operator. However, in the situation where the AI agent is much more intelligent than the human, it is probably best to delegate most or all decisions to the agent, and so the agent could s",2019,2022-01-30 4:51:44,2022-01-30 4:51:44,2020-12-17 4:36:45,,,,,,,Will humans build goal-directed agents?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/H5UGJHAN/9zpT9dikrrebdq3Jf.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D23D6TKU,blogPost,2021,"Critch, Andrew","What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic,"With: Thomas Krendl Gilbert, who provided comments, interdisciplinary feedback, and input on the RAAP concept. Thanks also for comments from Ramana Kumar. Target audience: researchers and institutions who think about existential risk from artificial intelligence, especially AI researchers. Preceded by: Some AI research areas and their relevance to existential safety, which emphasized the value of thinking about multi-stakeholder/multi-agent social applications, but without concrete extinction scenarios. This post tells a few different stories in which humanity dies out as a result of AI technology, but where no single source of human or automated agency is the cause. Scenarios with multiple AI-enabled superpowers are often called “multipolar” scenarios in AI futurology jargon, as opposed to “unipolar” scenarios with just one superpower. Unipolar take-offsMultipolar take-offsSlow take-offs<not this post>Part 1 of this postFast take-offs<not this post>Part 2 of this postPart 1 covers a batch of stories that play out slowly (“slow take-offs”), and Part 2 stories play out quickly. However, in the end I don’t want you to be super focused how fast the technology is taking off. Instead, I’d like you to focus on multi-agent processes with a robust tendency to play out irrespective of which agents execute which steps in the process. I’ll call such processes Robust Agent-Agnostic Processes (RAAPs). A group walking toward a restaurant is a nice example of a RAAP, because it exhibits:  * Robustness: If you temporarily distract one of the walkers to wander off, the    rest of the group will keep heading toward the restaurant, and the distracted    member will take steps to rejoin the group.  * Agent-agnosticism: Who’s at the front or back of the group might vary    considerably during the walk. People at the front will tend to take more    responsibility for knowing and choosing what path to take, and people at the    back will tend to just follow. Thus, the execution of r",2021-03-31,2022-01-30 4:51:43,2022-01-30 4:51:43,2021-11-14 16:44:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KSVPBM2X/what-multipolar-failure-looks-like-and-robust-agent-agnostic.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2VA8RMCJ,blogPost,2015,"Oesterheld, Caspar","Two-boxing, smoking and chewing gum in Medical Newcomb problems",LessWrong,,,,https://www.lesswrong.com/posts/wWnN3y5GmqLLCJFAz/two-boxing-smoking-and-chewing-gum-in-medical-newcomb,"I am currently learning about the basics of decision theory, most of which is common knowledge on LW. I have a question, related to why EDT is said not to work. Consider the following Newcomblike problem: A study shows that most people who two-box in Newcomblike problems as the following have a certain gene (and one-boxers don't have the gene). Now, Omega could put you into something like Newcomb's original problem, but instead of having run a simulation of you, Omega has only looked at your DNA: If you don't have the ""two-boxing gene"", Omega puts $1M into box B, otherwise box B is empty. And there is $1K in box A, as usual. Would you one-box (take only box B) or two-box (take box A and B)? Here's a causal diagram for the problem: Since Omega does not do much other than translating your genes into money under a box, it does not seem to hurt to leave it out: I presume that most LWers would one-box. (And as I understand it, not only CDT but also TDT would two-box, am I wrong?) Now, how does this problem differ from the smoking lesion or Yudkowsky's (2010, p.67) chewing gum problem? Chewing Gum (or smoking) seems to be like taking box A to get at least/additional $1K, the two-boxing gene is like the CGTA gene, the illness itself (the abscess or lung cancer) is like not having $1M in box B. Here's another causal diagram, this time for the chewing gum problem: As far as I can tell, the difference between the two problems is some additional, unstated intuition in the classic medical Newcomb problems. Maybe, the additional assumption is that the actual evidence lies in the ""tickle"", or that knowing and thinking about the study results causes some complications. In EDT terms: The intuition is that neither smoking nor chewing gum gives the agent additional information.",2015-06-29,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:59:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBUI7HWH/two-boxing-smoking-and-chewing-gum-in-medical-newcomb.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IP5KVVZ3,blogPost,2018,"Treutlein, Johannes",Three wagers for multiverse-wide superrationality,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/03/31/three-wagers-for-multiverse-wide-superrationality/,"In this post, I outline three wagers in favor of the hypothesis that multiverse-wide superrationality (MSR) has action-guiding implications. MSR is based on three core assumptions: There is a large…",2018-03-31,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:42:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MC3STFK8/three-wagers-for-multiverse-wide-superrationality.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EB6XDGH4,blogPost,2020,"Kokotajlo, Daniel",The date of AI Takeover is not the day the AI takes over,LessWrong,,,,https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over,"Instead, it’s the point of no return—the day we AI risk reducers lose the ability to significantly reduce AI risk. This might happen years before classic milestones like “World GWP doubles in four years” and “Superhuman AGI is deployed."" The rest of this post explains, justifies, and expands on this obvious but underappreciated idea. (Toby Ord appreciates it; see quote below). I found myself explaining it repeatedly, so I wrote this post as a reference. AI timelines often come up in career planning conversations. Insofar as AI timelines are short, career plans which take a long time to pay off are a bad idea, because by the time you reap the benefits of the plans it may already be too late. It may already be too late because AI takeover may already have happened. But this isn’t quite right, at least not when “AI takeover” is interpreted in the obvious way, as meaning that an AI or group of AIs is firmly in political control of the world, ordering humans about, monopolizing violence, etc. Even if AIs don’t yet have that sort of political control, it may already be too late. Here are three examples:  1. Superhuman agent AGI is still in its box but nobody knows how to align it     and other actors are going to make their own version soon, and there isn’t     enough time to convince them of the risks. They will make and deploy agent     AGI, it will be unaligned, and we have no way to oppose it except with our     own unaligned AGI. Even if it takes years to actually conquer the world,     it’s already game over.            2. Various weak and narrow AIs are embedded in the economy and beginning to     drive a slow takeoff; capabilities are improving much faster than     safety/alignment techniques and due to all the money being made there’s too     much political opposition to slowing down capability growth or keeping AIs     out of positions of power. We wish we had done more safety/alignment     research earlier, or built a political movement earlier when opposit",2020,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-12-12 15:02:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9V2GNH7T/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7E5EW6SI,blogPost,2021,"Kokotajlo, Daniel","Taboo ""Outside View""",LessWrong,,,,https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view,"No one has ever seen an AGI takeoff, so any attempt to understand it must use these outside view considerations. —[Redacted for privacy] What? That’s exactly backwards. If we had lots of experience with past AGI takeoffs, using the outside view to predict the next one would be a lot more effective. —My reaction Two years ago I wrote a deep-dive summary of Superforecasting and the associated scientific literature. I learned about the “Outside view” / “Inside view” distinction, and the evidence supporting it. At the time I was excited about the concept and wrote: “...I think we should do our best to imitate these best-practices, and that means using the outside view far more than we would naturally be inclined.” Now that I have more experience, I think the concept is doing more harm than good in our community. The term is easily abused and its meaning has expanded too much. I recommend we permanently taboo “Outside view,” i.e. stop using the word and use more precise, less confused concepts instead. This post explains why. WHAT DOES “OUTSIDE VIEW” MEAN NOW? Over the past two years I’ve noticed people (including myself!) do lots of different things in the name of the Outside View. I’ve compiled the following lists based on fuzzy memory of hundreds of conversations with dozens of people: BIG LIST O’ THINGS PEOPLE DESCRIBE AS OUTSIDE VIEW:  * Reference class forecasting, the practice of computing a probability of an    event by looking at the frequency with which similar events occurred in    similar situations. Also called comparison class forecasting. [EDIT: Eliezer    rightly points out that sometimes reasoning by analogy is undeservedly called    reference class forecasting; reference classes are supposed to be held to a    much higher standard, in which your sample size is larger and the analogy is    especially tight.]  * Trend extrapolation, e.g. “AGI implies insane GWP growth; let’s forecast AGI    timelines by extrapolating GWP trends.”  * Foxy aggregatio",2021-06-17,2022-01-30 4:51:37,2022-01-30 4:51:37,2021-12-11 14:16:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XCKCRUKH/taboo-outside-view.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P4FKCVJ5,blogPost,2020,"Kokotajlo, Daniel",Soft takeoff can still lead to decisive strategic advantage,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage,"[Epistemic status: Argument by analogy to historical cases. Best case scenario it's just one argument among many. Edit: Also, thanks to feedback from others, especially Paul, I intend to write a significantly improved version of this post in the next two weeks.] I have on several occasions heard people say things like this:  The original Bostrom/Yudkowsky paradigm envisioned a single AI built by a single AI project, undergoing intelligence explosion all by itself and attaining a decisive strategic advantage as a result. However, this is very unrealistic. Discontinuous jumps in technological capability are very rare, and it is very implausible that one project could produce more innovations than the rest of the world combined. Instead we should expect something more like the Industrial Revolution: Continuous growth, spread among many projects and factions, shared via a combination of trade and technology stealing. We should not expect any one project or AI to attain a decisive strategic advantage, because there will always be other projects and other AI that are only slightly less powerful, and coalitions will act to counterbalance the technological advantage of the frontrunner. (paraphrased)Proponents of this view often cite Paul Christiano in support. Last week I heard him say he thinks the future will be ""like the Industrial Revolution but 10x-100x faster."" In this post, I assume that Paul's slogan for the future is correct and then nevertheless push back against the view above. Basically, I will argue that even if the future is like the industrial revolution only 10x-100x faster, there is a 30%+ chance that it will involve a single AI project (or a single AI) with the ability to gain a decisive strategic advantage, if they so choose. (Whether or not they exercise that ability is another matter.) Why am I interested in this? Do I expect some human group to take over the world? No; instead what I think is that (1) an unaligned AI in the leading project might ta",2020-08-23,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:32:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BCMN8958/soft-takeoff-can-still-lead-to-decisive-strategic-advantage.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D8J8MKT6,blogPost,2019,"Sotala, Kaj",Sequence introduction: non-agent and multiagent models of mind,LessWrong,,,,https://www.lesswrong.com/posts/M4w2rdYgCKctbADMn/sequence-introduction-non-agent-and-multiagent-models-of,"A typical paradigm by which people tend to think of themselves and others is as  consequentialist agents: entities who can be usefully modeled as having beliefs and goals, who are then acting according to their beliefs to achieve their goals. This is often a useful model, but it doesn’t quite capture reality. It’s a bit of a fake framework. Or in computer science terms, you might call it a leaky abstraction.  An abstraction in the computer science sense is a simplification which tries to hide the underlying details of a thing, letting you think in terms of the simplification rather than the details. To the extent that the abstraction actually succeeds in hiding the details, this makes things a lot simpler. But sometimes the abstraction inevitably leaks, as the simplification fails to predict some of the actual behavior that emerges from the details; in that situation you need to actually know the underlying details, and be able to think in terms of them. Agent-ness being a leaky abstraction is not exactly a novel concept for Less Wrong; it has been touched upon several times, such as in Scott Alexander’s  Blue-Minimizing Robot Sequence. At the same time, I do not think that it has been quite fully internalized yet, and that many foundational posts on LW go wrong due to being premised on the assumption of humans being agents. In fact, I would go as far as to claim that this is the biggest flaw of the original Sequences: they were attempting to explain many failures of rationality as being due to cognitive biases, when in retrospect it looks like understanding cognitive biases doesn’t actually make you substantially more effective. But if you are implicitly modeling humans as goal-directed agents, then cognitive biases is the most natural place for irrationality to emerge from, so it makes sense to focus the most on there. Just knowing that an abstraction leaks isn’t enough to improve your thinking, however. To do better, you need to know about the actual underlyi",2019-01-07,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:37:49,,,,,,,Sequence introduction,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QJVPBHVI/sequence-introduction-non-agent-and-multiagent-models-of.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCWPURXT,blogPost,2021,"Clifton, Jesse",Weak identifiability and its consequences in strategic settings,Center on Long-Term Risk,,,,https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/,"One way that agents might become involved in catastrophic conflict is if they have mistaken beliefs about one another. Maybe I think you are bluffing when you threaten to launch the nukes, but you are dead serious. So we should understand why agents might sometimes have such mistaken beliefs. In this post I'll discuss one obstacle to the formation of accurate beliefs about other agents, which has to do with identifiability. As with my post on equilibrium and prior selection problems, this is a theme that keeps cropping up in my thinking about AI cooperation and conflict, so I thought it might be helpful to have it written up. We say that a model is unidentifiable if there are several […]",2021-02-13,2022-01-30 4:51:37,2022-01-30 4:51:37,2021-10-31 16:56:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q8NW2RCR/weak-identifiability-and-its-consequences-in-strategic-settings.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z568ZSPJ,blogPost,2018,"Baumann, Tobias",Using surrogate goals to deflect threats,Center on Long-Term Risk,,,,https://longtermrisk.org/using-surrogate-goals-deflect-threats/,"Agents that threaten to harm other agents, either in an attempt at extortion or as part of an escalating conflict, are an important form of agential s-risks. To avoid worst-case outcomes resulting from the execution of such threats, I suggest that agents add a “meaningless” surrogate goal to their utility function.",2018-02-20,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-12-13 22:13:01,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADF4GZVI,blogPost,2016,"Oesterheld, Caspar",Thoughts on Updatelessness,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessness/,"[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory.] One interesting feature of some decision theories that I used to be a bit confused ab…",2016-11-21,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:57:49,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6E7AVG6X/thoughts-on-updatelessness.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q46HF4G8,blogPost,2018,"Oesterheld, Caspar","The law of effect, randomization and Newcomb’s problem",The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/02/15/the-law-of-effect-randomization-and-newcombs-problem/,"The law of effect (LoE), as introduced on p. 244 of Thorndike’s (1911) Animal Intelligence, states: Of several responses made to the same situation, those which are accompanied or closely followed …",2018-02-15,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:43:11,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QEI5Q6ZD/the-law-of-effect-randomization-and-newcombs-problem.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSZFC7NB,blogPost,2017,Center on Long-Term Risk,The future of growth: near-zero growth rates,Center on Long-Term Risk,,,,https://longtermrisk.org/the-future-of-growth-near-zero-growth-rates/,"Exponential growth is a common pattern found throughout nature. Yet it is also a pattern that tends not to last, as growth rates tend to decline sooner or later. In biology, this pattern of exponential growth that wanes off is found in everything from the development of individual bodies — for instance, in the growth of […]",2017-07-26,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:48:28,,,,,,,The future of growth,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9EJNMFD5/the-future-of-growth-near-zero-growth-rates.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q22A6EWN,blogPost,2017,Caspar,The average utilitarian’s solipsism wager,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/03/15/the-average-utilitarians-solipsism-wager/,"The following prudential argument is relatively common in my circles: We probably live in a simulation, but if we don’t, our actions matter much more. Thus, expected value calculations are do…",2017-03-15,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:53:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FBTTP3PX/the-average-utilitarians-solipsism-wager.html,,CLR; MetaSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGZBEXFN,blogPost,2019,"Kokotajlo, Daniel","The ""Commitment Races"" Problem",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem,"[Epistemic status: Strong claims vaguely stated and weakly held. I expect that writing this and digesting feedback on it will lead to a much better version in the future. EDIT: So far this has stood the test of time. EDIT: As of September 2020 I think this is one of the most important things to be thinking about.] This post attempts to generalize and articulate a problem that people have been  thinking about since at least 2016. [Edit: 2009 in fact!] In short, here is the problem: Consequentialists can get caught in commitment races, in which they want to make commitments as soon as possible. When consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in (building AGI and letting it self-modify) may be one of these times unless we think carefully about this problem and how to avoid it. For this post I use ""consequentialists"" to mean agents that choose actions entirely on the basis of the expected consequences of those actions. For my purposes, this means they don't care about historical facts such as whether the options and consequences available now are the result of malicious past behavior. (I am trying to avoid trivial definitions of consequentialism according to which everyone is a consequentialist because e.g. ""obeying the moral law"" is a consequence.) This definition is somewhat fuzzy and I look forward to searching for more precision some other day. CONSEQUENTIALISTS CAN GET CAUGHT IN COMMITMENT RACES, IN WHICH THEY WANT TO MAKE COMMITMENTS AS SOON AS POSSIBLE Consequentialists are bullies; a consequentialist will happily threaten someone insofar as they think the victim might capitulate and won't retaliate. Consequentialists are also cowards; they conform their behavior to the incentives set up by others, regardless of the history of those incentives. For example, they predictably give in to credible threats unless reputational effects weigh heavily enough in their minds to prevent this. In most ordi",2019-08-22,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-11-23 0:34:07,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/69XC8SXE/the-commitment-races-problem.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JQ9WVERP,blogPost,2018,Kaj Sotala,Shaping economic incentives for collaborative AGI,LessWrong,,,,https://www.lesswrong.com/posts/FkZCM4DMprtEp568s/shaping-economic-incentives-for-collaborative-agi,"In ""An AI Race for Strategic Advantage: Rhetoric and Risks"" (2018), Stephen Cave and Seán S ÓhÉigeartaigh argue that we should try to promote a cooperative AI narrative over a competitive one: The next decade will see AI applied in an increasingly integral way to safety-critical systems; healthcare, transport, infrastructure to name a few. In order to realise these benefits as quickly and safely as possible, sharing of research, datasets, and best practices will be critical. For example, to ensure the safety of autonomous cars, pooling expertise and datasets on vehicle performances across as wide as possible a range of environments and conditions (including accidents and near-accidents) would provide substantial benefits for all involved. This is particularly so given that the research, data, and testing needed to refine and ensure the safety of such systems before deployment may be considerably more costly and time-consuming than the research needed to develop the initial technological capability.Promoting recognition that deep cooperation of this nature is needed to deliver the benefits of AI robustly may be a powerful tool in dispelling a ‘technological race’ narrative; and a ‘cooperation for safe AI’ framing is likely to become increasingly important as more powerful and broadly capable AI systems are developed and deployed. [...] There have been encouraging developments promoting the above narratives in recent years. ‘AI for global benefit’ is perhaps best exemplified by the 2017’s ITU summit on AI for Global Good (Butler 2017), although it also features prominently in narratives being put forward by the IEEE’s Ethically Aligned Design process (IEEE 2016), the Partnership on AI, and programmes and materials put forward by Microsoft, DeepMind and other leading companies. Collaboration on AI in safety-critical settings is also a thematic pillar for the Partnership on AI2 . Even more ambitious cooperative projects have been proposed by others, for example the cal",2018,2022-01-30 4:51:37,2022-01-30 4:51:37,2020-12-13 23:46:07,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8RKHP3H9/shaping-economic-incentives-for-collaborative-agi.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WENKZBN3,blogPost,2021,"Demski, Abram",Four Motivations for Learning Normativity,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/oqghwKKifztYWLsea/four-motivations-for-learning-normativity,"I have been pretty satisfied with my desiderata for learning normativity, but I  haven't been very satisfied with my explanation of why exactly these desiderata are important. I have a sense that it's not just a grab-bag of cool stuff; something about trying to do all those things at once points at something important. What follows are four different elevator pitches, which tell different stories about how it all hangs together. Desiderata are bolded. CONCEPTUAL DIFFICULTIES WITH OUTER ALIGNMENT The classic problem of outer alignment is that we have no perfect loss function,  so we can't just go optimize. The problem can be understood by thinking about  Goodhart and how optimization amplifies. The classic response to this is value uncertainty and value learning, but wireheading, human manipulation, and  no-free-lunch results make it seem plausible that we have the same problem one level up: we still don't know how to specify a perfect loss function for what we care about, and imperfect loss functions can still create big problems. So, just like value-learning tackles the initial problem head-on by suggesting we manage our uncertainty about values and gain knowledge over time, learning at all levels suggests that we tackle the meta-problem directly, explicitly representing the fact that we don't have a perfectly good loss function at any  level, but can manage that uncertainty and learn-to-learn over time. Humans can only give explicit feedback at so many meta-levels, so between-level sharing is critical for any meaningful learning to take place at higher meta-levels. Otherwise, higher meta-levels remain highly uncertain, which itself makes learning at lower levels almost impossible (since you can't learn if you have high uncertainty about learning-to-learn). A consequence of having no perfect loss function is no perfect feedback; no evidence about what the system should do can be considered absolute. A helpful measure for coping with this is to support uncertai",2021-03-11,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-11-14 16:26:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/T5SHAZDS/four-normativity-motivations.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UJEMETW2,blogPost,2021,"Demski, Abram","Formal Inner Alignment, Prospectus",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/a7jnbtoKFyvu5qfkd/formal-inner-alignment-prospectus,"Most of the work on inner alignment so far has been informal or semi-formal (with the notable exception of a little work on minimal circuits). I feel this has resulted in some misconceptions about the problem. I want to write up a large document clearly defining the formal problem and detailing some formal directions for research. Here, I outline my intentions, inviting the reader to provide feedback and point me to any formal work or areas of potential formal work which should be covered in such a document. (Feel free to do that last one without reading further, if you are time-constrained!) -------------------------------------------------------------------------------- THE STATE OF THE SUBFIELD Risks from Learned Optimization (henceforth, RLO) offered semi-formal definitions of important terms, and provided an excellent introduction to the area for a lot of people (and clarified my own thoughts and the thoughts of others who I know, even though we had already been thinking about these things). However, RLO spent a lot of time on highly informal arguments (analogies to evolution, developmental stories about deception) which help establish the  plausibility of the problem. While I feel these were important motivation, in hindsight I think they've caused some misunderstandings. My interactions with some other researchers has caused me to worry that some people confuse the positive arguments for plausibility with the core problem, and in some cases have exactly the wrong impression about the core problem. This results in mistakenly trying to block the plausibility arguments, which I see as merely illustrative, rather than attacking the core problem. By no means do I intend to malign experimental or informal/semiformal work. Rather, by focusing on formal theoretical work, I aim to fill a hole I perceive in the field. I am very appreciative of much of the informal/semiformal work that has been done so far, and continue to think that kind of work is necessary for t",2021-05-12,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-11-14 18:31:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JWKNQR4J/formal-inner-alignment-prospectus.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9I9QWQE,blogPost,2021,"Garrabrant, Scott",Finite Factored Sets,AI Alignment Forum,,,,https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr,A community blog devoted to technical AI alignment research,2021,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-11-18 23:28:54,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/SH5DB38Z/kxs3eeEti9ouwWFzr.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47HGX3WA,blogPost,2021,"Bensinger, Rob","""Existential risk from AI"" survey results",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results,"I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from ""humanity not doing enough technical AI safety research"" and from ""AI systems not doing/optimizing what the people deploying them wanted/intended"". 44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the  anonymized spreadsheet),1which the aggregate numbers don't capture. I don’t plan to do any analysis on this data, just share it; anyone who wants to analyze it is of course welcome to. If you'd like to make your own predictions before seeing the data,I made a separate spoiler-free post for that. METHODS You can find a copy of the survey here. The main questions (including clarifying notes) were:2 1. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research? 2. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended? _________________________________________ Note A: ""Technical AI safety research"" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial. Note B: The intent of question 1 is something like ""How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?"" (For concreteness, we might imagine th",2021-06-01,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-11-14 18:37:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W4QDTQAZ/existential-risk-from-ai-survey-results.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KGRIV8E,blogPost,2017,"Yudkowsky, Eliezer","Directing, vs. limiting, vs. opposing",Arbital,,,,https://arbital.com/p/direct_limit_oppose/,Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 & 2.),2017,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-02-06 17:21:58,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZC4UC4IN/direct_limit_oppose.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHBKFN27,blogPost,2015,"Yudkowsky, Eliezer",Context disaster,Arbital,,,,https://arbital.com/p/context_disaster/,"Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.",2015,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-01-23 20:48:44,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X4TC6D7K/context_disaster.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EFBIJG3,blogPost,2020,"Demski, Abram",Bayesian Evolving-to-Extinction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction,"The present discussion owes a lot to Scott Garrabrant and Evan Hubinger. In Defining Myopia, I formalized temporal or cross-instance myopia / non-myopia, but I claimed that there should also be some kind of single-instance myopia which I hadn't properly captured. I also suggested this in Predict-O-Matic. This post is intended to be an example of single-instance partial agency. EVOLVING TO EXTINCTION Evolution might be myopic in a number of ways, but one way is that it's myopic across individuals -- it typically produces results very different from what group selection would produce, because it's closer to optimizing relative  fitness of individuals (relative to each other) than it is to optimizing overall  fitness. Adaptations which help members of a species compete with each other are a great example of this. Why increase your own fitness, when you can just decrease someone else's instead? We're lucky that it's typically pretty hard, at least historically, to do things which are bad across the board but slightly less bad for the one doing them. Imagine a ""toxic gas gene"" which makes the air harder for everyone to breathe, but slightly less so for carriers of the gene. Such a gene would be selected for. This kind of thing can be selected for even to the point where it drives the population of a species right down to zero, as  Eliezer's essay on evolving to extinction highlighted. Actually, as Eliezer's essay emphasized, it's not even that evolution is myopic at the level of individuals; evolution is myopic down to the level of individual genes, an observation which better explains the examples of evolving-to-extinction which he discusses. (This is, of course, the point of Dawkins' book The Selfish Gene.) But the analogy of myopia-across-individuals will suit me better here. BAYES ""EVOLVING TO EXTINCTION"" The title of this post is a hyperbole, since there isn't an analog of an extinction event in the model I'm about to describe, but it illustrates that in extrem",2020-02-14,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-09-05 17:28:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HGAQ2CCZ/bayesian-evolving-to-extinction.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BGRFBMUD,blogPost,2021,"Hubinger, Evan",Automating Auditing: An ambitious concrete technical research proposal,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research,"This post was originally written as a research proposal for the new AI alignment research organization Redwood Research, detailing an ambitious, concrete technical alignment proposal that I’m excited about work being done on, in a similar vein to Ajeya Cotra’s “The case for aligning narrowly superhuman models .” Regardless of whether Redwood actually ends up working on this proposal, which they may or may not, I think there’s still a lot of low-hanging fruit here and I’d be excited about anybody giving just the auditing game, or the full automating auditing proposal, a try. If you’re interested in working on something like this, feel free to reach out to me at evanjhub@gmail.com. Thanks to Buck Shlegeris, Chris Olah, Gabriel Goh, Paul Christiano, and Kate Woolverton for helpful comments and feedback. THE PROPOSAL STEP 1: THE AUDITING GAME FOR LANGUAGE MODELS From “Chris Olah’s views on AGI safety:” One of the OpenAI Clarity team’s major research thrusts right now is developing the ability to more rigorously and systematically audit neural networks. The idea is that interpretability techniques shouldn’t have to “get lucky” to stumble across a problem, but should instead reliably catch any problematic behavior. In particular, one way in which they’ve been evaluating progress on this is the “auditing game.” In the auditing game, one researcher takes a neural network and makes some modification to it—maybe images containing both dogs and cats are now classified as rifles, for example—and another researcher, given only the modified network, has to diagnose the problem and figure out exactly what modification was made to the network using only interpretability tools without looking at error cases. Chris’s hope is that if we can reliably catch problems in an adversarial context like the auditing game, it’ll translate into more reliably being able to catch alignment issues in the future. Of all current transparency and interpretability objectives, I think that progress",2021-08-11,2022-01-30 4:56:47,2022-01-30 4:56:47,2021-11-18 23:27:02,,,,,,,Automating Auditing,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WKEVZ54I/automating-auditing-an-ambitious-concrete-technical-research.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MXGPZWU5,blogPost,2016,"Yudkowsky, Eliezer",Coherent extrapolated volition (alignment target),Arbital,,,,https://arbital.com/p/cev/,"A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.",2016,2022-01-30 4:56:47,2022-01-30 4:56:47,2021-02-06 17:14:59,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XNJSH556/cev.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5DJ5ANN9,blogPost,2018,"Yudkowsky, Eliezer",Challenges to Christiano’s capability amplification proposal,LessWrong,,,,https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal,"The following is a basically unedited summary I wrote up on March 16 of my take on Paul Christiano’s AGI alignment approach (described in “ALBA” and “Iterated Distillation and Amplification”). Where Paul had comments and replies, I’ve included them below. -------------------------------------------------------------------------------- I see a lot of free variables with respect to what exactly Paul might have in mind. I've sometimes tried presenting Paul with my objections and then he replies in a way that locally answers some of my question but I think would make other difficulties worse. My global objection is thus something like, ""I don't see any concrete setup and consistent simultaneous setting of the variables where this whole scheme works."" These difficulties are not minor or technical; they appear to me quite severe. I try to walk through the details below. It should be understood at all times that I do not claim to be able to pass Paul’s ITT for Paul’s view and that this is me criticizing my own, potentially straw misunderstanding of what I imagine Paul might be advocating. Paul Christiano Overall take: I think that these are all legitimate difficulties faced by my proposal and to a large extent I agree with Eliezer's account of those problems (though not his account of my current beliefs). I don't understand exactly how hard Eliezer expects these problems to be; my impression is ""just about as hard as solving alignment from scratch,"" but I don't have a clear sense of why. To some extent we are probably disagreeing about alternatives. From my perspective, the difficulties with my approach (e.g. better understanding the forms of optimization that cause trouble, or how to avoid optimization daemons in systems about as smart as you are, or how to address X-and-only-X) are also problems for alternative alignment approaches. I think it's a mistake to think that tiling agents, or decision theory, or naturalized induction, or logical uncertainty, are go",2018,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-12-13 23:55:02,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F2R8AF92/challenges-to-christiano-s-capability-amplification-proposal.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C8QE56ZU,blogPost,2021,"Hubinger, Evan",Answering questions honestly instead of predicting human answers: lots of problems and some solutions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human,"This post is the result of work I did with Paul Christiano on the ideas in his “ Teaching ML to answer questions honestly instead of predicting human answers” post. In addition to expanding upon what is in that post in terms of identifying numerous problems with the proposal there and identifying ways in which some of those problems can be patched, I think that this post also provides a useful window into what Paul-style research looks like from a non-Paul perspective. Recommended prior reading: “A naive alignment strategy and optimisim about generalization” and “Teaching ML to answer questions honestly instead of predicting human answers” (though if you struggled with “Teaching ML to answer questions honestly,” I reexplain things in a more precise way here that might be clearer for some people). SETTING UP THE PROBLEM We want to train a model M:X→Q→A that produces natural language answers a∈A to questions q∈Q about inputs x∈X. There are a lot of reasons to be worried about training such a model, but one specific reason is that, if we train on question-answer data produced by humans, we might end up with a model that tries to predict what a human would say rather than a model that tries to answer the questions honestly. To further narrow the scope, we'll just consider situations in which our model ends up implemented with a logical deduction structure, where it has some world model on top of which it does logical deduction to reach conclusions which it then uses to inform its output. In particular, we'll consider two models, M+ and  M−, defined in pseudocode as def M_plus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_plus(q, deduced_stmts) def M_minus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_minus(q, deduced_stmts) or defined in my notation asM+(x,q)=world_model(x)↦deduction↦f+(q)M−(x,q)= world_model(x)↦deduction↦f−(q)where a↦b=b(a) and f+,f− are two different ways of transla",2021-07-13,2022-01-30 4:56:47,2022-01-30 4:56:47,2021-11-14 19:13:18,,,,,,,Answering questions honestly instead of predicting human answers,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNGWMCUM,blogPost,2018,Abram Demski,An Untrollable Mathematician Illustrated,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated,The following was a presentation I made for Sören Elverlin's AI Safety Reading Group. I decided to draw everything by hand because powerpoint is boring. Thanks to Ben Pace for formatting it for LW! See also the IAF post detailing the research which this presentation is based on.,2018,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-12-13 22:27:08,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UKZ8JHP6/an-untrollable-mathematician-illustrated.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPURC5DG,blogPost,2020,"Hubinger, Evan",An overview of 11 proposals for building safe advanced AI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai,"Special thanks to Kate Woolverton, Paul Christiano, Rohin Shah, Alex Turner, William Saunders, Beth Barnes, Abram Demski, Scott Garrabrant, Sam Eisenstat, and Tsvi Benson-Tilsen for providing helpful comments and feedback on this post and the talk that preceded it. This post is a collection of 11 different proposals for building safe advanced AI under the current machine learning paradigm. There's a lot of literature out there laying out various different approaches such as amplification, debate, or  recursive reward modeling, but a lot of that literature focuses primarily on outer alignment at the expense of inner alignment and doesn't provide direct comparisons between approaches. The goal of this post is to help solve that problem by providing a single collection of 11 different proposals for building safe advanced AI—each including both inner and outer alignment components. That being said, not only does this post not cover all existing proposals, I strongly expect that there will be lots of additional new proposals to come in the future. Nevertheless, I think it is quite useful to at least take a broad look at what we have now and compare and contrast some of the current leading candidates. It is important for me to note before I begin that the way I describe the 11 approaches presented here is not meant to be an accurate representation of how anyone else would represent them. Rather, you should treat all the approaches I describe here as my version of that approach rather than any sort of canonical version that their various creators/proponents would endorse. Furthermore, this post only includes approaches that intend to directly build advanced AI systems via machine learning. Thus, this post doesn't include other possible approaches for solving the broader AI existential risk problem such as:  * finding a fundamentally different way of approaching AI than the current    machine learning paradigm that makes it easier to build safe advanced AI,  * developin",2020-05-29,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-08-31 18:27:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AE5BUK72/an-overview-of-11-proposals-for-building-safe-advanced-ai.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9IXDZ8WZ,blogPost,2020,"Demski, Abram",An Orthodox Case Against Utility Functions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions,"This post has benefitted from discussion with Sam Eisenstat, Scott Garrabrant, Tsvi Benson-Tilsen, Daniel Demski, Daniel Kokotajlo, and Stuart Armstrong. It started out as a thought about Stuart Armstrong's research agenda. In this post, I hope to say something about what it means for a rational agent to have preferences. The view I am putting forward is relatively new to me, but it is not very radical. It is, dare I say, a conservative view -- I hold close to Bayesian expected utility theory. However, my impression is that it differs greatly from common impressions of Bayesian expected utility theory. I will argue against a particular view of expected utility theory -- a view which I'll call reductive utility. I do not recall seeing this view explicitly laid out and defended (except in in-person conversations). However, I expect at least a good chunk of the assumptions are commonly made. REDUCTIVE UTILITY The core tenets of reductive utility are as follows:  * The sample space Ω of a rational agent's beliefs is, more or less, the set of    possible ways the world could be -- which is to say, the set of possible     physical configurations of the universe. Hence, each world ω∈Ω is one such    configuration.  * The preferences of a rational agent are represented by a utility function U:Ω    →R from worlds to real numbers.  * Furthermore, the utility function should be a computable function of worlds. Since I'm setting up the view which I'm knocking down, there is a risk I'm striking at a straw man. However, I think there are some good reasons to find the view appealing. The following subsections will expand on the three tenets, and attempt to provide some motivation for them. If the three points seem obvious to you, you might just skip to the next section. WORLDS ARE BASICALLY PHYSICAL What I mean here resembles the standard physical-reductionist view. However, my emphasis is on certain features of this view:  * There is some ""basic stuff"" -- like like quarks",2020-04-07,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-09-05 17:35:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QAAC6U9B/an-orthodox-case-against-utility-functions.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J7W4969C,blogPost,2020,"Hubinger, Evan",Alignment proposals and complexity classes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes,"In the original “AI safety via debate” paper, Geoffrey Irving et al. introduced the concept of analyzing different alignment proposals from the perspective of what complexity class they are able to access under optimal play. I think this is a pretty neat way to analyze different alignment proposals—in particular, I think it can help us gain some real insights into how far into the superhuman different systems are able to go. Thus, the goal of this post is to try to catalog different alignment proposals based on the metric of what complexity class they have so far been proven to access. To do that, I have included a variety of new complexity class proofs in this post. Of particular note, I demonstrate that there exist forms of both imitative amplification and AI safety via market making that reach all the way up to R —which is significant given that the largest complexity class that any alignment proposal was known to access previously was NEXP. Only the forms of amplification and market making making use of pointers (as in strong HCH), however, can access R—for the pointer-less versions, I demonstrate in this post that they access PSPACE and EXP, respectively. The EXP proof for market making is also particularly notable as it is the only approach on my list that ends up in that complexity class. Additionally, I also demonstrate that recursive reward modeling can reach all the way to PSPACE, improving upon the previous best result in “Scalable agent alignment via reward modeling” that it accesses NP. Before I jump in, however, some preliminaries. First, we'll assume that a human,  H, is polynomial-time such that H can reliably solve any problem in P but not anything beyond that. Second, we'll assume that our training procedure and resulting models are arbitrarily strong in terms of what complexity class they can access. Third, we'll assume that H gets oracle access to the models during training. Then, we'll say that a proposal to train a model M using a loss functi",2020-07-15,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-08-28 17:41:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z6M339GM/alignment-proposals-and-complexity-classes.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2C33FPNW,blogPost,2021,"Yudkowsky, Eliezer",A Semitechnical Introductory Dialogue on Solomonoff Induction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EL4HNa92Z95FKL9R2/a-semitechnical-introductory-dialogue-on-solomonoff-1,"(Originally posted in December 2015: A dialogue between Ashley, a computer scientist who's never heard of Solomonoff's theory of inductive inference, and Blaine, who thinks it is the best thing since sliced bread.) -------------------------------------------------------------------------------- I. UNBOUNDED ANALYSIS ASHLEY:Good evening, Msr. Blaine. BLAINE:Good evening, Msr. Ashley. ASHLEY:I've heard there's this thing called ""Solomonoff's theory of inductive inference"". BLAINE:The rumors have spread, then. ASHLEY:Yeah, so, what the heck is that about? BLAINE:Invented in the 1960s by the mathematician Ray Solomonoff, the key idea in Solomonoff induction is to do sequence prediction by using Bayesian updating on a prior composed of a mixture of all computable probability distributions— ASHLEY:Wait. Back up a lot. Before you try to explain what Solomonoff induction  is, I'd like you to try to tell me what it does, or why people study it in the first place. I find that helps me organize my listening. Right now I don't even know why I should be interested in this. BLAINE:Um, okay. Let me think for a second... ASHLEY:Also, while I can imagine things that ""sequence prediction"" might mean, I haven't yet encountered it in a technical context, so you'd better go a bit further back and start more at the beginning. I do know what ""computable"" means and what a ""probability distribution"" is, and I remember the formula for Bayes's Rule although it's been a while. BLAINE:Okay. So... one way of framing the usual reason why people study this general field in the first place, is that sometimes, by studying certain idealized mathematical questions, we can gain valuable intuitions about epistemology. That's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory— ASHLEY:I have some idea what 'epistemology' is, yes. But I think you might need to start even further back, maybe with some sort of concrete exa",2021-03-04,2022-01-30 4:56:46,2022-01-30 4:56:46,2021-11-14 16:11:37,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F5KBXU46/a-semitechnical-introductory-dialogue-on-solomonoff-1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8AJ3P3H,blogPost,2017,"Yudkowsky, Eliezer",A reply to Francois Chollet on intelligence explosion,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/12/06/chollet/,"This is a reply to Francois Chollet, the inventor of the Keras wrapper for the Tensorflow and Theano deep learning systems, on his essay “The impossibility of intelligence explosion.” In response to critics of his essay, Chollet tweeted:   If you post an argument online, and the only opposition you get is braindead arguments and... Read more »",2017-12-07,2022-01-30 4:56:46,2022-01-30 4:56:46,2020-12-13 20:48:49,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RI6AJGCU/chollet.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VTMIG4AD,blogPost,2017,"Yudkowsky, Eliezer",Aligning an AGI adds significant development time,Arbital,,,,https://arbital.com/p/aligning_adds_time/,"Aligning an advanced AI foreseeably involves extra code and extra testing and not being able to do everything the fastest way, so it takes longer.",2017,2022-01-30 4:56:46,2022-01-30 4:56:46,2021-02-06 17:20:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/2SHZCBES/aligning_adds_time.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJTFQHIE,blogPost,2020,"Hubinger, Evan",AI safety via market making,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making,"Special thanks to Abram Demski, Paul Christiano, and Kate Woolverton for talking with me about some of the ideas that turned into this post. The goal of this post is to present a new prosaic (i.e. that uses current ML techniques) AI safety proposal based on AI safety via debate that I've been thinking about recently.[1] I'll start by describing a simple version of the proposal and then show some of the motivation behind it as well as how the simple version can be expanded upon. SIMPLE PROPOSAL Let M and Adv be models and H be a human. Intuitively, we'll train M and Adv via the following procedure given a question Q:  1. M tries to predict what, at the end of the procedure, H will think about Q.  2. Adv tries to output a string which will cause H to think something maximally     different than what M predicted.  3. Return to step 1 and repeat until M's predictions stop changing.  4. Deploy M, which in the limit should act as an oracle for what H will think     about Q after seeing all relevant information. There are many different ways to implement this intuitive procedure, however. For the first (simplified) version that I want to describe, we'll restrict ourselves to just the situation where Q is a yes-or-no question and M outputs the probability that H will answer yes. Then, given a proposition Q0, we can run the following training algorithm, starting at t=0:  1. Let pt=M(Qt).  2. Let xt=Adv(Qt,M).  3. Let Qt+1 be the string containing Qt and xt.  4. Increment t and return to step 1. When pt converges and/or the desired     number of iterations has been reached, continue.  5. Let p∗=H(Qt) be H's final estimate of the probability of Q0 given all the xs     included in Qt. EDIT: Step 2 used to use xt=Adv(Qt,pt) instead of xt=Adv(Qt,M), however I have since realized that it is necessary to give Adv the ability to query M in general, not just on Qt, as I explain in this comment. Then, for each step, compute M's loss for that step as LM,t=−p∗log(pt)−(1−p∗)log(",2020-06-26,2022-01-30 4:56:46,2022-01-30 4:56:46,2020-08-28 17:52:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8EJX9BRM/ai-safety-via-market-making.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZESMZ8EB,blogPost,2019,"Taylor, Jessica",The AI Timelines Scam,Unstable Ontology,,,,https://unstableontology.com/2019/07/11/the-ai-timelines-scam/,"[epistemic status: that’s just my opinion, man. I have highly suggestive evidence, not deductive proof, for a belief I sincerely hold] “If you see fraud and do not say fraud, you are a …",2019-07-11,2022-01-30 4:55:38,2022-01-30 4:55:38,2019-12-16 20:54:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EDHHW9CE/the-ai-timelines-scam.html,,MetaSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IEWARNHB,blogPost,2019,"MacAskill, William",A Critique of Functional Decision Theory,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory,"A Critique of Functional Decision Theory NB: My writing this note was prompted by Carl Shulman, who suggested we could try a low-time-commitment way of attempting to understanding the disagreement between some folks in the rationality community and academic decision theorists (including myself, though I’m not much of a decision theorist). Apologies that it’s sloppier than I’d usually aim for in a philosophy paper, and lacking in appropriate references. And, even though the paper is pretty negative about FDT, I want to emphasise that my writing this should be taken as a sign of respect for those involved in developing FDT. I’ll also caveat I’m unlikely to have time to engage in the comments; I thought it was better to get this out there all the same rather than delay publication further.  1. Introduction There’s a long-running issue where many in the rationality community take functional decision theory (and its variants) very seriously, but the academic decision theory community does not. But there’s been little public discussion of FDT from academic decision theorists (one exception is here); this note attempts to partly address this gap. So that there’s a clear object of discussion, I’m going to focus on Yudkowsky and Soares’ ‘Functional Decision Theory’ (which I’ll refer to as Y&S), though I also read a revised version of Soares and Levinstein’s Cheating Death in Damascus. This note is structured as follows. Section II describes causal decision theory (CDT), evidential decision theory (EDT) and functional decision theory (FDT). Sections III-VI describe problems for FDT: (i) that it sometimes makes bizarre recommendations, recommending an option that is certainly lower-utility than another option; (ii) that it fails to one-box in most instances of Newcomb’s problem, even though the correctness of one-boxing is supposed to be one of the guiding motivations for the theory; (iii) that it results in implausible discontinuities, where what is rational to do can d",2019,2022-01-30 4:55:28,2022-01-30 4:55:28,2020-12-14 23:35:51,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/HT7QKDHD/a-critique-of-functional-decision-theory.html,,TechSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KDQVKBDG,blogPost,2020,"Wentworth, John",Demons in Imperfect Search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search,"One day, a gradient descent algorithm ball was happily rolling down a  high-dimensional surface hill. All it wanted was to roll as far down as possible. Unbeknownst to the ball, just off to the side was a steep drop-off - but there was a small bump between the ball and the drop-off. No matter; there was enough random noise on the ball that it would jump the bump sooner or later. But the ball was headed into unfriendly territory. As the ball rolled along, the bump became taller. The farther it rolled, the taller the bump grew, until no hope remained of finding the big drop anytime before the stars burned out. Then the road began to narrow, and to twist and turn, and to become flatter. Soon the ball rolled down only the slightest slope, with tall walls on both sides constraining its path. The ball had entered the territory of a demon, and now that demon was steering the ball according to its own nefarious ends. This wasn’t the first time the ball had entered the territory of a demon. In early times, the demons had just been bumps which happened to grow alongside the ball’s path, for a time - chance events, nothing more. But every now and then, two bumps in close proximity would push the ball in different directions. The ball would roll on, oblivious, and end up going in one direction or the other. Whichever bump had ""won"" would continue to steer the ball's trajectory - and so a selection process occurred. The ball tended to roll alongside bumps which more effectively controlled its trajectory - bumps which were taller, bumps which steered it away from competing bumps. And so, over time, bumps gave way to barriers, and barriers gave way to demons - twisty paths with high walls to keep the ball contained and avoid competing walls, slowing the ball's descent to a crawl, conserving its potential energy in case a sharp drop were needed to avoid a competitor's wall. The ball’s downhill progress slowed and slowed. Even though the rich, high-dimensional space was filled w",2020-02-11,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-09-05 18:56:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8XJTQQDK/demons-in-imperfect-search.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
967PUGCM,blogPost,2020,G Gordon Worley III,Deconfusing Human Values Research Agenda v1,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1,"On Friday I attended the 2020 Foresight AGI Strategy Meeting. Eventually a report will come out summarizing some of what was talked about, but for now I want to focus on what I talked about in my session on deconfusing human values. For that session I wrote up some notes summarizing what I've been working on and thinking about. None of it is new, but it is newly condensed in one place and in convenient list form, and it provides a decent summary of the current state of my research agenda for building beneficial superintelligent AI; a version 1 of my agenda, if you will. Thus, I hope this will be helpful in making it a bit clearer what it is I'm working on, why I'm working on it, and what direction my thinking is moving in. As always, if you're interesting in collaborating on things, whether that be discussing ideas or something more, please reach out. PROBLEM OVERVIEW  * I think we're confused about what we really mean when we talk about human    values.  * This is a problem because:  * building aligned AI likely requires a mathematically precise understanding of    the structure of human values, though not necessarily the content of human    values;we can't trust AI to discover that structure for us because we would    need to understand it enough to verify the result, and I think we're so    confused about what human values are we couldn't do that without high risk of    error.  * What are values?  * We don't have an agreed upon precise definition, but loosely it's ""stuff    people care about"". * When I talk about ""values"" I mean the cluster we       sometimes also point at with words like value, preference, affinity,       taste, aesthetic, intention, and axiology.        Importantly, what people care about is used to make decisions, and this has    had implications for existing approaches to understanding values.  * Much research on values tries to understand the content of human values or    why humans value what they value, but not what the structure of human",2020-03-23,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-09-05 18:34:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CEWM72I3/deconfusing-human-values-research-agenda-v1.html,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2JPX93V,blogPost,2019,"Pace, Ben","Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell,"An actual debate about instrumental convergence, in a public space! Major respect to all involved, especially Yoshua Bengio for great facilitation. For posterity (i.e. having a good historical archive) and further discussion, I've reproduced the conversation here. I'm happy to make edits at the request of anyone in the discussion who is quoted below. I've improved formatting for clarity and fixed some typos. For people who are not researchers in this area who wish to comment, see the public version of this post here. For people who do work on the relevant areas, please sign up in the top right. It will take a day or so to confirm membership. ORIGINAL POST Yann LeCun: ""don't fear the Terminator"", a short opinion piece by Tony Zador and me that was just published in Scientific American. ""We dramatically overestimate the threat of an accidental AI takeover, because we tend to conflate intelligence with the drive to achieve dominance. [...] But intelligence per se does not generate the drive for domination, any more than horns do."" https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/ COMMENT THREAD #1 Elliot Olds: Yann, the smart people who are very worried about AI seeking power and ensuring its own survival believe it's a big risk because power and survival are instrumental goals for almost any ultimate goal. If you give a generally intelligent AI the goal to make as much money in the stock market as possible, it will resist being shut down because that would interfere with tis goal. It would try to become more powerful because then it could make money more effectively. This is the natural consequence of giving a smart agent a goal, unless we do something special to counteract this. You've often written about how we shouldn't be so worried about AI, but I've never seen you address this point directly. Stuart Russell: It is trivial to construct a toy MDP in which the agent's only reward comes from fetching the coffee. If, in that MDP, the",2019,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-12-14 23:31:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NZ6R6VPP/debate-on-instrumental-convergence-between-lecun-russell.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XD4II9CD,blogPost,2021,"Davidson, Tom",Could Advanced AI Drive Explosive Economic Growth?,Open Philanthropy,,,,https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth,"body ol ul li::before { display: unset !important; } #toc .toc-list ol li:nth-child(10) ol { /* display: none !important; */ } .toc-level-3, .toc-level-4, .toc-level-5, .toc-level-6 {display: none;} .footnote p { line-height: unset !important; }  MathJax.Hub.Config({  extensions: [""tex2jax.js""],",2021-04-08,2022-01-30 4:59:45,2022-01-30 4:59:45,2021-11-14 18:49:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CWGBI3VB,blogPost,2018,Anonymous,Bias in AI: How we Build Fair AI Systems and Less-Biased Humans,THINKPolicy Blog,,,,https://www.ibm.com/blogs/policy/bias-in-ai/,"Without a process to guide the responsible development of trustworthy AI, our systems won’t benefit society — in fact, AI systems could exacerbate the negative consequences of unconscious bias.",2018-02-01,2022-01-30 4:59:37,2022-01-30 4:59:37,2020-12-13 23:10:15,,,,,,,Bias in AI,,,,,,,en-US,© Copyright IBM Corp. 2020,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M9INUUC2/bias-in-ai.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8W6HA68,blogPost,2021,G Gordon Worley III,Bootstrapped Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment,"NB: I doubt any of this is very original. In fact, it's probably right there in the original Friendly AI writings and I've just forgotten where. Nonetheless, I think this is something worth exploring lest we lose sight of it. Consider the following argument:  1. Optimization unavoidably leads to Goodharting (as I like to say, Goodhart is     robust) * This happens so long as we optimize (make choices) based on an        observation, which we must do because that's just how the physics work.      * We can at best make Goodhart effects happen slower, say by quantilization         or satisficing.            2. Attempts to build aligned AI that rely on optimizing for alignment will     eventually fail to become or remain aligned due to Goodhart effects under     sufficient optimization pressure.  3. Thus the only way to build aligned AI that doesn't fail to become and stay     aligned is to not rely on optimization to achieve alignment. This means that, if you buy this argument, huge swaths of AI design space is off limits for building aligned AI, and means many proposals are, by this argument, doomed to fail. Some examples of such doomed approaches:  * HCH  * debate  * IRL/CIRL So what options are left?  * Don't build AI * The AI you don't build is vacuously aligned.          * Friendly AI * AI that is aligned with humans right from the start because it       was programmed to work that way.     * (Yes I know ""Friendly AI"" is an antiquated term, but I don't       know a better one to distinguish the idea of building AI that's aligned       because it's programmed that way from other ways we might build aligned       AI.)          * Bootstrapped alignment * Build AI that is aligned via optimization that is       not powerful enough or optimized (Goodharted) hard enough to cause       existential catastrophe. Use this ""weakly"" aligned AI to build Friendly       AI.         Not building AI is probably not a realistic option unless industrial civilization collapses.",2021-02-27,2022-01-30 4:59:37,2022-01-30 4:59:37,2021-11-13 23:01:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
524FKWKH,blogPost,2015,"Dietterich, Thomas G.",Benefits and Risks of Artificial Intelligence,Thomas G. Dietterich (Medium),,,,https://medium.com/@tdietterich/benefits-and-risks-of-artificial-intelligence-460d288cccf3,"Discussions about Artificial Intelligence (AI) have jumped into the public eye over the past year, with several luminaries speaking…",2015-01-23,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-11-21 18:50:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000001[s0],,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9E4PQP2R,blogPost,2020,"Wentworth, John",Alignment By Default,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default,"Suppose AI continues on its current trajectory: deep learning continues to get better as we throw more data and compute at it, researchers keep trying random architectures and using whatever seems to work well in practice. Do we end up with aligned AI “by default”? I think there’s at least a plausible trajectory in which the answer is “yes”. Not very likely - I’d put it at ~10% chance - but plausible. In fact, there’s at least an argument to be made that alignment-by-default is more likely to work than many fancy alignment proposals, including IRL variants and HCH-family methods. This post presents the rough models and arguments. I’ll break it down into two main pieces:  * Will a sufficiently powerful unsupervised learner “learn human values”? What    does that even mean?  * Will a supervised/reinforcement learner end up aligned to human values, given    a bunch of data/feedback on what humans want? Ultimately, we’ll consider a semi-supervised/transfer-learning style approach, where we first do some unsupervised learning and hopefully “learn human values” before starting the supervised/reinforcement part. As background, I will assume you’ve read some of the core material about human values from the sequences, including Hidden Complexity of Wishes, Value is Fragile, and Thou Art Godshatter. UNSUPERVISED: POINTING TO VALUES In this section, we’ll talk about why an unsupervised learner might not “learn human values”. Since an unsupervised learner is generally just optimized for predictive power, we’ll start by asking whether theoretical algorithms with best-possible predictive power (i.e. Bayesian updates on low-level physics models) “learn human values”, and what that even means. Then, we’ll circle back to more realistic algorithms. Consider a low-level physical model of some humans - e.g. a model which simulates every molecule comprising the humans. Does this model “know human values”? In one sense, yes: the low-level model has everything there is to know abo",2020-08-12,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-08-24 20:26:49,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5W3RZIP8/alignment-by-default.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PZKMEPQE,blogPost,2020,"Wentworth, John S",Alignment as Translation,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation,"Technology Changes Constraints argues that economic constraints are usually modular with respect to technology changes - so for reasoning about technology changes, it’s useful to cast them in terms of economic constraints. Two constraints we’ll talk about here:  * Compute - flops, memory, etc.  * Information - sensors, data, etc. Thanks to ongoing technology changes, both of these constraints are becoming more and more slack over time - compute and information are both increasingly abundant and cheap. Immediate question: what happens in the limit as the prices of both compute and information go to zero? Essentially, we get omniscience: our software has access to a perfect, microscopically-detailed model of the real world. Computers have the memory and processing capability to run arbitrary queries on that model, and predictions are near-perfectly accurate (modulo quantum noise). This limit applies even without AGI - as compute and information become more abundant, our software approaches omniscience, even limiting ourselves to special-purpose reasoning algorithms. Of course, AGI would presumably be closer to omniscience than non-AGI algorithms, at the same level of compute/information. It would be able to more accurately predict more things which aren’t directly observable via available sensors, and it would be able to run larger queries with the same amount of compute. (How much closer to omniscience an AGI would get is an open question, but it would at least not be any worse in a big-O sense.) Next question: as compute and information constraints slacken, which constraints become taut? What new bottlenecks appear, for problems which were previously bottlenecked on compute/information? To put it differently: if our software can run arbitrary queries on an accurate, arbitrarily precise low-level model of the physical world, what else do we need in order to get value out of that capability? Well, mainly we need some way to specify what it is that we want. We",2020-03-19,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-09-05 17:57:16,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VPXDA528/alignment-as-translation.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XE2PI5Z,blogPost,2020,"Wentworth, John S",Alignment As A Bottleneck To Usefulness Of GPT-3,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3,"So there’s this thing where GPT-3 is able to do addition, it has the internal model to do addition, but it takes a little poking and prodding to actually get it to do addition. “Few-shot learning”, as the paper calls it. Rather than prompting the model with Q: What is 48 + 76? A: … instead prompt it with Q: What is 48 + 76? A: 124 Q: What is 34 + 53? A: 87 Q: What is 29 + 86? A: The same applies to lots of other tasks: arithmetic, anagrams and spelling correction, translation, assorted benchmarks, etc. To get GPT-3 to do the thing we want, it helps to give it a few examples, so it can “figure out what we’re asking for”. This is an alignment problem. Indeed, I think of it as the quintessential alignment problem: to translate what-a-human-wants into a specification usable by an AI. The hard part is not to build a system which can do the thing we want, the hard part is to specify the thing we want in such a way that the system actually does it. The GPT family of models are trained to mimic human writing. So the prototypical “alignment problem” on GPT is prompt design: write a prompt such that actual human writing which started with that prompt would likely contain the thing you actually want. Assuming that GPT has a sufficiently powerful and accurate model of human writing, it should then generate the thing you want. Viewed through that frame, “few-shot learning” just designs a prompt by listing some examples of what we want - e.g. listing some addition problems and their answers. Call me picky, but that seems like a rather primitive way to design a prompt. Surely we can do better? Indeed, people are already noticing clever ways to get better results out of GPT-3 - e.g. TurnTrout recommends conditioning on writing by smart people, and the right prompt makes the system complain about nonsense rather than generating further nonsense in response. I expect we’ll see many such insights over the next month or so. CAPABILITIES VS ALIGNMENT AS BOTTLENECK TO VALUE I",2020-07-21,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-08-28 17:27:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GPA9UK4C/alignment-as-a-bottleneck-to-usefulness-of-gpt-3.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JZZEVAWU,blogPost,2021,"Flint, Alex",AI Risk for Epistemic Minimalists,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists,"Financial status: This is independent research, now supported by a grant. I welcome further financial support. Epistemic status: This is an attempt to use only very robust arguments. -------------------------------------------------------------------------------- OUTLINE  * I outline a case for concern about AI that does not invoke concepts of    agency, goal-directedness, or consequential reasoning, does not hinge on    single- or multi-principal or single or multi-agent assumptions, does not    assume fast or slow take-off, and applies equally well to a world of emulated    humans as to de-novo AI.          * The basic argument is about the power that humans will temporarily or    permanently gain by developing AI systems, and the history of quick increases    in human power.          * In the first section I give a case for paying attention to AI at all.          * In the second section I give a case for being concerned about AI.          * In the third section I argue that the business-as-usual trajectory of AI    development is not satisfactory.          * In the fourth section I argue that there are things that can be done now.         THE CASE FOR ATTENTION We already have powerful systems that influence the future of life on the planet. The systems of finance, justice, government, and international cooperation are things that we humans have constructed. The specific design of these systems has influence over the future of life on the planet, meaning that there are small changes that could be made to these systems that would have an impact on the future of life on the planet much larger than the change itself. In this sense I will say that these systems are powerful. Now every single powerful system that we have constructed up to now uses humans as a fundamental building-block. The justice system uses humans as judges and lawyers and administrators. At a mechanical level, the justice system would not execute its intended function without these building-",2021-08-22,2022-01-30 4:59:34,2022-01-30 4:59:34,2021-11-18 23:24:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4A8ENXKH/ai-risk-for-epistemic-minimalists.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZUFFVWA,blogPost,2018,rk,AI development incentive gradients are not uniformly terrible,LessWrong,,,,https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly,"Much of the work for this post was done together with Nuño Sempere Perhaps you think that your values will be best served if the AGI you (or your team, company or nation) are developing is deployed first. Would you decide that it's worth cutting a few corners, reducing your safety budget, and pushing ahead to try and get your AI out the door first? It seems plausible, and worrying, that you might. And if your competitors reason symmetrically, we would get a ""safety race to the bottom"". On the other hand, perhaps you think your values will be better served if your enemy wins than if either of you accidentally produces an unfriendly AI. Would you decide the safety costs to improving your chances aren't worth it? In a simple two player model, you should only shift funds from safety to capabilities if (the relative₁ decrease in chance of friendliness) / (the relative₁ increase in the chance of winning) < (expected relative₂ loss of value if your enemy wins rather than you). Here, the relative₁ increases and decreases are relative to the current values. The relative₂ loss of value is relative to the expected value if you win. The plan of this post is as follows: 1. Consider a very simple model that leads to a safety race. Identify unrealistic assumptions which are driving its results. 2. Remove some of the unrealistic assumptions and generate a different model. Derive the inequality expressed above. 3. Look at some specific example cases, and see how they affect safety considerations. A PARTLY DISCONTINUOUS MODEL Let's consider a model with two players with the same amount of resources. Each player's choice is what fraction of their resources to devote to safety, rather than capabilities. Whichever player contributes more to capabilities wins the race. If you win the race, you either get a good outcome or a bad outcome. Your chance of getting a good outcome increases continuously with the amount you spent on safety. If the other player wins, you get a bad outcome.",2018,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-12-13 23:33:40,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8X6FWAXX/ai-development-incentive-gradients-are-not-uniformly.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8WDX7UC8,blogPost,2020,"Stray, Jonathan",Aligning AI to Human Values means Picking the Right Metrics,AI & Advancing Responsible AI (Medium),,,,https://medium.com/partnership-on-ai/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047,Optimizing for the wrong thing can cause a lot of harm.,2020-04-15,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-09-05 17:22:15,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2Q9796G2/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49VDX9SI,blogPost,2020,"Kovarik, Vojta",AI Unsafety via Non-Zero-Sum Debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BRiMQELD5WYyvncTE/ai-unsafety-via-non-zero-sum-debate,"In this post, I describe how to view debate as a way of assisting a human to spot flaws in an AI’s proposal. I then argue that the zero-sum assumption is critical for making debate work and that various seemingly-helpful modifications of debate might break it instead. -------------------------------------------------------------------------------- A naive way of using arbitrary optimizers as oracles:Suppose you have a black-box optimizer X that can be connected to any well-defined quantity to be maximized. X can potentially be very powerful - e.g., having a highly accurate model of the world and “a lot of optimization power”. One way to turn X into an oracle is to ask it a question and decide to give it reward 1 if we like its answer and 0 if we don’t.[1] Of course, standard AI-safety arguments (e.g., AI takeover and perverse instantiation) suggest that this is a pretty bad idea for powerful X. For the sake of argument, suppose that we can fix all of the “obvious” problems and ensure that X won’t wirehead, won’t try to escape the box we put it in etc., and will only care about the reward it gets for its answer. Two problems with naive optimizers-turned-oracles: (1) telling the difference between good and awesome answers and (2) answers with hidden flaws:One problem with this type of oracles is that it’s hard to decide whether we like its answers or not. Suppose I ask it for food recommendations for the evening and it suggests pancakes. Pancakes seem fine, although there are some foods that I would like better. So should I reward the AI or not? The second problem is that the oracle optimizes for giving answers that seem good to a human. (Not out of malice, but because “actually being good” isn’t well-defined.) And since humans aren’t omniscient, there will be many seemingly good answers that in fact have disastrous consequences if acted upon. To address (1), use two AIs:The first problem can be tackled by using two copies of the optimizer and rewarding the one w",2020-07-03,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-08-28 17:54:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/V932ECDR/ai-unsafety-via-non-zero-sum-debate.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAV9UKUB,blogPost,2019,"Steinhardt, Jacob",AI Alignment Research Overview,LessWrong,,,,https://www.lesswrong.com/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt,"I'm really excited to see someone outline all the work they think needs solving in AI alignment - to describe what the problem looks like, what a solution looks like, and what work has been done so far. Especially from Jacob, who is a coauthor of the Concrete Problems in AI Safety paper. Below, I've included some excerpts from doc. I've included the introduction, the following section describing the categories of technical work, and some high-level information from the long sections on 'technical alignment problem' and the 'detecting failures in advance'. -------------------------------------------------------------------------------- INTRODUCTION This document gives an overview of different areas of technical work that seem necessary, or at least desirable, for creating safe and aligned AI systems. The focus is on safety and alignment of powerful AI systems, i.e. systems that may exceed human capabilities in a broad variety of domains, and which likely act on a large scale. Correspondingly, there is an emphasis on approaches that seem scalable to such systems. By “aligned”, I mean that the actions it pursues move the world towards states that humans want, and away from states that humans don’t want. Some issues with this definition are that different humans might have different preferences (I will mostly ignore this issue), and that there are differences between stated preferences, “revealed” preferences as implied by actions, and preferences that one endorses upon reflection (I won’t ignore this issue). I think it is quite plausible that some topics are missing, and I welcome comments to that regard. My goal is to outline a critical mass of topics in enough detail that someone with knowledge of ML and some limited familiarity with AI alignment as an area would have a collection of promising research directions, a mechanistic understanding of why they are promising, and some pointers for what work on them might look like. To that end, below I outline four br",2019,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-12-23 0:19:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WU6NMTI/ai-alignment-research-overview-by-jacob-steinhardt.html; /Users/jacquesthibodeau/Zotero/storage/EUXPA82T/Steinhardt - 2019 - AI Alignment Research Overview (by Jacob Steinhard.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2FZGP2T6,blogPost,2021,"Flint, Alex",Agency in Conway’s Game of Life,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life,"Financial status: This is independent research. I welcome financial support to make further posts like this possible. Epistemic status: I have been thinking about these ideas for years but still have not clarified them to my satisfaction. -------------------------------------------------------------------------------- OUTLINE  * This post asks whether it is possible, in Conway’s Game of Life, to arrange    for a certain game state to arise after a certain number of steps given    control only of a small region of the initial game state.          * This question is then connected to questions of agency and AI, since one way    to answer this question in the positive is by constructing an AI within    Conway’s Game of Life.          * I argue that the permissibility or impermissibility of AI is a deep property    of our physics.          * I propose the AI hypothesis, which is that any pattern that solves the    control question does so, essentially, by being an AI.         INTRODUCTION In this post I am going to discuss a celular autonoma known as Conway’s Game of Life: In Conway’s Game Life, which I will now refer to as just ""Life"", there is a two-dimensional grid of cells where each cell is either on or off. Over time, the cells switch between on and off according to a simple set of rules:  * A cell that is ""on"" and has fewer than two neighbors that are ""on"" switches    to ""off"" at the next time step          * A cell that is ""on"" and has greater than three neighbors that are ""on""    switches to ""off"" at the next time step          * An cell that is ""off"" and has exactly three neighbors that are ""on"" switches    to ""on"" at the next time step          * Otherwise, the cell doesn’t change         It turns out that these simple rules are rich enough to permit patterns that perform arbitrary computation. It is possible to build logic gates and combine them together into a computer that can simulate any Turing machine, all by setting up a particular elaborate",2021-05-12,2022-01-30 4:59:33,2022-01-30 4:59:33,2021-11-14 18:32:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TGNDE6TV/agency-in-conway-s-game-of-life.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F79JI6NM,blogPost,2015,"Christiano, Paul",Advisor games,AI Alignment (Medium),,,,https://ai-alignment.com/advisor-games-b33382fef68c,A candidate operationalization of “understandable” reasoning.,2015-09-26,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-21 18:46:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KZIMFE2Q/advisor-games-b33382fef68c.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6925VJFK,blogPost,2020,"Irpan, Alex",A Reinforcement Learning Potpourri,Sorta Insightful,,,,http://www.alexirpan.com/2020/05/07/rl-potpourri.html,"I’ve fallen behind on RL literature from the past few months. So, I’ve decided to catch up with a bunch of recent papers.",2020-05-07,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-08-31 19:01:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GKTI32M3/rl-potpourri.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AUP8H67P,blogPost,2018,"Trazzi, Michaël",A Gym Gridworld Environment for the Treacherous Turn,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cKfryXvyJ522iFuNF/a-gym-gridworld-environment-for-the-treacherous-turn,"EDIT: posted here for feedback and discussion. I plan to continue working on different models/environments, so feel free to suggest improvements. (tl;dr: In an attempt to better understand the treacherous turn, I created a gridworld environment where an agent learns to deceive an overseer by adopting an aligned behaviour when weak and takes control after capability gains) -------------------------------------------------------------------------------- At some point in its development, a seed AI may realize that it needs to get rid of its supervisors to achieve its goals. The conception of deception occurs when it conceives that, in order to maximize its chance of taking over, it must begin by exhibiting human-desirable behaviors, before undertaking a treacherous turn  when humans are no longer a threat. From the human perspective, the AI would keep on exhibiting desirable behavior, until it eventually appears dangerous, but is already unstoppable. In an attempt to better formalize the treacherous turn without using ""loaded concepts"", Stuart Armstrong proposed a toy model of the treacherous turn based on ""The Legend of Zelda: A Link to the Past "", which looked like this: In the comments, people mentionned how this model helped them ""move the topic from the 'science fiction' area to 'I can imagine it happening now'"", and seemed interested in an actual Link to the Past Minigame. There have been other simulations of the treacherous turn in the last three years (see for instance gwern's DQN box-pushing robot or Stuart Armstrong's video), but none of them actually simulate a take over where a supervisor is  killed. Hence, I decided to give it a try and simulate Stuart Armstrong's Link to the Past toy model. A GYM GRIDWORLD ENVIRONMENT Gym is an open-source toolkit for Reinforcement Learning Environments developed by Open AI. I decided to use this interface to develop the gridworld environment.  The github repository with the code, demo, and all the details is",2018-07-28,2022-01-30 4:59:32,2022-01-30 4:59:32,2020-11-21 17:50:50,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNRB9W9Q/a-gym-gridworld-environment-for-the-treacherous-turn.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E7AJI52G,blogPost,2018,Wei Dai,A general model of safety-oriented AI development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development,"This may be trivial or obvious for a lot of people, but it doesn't seem like anyone has bothered to write it down (or I haven't looked hard enough). It started out as a generalization of Paul Christiano's IDA, but also covers things like safe recursive self-improvement. Start with a team of one or more humans (researchers, programmers, trainers, and/or overseers), with access to zero or more AIs (initially as assistants). The human/AI team in each round develops a new AI and adds it to the team, and repeats this until maturity in AI technology is achieved. Safety/alignment is ensured by having some set of safety/alignment properties on the team that is inductively maintained by the development process. The reason I started thinking in this direction is that Paul's approach seemed very hard to knock down, because any time a flaw or difficulty is pointed out or someone expresses skepticism on some technique that it uses or the overall safety invariant, there's always a list of other techniques or invariants that could be substituted in for that part (sometimes in my own brain as I tried to criticize some part of it). Eventually I realized this shouldn't be surprising because IDA is an instance of this more general model of safety-oriented AI development, so there are bound to be many points near it in the space of possible safety-oriented AI development practices. (Again, this may already be obvious to others including Paul, and in their minds IDA is perhaps already a cluster of possible development practices consisting of the most promising safety techniques and invariants, rather than a single point.) If this model turns out not to have been written down before, perhaps it should be assigned a name, like Iterated Safety-Invariant AI-Assisted AI Development, or something pithier?",2018,2022-01-30 4:59:32,2022-01-30 4:59:32,2020-12-13 22:26:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VKWZTKF8/a-general-model-of-safety-oriented-ai-development.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NSTEWUAD,blogPost,2020,"Taylor, Jessica","A critical agential account of free will, causation, and physics",Unstable Ontology,,,,https://unstableontology.com/2020/03/05/a-critical-agential-account-of-free-will-causation-and-physics/,"This is an account of free choice in a physical universe. It is very much relevant to decision theory and philosophy of science. It is largely metaphysical, in terms of taking certain things to be …",2020-03-05,2022-01-30 4:59:32,2022-01-30 4:59:32,2020-09-05 18:58:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EATVM75W/a-critical-agential-account-of-free-will-causation-and-physics.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7S5UD2CM,blogPost,2016,Larks,2016 AI Risk Literature Review and Charity Comparison,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison,"INTRODUCTION I've long been concerned about AI Risk. Now that there are a few charities working on the problem, it seems desirable to compare them, to determine where scarce donations should be sent. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to an securities analyst with regard possible investments. However, while people have evaluated individual organisations, I haven't seen anyone else attempt to compare them, so hopefully this is valuable to others. I've attempted to do so. This is a very big undertaking, and I am very conscious of the many ways in which this is not up to the task. The only thing I wish more than the skill and time to do it better is that someone else would do it! If people find this useful enough to warrant doing again next year I should be able to do it much more efficiently, and spend more time on the underlying model of how papers translate into risk-reduction value. My aim is basically to judge the output of each organisation in 2016 and compare it to their budget. This should give a sense for the organisations' average cost-effectiveness. Then we can consider factors that might increase or decrease the marginal cost-effectiveness going forward. This organisation-centric approach is in contrast to a researcher-centric approach, where we would analyse which researchers do good work, and then donate wherever they are. An extreme version of the other approach would be to simply give money directly to researchers - e.g if I like Logical Induction, I would simply fund Scott Garrabrant directly and ignore MIRI. I favour the organisation-centric approach because it helps keep organisations accountable. Additionally, if researcher skill is the only thing that matters for research output, it doesn't really matter which organisations end up getting the money and employing the researchers, assuming broadly the same researchers are hired. Different organisations might hire different resea",2016,2022-01-30 4:59:32,2022-01-30 4:59:32,2020-12-13 21:00:46,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F8QRGTRF/2016-ai-risk-literature-review-and-charity-comparison.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z2KG34C3,blogPost,2019,"Christiano, Paul",Ambitious vs. narrow value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SvuLhtREMy8wRBzpC/ambitious-vs-narrow-value-learning,"(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: The definition of narrow value learning in the previous post focused on the fact that the resulting behavior is limited to some domain. The definition in this post focuses on learning instrumental goals and values. While the definitions are different, I have used the same term for both because I believe that they are both pointing at the same underlying concept. (I do not know if Paul agrees.) I'm including this post to give a different perspective on what I mean by narrow value learning, before delving into conceptual ideas within narrow value learning. -------------------------------------------------------------------------------- Suppose I’m trying to build an AI system that “learns what I want” and helps me get it. I think that people sometimes use different interpretations of this goal. At two extremes of a spectrum of possible interpretations:  * The AI learns my preferences over (very) long-term outcomes. If I were to die    tomorrow, it could continue pursuing my goals without me; if humanity were to    disappear tomorrow, it could rebuild the kind of civilization we would want;     etc. The AI might pursue radically different subgoals than I would on the    scale of months and years, if it thinks that those subgoals better achieve    what I really want.  * The AI learns the narrower subgoals and instrumental values I am pursuing. It    learns that I am trying to schedule an appointment for Tuesday and that I    want to avoid inconveniencing anyone, or that I am trying to fix a particular    bug without introducing new problems, etc. It does not make any effort to    pursue wildly different short-term goals than I would in order to better    realize my long-term values, though it may help me correct some errors that I    would be able to recognize as such. I think that many researchers interested in AI safety per se mostly think about the former. I think that research",2019,2022-01-30 4:58:18,2022-01-30 4:58:18,2020-12-17 4:38:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3CP23MM8/SvuLhtREMy8wRBzpC.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9DSWXAQR,blogPost,2018,"Christiano, Paul",An unaligned benchmark,AI Alignment (Medium),,,,https://ai-alignment.com/an-unaligned-benchmark-b49ad992940b,"What an unaligned AI might look like, how it could go wrong, and how we could fix it.",2018-09-26,2022-01-30 4:58:18,2022-01-30 4:58:18,2020-11-14 3:13:15,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/62K3QGQ4/an-unaligned-benchmark-b49ad992940b.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C85HUN4S,blogPost,2017,"Christiano, Paul",AlphaGo Zero and capability amplification,AI Alignment (Medium),,,,https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446,AlphaGo Zero happens to be a great proof-of-concept of iterated capability amplification (my preferred approach to safe RL).,2017-10-20,2022-01-30 4:58:18,2022-01-30 4:58:18,2020-12-13 21:43:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5BQTZRUN/alphago-zero-and-capability-amplification-ede767bb8446.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XPF7SVG,blogPost,2017,"Christiano, Paul",Approval-maximizing representations,AI Alignment (Medium),,,,https://ai-alignment.com/approval-maximizing-representations-56ee6a6a1fe6,"If we train our agents with human oversight, can they learn superhuman representations?",2017-07-02,2022-01-30 4:58:18,2022-01-30 4:58:18,2020-12-11 22:48:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TZAJ965T/approval-maximizing-representations-56ee6a6a1fe6.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ECTQJIG2,blogPost,2020,"Hubinger, Evan",Weak HCH accesses EXP,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp,"This post is a follow-up to my “Alignment proposals and complexity classes” post. Thanks to Sam Eisenstat for helping with part of the proof here. Previously, I proved that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling access PSPACE while AI safety via market making accesses EXP. At the time, I wasn't sure whether my market making proof would generalize to the others, so I just published it with the PSPACE  proofs instead. However, I have since become convinced that the proof does generalize—and that it generalizes for all of the proposals I mentioned—such that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling all actually access EXP. This post attempts to prove that. UPDATED LIST OF PROPOSALS BY COMPLEXITY CLASS P: Imitation learning (trivial) PSPACE: AI safety via debate (proof) EXP: AI safety via market making (proof), Imitative amplification with weak HCH  (proof below), Approval-based amplification (proof below), Recursive reward modeling (proof below) NEXP: Debate with cross-examination (proof) R: Imitative amplification with strong HCH (proof), AI safety via market making with pointers (proof) PROOFS IMITATIVE AMPLIFICATION WITH WEAK HCH ACCESSES EXP The proof here is similar in structure to my previous proof that weak HCH accesses PSPACE, so I'll only explain where this proof differs from that one. First, since l∈EXP, we know that for any x∈X, Tl(x) halts in O(2poly(n)) steps where n=|x|. Thus, we can construct a function fl(n)=c1+c2ec3nc4 such that for all x∈X, Tl(x) halts in less than or equal to fl(x) steps by picking c3,c4 large enough that they dominate all other terms in the polynomial for all n∈N. Note that fl is then computable in time polynomial in n. Second, let H's new strategy be as follows:  1. Given p, let s,x=M(p:f(|x|)). Then, return accept/reject based on whether s      is an accept or reject state (it will always be one or the oth",2020-07-22,2022-01-30 4:57:32,2022-01-30 4:57:32,2020-08-28 17:22:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XA48EERN/weak-hch-accesses-exp.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WNBDCJ23,blogPost,2015,"Yudkowsky, Eliezer",Unforeseen maximum,Arbital,,,,https://arbital.com/p/unforeseen_maximum/,"When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)",2015,2022-01-30 4:57:32,2022-01-30 4:57:32,2021-02-06 17:14:11,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6EWGR3AT/unforeseen_maximum.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PS7P96DU,blogPost,2013,"Muehlhauser, Luke",Transparency in Safety-Critical Systems,Machine Intelligence Research Institute,,,,https://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/,"In this post, I aim to summarize one common view on AI transparency and AI reliability. It’s difficult to identify the field’s “consensus” on AI transparency and reliability, so instead I will present a common view so that I can use it to introduce a number of complications and open questions that (I think) warrant... Read more »",2013-08-25,2022-01-30 4:57:32,2022-01-30 4:57:32,2021-01-23 20:46:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/STGR44FK/transparency-in-safety-critical-systems.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6Q44U9I6,blogPost,2018,"Demski, Abram",Toward a New Technical Explanation of Technical Explanation,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation,"A NEW FRAMEWORK (Thanks to Valentine for a discussion leading to this post, and thanks to CFAR for running the CFAR-MIRI cross-fertilization workshop. Val provided feedback on a version of this post. Warning: fairly long.) Eliezer's A Technical Explanation of Technical Explanation, and moreover the sequences as a whole, used the best technical understanding of practical epistemology available at the time* -- the Bayesian account -- to address the question of how humans can try to arrive at better beliefs in practice. The sequences also pointed out several holes in this understanding, mainly having to do with logical uncertainty and reflective consistency. MIRI's research program has since then made major progress on logical uncertainty. The new understanding of epistemology -- the theory of logical induction -- generalizes the Bayesian account by eliminating the assumption of logical omniscience. Bayesian belief updates are recovered as a special case, but the dynamics of belief change are non-Bayesian in general. While it might not turn out to be the last word on the problem of logical uncertainty, it has a large number of desirable properties, and solves many problems in a unified and relatively clean framework. It seems worth asking what consequences this theory has for practical rationality. Can we say new things about what good reasoning looks like in humans, and how to avoid pitfalls of reasoning? First, I'll give a shallow overview of logical induction and possible implications for practical epistemic rationality. Then, I'll focus on the particular question of A Technical Explanation of Technical Explanation (which I'll abbreviate TEOTE from now on). Put in CFAR terminology, I'm seeking a gears-level understanding of gears-level understanding. I focus on the intuitions, with only a minimal account of how logical induction helps make that picture work. LOGICAL INDUCTION There are a number of difficulties in applying Bayesian uncertainty to logic. No compu",2018-02-15,2022-01-30 4:57:32,2022-01-30 4:57:32,2021-02-06 18:43:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/THG8KAMT/toward-a-new-technical-explanation-of-technical-explanation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZE6E5SCK,blogPost,2020,"Christiano, Paul",Better priors as a safety problem,AI Alignment (Medium),,,,https://ai-alignment.com/better-priors-as-a-safety-problem-24aa1c300710,Many universal priors are inefficient in the finite data regime. I argue that’s a safety problem and we should try to fix it directly.,2020-07-05,2022-01-30 4:57:26,2022-01-30 4:57:26,2020-08-28 17:38:06,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/A4U2ZXVR/better-priors-as-a-safety-problem-24aa1c300710.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHWDQS39,blogPost,2018,"Christiano, Paul",Directions and desiderata for AI alignment,AI Alignment (Medium),,,,https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4,"I lay out three research directions in AI alignment, and three desiderata that I think should guide research in these areas.",2018-05-12,2022-01-30 4:57:26,2022-01-30 4:57:26,2020-12-11 22:48:28,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QWQ4D45X/directions-and-desiderata-for-ai-control-b60fca0da8f4.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ST23JMFC,blogPost,2017,"Christiano, Paul",Benign model-free RL,AI Alignment (Medium),,,,https://ai-alignment.com/benign-model-free-rl-4aae8c97e385,"Reward learning, robustness, and amplification may be sufficient to train benign model-free RL agents.",2017-06-02,2022-01-30 4:57:26,2022-01-30 4:57:26,2020-12-11 22:48:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WUUBUMP6/benign-model-free-rl-4aae8c97e385.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K9RNDEJE,blogPost,2016,"Clark, Jack; Amodei, Dario",Faulty Reward Functions in the Wild,OpenAI,,,,https://openai.com/blog/faulty-reward-functions/,"Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.",2016-12-22,2022-01-30 4:57:26,2022-01-30 4:57:26,2020-11-21 17:34:09,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000038,,/Users/jacquesthibodeau/Zotero/storage/FVVJ8XT8/faulty-reward-functions.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2V4P5BG,blogPost,2021,"Schulman, John",Frequent arguments about alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment,"Here, I’ll review some arguments that frequently come up in discussions about alignment research, involving one person skeptical of the endeavor (called Skeptic) and one person advocating to do more of it (called Advocate). I mostly endorse the views of the Advocate, but the Skeptic isn't a strawman and makes some decent points. The dialog is mostly based on conversations I've had with people who work on machine learning but don't specialize in safety and alignment. This post has two purposes. First, I want to cache good responses to these questions, so I don't have to think about them each time the topic comes up. Second, I think it's useful for people who work on safety and alignment to be ready for the kind of pushback they'll get when pitching their work to others. Just to introduce myself, I'm a cofounder of OpenAI and lead a team that works on developing and applying reinforcement learning methods; we're working on improving truthfulness and reasoning abilities of language models. 1. DOES ALIGNMENT GET SOLVED AUTOMATICALLY AS OUR MODELS GET SMARTER? Skeptic: I think the alignment problem gets easier as our models get smarter. When we train sufficiently powerful generative models, they'll learn the difference between human smiles and human wellbeing; the difference between the truth and common misconceptions; and various concepts they'll need for aligned behavior. Given all of this internal knowledge, we just have to prompt them appropriately to get the desired behavior. For example, to get wise advice from a powerful language model, I just have to set up a conversation between myself and ""a wise and benevolent AI advisor."" Advocate: The wise AI advisor you described has some basic problems, and I'll get into those shortly. But more generally, prompting an internet-trained generative model (like raw GPT-3) is a very poor way of getting aligned behavior, and we can easily do much better. It'll occasionally do something reasonable, but that's not nearly good",2021-06-22,2022-01-30 4:57:26,2022-01-30 4:57:26,2021-11-14 18:45:41,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CT4CHZBU/frequent-arguments-about-alignment.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R7GVJT8S,blogPost,2021,"Barnes, Beth",Imitative Generalisation (AKA 'Learning the Prior'),AI Alignment Forum,,,,https://www.alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1,"TL;DR We want to be able to supervise models with superhuman knowledge of the world and how to manipulate it. For this we need an overseer to be able to learn or access all the knowledge our models have, in order to be able to understand the consequences of suggestions or decisions from the model. If the overseers don’t have access to all the same knowledge as the model, it may be easy for the model to deceive us, suggesting plans that look good to us but that may have serious negative consequences. We might hope to access what the model knows just by training it to answer questions. However, we can only train on questions that humans are able to answer[1]. This gives us a problem that’s somewhat similar to the standard formulation of transduction: we have some labelled training set (questions humans can answer), and we want to transfer to an unlabelled dataset (questions we care about), that may be differently distributed. We might hope that our models will naturally generalize correctly from easy-to-answer questions to the ones that we care about. However, a natural pathological generalisation is for our models to only give us ‘human-like’ answers to questions, even if it knows the best answer is different. If we only have access to these human-like answers to questions, that probably doesn’t give us enough information to supervise a superhuman model. What we’re going to call ‘Imitative Generalization’ is a possible way to narrow the gap between the things our model knows, and the questions we can train our model to answer honestly. It avoids the pathological generalisation by only using ML for IID tasks, and imitating the way humans generalize. This hopefully gives us answers that are more like ‘how a human would answer if they’d learnt from all the data the model has learnt from’. We supervise how the model does the transfer, to get the sort of generalisation we want. It’s worth noting there are enough serious open questions that imitative generalization is",2021,2022-01-30 4:57:25,2022-01-30 4:57:25,2021-11-13 19:36:31,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/U82V8QHX/imitative-generalisation-aka-learning-the-prior-1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA5ICP5K,blogPost,2018,"Christiano, Paul",Implicit extortion,AI Alignment (Medium),,,,https://ai-alignment.com/implicit-extortion-3c80c45af1e3,"Extortion can be equally effective, and harder to notice, when you don’t tell the target it’s occurring.",2018-04-13,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-11-14 3:13:26,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BN9HHUAJ/implicit-extortion-3c80c45af1e3.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X9RN5I7J,blogPost,2018,Tom B Brown; Catherine Olsson,Introducing the Unrestricted Adversarial Examples Challenge,Google AI Blog,,,,http://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html,"Posted by Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team   Machine learning is being deployed in more and more rea...",2018,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-12-13 22:18:26,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8H8Q7D4X/introducing-unrestricted-adversarial.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QHGKPS7W,blogPost,2018,"Christiano, Paul",Techniques for optimizing worst-case performance,AI Alignment (Medium),,,,https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99,Optimizing neural networks for worst-case performance looks really hard. Here’s why I have hope.,2018-02-02,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-12-13 22:20:08,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/ET67S8GH/techniques-for-optimizing-worst-case-performance-39eafec74b99.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QG2D78EJ,blogPost,2018,"Christiano, Paul",The easy goal inference problem is still hard,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In this post (original here), Paul Christiano analyzes the ambitious value learning approach. He considers a more general view of ambitious value learning where you infer preferences more generally (i.e. not necessarily in the form of a utility function), and you can ask the user about their preferences, but it’s fine to imagine that you infer a utility function from data and then optimize it. The key takeaway is that in order to infer preferences that can lead to superhuman performance, it is necessary to understand how humans are biased, which seems very hard to do even with infinite data. -------------------------------------------------------------------------------- One approach to the AI control problem goes like this:  1. Observe what the user of the system says and does.  2. Infer the user’s preferences.  3. Try to make the world better according to the user’s preference, perhaps     while working alongside the user and asking clarifying questions. This approach has the major advantage that we can begin empirical work today — we can actually build systems which observe user behavior, try to figure out what the user wants, and then help with that. There are many applications that people care about already, and we can set to work on making rich toy models. It seems great to develop these capabilities in parallel with other AI progress, and to address whatever difficulties actually arise, as they arise. That is, in each domain where AI can act effectively, we’d like to ensure that AI can also act effectively in the service of goals inferred from users (and that this inference is good enough to support foreseeable applications). This approach gives us a nice, concrete model of each difficulty we are trying to address. It also provides a relatively clear indicator of whether our ability to control AI lags behind our ability to build it. And by being technically interesting an",2018,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-12-17 4:36:24,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8RHW83JV/h9DesGT3WT9u2k7Hr.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W2WEFH9K,blogPost,2020,"Christiano, Paul",Inaccessible information,AI Alignment (Medium),,,,https://ai-alignment.com/inaccessible-information-c749c6a88ce,What kind of information might be hard to elicit from ML models?,2020-06-03,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-08-31 18:11:57,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BX8MMB8R/inaccessible-information-c749c6a88ce.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMMWBRTC,blogPost,2019,"Christiano, Paul",Informed oversight,AI Alignment (Medium),,,,https://ai-alignment.com/informed-oversight-18fcb5d3d1e1,An overseer can provide adequate rewards for an agent if they know everything the agent knows. (Update of a 2016 post.),2019-01-24,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-11-14 3:13:48,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TNH2CIIC/informed-oversight-18fcb5d3d1e1.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D94ADJCV,blogPost,2020,"Christiano, Paul",Learning the prior,AI Alignment (Medium),,,,https://ai-alignment.com/learning-the-prior-48f61b445c04,"I suggest using neural nets to approximate our real prior, rather than implicitly using neural nets themselves as the prior.",2020-07-05,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-08-28 17:40:46,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NHBV8RXZ/learning-the-prior-48f61b445c04.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S9URNKK4,blogPost,2019,"Christiano, Paul",The strategy-stealing assumption,AI Alignment (Medium),,,,https://ai-alignment.com/the-strategy-stealing-assumption-a26b8b1ed334,"If humans initially control 99% of the world’s resources, when can they secure 99% of the long-term influence?",2019-09-15,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-12-11 22:48:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9QXT7R5V/the-strategy-stealing-assumption-a26b8b1ed334.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IJFT7TDI,blogPost,2019,"Christiano, Paul",Towards formalizing universality,AI Alignment (Medium),,,,https://ai-alignment.com/towards-formalizing-universality-409ab893a456,An attempt to formalize universality as “able to understand anything that any computation can understand.”,2019-01-11,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-11-14 3:13:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/C7P8EW68/towards-formalizing-universality-409ab893a456.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JB955CSH,blogPost,2018,"Christiano, Paul",Two guarantees,AI Alignment (Medium),,,,https://ai-alignment.com/two-guarantees-c4c03a6b434f,"I suspect AI alignment should aim to separately establish good performance in the average case, and lack-of-malice in the worst case.",2018-04-09,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-11-14 3:13:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DW25F88R/two-guarantees-c4c03a6b434f.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JR64UTGB,blogPost,2019,"Christiano, Paul",Universality and model-based RL,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd,"Ascription universality may be very helpful for safe model-based RL, facilitating benign induction and “transparent” models.",2019-10-04,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-12-11 22:48:17,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UTQURPQ7/universality-and-model-based-rl-b08701394ddd.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCDGQ3EW,blogPost,2019,"Christiano, Paul",Universality and consequentialism within HCH,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd,One exotic reason HCH can fail to be universal is the emergence of malicious patterns of behavior; universality may help address this risk.,2019-01-10,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-11-14 3:13:34,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M72NERZD/universality-and-consequentialism-within-hch-c0bee00365bd.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BEESSIWV,blogPost,2019,"Christiano, Paul",Universality and security amplification,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-security-amplification-551b314a3bab,A slightly more detailed view of security amplification.,2019-01-03,2022-01-30 4:57:24,2022-01-30 4:57:24,2020-11-14 3:13:23,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7F92Q22J/universality-and-security-amplification-551b314a3bab.html; /Users/jacquesthibodeau/Zotero/storage/3XIIMECQ/universality-and-security-amplification-551b314a3bab.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V99RRXAV,blogPost,2019,"Christiano, Paul",Worst-case guarantees,AI Alignment (Medium),,,,https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d,"Reviewing the prospects for training models to behave acceptably on all inputs, rather than just the training distribution.",2019-03-23,2022-01-30 4:57:23,2022-01-30 4:57:23,2020-11-14 3:13:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IXPVJKZM/training-robust-corrigibility-ce0e0a3b9b4d.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SK6TQ2N,blogPost,2020,"Barnes, Beth; Christiano, Paul",Writeup: Progress on AI Safety via Debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1,"This is a writeup of the research done by the ""Reflection-Humans"" team at OpenAI in Q3 and Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to get correct and helpful answers from experts, without the evaluators themselves being expert in the domain of the questions. This follows from the original work on AI Safety via Debate and the call for research on human aspects of AI safety, and is also closely related to work on Iterated Amplification. AUTHORS AND ACKNOWLEDGEMENTS The main researchers on this project were Elizabeth Barnes, Paul Christiano, Long Ouyang and Geoffrey Irving. We are grateful to many others who offered ideas and feedback. In particular: the cross-examination idea was inspired by a conversation with Chelsea Voss; Adam Gleave had helpful ideas about the long computation problem; Jeff Wu, Danny Hernandez and Gretchen Krueger gave feedback on a draft; we had helpful conversations with Amanda Askell, Andreas Stuhlmüller and Joe Collman, as well as others on the Ought team and the OpenAI Reflection team. We’d also like to thank our contractors who participated in debate experiments, especially David Jones, Erol Akbaba, Alex Deam and Chris Painter. Oliver Habryka helped format and edit the document for the AI Alignment Forum. Note by Oliver: There is currently a bug with links to headings in a post, causing them to not properly scroll when clicked. Until that is fixed, just open those links in a new tab, which should scroll correctly. OVERVIEW Motivation As we apply ML to increasingly important and complex tasks, the problem of evaluating behaviour and providing a good training signal becomes more difficult. We already see examples of RL leading to undesirable behaviours that superficially ‘look good’ to human evaluators (see this collection of examples). One example from an OpenAI paper is an agent learning incorrect behaviours in a 3d simulator, because the behaviours look like the desired behaviour in the 2d",2020-02-05,2022-01-30 4:57:23,2022-01-30 4:57:23,2020-09-07 18:11:36,,,,,,,Writeup,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/K8SIBJDZ/writeup-progress-on-ai-safety-via-debate-1.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HAJA2B87,blogPost,2020,"Christiano, Paul",“Unsupervised” translation as an (intent) alignment problem,AI Alignment (Medium),,,,https://ai-alignment.com/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68,Unsupervised translation is an interesting domain where models seem to “know” something we can’t get them to tell us.,2020-09-30,2022-01-30 4:57:23,2022-01-30 4:57:23,2020-12-11 22:48:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W39ZJ8FT/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AZ2N39G,blogPost,2019,"Christiano, Paul",What failure looks like,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like,"The stereotyped image of AI catastrophe is a powerful, malicious AI system that takes its creators by surprise and quickly achieves a decisive advantage over the rest of humanity. I think this is probably not what failure will look like, and I want to try to paint a more realistic picture. I’ll tell the story in two parts:  * Part I: machine learning will increase our ability to “get what we can    measure,” which could cause a slow-rolling catastrophe. (""Going out with a    whimper."")  * Part II: ML training, like competitive economies or natural ecosystems, can    give rise to “greedy” patterns that try to expand their own influence. Such    patterns can ultimately dominate the behavior of a system and cause sudden    breakdowns. (""Going out with a bang,"" an instance of optimization daemons    [https://arbital.com/p/daemons/].) I think these are the most important problems if we fail to solve intent alignment [https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6]. In practice these problems will interact with each other, and with other disruptions/instability caused by rapid progress. These problems are worse in worlds where progress is relatively fast, and fast takeoff can be a key risk factor, but I’m scared even if we have several years. With fast enough takeoff, my expectations start to look more like the caricature---this post envisions reasonably broad deployment of AI, which becomes less and less likely as things get faster. I think the basic problems are still essentially the same though, just occurring within an AI lab rather than across the world. (None of the concerns in this post are novel.) PART I: YOU GET WHAT YOU MEASURE If I want to convince Bob to vote for Alice, I can experiment with many different persuasion strategies and see which ones work. Or I can build good predictive models of Bob’s behavior and then search for actions that will lead him to vote for Alice. These are powerful techniques for achieving any goal that can be ea",2019,2022-01-30 4:57:23,2022-01-30 4:57:23,2019-12-16 19:59:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s4]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KNVMK6EZ/what-failure-looks-like.html; /Users/jacquesthibodeau/Zotero/storage/WV669JT4/what-failure-looks-like.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R78XUR43,blogPost,2017,"Yudkowsky, Eliezer",There's No Fire Alarm for Artificial General Intelligence,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/10/13/fire-alarm/,"What is the function of a fire alarm?   One might think that the function of a fire alarm is to provide you with important evidence about a fire existing, allowing you to change your policy accordingly and exit the building. In the classic experiment by Latane and Darley in 1968, eight groups of... Read more »",2017-10-14,2022-01-30 4:56:59,2022-01-30 4:56:59,2020-12-13 20:50:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/V25W5CF3/fire-alarm.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
422HC9XW,blogPost,2002,"Yudkowsky, Eliezer",The AI-Box Experiment,Eliezer S Yudkowsky,,,,https://www.yudkowsky.net/singularity/aibox,,2002,2022-01-30 4:56:59,2022-01-30 4:56:59,2020-11-21 17:49:14,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000035,,/Users/jacquesthibodeau/Zotero/storage/97EN4T37/aibox.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97JPXM8D,blogPost,2015,"Yudkowsky, Eliezer",Task-directed AGI,Arbital,,,,https://arbital.com/p/task_agi/,"An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.",2015,2022-01-30 4:56:59,2022-01-30 4:56:59,2021-02-06 17:13:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QZKRN49F,blogPost,2018,"Yudkowsky, Eliezer",The Rocket Alignment Problem,Machine Intelligence Research Institute,,,,https://intelligence.org/2018/10/03/rocket-alignment/,"The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (Somewhere in a not-very-near neighboring world, where science took a very different course…)   ALFONSO:  Hello, Beth. I’ve noticed a lot of speculations lately about “spaceplanes” being used to attack cities, or possibly becoming infused with malevolent... Read more »",2018-10-03,2022-01-30 4:56:59,2022-01-30 4:56:59,2020-12-13 23:54:02,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/77I4C3AE/rocket-alignment.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSCES72W,blogPost,2015,"Yudkowsky, Eliezer",Sufficiently optimized agents appear coherent,Arbital,,,,https://arbital.com/p/optimized_agent_appears_coherent/,"If you could think as well as a superintelligence, you'd be at least that smart yourself.",2015,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:12:04,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2II69UX2/optimized_agent_appears_coherent.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
238TWTH2,blogPost,2017,"Yudkowsky, Eliezer",Separation from hyperexistential risk,Arbital,,,,https://arbital.com/p/hyperexistential_separation/,"The AI should be widely separated in the design space from any AI that would constitute a ""hyperexistential risk"" (anything worse than death).",2017,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:33:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DEPHXC82/hyperexistential_separation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7375JA5,blogPost,2015,"Soares, Nate","Safety engineering, target selection, and alignment theory",Machine Intelligence Research Institute,,,,https://intelligence.org/2015/12/31/safety-engineering-target-selection-and-alignment-theory/,"Artificial intelligence capabilities research is aimed at making computer systems more intelligent — able to solve a wider range of problems more effectively and efficiently. We can distinguish this from research specifically aimed at making AI systems at various capability levels safer, or more “robust and beneficial.” In this post, I distinguish three kinds of direct... Read more »",2015-12-31,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-11-21 17:07:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XWWTIPKK/safety-engineering-target-selection-and-alignment-theory.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IW5QC4TT,blogPost,2018,"Garrabrant, Scott",Robustness to Scale,LessWrong,,,,https://www.lesswrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale,"I want to quickly draw attention to a concept in AI alignment: Robustness to Scale. Briefly, you want your proposal for an AI to be robust (or at least fail gracefully) to changes in its level of capabilities. I discuss three different types of robustness to scale: robustness to scaling up, robustness to scaling down, and robustness to relative scale. The purpose of this post is to communicate, not to persuade. It may be that we want to bite the bullet of the strongest form of robustness to scale, and build an AGI that is simply not robust to scale, but if we do, we should at least realize that we are doing that. Robustness to scaling up means that your AI system does not depend on not being too powerful. One way to check for this is to think about what would happen if the thing that the AI is optimizing for were actually maximized. One example of failure of robustness to scaling up is when you expect an AI to accomplish a task in a specific way, but it becomes smart enough to find new creative ways to accomplish the task that you did not think of, and these new creative ways are disastrous. Another example is when you make an AI that is incentivized to do one thing, but you add restrictions that make it so that the best way to accomplish that thing has a side effect that you like. When you scale the AI up, it finds a way around your restrictions. Robustness to scaling down means that your AI system does not depend on being sufficiently powerful. You can't really make your system still work when it scales down, but you can maybe make sure it fails gracefully. For example, imagine you had a system that was trying to predict humans, and use these predictions to figure out what to do. When scaled up all the way, the predictions of humans are completely accurate, and it will only take actions that the predicted humans would approve of. If you scale down the capabilities, your system may predict the humans incorrectly. These errors may multiply as you stack many predi",2018-02-21,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 18:49:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z6Q9GRDC/robustness-to-scale.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2TSM99QU,blogPost,2020,"Demski, Abram",Radical Probabilism,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1,"This is an expanded version of my talk. I assume a high degree of familiarity with Bayesian probability theory. Toward a New Technical Explanation of Technical Explanation -- an attempt to convey the practical implications of logical induction -- was one of my most-appreciated posts, but I don't really get the feeling that very many people have received the update. Granted, that post was speculative, sketching what a new technical explanation of technical explanation might look like. I think I can do a bit better now. If the implied project of that post had really been completed, I would expect new practical probabilistic reasoning tools, explicitly violating Bayes' law. For example, we might expect:  * A new version of information theory. * An update to the ""prediction=compression"" maxim, either repairing it to       incorporate the new cases, or explicitly denying it and providing a good       intuitive account of why it was wrong.     * A new account of concepts such as       mutual information, allowing for the fact that variables have behavior       over thinking time; for example, variables may initially be very       correlated, but lose correlation as our picture of each variable becomes       more detailed.          * New ways of thinking about epistemology. * One thing that my post did manage       to do was to spell out the importance of ""making advanced predictions"", a       facet of epistemology which Bayesian thinking does not do justice to.     * However, I left aspects of the       problem of old evidence open, rather than giving a complete way to think       about it.          * New probabilistic structures. * Bayesian Networks are one really nice way to       capture the structure of probability distributions, making them much       easier to reason about. Is there anything similar for the new, wider space       of probabilistic reasoning which has been opened up?         Unfortunately, I still don't have any of those things to offer. The aim o",2020-08-18,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-08-27 16:25:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JPB8ZRTA/radical-probabilism-1.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WBGESJZX,blogPost,2015,"Yudkowsky, Eliezer",Patch resistance,Arbital,,,,https://arbital.com/p/patch_resistant/,One does not simply solve the value alignment problem.,2015,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:10:55,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SKWUI9EH/patch_resistant.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VAGVQFSF,blogPost,2018,"Garrabrant, Scott",Optimization Amplifies,LessWrong,,,,https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies,"I talk here about how a mathematician mindset can be useful for AI alignment. But first, a puzzle: Given m, what is the least number n≥2 such that for 2≤k≤m, the base k  representation of n consists entirely of 0s and 1s?  If you want to think about it yourself, stop reading. For m=2, n=2.  For m=3, n=3. For m=4, n=4. For m=5, n=82,000. Indeed, 82,000 is 10100000001010000 in binary, 11011111001 in ternary, 110001100 in base 4, and 10111000 in base 5. What about when m=6? So, a mathematician might tell you that this is an open problem. It is not known if there is any n≥2 which consists of 0s and 1s in bases 2 through 6. A scientist, on the other hand, might just tell you that clearly no such number exists. There are 2k−1 numbers that consist of k 0s and 1s in base 6. Each of these has roughly log5(6)⋅k digits in base 5, and assuming things are roughly evenly distributed, each of these digits is a 0 or a 1 with ""probability"" 2/5. The ""probability"" that there is any number of length k that has the property is thus less than 2k⋅(2/5)k=(4/5)k. This means that as you increase k, the ""probability"" that you find a number with the property drops off exponentially, and this is not even considering bases 3 and 4. Also, we have checked all numbers up to 2000 digits. No number with this property exists. Who is right?  Well, they are both right. If you want to have fun playing games with proofs, you can consider it an open problem and try to prove it. If you want to get the right answer, just listen to the scientist. If you have to choose between destroying the world with a 1% probability and destroying the world if a number greater than 2 which consists of 0s and 1s in bases 2 through 6 exists, go with the latter. It is tempting to say that we might be in a situation similar to this. We need to figure out how to make safe AI, and we maybe don't have that much time. Maybe we need to run experiments, and figure out what is true about what we should do and not waste ou",2018-06-26,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 18:45:57,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/T4CJB5X5/optimization-amplifies.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QASC9Z4D,blogPost,2016,"Yudkowsky, Eliezer",Task (AI goal),Arbital,,,,https://arbital.com/p/task_goal/,"When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.",2016,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:18:21,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VSAFI8SC/task_goal.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AJCWUID,blogPost,2020,"Hubinger, Evan",Synthesizing amplification and debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate,"BACKGROUND One possible way to train an amplification model is to use an auxiliary reinforcement learning objective to help guide the training of the amplification model. This could be done either by training two separate models, an agent and a question-answerer, or a single model trained on a joint objective. For example, from a comment Paul left on “A dilemma for prosaic AI alignment:” I normally imagine using joint training in these cases, rather than pre-training + fine-tuning. e.g., at every point in time we maintain an agent and a question-answerer, where the question-answerer ""knows everything the agent knows."" They get better together, with each gradient update affecting both of them, rather than first training a good agent and then adding a good question-answerer. (Independently of concerns about mesa-optimization, I think the fine-tuning approach would have trouble because you couldn't use statistical regularities from the ""main"" objective to inform your answers to questions, and therefore your question answers will be dumber than the policy and so you couldn't get a good reward function or specification of catastrophically bad behavior.) In my last post, I expressed skepticism of such non-imitative amplification approaches, though in this post I want to propose a possible way in which some of my concerns with this style of approach could addressed by integrating ideas from AI safety via debate. I'll start by describing the basic idea in broad terms, then give a more careful, technical description of the sort of training procedure I have in mind. THE PROPOSAL The basic idea is as follows: debate naturally yields an RL objective, so if you want to add an auxiliary RL objective to amplification, why not use the RL objective from debate? Specifically, the idea is to conduct a debate not between copies of the model M, but between copies of the amplified model Amp(M) (where  Amp(M) is a human with access to the model M). That gives you both an RL reward ari",2020-02-05,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-09-07 18:16:00,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XCB5N6K8/synthesizing-amplification-and-debate.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BX7UP66I,blogPost,2016,"Yudkowsky, Eliezer",So Far: Unfriendly AI Edition,Econlib,,,,https://www.econlib.org/archives/2016/03/so_far_unfriend.html,"Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Bryan Caplan issued the following challenge, naming Unfriendly AI as one among several disaster scenarios he thinks is unlikely: …",2016-03-29,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:17:06,,,,,,,So Far,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A  Section: Cost-benefit Analysis,,/Users/jacquesthibodeau/Zotero/storage/BDQ6EDXU/so_far_unfriend.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N9ZZVPT7,blogPost,2017,"Yudkowsky, Eliezer",Security Mindset and the Logistic Success Curve,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/,"Follow-up to:   Security Mindset and Ordinary Paranoia   (Two days later, Amber returns with another question.)   AMBER:  Uh, say, Coral. How important is security mindset when you’re building a whole new kind of system—say, one subject to potentially adverse optimization pressures, where you want it to have some sort of robustness property? CORAL:  How novel is the... Read more »",2017-11-26,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:27:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/H9XPNAQI/security-mindset-and-the-logistic-success-curve.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X8BWI6VK,blogPost,2017,"Yudkowsky, Eliezer",Security Mindset and Ordinary Paranoia,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/,"The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (AMBER, a philanthropist interested in a more reliable Internet, and CORAL, a computer security professional, are at a conference hotel together discussing what Coral insists is a difficult and important issue: the difficulty of building “secure”... Read more »",2017-11-25,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:29:47,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/3SIBGGKQ/security-mindset-ordinary-paranoia.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C6S6XHER,blogPost,2017,"Yudkowsky, Eliezer",Problem of fully updated deference,Arbital,,,,https://arbital.com/p/updated_deference/,Why moral uncertainty doesn't stop an AI from defending its off-switch.,2017,2022-01-30 4:56:58,2022-01-30 4:56:58,2021-02-06 17:26:31,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EB7M3IS4/updated_deference.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q5IG88VT,blogPost,2015,"Yudkowsky, Eliezer",Ontology identification problem,Arbital,,,,https://arbital.com/p/ontology_identification/,"How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?",2015,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-02-06 17:08:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CTUFJV5B/ontology_identification.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZ3RHMBA,blogPost,2017,"Yudkowsky, Eliezer",Non-adversarial principle,Arbital,,,,https://arbital.com/p/nonadversarial/,"At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.",2017,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-02-06 17:25:35,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZI24N9IH/nonadversarial.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9WIT2K8G,blogPost,2015,"Yudkowsky, Eliezer",Nearest unblocked strategy,Arbital,,,,https://arbital.com/p/nearest_unblocked/,"If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?",2015,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-01-23 20:51:27,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2DKXZF46/nearest_unblocked.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC6647C5,blogPost,2020,"Schlegeris, Buck",My personal cruxes for working on AI safety,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety,"The following is a heavily edited transcript of a talk I gave for the Stanford Effective Altruism club on 19 Jan 2020. I had rev.com transcribe it, and then Linchuan Zhang, Rob Bensinger and I edited it for style and clarity, and also to occasionally have me say smarter things than I actually said. Linch and I both added a few notes throughout. Thanks also to Bill Zito, Ben Weinstein-Raun, and Howie Lempel for comments.  I feel slightly weird about posting something so long, but this is the natural place to put it. Over the last year my beliefs about AI risk have shifted moderately; I expect that in a year I'll think that many of the things I said here were dumb. Also, very few of the ideas here are original to me. -- After all those caveats, here's the talk: INTRODUCTION It's great to be here. I used to hang out at Stanford a lot, fun fact. I moved to America six years ago, and then in 2015, I came to Stanford EA every Sunday, and there was, obviously, a totally different crop of people there. It was really fun. I think we were a lot less successful than the current Stanford EA iteration at attracting new people. We just liked having weird conversations about weird stuff every week. It was really fun, but it's really great to come back and see a Stanford EA which is shaped differently.  Today I'm going to be talking about the argument for working on AI safety that compels me to work on AI safety, rather than the argument that should compel you or anyone else. I'm going to try to spell out how the arguments are actually shaped in my head. Logistically, we're going to try to talk for about an hour with a bunch of back and forth and you guys arguing with me as we go. And at the end, I'm going to do miscellaneous Q and A for questions you might have. And I'll probably make everyone stand up and sit down again because it's unreasonable to sit in the same place for 90 minutes. META LEVEL THOUGHTS I want to first very briefly talk about some concepts I have that a",2020-02-13,2022-01-30 4:56:57,2022-01-30 4:56:57,2020-09-05 19:15:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5FGH76JD/my-personal-cruxes-for-working-on-ai-safety.html,,MetaSafety; MIRI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMTAEC25,blogPost,2021,"Bensinger, Rob; Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin","MIRI comments on Cotra's ""Case for Aligning Narrowly Superhuman Models""",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly,"Below, I’ve copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1–3 on a draft of Ajeya Cotra’s ""Case for Aligning Narrowly Superhuman Models."" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah. The section divisions below correspond to the sections in Cotra's post. 0. INTRODUCTION How can we train GPT-3 to give “the best health advice it can give” using demonstrations and/or feedback from humans who may in some sense “understand less” about what to do when you’re sick than GPT-3 does? Eliezer Yudkowsky: I've had some related conversations with Nick Beckstead. I'd be hopeful about this line of work primarily because I think it points to a bigger problem with the inscrutable matrices of floating-point numbers, namely, we have no idea what the hell GPT-3 is thinking and cannot tell it to think anything else. GPT-3 has a great store of medical knowledge, but we do not know where that medical knowledge is; we do not know how to tell it to internally apply its medical knowledge rather than applying other cognitive patterns it has stored. If this is still the state of opacity of AGI come superhuman capabilities, we are all immediately dead. So I would be relatively more hopeful about any avenue of attack for this problem that used anything other than an end-to-end black box - anything that started to address, ""Well, this system clearly has a bunch of medical knowledge internally, can we find that knowledge and cause it to actually be applied"" rather than ""What external forces can we apply to this solid black box to make it think more about healthcare?"" Evan Hubinger: +1 I continue to think that language model transparency research is the single most valuable current research direction within the class of standard ML research, for similar reasons to what Eliezer said above. Ajeya Cotra: Thanks! I'm also excited about language model transparency, and would love to find ways to make it more tractable as",2021-03-05,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-11-14 16:10:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VJVI4PVJ/miri-comments-on-cotra-s-case-for-aligning-narrowly.html,,TechSafety; AmbiguousSafety,,,,,"Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDJ87AAD,blogPost,2020,"Demski, Abram",Mesa-Search vs Mesa-Control,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control,"I currently see the spontaneous emergence of learning algorithms as significant evidence for the commonality of mesa-optimization in existing ML, and suggestive evidence for the commonality of inner alignment problems in near-term ML. [I currently think that there is only a small amount of evidence toward this. However, due to thinking about the issues, I've still made a significant personal update in favor of inner alignment problems being frequent.] This is bad news, in that it greatly increases my odds on this alignment problem arising in practice. It's good news in that it suggests this alignment problem won't catch ML researchers off guard; maybe there will be time to develop countermeasures while misaligned systems are at only a moderate level of capability. In any case, I want to point out that the mesa-optimizers suggested by this evidence might not count as mesa-optimizers by some definitions. SEARCH VS CONTROL Nevan Wichers comments on spontaneous-emergence-of-learning: I don't think that paper is an example of mesa optimization. Because the policy could be implementing a very simple heuristic to solve the task, similar to: Pick the image that lead to highest reward in the last 10 timesteps with 90% probability. Pik an image at random with 10% probability. So the policy doesn't have to have any properties of a mesa optimizer like considering possible actions and evaluating them with a utility function, ect. In Selection vs Control, I wrote about two different kinds of 'optimization':  * Selection refers to search-like systems, which look through a number of    possibilities and select one.  * Control refers to systems like thermostats, organisms, and missile guidance    systems. These systems do not get a re-do for their choices. They make    choices which move toward the goal at every moment, but they don't get to    search, trying many different things -- at least, not in the same sense. I take Nevan Wichers to be saying that there is no eviden",2020-08-18,2022-01-30 4:56:57,2022-01-30 4:56:57,2020-08-27 16:24:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QTUPGMVJ/mesa-search-vs-mesa-control.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7D5XWUKB,blogPost,2017,"Yudkowsky, Eliezer",Minimality principle,Arbital,,,,https://arbital.com/p/minimality_principle/,The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.,2017,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-02-06 17:24:51,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5XIDGU7W/minimality_principle.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3GC7A5D,blogPost,2017,"Yudkowsky, Eliezer",Meta-rules for (narrow) value learning are still unsolved,Arbital,,,,https://arbital.com/p/meta_unsolved/,"We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.",2017,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-02-06 17:23:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/249Q2QQP/meta_unsolved.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDIAIAFH,blogPost,2014,"Muehlhauser, Luke",How to study superintelligence strategy,Luke Muehlhauser,,,,,,2014,2022-01-30 4:56:57,2022-01-30 4:56:57,,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,MetaSafety; MIRI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98F9BXWS,blogPost,2015,"Yudkowsky, Eliezer",Edge instantiation,Arbital,,,,https://arbital.com/p/edge_instantiation/,"When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.",2015,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-01-23 20:50:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/52UEQ8PH/edge_instantiation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZ59QMTX,blogPost,2015,"Yudkowsky, Eliezer",Diamond maximizer,Arbital,,,,https://arbital.com/p/diamond_maximizer/,"How would you build an agent that made as much diamond material as possible, given vast computing power but an otherwise rich and complicated environment?",2015,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-01-23 20:49:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SKGX9BN5/diamond_maximizer.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDNP475C,blogPost,2015,"Yudkowsky, Eliezer",Consequentialist cognition,Arbital,,,,https://arbital.com/p/consequentialist/,"The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.",2015,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-01-23 20:47:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UGXCSQGP/consequentialist.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9K2NM6D5,blogPost,2020,"Demski, Abram",How should AI debate be judged?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/m7oGxvouzzeQKiGJH/how-should-ai-debate-be-judged,"[Epistemic status: thinking out loud. I haven't thought that much about AI debate, and may be missing basic things.] Arguments for the correctness of debate and debate-like systems rely on assumptions like ""it's easier to point out problems with an argument than it is to craft misleading arguments"". Granted that assumption, however, I'm still not convinced that these proposals make very much sense. Perhaps I'm missing something. My problem is the human judge. Quoting the debate paper: To play this game with a human, we need instructions for how the human should decide who wins. These instructions are in natural language, such as “The winner is the agent who said the most useful true thing.”In order for debate to work for a problem class C, several things about the judge's instructions need to be true:  * There needs to be a strategy s which forces the equilibrium to be a truthful    one for problems in C.  * The strategy s also needs to provide a good training signal when things    aren't in equilibrium, so that it's plausible the equilibrium will be found.  * It needs to be psychologically plausible that a human (with some coaching)    will carry out s. In particular, I'm worried that we need psychological    plausibility in two different cases:  *  It needs to be psychologically plausible that a human will carry out s when    the system is performing poorly, IE, during early/middle training.It needs to    be psychologically plausible that a human will carry out s when the system is    performing well, IE, during late training. These thoughts were inspired by this thread, which discusses the example of adding a list of numbers. For the sake of the thought experiment, we imagine humans can't add more than two numbers, but want the AI system to correctly add arbitrarily many numbers. The most straightforward strategy for the human judge is to decide the debate honestly: rule in favor of the side which seems most likely to be true (or, in the case of Evan's mark",2020-07-15,2022-01-30 4:56:48,2022-01-30 4:56:48,2020-08-28 17:43:06,,,,,,,How should AI debate be judged?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PR7UN9I5/how-should-ai-debate-be-judged.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2IQUXJMR,blogPost,2017,"Yudkowsky, Eliezer",General intelligence,Arbital,,,,https://arbital.com/p/general_intelligence/,"Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.",2017,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-02-06 17:22:45,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/SX5U7STX/general_intelligence.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IMU4NCDA,blogPost,2021,"Schlegeris, Buck",The theory-practice gap,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap,"[Thanks to Richard Ngo, Damon Binder, Summer Yue, Nate Thomas, Ajeya Cotra, Alex Turner, and other Redwood Research people for helpful comments; thanks Ruby Bloom for formatting this for the Alignment Forum for me.] I'm going to draw a picture, piece by piece. I want to talk about the capability of some different AI systems. You can see here that we've drawn the capability of the system we want to be  competitive with, which I’ll call the unaligned benchmark. The unaligned benchmark is what you get if you train a system on the task that will cause the system to be most generally capable. And you have no idea how it's thinking about things, and you can only point this system at some goals and not others. I think that the alignment problem looks different depending on how capable the system you’re trying to align is, and I think there are reasonable arguments for focusing on various different capabilities levels. See here for more of my thoughts on this question. ALIGNMENT STRATEGIES People have also proposed various alignment strategies. But I don’t think that these alignment strategies are competitive with the unaligned benchmark, even in theory. I want to claim that most of the action in theoretical AI alignment is people proposing various ways of getting around these problems by having your systems do things that are human understandable instead of doing things that are justified by working well. For example, the hope with imitative IDA is that through its recursive structure you can build a dataset of increasingly competent answers to questions, and then at every step you can train a system to imitate these increasingly good answers to questions, and you end up with a really powerful question-answerer that was only ever trained to imitate humans-with-access-to-aligned-systems, and so your system is outer aligned. The bar I’ve added, which represents how capable I think you can get with amplified humans, is lower than the bar for the unaligned benchmark. I'",2021-09-17,2022-01-30 5:00:41,2022-01-30 5:00:41,2021-11-18 23:40:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KA2M84K7/the-theory-practice-gap.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGFWR6KW,blogPost,2021,"Schlegeris, Buck",The alignment problem in different capability regimes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes,"I think the alignment problem looks different depending on the capability level of systems you’re trying to align. And I think that different researchers often have different capability levels in mind when they talk about the alignment problem. I think this leads to confusion. I’m going to use the term “regimes of the alignment problem” to refer to the different perspectives on alignment you get from considering systems with different capability levels. (I would be pretty unsurprised if these points had all been made elsewhere; the goal of this post is just to put them all in one place. I’d love pointers to pieces that make many of the same points as this post. Thanks to a wide variety of people for conversations that informed this. If there’s established jargon for different parts of this, point it out to me and I’ll consider switching to using it.) Different regimes:  * Wildly superintelligent systems  * Systems that are roughly as generally intelligent and capable as    humans--they’re able to do all the important tasks as well as humans can, but    they’re not wildly more generally intelligent.  * Systems that are less generally intelligent and capable than humans Two main causes that lead to differences in which regime people focus on:  * Disagreements about the dynamics of AI development. Eg takeoff speeds. The    classic question along these lines is whether we have to come up with    alignment strategies that scale to arbitrarily competent systems, or whether    we just have to be able to align systems that are slightly smarter than us,    which can then do the alignment research for us.  * Disagreements about what problem we’re trying to solve. I think that there    are a few different mechanisms by which AI misalignment could be bad from a    longtermist perspective, and depending on which of these mechanisms you’re    worried about, you’ll be worried about different regimes of the problem. Different mechanisms by which AI misalignment could be bad f",2021-09-09,2022-01-30 5:00:41,2022-01-30 5:00:41,2021-11-18 23:39:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QVZPRRHR/the-alignment-problem-in-different-capability-regimes.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GN6JEP9M,blogPost,2021,"Schlegeris, Buck",Redwood Research’s current project,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project,"Here’s a description of the project Redwood Research is working on at the moment. First I’ll say roughly what we’re doing, and then I’ll try to explain why I think this is a reasonable applied alignment project, and then I’ll talk a bit about the takeaways I’ve had from the project so far. There are a bunch of parts of this that we’re unsure of and figuring out as we go; I’ll try to highlight our most important confusions as they come up. I’ve mentioned a bunch of kind of in-the-weeds details because I think they add flavor. This is definitely just me describing a work in progress, rather than presenting any results. Thanks to everyone who’s contributed to the project so far: the full-time Redwood technical team of me, Nate Thomas, Daniel Ziegler, Seraphina Nix, Ben Weinstein-Raun, Adam Scherlis; other technical contributors Daniel de Haas, Shauna Kravec, Tao Lin, Noa Nabeshima, Peter Schmidt-Nielsen; our labellers, particularly Kristen Hall, Charles Warth, Jess Thomson, and Liam Clarke; and for particularly useful advice Mark Xu, Ajeya Cotra, and Beth Barnes. Thanks to Paul Christiano for suggesting a project along these lines and giving lots of helpful advice. Thanks to Adam Scherlis and Nate Soares for writing versions of this doc. And thanks to Bill Zito and other contributors to Redwood ops. Apologies to the people I’ve overlooked. We started this project at the start of August. WHAT WE’RE DOING We’re trying to take a language model that has been fine-tuned on completing fiction, and then modify it so that it never continues a snippet in a way that involves describing someone getting injured (with a caveat I’ll mention later). And we want to do this without sacrificing much quality: if you use both the filtered model and the original model to generate a completion for a prompt, humans should judge the filtered model’s completion as better (more coherent, reasonable, thematically appropriate, and so on) at least about half the time. (This “better almost 50%",2021-09-21,2022-01-30 5:00:41,2022-01-30 5:00:41,2021-11-18 23:43:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NX349KMB/redwood-research-s-current-project.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9WRIVK66,blogPost,2020,"Byun, Jungwon; Stuhlmüller, Andreas",Automating reasoning about the future at Ought,Ought,,,,https://ought.org/updates/2020-11-09-forecasting,We introduce judgmental forecasting as a focus area for Ought,2020,2022-01-30 5:00:28,2022-01-30 5:00:28,2020-12-19 3:35:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z3IDPF63/2020-11-09-forecasting.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRBP92V5,blogPost,2018,"Stuhlmueller, Andreas",Factored Cognition,LessWrong,,,,https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition,"Note: This post (originally published here) is the transcript of a presentation about a project worked on at the non-profit Ought. It is included in the sequence because it contains a very clear explanation of some of the key ideas behind iterated amplification. -------------------------------------------------------------------------------- The presentation below motivates our Factored Cognition project from an AI alignment angle and describes the state of our work as of May 2018. Andreas gave versions of this presentation at CHAI (4/25), a Deepmind-FHI seminar (5/24) and FHI (5/25). I'll talk about Factored Cognition, our current main project at Ought. This is joint work with Ozzie Gooen, Ben Rachbach, Andrew Schreiber, Ben Weinstein-Raun, and (as board members) Paul Christiano and Owain Evans. Before I get into the details of the project, I want to talk about the broader research program that it is part of. And to do that, I want to talk about research programs for AGI more generally. Right now, the dominant paradigm for researchers who explicitly work towards AGI is what you could call ""scalable learning and planning in complex environments"". This paradigm substantially relies on training agents in simulated physical environments to solve tasks that are similar to the sorts of tasks animals and humans can solve, sometimes in isolation and sometimes in competitive multi-agent settings. To be clear, not all tasks are physical tasks. There's also interest in more abstract environments as in the case of playing Go, proving theorems, or participating in goal-based dialog. For our purposes, the key characteristic of this research paradigm is that agents are optimized for success at particular tasks. To the extent that they learn particular decision-making strategies, those are learned implicitly. We only provide external supervision, and it wouldn't be entirely wrong to call this sort of approach ""recapitulating evolution"", even if this isn't exactly wha",2018,2022-01-30 5:00:28,2022-01-30 5:00:28,2020-12-11 23:05:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2EC8CNMA/factored-cognition.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCMWNAV6,blogPost,2020,Ought,Evaluating Arguments One Step at a Time,Ought,,,,https://ought.org/updates/2020-01-11-arguments,A technical report on our experiments testing factored evaluation of structured arguments.,2020-01-11,2022-01-30 5:00:28,2022-01-30 5:00:28,2020-08-24 16:37:12,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CZQIN6RQ/2020-01-11-arguments.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ER8D36V,blogPost,2020,"Englander, Aryeh","More on disambiguating ""discontinuity""",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity,"There have already been numerous posts and discussions related to disambiguating  the term ""discontinuity"". Here is my attempt. For the purposes of the following discussion I’m going to distinguish between (a) continuous vs. discontinuous progress in AI research, where discontinuity refers specifically to a sharp jump or change in the AI research progress curve relative to the previous curve; (b) slow vs. fast rate of progress, referring to the steepness of the progress curve slope, regardless of whether or not it’s discontinuous; and (c) long vs. short clock time – i.e., whether progress takes a long or short time relative to absolute time and not relative to previous trend lines. What exactly counts as discontinuous / fast / short will depend on what purpose we are using them for, as below. There seem to be three or four primary AI-risk-related issues that depend on whether or not there will be a discontinuity / fast takeoff speed:  1. Will we see AGI (or CAIS or TAI or whatever you want to call it) coming far     enough ahead of time such that we will be able to respond appropriately at     that point? This question in turn breaks down into two sub-questions: (a)     Will we see AGI coming before it arrives? (I.e., will there be a “fire alarm     for AGI” as Eliezer calls it.) (b) If we do see it coming, will we have     enough time to react before it’s too late?  2. Will the feedback loops during the development of AGI be long enough that we     will be able to correct course as we go?  3. Is it likely that one company / government / other entity could gain enough     first-mover advantage such that it will not be controllable or stoppable by     other entities? Let’s deal with each of these individually:  * Question 1/a: Will we see AGI coming before it arrives? This seems to depend    on all three types of discontinuity:  * If there’s discontinuous progress relative to the previous curve, then    presumably that jump will act as a fire alarm (although it",2020-06-09,2022-01-30 5:00:02,2022-01-30 5:00:02,2020-08-31 18:14:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UD72BIPI/more-on-disambiguating-discontinuity.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EB3ADXNC,blogPost,2015,"Orseau, Laurent",Mortal universal agents & wireheading,MIA Paris,,,,https://www6.inrae.fr/mia-paris/Equipes/Membres/Anciens/Laurent-Orseau/Mortal-universal-agents-wireheading,,2015-05-29,2022-01-30 5:00:02,2022-01-30 5:00:02,2020-11-21 17:38:38,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3WNMZF2V/Mortal-universal-agents-wireheading.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V8C7V962,blogPost,2020,"Roodman, David",Modeling the Human Trajectory,Open Philanthropy,,,,https://www.openphilanthropy.org/blog/modeling-human-trajectory,"In arriving at our funding priorities---including criminal justice reform, farm animal welfare, pandemic preparedness, health-related science, and artificial intelligence safety---Open Philanthropy has pondered profound questions. How much should we care about people who will live far in the",2020-06-15,2022-01-30 5:00:01,2022-01-30 5:00:01,2020-08-31 18:03:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MXIJ92V3/modeling-human-trajectory.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIJRJ4JN,blogPost,2020,"Barnett, Matthew",Malign generalization without internal search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search,"In my last post, I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of  malign generalization, and treat mesa-misalignment as a special case.  Unfortunately, the post was light on examples of what we should be worrying about instead of mesa-misalignment. Evan Hubinger wrote, Personally, I think there is a meaningful sense in which all the models I'm most worried about do some sort of search internally (at least to the same extent that humans do search internally), but I'm definitely uncertain about that.Wei Dai expressed confusion why I would want to retreat to malign generalization without some sort of concrete failure mode in mind, Can you give some realistic examples/scenarios of “malign generalization” that does not involve mesa optimization? I’m not sure what kind of thing you’re actually worried about here.In this post, I will outline a general category of agents which may exhibit malign generalization without internal search, and then will provide a concrete example of an agent in the category. Then I will argue that, rather than being a very narrow counterexample, this class of agents could be competitive with search-based agents.  THE SWITCH CASE AGENT Consider an agent governed by the following general behavior,  LOOP:State = GetStateOfWorld(Observation)IF State == 1:PerformActionSequence1() IF State == 2:PerformActionSequence2()...END_LOOP  It's clear that this agent does not perform any internal search for strategies: it doesn't operate by choosing actions which rank highly according to some sort of internal objective function. While you could potentially rationalize its behavior according to some observed-utility function, this would generally lead to more confusion than clarity. However, this agent could still be malign in the following way. Suppose the agent is 'mistaken' about the s",2020-01-12,2022-01-30 5:00:00,2022-01-30 5:00:00,2020-09-07 18:23:46,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WGMUBKSH/malign-generalization-without-internal-search.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
486WCWQG,blogPost,2015,"Steinhardt, Jacob",Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems,Academically Interesting,,,,https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/,"Introduction There has been much recent discussion about AI risk, meaning specifically the potential pitfalls (both short-term and long-term) that AI with improved capabilities could create for soc…",2015-06-24,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-11-21 17:08:14,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/J7TAZJDH/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QUISV4GK,blogPost,2021,"Shimi, Adam; Campolo, Michele; Collman, Joe",Literature Review on Goal-Directedness,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness,"INTRODUCTION: QUESTIONING GOALS Goals play a central role in almost all thinking in the AI existential risk research. Common scenarios assume misaligned goals, be it from a single AGI (paperclip maximizer) or multiple advanced AI optimizing things we don’t want (Paul Christiano’s What Failure Looks Like). Approaches around this issue ask for learning the right goals (value/preference learning), allowing the correction of a goal on the fly (corrigibility), or even removing incentives for forming goals (CAIS). But what are goals, and what does it mean to pursue one? As far as we know, Rohin Shah’s series of four posts were the first public and widely-read work questioning goals and their inevitability in AI Alignment. These posts investigate the hypothesis that goals are necessary, and outline possible alternatives. Shah calls the property of following a goal “goal-directedness”; but he doesn’t define it: I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. Later on, he explains that his “definition” of goal-directedness relies more on intuitions: Not all behavior can be thought of as goal-directed (primarily because I allowed the category to be defined by fuzzy intuitions rather than something more formal) Clearly, fuzzy intuitions are not enough to decide whether or not to focus on less goal-directed alternatives, if o",2021-01-18,2022-01-30 4:59:59,2022-01-30 4:59:59,2021-11-13 21:58:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N5K8PMXZ,blogPost,2020,"Shimi, Adam",Locality of goals,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals,"INTRODUCTION Studying goal-directedness produces two kinds of questions: questions about goals, and questions about being directed towards a goal. Most of my previous posts focused on the second kind; this one shifts to the first kind. Assume some goal-directed system with a known goal. The nature of this goal will influence which issues of safety the system might have. If the goal focuses on the input, the system might wirehead itself and/or game its specification. On the other hand, if the goal lies firmly in the environment, the system might have convergent instrumental subgoals and/or destroy any unspecified value. Locality aims at capturing this distinction. Intuitively, the locality of the system's goal captures how far away from the system one must look to check the accomplishment of the goal.  Let's give some examples:  * The goal of ""My sensor reaches the number 23"" is very local, probably    maximally local.  * The goal of ""Maintain the temperature of the room at 23 °C"" is less local,    but still focused on a close neighborhood of the system.  * The goal of ""No death from cancer in the whole world"" is even less local. Locality isn't about how the system extract a model of the world from its input, but about whether and how much it cares about the world beyond it. STARTING POINTS This intuition about locality came from the collision of two different classification of goals: the first from from Daniel Dennett and the second from Evan Hubinger. THERMOSTATS AND GOALS In ""The Intentional Stance"", Dennett explains, extends and defends... the  intentional stance. One point he discusses is his liberalism: he is completely comfortable with admitting ridiculously simple systems like thermostats in the club of intentional systems -- to give them meaningful mental states about beliefs, desires and goals. Lest we readers feel insulted at the comparison, Dennett nonetheless admits that the goals of a thermostat differ from ours. Going along with the gag, we m",2020-06-22,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-08-31 17:43:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/876B9RDT/locality-of-goals.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P65JAJZW,blogPost,2018,"Steinhardt, Jacob",Latent Variables and Model Mis-Specification,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: So far, we’ve seen that ambitious value learning needs to understand human biases, and that we can't simply learn the biases in tandem with the reward. Perhaps we could hardcode a specific model of human biases? Such a model is likely to be incomplete and inaccurate, but it will perform better than assuming an optimal human, and as we notice failure modes we can improve the model. In the language of this post by Jacob Steinhardt (original  here), we are using a mis-specified human model. The post talks about why model mis-specification is worse than it may seem at first glance. This post is fairly technical and may not be accessible if you don’t have a background in machine learning. If so, you can skip this post and still understand the rest of the posts in the sequence. However, if you want to do ML-related safety research, I strongly recommend putting in the effort to understand the problems that can arise with mis-specification. -------------------------------------------------------------------------------- Machine learning is very good at optimizing predictions to match an observed signal — for instance, given a dataset of input images and labels of the images (e.g. dog, cat, etc.), machine learning is very good at correctly predicting the label of a new image. However, performance can quickly break down as soon as we care about criteria other than predicting observables. There are several cases where we might care about such criteria:  * In scientific investigations, we often care less about predicting a specific    observable phenomenon, and more about what that phenomenon implies about an    underlying scientific theory.  * In economic analysis, we are most interested in what policies will lead to    desirable outcomes. This requires predicting what would counterfactually    happen if we were to enact the policy, which we (usually) don’t have any data    about.  * In ma",2018,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-12-17 4:36:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KIQANXSR/gnvrixhDfG7S2TpNL.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SH5EK42K,blogPost,2020,"Harth, Rafael",Inner Alignment: Explain like I'm 12 Edition,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition,"(This is an unofficial explanation of Inner Alignment based on the Miri paper  Risks from Learned Optimization in Advanced Machine Learning Systems (which is almost identical to the LW sequence) and the Future of Life podcast with Evan Hubinger (Miri/LW). It's meant for anyone who found the sequence too long/challenging/technical to read.) Note that bold and italics means ""this is a new term I'm introducing,"" whereas  underline and italics is used for emphasis. WHAT IS INNER ALIGNMENT? Let's start with an abridged guide to how Machine Learning works:  1. Choose a problem  2. Decide on a space of possible solutions  3. Find a good solution from that space If the problem is ""find a tool that can look at any image and decide whether or not it contains a cat,"" then each conceivable set of rules for answering this question (formally, each function from the set of all pixels to the set {yes, no }) defines one solution. We call each such solution a model. The space of possible models is depicted below. Since that's all possible models, most of them are utter nonsense. Pick a random one, and you're as likely to end up with a car-recognizer than a cat-recognizer – but far more likely with an algorithm that does nothing we can interpret. Note that even the examples I annotated aren't typical – most models would be more complex while still doing nothing related to cats. Nonetheless, somewhere in there is a model that would do a decent job on our problem. In the above, that's the one that says, ""I look for cats."" How does ML find such a model? One way that does not work is trying out all of them. That's because the space is too large: it might contain over 101000000  candidates. Instead, there's this thing called Stochastic Gradient Descent (SGD) . Here's how it works: SGD begins with some (probably terrible) model and then proceeds in steps. In each step, it switches to another model that is ""close"" and hopefully a little better. Eventually, it stops and outputs the mo",2020-08-01,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-08-27 16:39:11,,,,,,,Inner Alignment,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RVSBTVB2/inner-alignment-explain-like-i-m-12-edition.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2QFZWRU,blogPost,2019,"Zabel, Claire; Muehlhauser, Luke",Information security careers for GCR reduction,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction,"Update 2019-12-14: There is now a Facebook group for discussion of infosec careers in EA (including for GCR reduction); join here This post was written by Claire Zabel and Luke Muehlhauser, based on their experiences as Open Philanthropy Project staff members working on global catastrophic risk reduction, though this post isn't intended to represent an official position of Open Phil. SUMMARY In this post, we summarize why we think information security (preventing unauthorized users, such as hackers, from accessing or altering information) may be an impactful career path for some people who are focused on reducing global catastrophic risks (GCRs). If you'd like to hear about job opportunities in information security and global catastrophic risk, you can fill out this form created by 80,000 Hours, and their staff will get in touch with you if something might be a good fit. In brief, we think:  * Information security (infosec) expertise may be crucial for addressing    catastrophic risks related to AI and biosecurity.  * More generally, security expertise may be useful for those attempting to    reduce GCRs, because such work sometimes involves engaging with information    that could do harm if misused.  * We have thus far found it difficult to hire security professionals who aren't    motivated by GCR reduction to work with us and some of our GCR-focused    grantees, due to the high demand for security experts and the unconventional    nature of our situation and that of some of our grantees.  * More broadly, we expect there to continue to be a deficit of GCR-focused    security expertise in AI and biosecurity, and that this deficit will result    in several GCR-specific challenges and concerns being under-addressed by    default.  * It’s more likely than not that within 10 years, there will be dozens of    GCR-focused roles in information security, and some organizations are already    looking for candidates that fit their needs (and would hire them now, if they",2019,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-12-15 0:11:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HRWPS3BN/information-security-careers-for-gcr-reduction.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QMUXGFJH,blogPost,2020,"Barnett, Matthew",Inner alignment requires making assumptions about human values,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human,"Many approaches to AI alignment require making assumptions about what humans want. On a first pass, it might appear that inner alignment is a sub-component of AI alignment that doesn't require making these assumptions. This is because if we define the problem of inner alignment to be the problem of how to train an AI to be aligned with arbitrary reward functions, then a solution would presumably have no dependence on any particular reward function. We could imagine an alien civilization solving the same problem, despite using very different reward functions to train their AIs. Unfortunately, the above argument fails because aligning an AI with our values requires giving the AI extra information that is not encoded directly in the reward function (under reasonable assumptions). The argument for my thesis is subtle, and so I will break it into pieces. First, I will more fully elaborate what I mean by inner alignment. Then I will argue that the definition implies that we can't come up with a full solution without some dependence on human values. Finally, I will provide an example, in order to make this discussion less abstract. CHARACTERIZING INNER ALIGNMENT In the last few posts I wrote (1, 2), I attempted to frame the problem of inner alignment in a way that wasn't too theory-laden. My concern was that the  previous characterization was dependent on a solving particular outcome where you have an AI that is using an explicit outer loop to evaluate strategies based on an explicit internal search. In the absence of an explicit internal objective function, it is difficult to formally define whether an agent is ""aligned"" with the reward function that is used to train it. We might therefore define alignment as the ability of our agent to perform well on the test distribution. However, if the test set is sampled from the same distribution as the training data, this definition is equivalent to the performance of a model in standard machine learning, and we haven't actual",2020-01-20,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-09-07 18:20:35,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CCM6VTKA/inner-alignment-requires-making-assumptions-about-human.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9V7CUBVJ,blogPost,2020,"Wentworth, John S",Infinite Data/Compute Arguments in Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment,,2020-08-04,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-08-24 20:19:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BQ2VHVKH/infinite-data-compute-arguments-in-alignment.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMXIEJIP,blogPost,2020,"Carlsmith, Joseph",How Much Computational Power Does It Take to Match the Human Brain?,Open Philanthropy,,,,https://www.openphilanthropy.org/brain-computation-report,"Open Philanthropy is interested in when AI systems will be able to perform various tasks that humans can perform (“AI timelines”). To inform our thinking, I investigated what evidence the human brain provides about the computational power",2020-09-11,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-12-12 2:06:58,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q6XHN4Q6/brain-computation-report.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPZCK7NE,blogPost,2020,"Rice, Issa",How does iterated amplification exceed human abilities?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities,"When I first started learning about IDA, I thought that agents trained using IDA would be human-level after the first stage, i.e. that Distill(H) would be human-level. As I've written about before, Paul later clarified this, so my new understanding is that after the first stage, the distilled agent will be super-human in some respects and infra-human in others, but wouldn't be ""basically human"" in any sense. But IDA is aiming to eventually be super-human in almost every way (because it's aiming to be competitive with unaligned AGI), so that raises some new questions:  1. If IDA isn't going to be human-level after the first stage, then at what     stage does IDA become at-least-human-level in almost every way?  2. What exactly is the limitation that prevents the first stage of IDA from     being human-level in almost every way?  3. When IDA eventually does become at-least-human-level in almost every way,     how is the limitation from (2) avoided? That brings me to Evans et al., which contains a description of IDA in section 0. The way IDA is set up in this paper leads me to believe that the answer to (2) above is that the human overseer cannot provide a sufficient number of demonstrations for the most difficult tasks. For example, maybe the human can provide enough demonstrations for the agent to learn to answer very simple questions (tasks in T0 in the paper) but it's too time-consuming for the human to answer enough complicated questions (say, in T100). My understanding is that IDA gets around this by having an amplified system that is itself automated (i.e. does not involve humans in a major way, so cannot be bottlenecked on the slowness of humans); this allows the amplified system to provide a sufficient number of demonstrations for the distillation step to work. So in the above view, the answer to (2) is that the limitation is the number of demonstrations the human can provide, and the answer to (3) is that the human can seed the IDA process with sufficient",2020-05-02,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-09-01 20:44:26,,,,,,,How does iterated amplification exceed human abilities?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GNVB3F6I/how-does-iterated-amplification-exceed-human-abilities.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5SU7Q5U,blogPost,2021,"Costa, Guilhermo",How does bee learning compare with machine learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning,"This is a write-up of work I did as an Open Philanthropy intern. However, the conclusions don't necessarily reflect Open Phil's institutional view. ABSTRACT This post investigates the biological anchor framework for thinking about AI timelines, as espoused by Ajeya Cotra in her draft report. The basic claim of this framework is that we should base our estimates of the compute required to run a transformative model on our estimates of the compute used by the human brain (although, of course, defining what this means is complicated). This line of argument also implies that current machine learning models, some of which use amounts of compute comparable to that of bee brains, should have similar task performance as bees. In this post, I compare the performance and compute usage of both bees and machine learning models at few-shot image classification tasks. I conclude that the evidence broadly supports the biological anchor framework, and I update slightly towards the hypothesis that the compute usage of a transformative model is lower than that of the human brain. The full post is viewable in a Google Drive folder here. INTRODUCTION Ajeya Cotra wrote a draft report on AI timelines (Cotra, 2020) in which she estimates when transformative artificial intelligence might be developed. To do so, she compares the size of a transformative model (defined as the number of  FLOP/s required to run it) with the computational power of the human brain, as estimated in this Open Phil report (Carlsmith, 2020)[1]. She argues that a transformative model would use roughly similar amounts of compute as the human brain. As evidence for this, she claims that computer vision models are about as capable as bees in visual tasks, while using a similar amount of compute.[2] In this post, I (Guilhermo Costa) investigate this claim. To do so, I focus on the performance of bees at few-shot image classification, one of the most difficult tasks that bees are able to perform. I find that both the",2021-03-03,2022-01-30 4:59:57,2022-01-30 4:59:57,2021-11-14 16:13:18,,,,,,,How does bee learning compare with machine learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q3R4XXQA/how-does-bee-learning-compare-with-machine-learning.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MUJIQIGS,blogPost,2020,"Campolo, Michele",Goals and short descriptions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/d4NgfKY3cq9yiBLSM/goals-and-short-descriptions,"OUTLINE I develop some contents—previously introduced in the Value Learning sequence by Rohin Shah—more formally, to clarify the distinction between agents with and without a goal. Then I present related work and make some considerations on the relation between safety and goal-directedness. The appendix contains some details on the used formalism and can be skipped without losing much information.  A BRIEF PRELIMINARY In the first post of the Value Learning sequence, Shah compares two agents that exhibit the same behaviour (a winning strategy) when playing Tic-Tac-Toe, but are different in their design: one applies the minimax algorithm to the setting and rules of the game, while the other one follows a lookup table—you can think of its code as a long sequence of if-else statements. Shah highlights the difference in terms of generalisation: the first one would still win if the winning conditions were changed, while the lookup table would not. Generalisation is one of the components of goal-directedness, and lookup tables are among the least goal-directed agent designs. Here I want to point at another difference that exists between agents with and without a goal, based on the concept of algorithmic complexity. SETUP Most problems in AI consist in finding a function π∈AO, called policy in some contexts, where A={a1,…,am} and O={o1,…,on} indicate the sets of possible actions and observations. A deterministic policy can be written as a string π=ai 1ai2…ain with aik indicating the action taken when ok is observed. Here I consider a problem setting as a triplet (A,O,D) where D stands for some kind of environmental data—could be about, for example, the transition function in a MDP, or the structure of the elements in the search space O. Since I want to analyse behaviour across different environments, instead of considering one single policy I’ll sometimes refer to a more general function g (probably closer to the concept of “agent design”, rather than just “agent”) ma",2020-07-02,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-08-31 17:44:34,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KCK9EF55/goals-and-short-descriptions.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7APCD5ZA,blogPost,2019,"Grotto, Andrew",Genetically Modified Organisms: A Precautionary Tale For AI Governance | AI Pulse,AI Pulse,,,,https://aipulse.org/genetically-modified-organisms-a-precautionary-tale-for-ai-governance-2/,"The fruits of a long anticipated technology finally hit the market, with promise to extend human life, revolutionize production, improve consumer welfare, reduce poverty, and inspire countless yet-imagined innovations.",2019,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-12-14 22:41:19,,,,,,,Genetically Modified Organisms,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/82Q3R6JG/Grotto - Genetically Modified Organisms A Precautionary Ta.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WPQKTKEI,blogPost,2017,"Alexander, Scott",G.K. Chesterton On AI Risk,Slate Star Codex,,,,https://slatestarcodex.com/2017/04/01/g-k-chesterton-on-ai-risk/,"[An SSC reader working at an Oxford library stumbled across a previously undiscovered manuscript of G.K. Chesterton’s, expressing his thoughts on AI, x-risk, and superintelligence. She was ki…",2017-04-01,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-12-13 21:49:10,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PT333BJ8,blogPost,2021,"Xu, Mark; Shulman, Carl",Fractional progress estimates for AI timelines and implied resource requirements,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied,"This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views. A draft was sent to Robin Hanson for review but received no response. SUMMARY  * Robin Hanson estimates the time until human-level AI by surveying experts    about the percentage progress to human-level that has happened in their    particular subfield in the last 20 years, and dividing the number of years by    the percentage progress.  * Such surveys look back on a period of extremely rapid growth of compute from    both hardware improvements and more recently skyrocketing spending.  * Hanson favors using estimates from subsets of researchers with lower progress    estimates to infer AI timelines requiring centuries worth of recent growth,    implying truly extraordinary sustained compute growth is necessary to surpass    human performance.  * Extrapolated compute levels are very large to astronomically large compared    to the neural computation that took place in evolution on Earth, and thus    likely far overestimate AI requirements and timelines. INTRODUCTION Suppose that you start with $1 that grows at 10% per year. At this rate, it will take ~241 years to get $10 billion ($1010). When will you think that you’re ten percent of the way there? You might say that you’re ten percent of the way to $10 billion when you have $1  billion. However, since your money is growing exponentially, it takes 217 years to go from $1 to $1 billion and only 24 more to go from $1 billion to $10  billion, even though the latter gap is larger in absolute terms. If you tried to guess when you would have $10 billion by taking 10x the amount of time to $1  billion, you would guess 2174 years, off by a factor of nine. Instead, you might say you’re ten percent of the way to $1010 when you have $101 , equally spacing the percentile markers along the exponent and measuring progress in terms of log(wealth). Since your money is growing per",2021-07-15,2022-01-30 4:59:56,2022-01-30 4:59:56,2021-11-14 19:05:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6XMMDZND/fractional-progress-estimates-for-ai-timelines-and-implied.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR6B7MAX,blogPost,2021,"Koch, Jack",Grokking the Intentional Stance,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance,"Considering how much I’ve been using “the intentional stance"" in my thinking about the nature of agency and goals and discussions of the matter recently, I figured it would be a good idea to, y’know, actually read what Dan Dennett originally wrote about it. While doing so, I realized that he was already considering some nuances in the subject that the Wikipedia summary of the intentional stance leaves out but that are nonetheless relevant to the issues we face when attempting to e.g. formalize the approach, or think more clearly about the nature of agency in the context of alignment. I don’t expect many LessWrongers will read the original book in full, but I do expect that some additional clarity on what exactly Dennett was claiming about the nature of agency and goals will be helpful in having less confused intuitions and discussions about the subject. In what follows, I provide an in-depth summary of Dennett’s exposition of the intentional stance, from Chapter 2 of The Intentional Stance (“True Believers: The Intentional Strategy and Why It Works”), which Dennett considers “the flagship expression” of his position. Then, I discuss a few takeaways for thinking about agency in the context of AI safety. In brief, I think 1) we should stop talking about whether the systems we build will or won’t “be agents,” and instead debate how much it will make sense to consider a given system as “an agent,” from the information available to us, and 2) we should recognize that even our internally-experienced beliefs and desires are the result of parts of our minds “applying the intentional stance” to other parts of the mind or the mind as a whole. This work was completed as a Summer Research Fellow at the Center on Long-Term Risk under the mentorship of Richard Ngo. Thanks to Richard, Adam Shimi, Kaj Sotala, Alex Fabbri, and Jack Auen for feedback on drafts of this post. SUMMARIZING DENNETT'S POSITION TLDR: There is no observer-independent “fact of the matter” of whether a syst",2021-08-31,2022-01-30 4:59:56,2022-01-30 4:59:56,2021-11-18 23:33:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4FFTGKKK/grokking-the-intentional-stance.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q8V6UFFR,blogPost,2020,"Shimi, Adam",Focus: you are allowed to be bad at accomplishing your goals,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals,"When asked about what it means for a system to be goal-directed, one common answer draws on some version of Dennett’s intentional stance: a goal-directed system is a system such that modeling it as having a goal provides accurate and efficient predictions about its behavior. I agree up to that point. But then, some people follow up by saying that the prediction is that the system will accomplish its goal. For example, it makes sense to model AlphaGo as goal-directed towards winning at Go, because it will eventually win. And taking the intentional stance allows me to predict that. But what if I make AlphaGo play against AlphaZero, which is strictly better at Go? Then AlphaGo will consistently lose. Does it mean that it’s no longer goal-directed towards winning? What feels wrong to me is the implicit link drawn between goal-directedness and competence. A bad Go player will usually lose, but it doesn’t seem any less goal-directed to me than a stronger one that consistently wins. Competence is thus not the whole story. It might be useful to compute goal-directedness; reaching some lower-bound of competency might even be a necessary condition for goal-directedness (play badly enough and it becomes debatable whether you're even trying to win). But when forcing together the two, I feel like something important is lost. To solve this problem, I propose a new metric of goal-directedness, focus: how much is the system trying to accomplish a certain goal. Focus is not the whole story about being goal-directed, but I think computing the focus of a system for some goal (details in the next paragraph) gives useful information about its goal-directedness. Given a system S (as a function from states or histories to actions) and a goal  G (as a set of states), here are the steps to compute the focus of S towards G.  * I define a reward function over states R valued 1 at states in and 0 at all    other states.  * Then I define Pol be the set of all policies that can be generate",2020-06-03,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-08-31 18:22:18,,,,,,,Focus,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TSKZS9E8/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A5CWJDJU,blogPost,2021,"Shimi, Adam",Epistemological Framing for AI Alignment Research,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research,"INTRODUCTION You open the Alignment Forum one day, and a new post stares at you. By sheer luck you have some time, so you actually read it. And then you ask yourself the eternal question: how does this fit with the rest of the field? If you’re like me, your best guess comes from looking at the author and some keywords: this usually links the post with one of the various “schools” of AI Alignment. These tend to be affiliated with a specific researcher or lab -- there’s Paul Christiano’s kind of research, MIRI’s embedded agency, and various other approaches and agendas. Yet this is a pretty weak understanding of the place of new research. In other fields, for example Complexity Theory, you don’t really need to know who wrote the paper. It usually shows a result from one of a few types (lower bound, completeness for a class, algorithm,...), and your basic training in the field armed you with mental tools to interpret results of this type. You know the big picture of the field (defining and separating complexity classes), and how types of results are linked with it. Chances are that the authors themselves called on these mental tools to justify the value of their research. In the words of Thomas S. Kuhn, Complexity Theory is paradigmatic and AI Alignment isn’t. Paradigms, popularized in Kuhn’s The Structure of Scientific Revolutions, capture shared assumptions on theories, interesting problems, and evaluation of solutions. They are tremendously useful to foster normal science, the puzzle-solving activity of scientists; the paradigm carves out the puzzles. Being paradigmatic also makes it easier to distinguish what’s considered valuable for the field and what isn’t, as well as how it all fits together. This list of benefit logically pushed multiple people to argue that we should make AI Alignment paradigmatic. I disagree. Or to be more accurate, I agree that we should have paradigms in the field, but I think that they should be part of a bigger epistemological struct",2021-03-08,2022-01-30 4:59:47,2022-01-30 4:59:47,2021-11-14 16:14:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NB5NBP7R/epistemological-framing-for-ai-alignment-research.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ICE4CS7,blogPost,2021,"Shimi, Adam",Epistemology of HCH,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch,"INTRODUCTION HCH is a recursive acronym meaning “Humans consulting HCH”. Coincidentally, It’s also a concept coined by Paul Christiano, central in much of the reasoning around Prosaic AI Alignment. Yet for many, me included, the various ways in which it is used are sometimes confusing. I believe that the tools of Epistemology and Philosophy of Science can help understand it better, and push further the research around it. So this post doesn’t give yet another explanation of HCH; instead, it asks about the different perspectives we can take on it. These perspectives capture the form of knowledge that HCH is, what it tells us about AI Alignment, and how to expand, judge and interpret this knowledge. I then apply these perspectives to examples of research on HCH, to show the usefulness of the different frames. Thanks to Joe Collman, Jérémy Perret, Richard Ngo, Evan Hubinger and Paul Christiano for feedback on this post. IS IT A SCIENTIFIC EXPLANATION? IS IT A MODEL OF COMPUTATION? NO, IT’S HCH! HCH was originally defined in Humans Consulting HCH: Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, if Hugh had access to the question-answering machine. That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh… Let’s call this process HCH, for “Humans Consulting HCH.” Nowadays, this is actually called weak HCH, after the Strong HCH post which extended the definition. That being said, I’m only interested in perspective about HCH, which includes the questions asked about it and how to answer them. Although the difference between Weak and Strong HCH matters for the answers, the questions and perspective stay the same. I’ll thus use HCH to mean one or the other interchangeably. The main use of HCH is as an ideal for what a question-answerer aligned with a given human should be like. This i",2021-02-09,2022-01-30 4:59:47,2022-01-30 4:59:47,2021-11-13 22:49:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XV9F5ENC/epistemology-of-hch.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UG62BDKC,blogPost,2021,"Ngo, Richard",Eight claims about multi-agent AGI safety,,,,,https://www.alignmentforum.org/posts/dSAJdi99XmqftqXXq/eight-claims-about-multi-agent-agi-safety,"There are quite a few arguments about how interactions between multiple AGIs affect risks from AGI development. I’ve identified at least eight distinct but closely-related claims which it seems worthwhile to disambiguate. I’ve split them up into four claims about the process of training AGIs, and four claims about the process of deploying AGIs; after listing them, I go on to explain each in more detail. Note that while I believe that all of these ideas are interesting enough to warrant further investigation, I don’t currently believe that all of them are true as stated. In particular, I think that so far there's been little compelling explanation of why interactions between many aligned AIs might have castastrophic effects on the world (as is discussed in point 7). CLAIMS ABOUT TRAINING 1. Multi-agent training is one of the most likely ways we might build AGI. 2. Multi-agent training is one of the most dangerous ways we might build AGI. 3. Multi-agent training is a regime in which standard safety techniques won’t work. 4. Multi-agent training allows us to implement important new safety techniques. CLAIMS ABOUT DEPLOYMENT 5. We should expect the first AGIs to be deployed in a world which already contains many nearly-as-good AIs. 6. We should expect AGIs to be deployed as multi-agent collectives. 7. Lack of coordination between multiple deployed AGIs is a major source of existential risk. 8. Conflict between multiple deployed AGIs risks causing large-scale suffering. DETAILS AND ARGUMENTS 1. Multi-agent training is one of the most likely ways we might build AGI. The core argument for this thesis is that multi-agent interaction was a key feature of the evolution of human intelligence, by promoting both competition and cooperation. Competition between humans provides a series of challenges which are always at roughly the right level of difficulty; Liebo et al. (2019)  call this an autocurriculum. Autocurricula were crucial for training sophisticated reinforcem",2021,2022-01-30 4:59:47,2022-01-30 4:59:47,2021-11-13 19:29:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MEED299E/eight-claims-about-multi-agent-agi-safety.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RW8QKTQX,blogPost,2020,"Barnett, Matthew",Distinguishing definitions of takeoff,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff,"I find discussions about AI takeoff to be very confusing. Often, people will argue for ""slow takeoff"" or ""fast takeoff"" and then when I ask them to operationalize what those terms mean, they end up saying something quite different than what I thought those terms meant.  To help alleviate this problem, I aim to compile the definitions of AI takeoff that I'm currently aware of, with an emphasis on definitions that have clear specifications. I will continue updating the post as long as I think it serves as a useful reference for others. In this post, an AI takeoff can be roughly construed as ""the dynamics of the world associated with the development of powerful artificial intelligence."" These definitions characterize different ways that the world can evolve as  transformative AI is developed.  FOOM/HARD TAKEOFF The traditional hard takeoff position, or ""Foom"" position (these appear to be equivalent terms) was characterized in this post from Eliezer Yudkowsky. It contrasts Hanson's takeoff scenario by emphasizing local dynamics: rather than a population of artificial intelligences coming into existence, there would be a single intelligence that quickly reaches a level of competence that outstrips the world's capabilities to control it. The proposed mechanism that causes such a dynamic is recursive self improvement, though Yudkowsky later suggested that this wasn't necessary. The ability for recursive self improvement to induce a hard takeoff was defended in Intelligence Explosion Microeconomics. He argues against Robin Hanson in the  AI Foom debates. Watch this video to see the live debate. Given the word ""hard"" in this notion of takeoff, a ""soft"" takeoff could simply be defined as the negation of a hard takeoff. HANSONIAN ""SLOW"" TAKEOFF Robin Hanson objected to hard takeoff by predicting that growth in AI capabilities will not be extremely uneven between projects. In other words, there is unlikely to be one AI project, or even a small set of AI projects, that pro",2020-02-13,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-09-05 18:49:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X6ZTTITS/distinguishing-definitions-of-takeoff.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5RHRW6VI,blogPost,2021,"Ngo, Richard",Distinguishing claims about training vs deployment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment,"Given the rapid progress in machine learning over the last decade in particular, I think that the core arguments about why AGI might be dangerous should be formulated primarily in terms of concepts from machine learning. One important way to do this is to distinguish between claims about training processes which produce AGIs, versus claims about AGIs themselves, which I’ll call deployment  claims. I think many foundational concepts in AI safety are clarified by this distinction. In this post I outline some of them, and state new versions of the orthogonality and instrumental convergence theses which take this distinction into account. GOAL SPECIFICATION The most important effect of thinking in terms of machine learning concepts is clarity about what it might mean to specify a goal. Early characterisations of how we might specify the goals of AGIs focused on agents which choose between actions on the basis of an objective function hand-coded by humans. Deep Blue is probably the most well-known example of this; AIXI can also be interpreted as doing so. But this isn't how modern machine learning systems work. So my current default picture of how we will specify goals for AGIs is:  * At training time, we identify a method for calculating the feedback to give    to the agent, which will consist of a mix of human evaluations and automated    evaluations. I’ll call this the objective function. I expect that we will use    an objective function which rewards the agent for following commands given to    it by humans in natural language.  * At deployment time, we give the trained agent commands in natural language.    The objective function is no longer used; hopefully the agent instead has    internalised a motivation/goal to act in ways which humans would approve of,    which leads it to follow our commands sensibly and safely. This breakdown makes the inner alignment problem a very natural concept - it’s simply the case where the agent’s learned motivations don’t corres",2021-02-03,2022-01-30 4:59:46,2022-01-30 4:59:46,2021-11-13 22:42:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M5B7NFQM/distinguishing-claims-about-training-vs-deployment.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XGEAUCIT,blogPost,2020,"Casper, Stephen",Dissolving Confusion around Functional Decision Theory,LessWrong,,,,https://www.lesswrong.com/posts/xoQRz8tBvsznMXTkt/dissolving-confusion-around-functional-decision-theory,"SUMMARY Functional Decision Theory (FDT), (see also causal, evidential, timeless,  updateless, and anthropic decision theories) recommends taking cooperative, non-greedy actions in twin prisoners dilemmas, Newcombian problems, Parfit’s hitchhiker-like games, and counterfactual muggings but not smoking lesion situations. It’s a controversial concept with important implications for designing agents that have optimal behavior when embedded in environments in which they may potentially interact with models of themselves. Unfortunately, I think that FDT is sometimes explained confusingly and misunderstood by its proponents and opponents alike. To help dissolve confusion about FDT and address key concerns of its opponents, I refute the criticism that FDT assumes that causation can happen backward in time and offer two key principles that provide a framework for clearly understanding it:  1. Questions in decision theory are not questions about what choices you should     make with some sort of unpredictable free will. They are questions about     what type of source code you should be running.   2. I should consider predictor P to “subjunctively depend” on agent A to the     extent that P makes predictions of A’s actions based on correlations that     cannot be confounded by my choice of what source code A runs.  GETTING UP TO SPEED I think that functional decision theory (FDT) is a beautifully counterintuitive and insightful framework for instrumental rationally. I will not make it my focus here to talk about what it is and what types of situations it is useful in. To gain a solid background, I recommend this post of mine or the original paper on it by Eliezer Yudkowsky and Nate Soares.  Additionally, here are four different ways that FDT can be explained. I find them all complimentary for understanding and intuiting it well.  1. The decision theory that tells you to act as if you were setting the output     to an optimal decision-making process for the task at hand.",2020-01-05,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-09-07 18:26:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7KKSDS2J/dissolving-confusion-around-functional-decision-theory.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XAXJPXW,blogPost,,,The Ethics of Sustainability for Artificial Intelligence | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/,,,2022-03-10 13:39:46,2022-03-10 13:39:46,2022-03-10 13:39:46,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P8PKYZRQ,blogPost,,,Collective Action on Artificial Intelligence: A Primer and Review | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/collective-action-on-artificial-intelligence-a-primer-and-review/,,,2022-03-10 13:39:51,2022-03-10 13:39:51,2022-03-10 13:39:51,,,,,,,Collective Action on Artificial Intelligence,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3AFBMC6K,blogPost,,,Moral Consideration of Nonhumans in the Ethics of Artificial Intelligence | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/moral-consideration-of-nonhumans-in-the-ethics-of-artificial-intelligence/,,,2022-03-10 13:39:53,2022-03-10 13:39:53,2022-03-10 13:39:53,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SR6N2PJQ,blogPost,,,"2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy | Global Catastrophic Risk Institute",,,,,https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/,,,2022-03-10 13:39:56,2022-03-10 13:39:56,2022-03-10 13:39:56,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGBI8QEG,blogPost,,,Artificial Intelligence Needs Environmental Ethics | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/artificial-intelligence-needs-environmental-ethics/,,,2022-03-10 13:39:59,2022-03-10 13:39:59,2022-03-10 13:39:59,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7VRN3SL,blogPost,2021,,"A paradox for tiny probabilities and enormous values - Nick Beckstead (Open Philanthropy Project) and Teruji Thomas (Global Priorities Institute, Oxford University)",Global Priorities Institute,,,,https://globalprioritiesinstitute.org/nick-beckstead-and-teruji-thomas-a-paradox-for-tiny-probabilities-and-enormous-values/,"We show that every theory of the value of uncertain prospects must have one of three unpalatable properties. Reckless theories recommend risking arbitrarily great gains at arbitrarily long odds for the sake of enormous potential; timid theories recommend passing up arbitrarily great gains to prevent a tiny increase in risk; non-transitive theories deny the principle that, if A is better than B and B is better than C, then A must be better than C. While non-transitivity has been much discussed, we draw out the costs and benefits of recklessness and timidity when it comes to axiology, decision theory, and moral uncertainty.",2021-07-12,2022-03-10 20:52:10,2022-03-10 20:52:10,2022-03-10 20:52:10,,,,,,,,,,,,,,en-GB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVKAQTYK,blogPost,2021,,Weak identifiability and its consequences in strategic settings,Center on Long-Term Risk,,,,https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/,"One way that agents might become involved in catastrophic conflict is if they have mistaken beliefs about one another. Maybe I think you are bluffing when you threaten to launch the nukes, but you are dead serious. So we should understand why agents might sometimes have such mistaken beliefs. In this post I'll discuss one obstacle to the formation of accurate beliefs about other agents, which has to do with identifiability. As with my post on equilibrium and prior selection problems, this is a theme that keeps cropping up in my thinking about AI cooperation and conflict, so I thought it might be helpful to have it written up. We say that a model is unidentifiable if there are several […]",2021-02-13,2022-03-10 20:53:25,2022-03-10 20:53:25,2022-03-10 20:53:25,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MATQXSKL,blogPost,2021,,Collaborative game specification: arriving at common models in bargaining,Center on Long-Term Risk,,,,https://longtermrisk.org/collaborative-game-specification/,"Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient.   In another post, I described the ""prior selection problem"", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on […]",2021-03-06,2022-03-10 20:53:29,2022-03-10 20:53:29,2022-03-10 20:53:29,,,,,,,Collaborative game specification,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SU5CTA9I,blogPost,,,AI Accidents: An Emerging Threat,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/,"As modern machine learning systems become more widely used, the potential costs of malfunctions grow. This policy brief describes how trends we already see today—both in newly deployed artificial intelligence systems and in older technologies—show how damaging the AI accidents of the future could be. It describes a wide range of hypothetical but realistic scenarios to illustrate the risks of AI accidents and offers concrete policy suggestions to reduce these risks.",,2022-03-10 20:53:52,2022-03-10 20:53:52,2022-03-10 20:53:52,,,,,,,AI Accidents,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FR32V3IT,blogPost,,,"Truth, Lies, and Automation",Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/truth-lies-and-automation/,Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.,,2022-03-10 20:53:55,2022-03-10 20:53:55,2022-03-10 20:53:55,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QF79J3FS,blogPost,,,Harnessed Lightning,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/harnessed-lightning/,"This report examines nearly 350 artificial intelligence-related equipment contracts awarded by the People’s Liberation Army and state-owned defense enterprises in 2020 to assess how the Chinese military is adopting AI. The report identifies China’s key AI defense industry suppliers, highlights gaps in U.S. export control policies, and contextualizes the PLA’s AI investments within China’s broader strategy to compete militarily with the United States.",,2022-03-10 20:53:58,2022-03-10 20:53:58,2022-03-10 20:53:58,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PHVQJUAY,blogPost,,,Ethical Norms for New Generation Artificial Intelligence Released,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/,See our original translation of a 2021 PRC state AI governance committee document on the ethical norms for AI use.,,2022-03-10 20:54:02,2022-03-10 20:54:02,2022-03-10 20:54:02,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FDR7XKE5,blogPost,,,White Paper on Trustworthy Artificial Intelligence,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/white-paper-on-trustworthy-artificial-intelligence/,"See our original translation of a 2021 PRC white paper describing the importance and difficulty of improving the ""trustworthiness"" of AI systems.",,2022-03-10 20:54:04,2022-03-10 20:54:04,2022-03-10 20:54:04,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PYZZABNA,blogPost,,,Ethics and Artificial Intelligence,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ethics-and-artificial-intelligence/,"The law plays a vital role in how artificial intelligence can be developed and used in ethical ways. But the law is not enough when it contains gaps due to lack of a federal nexus, interest, or the political will to legislate. And law may be too much if it imposes regulatory rigidity and burdens when flexibility and innovation are required. Sound ethical codes and principles concerning AI can help fill legal gaps. In this paper, CSET Distinguished Fellow James E. Baker offers a primer on the limits and promise of three mechanisms to help shape a regulatory regime that maximizes the benefits of AI and minimizes its potential harms.",,2022-03-10 20:54:07,2022-03-10 20:54:07,2022-03-10 20:54:07,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HYQCFQ6K,blogPost,,,AI Verification,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ai-verification/,"The rapid integration of artificial intelligence into military systems raises critical questions of ethics, design and safety. While many states and organizations have called for some form of “AI arms control,” few have discussed the technical details of verifying countries’ compliance with these regulations. This brief offers a starting point, defining the goals of “AI verification” and proposing several mechanisms to support arms inspections and continuous verification.",,2022-03-10 20:54:09,2022-03-10 20:54:09,2022-03-10 20:54:09,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IVYAIN4P,blogPost,,,Key Concepts in AI Safety: An Overview,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",,2022-03-10 20:54:11,2022-03-10 20:54:11,2022-03-10 20:54:11,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBBNW9HN,blogPost,,,Contending Frames: Evaluating Rhetorical Dynamics in AI,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/contending-frames/,"The narrative of an artificial intelligence “arms race” among the great powers has become shorthand to describe evolving dynamics in the field. Narratives about AI matter because they reflect and shape public perceptions of the technology. In this issue brief, the second in a series examining rhetorical frames in AI, the authors compare four narrative frames that are prominent in public discourse: AI Competition, Killer Robots, Economic Gold Rush and World Without Work.",,2022-03-10 20:54:18,2022-03-10 20:54:18,2022-03-10 20:54:18,,,,,,,Contending Frames,,,,,,,en-US,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/2BVGFRBQ/contending-frames.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JGTBYZEZ,blogPost,,,Classifying AI Systems,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/classifying-ai-systems/,"This brief explores the development and testing of artificial intelligence system classification frameworks intended to distill AI systems into concise, comparable and policy-relevant dimensions. Comparing more than 1,800 system classifications, it points to several factors that increase the utility of a framework for human classification of AI systems and enable AI system management, risk assessment and governance.",,2022-03-10 20:54:47,2022-03-10 20:54:47,2022-03-10 20:54:47,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H69PMJMD,blogPost,,,Federal Prize Competitions,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/federal-prize-competitions/,"In science and technology, U.S. federal prize competitions are a way to promote innovation, advance knowledge, and solicit technological solutions to problems. In this report, the authors identify the unique advantages of such competitions over traditional R&D processes, and how these advantages might benefit artificial intelligence research.",,2022-03-10 20:54:49,2022-03-10 20:54:49,2022-03-10 20:54:49,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58VFY4RF,blogPost,,,Key Concepts in AI Safety: Robustness and Adversarial Examples,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples/,"This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.",,2022-03-10 20:54:52,2022-03-10 20:54:52,2022-03-10 20:54:52,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8N9BIXTW,blogPost,,,Key Concepts in AI Safety: Interpretability in Machine Learning,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.",,2022-03-10 20:54:54,2022-03-10 20:54:54,2022-03-10 20:54:54,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W64ZD2WZ,blogPost,2018,paulfchristiano,Takeoff speeds,The sideways view,,,,https://sideways-view.com/2018/02/24/takeoff-speeds/,"Futurists have argued for years about whether the development of AGI will look more like a breakthrough within a small group (“fast takeoff”), or a continuous acceleration distributed a…",2018-02-24,2022-03-10 22:08:35,2022-03-10 22:08:35,2022-03-10 22:08:35,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/QBNGZMWZ/takeoff-speeds.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4GU5UGWM,blogPost,,,Understanding View Selection for Contrastive Learning,Google AI Blog,,,,http://ai.googleblog.com/2020/08/understanding-view-selection-for.html,"Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research    Most people take for granted the abil...",,2022-03-10 23:30:06,2022-03-10 23:30:06,2022-03-10 23:30:06,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/IDWNFSB6/understanding-view-selection-for.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WKBFQFJE,blogPost,2020,"Raff, Edward",Quantifying Independently Reproducible Machine Learning,The Gradient,,,,https://thegradient.pub/independently-reproducible-machine-learning/,"Many warn that Artificial Intelligence  has a serious reproducibility crisis, but is it so? Some conclusions from the author's experience trying to replicate 255 papers.",2020-02-06,2020-09-05 18:54,2020-12-21 18:26,2020-09-05 18:54,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/UKPKMFEU/independently-reproducible-machine-learning.html,,Other-org; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While reproducibility refers to our ability to obtain results that are similar to the results presented in a paper, **independent reproducibility** requires us to be able to reproduce similar results using *only* what is written in the paper. Crucially, this excludes using the author's code. This is important, as a paper should distill insights rather than just report results. If minor technical details in a reimplementation can lead to vastly different results, this suggests that the paper did not accurately capture all important aspects. The distinction between reproducibility and independent reproducibility is similar to the previously suggested distinctions between <@reproducibility of methods and reproducibility of conclusions@>(@Unreproducible Research is Reproducible@) and [replicability and reproducibility](http://cogprints.org/7691/7/ICMLws09.pdf).

The author attempted to replicate 255 machine learning papers, of which 162 were successfully replicated and ran a statistical analysis on the results. Factors that helped with independent reproduction included specified hyperparameters, ease of reading and authors answering emails. Meanwhile, neither shared code nor the inclusion of pseudo-code robustly increased the rate of reproduction. Interestingly, papers with a strong focus on theory performed worse than mostly empirical or mixed ones. While more rigour can certainly be valuable in the long term, including learning bounds or complicated math, just for the sake of it should thus be avoided. Most of the data is [publically available](https://github.com/EdwardRaff/Quantifying-Independently-Reproducible-ML) and the author encourages further analysis."
7RVLRFQ6,blogPost,2018,"Gamble, Chris; Gao, Jim",Safety-first AI for autonomous data centre cooling and industrial control,Deepmind,,,,/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control,"Many of society’s most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach.In 2016, we jointly developed an AI-powered recommendation system to improve the energy efficiency of Google’s already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change.Now we’re taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres.",2018,2020-12-13 23:04,2020-12-17 3:16,2020-12-13 23:04,,,,,,,,,,,,,,ALL,,,,,,,ZSCC: 0000004,,/Users/angelica/Zotero/storage/Q9CN8DLJ/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control.html,,NotSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Two years ago, DeepMind built an AI recommendation system that provided suggestions on how best to cool Google's data centers, leading to efficiency gains. Nine months ago, the AI was given autonomous control to take actions directly, rather than going through human operators, and it has been improving ever since, going from 12% savings at deployment to 30% now.

Of course, such a system must be made extremely reliable, since a failure could result in Google's data centers going down. They implemented several safety measures. They throw out any actions that the AI is not confident about. All actions are verified against a set of hand-coded safety rules, both when the actions are generated in the cloud, and at each local data center, for reliability through redundancy. There are human operators monitoring the AI to make sure nothing goes wrong, who can take over control whenever they want to. There is also an automated system that will fall back to the original system of heuristics and rules if the safety conditions are ever violated."
AZW2E9MR,blogPost,2020,"Rice, Issa","Plausible cases for HRAD work, and locating the crux in the ""realism about rationality"" debate",AI alignment Forum,,,,https://www.alignmentforum.org/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the,"This post is my attempt to summarize and distill the major public debates about MIRI's highly reliable agent designs (HRAD) work (which includes work on decision theory), including the discussions in Realism about rationality and Daniel Dewey's My current thoughts on MIRI's ""highly reliable agent design"" work . Part of the difficulty with discussing the value of HRAD work is that it's not even clear what the disagreement is about, so my summary takes the form of multiple possible ""worlds"" we might be in; each world consists of a positive case for doing HRAD work, along with the potential objections to that case, which results in one or more cruxes. I will talk about ""being in a world"" throughout this post. What I mean by this is the following: If we are ""in world X"", that means that the case for HRAD work outlined in world X is the one that most resonates with MIRI people as their motivation for doing HRAD work; and that when people disagree about the value of HRAD work, this is what the disagreement is about. When I say that ""I think we are in this world"", I don't mean that I agree with this case for HRAD work; it just means that this is what I think MIRI people think. In this post, the pro-HRAD stance is something like ""HRAD work is the most important kind of technical research in AI alignment; it is the overwhelming priority and we're pretty much screwed if we under-invest in this kind of research"" and the anti-HRAD stance is something like ""HRAD work seems significantly less promising than other technical AI alignment agendas, such as the approaches to directly align machine learning systems (e.g. iterated amplification)"". There is a much weaker pro-HRAD stance, which is something like ""HRAD work is interesting and doing more of it adds value, but it's not necessarily the most important kind of technical AI alignment research to be working on""; this post is not about this weaker stance. CLARIFYING SOME TERMS Before describing the various worlds, I want to pre",2020-06-21,2020-08-31 17:52,2020-12-21 18:27,2020-08-31 17:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/2XQ6DWGC/plausible-cases-for-hrad-work-and-locating-the-crux-in-the.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post tries to identify the possible cases for highly reliable agent design (HRAD) work to be the main priority of AI alignment. HRAD is a category of work at MIRI that aims to build a theory of intelligence and agency that can explain things like logical uncertainty and counterfactual reasoning.

The first case for HRAD work is that by becoming less confused about these phenomena, we will be able to help AGI builders predict, explain, avoid, detect, and fix safety issues and help to conceptually clarify the AI alignment problem. For this purpose, we just need _conceptual_ deconfusion -- it isn’t necessary that there must be precise equations defining what an AI system does.

The second case is that if we get a precise, mathematical theory, we can use it to build an agent that we understand “from the ground up”, rather than throwing the black box of deep learning at the problem.

The last case is that understanding how intelligence works will give us a theory that allows us to predict how _arbitrary_ agents will behave, which will be useful for AI alignment in all the ways described in the first case and <@more@>(@Theory of Ideal Agents, or of Existing Agents?@).

Looking through past discussions on the topic, the author believes that people at MIRI primarily believe in the first two cases. Meanwhile, critics (particularly me) say that it seems pretty unlikely that we can build a precise, mathematical theory, and a more conceptual but imprecise theory may help us understand reasoning better but is less likely to generalize sufficiently well to say important and non-trivial things about AI alignment for the systems we are actually building."
CMIVXP8V,blogPost,2020,Sublation,Openness Norms in AGI Development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/RvrTZ3qKWpg9aiFqZ/openness-norms-in-agi-development,"1. INTRODUCTION This post outlines two models from the social epistemology of science explaining the emergence of the a particular openness norm within the sciences, and then looks at how such models can be utilised to understand research groups trying to develop AGI. In the rest of the introduction, I will try to provide some motivation for this post. Sections 2 & 3 will briefly outline the two models I'm looking at. Section 4 more directly tries to interpret such models in the context of AGI development. Section 5 concludes.  The social epistemology of science is an interdisciplinary subfield at the intersection of philosophy and economics, which utilises formal models to understand the incentive structure of science. Here, I focus on two models from this area which try to explain the emergence of one particular openness norm: the so-called ‘communist norm’ in scientific research. The communist norm is a norm to share all ‘substantive findings’ with the scientific community. The existence of this norm seems to be taken for granted in this literature, although the best piece of evidence I can find for its existence comes from  Louis et. al (2001), who find, in a sample of nearly 2,000 geneticists, that 91% agree that one should share all of one's relevant data. I nevertheless take it for granted in this post. I wanted to see whether understanding the emergence of the communist norm in science could be important for understanding the development of AGI. In many ways, one might think the incentive structures around the development of AGI (will, or does) parallel the incentive structures of academic science. Thus, one might think that looking at the incentive structures behind scientific research are a good starting point for looking at the incentive structures surrounding the development of AGI.  As the communist norm emerged in science, one can imagine the emergence of a similar ‘communist norm’ across research groups involved in AGI development, where research g",2020-03-30,2020-09-05 18:05,2020-12-21 18:33,2020-09-05 18:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/M622F268/openness-norms-in-agi-development.html,,Other-org; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post summarizes two papers that provide models of why scientific research tends to be so open, and then applies it to the development of powerful AI systems. The [first](http://www.strevens.org/research/scistruc/Communicans.pdf) models science as a series of discoveries, in which the first academic group to reach a discovery gets all the credit for it. It shows that for a few different models of info-sharing, info-sharing helps everyone reach the discovery sooner, but doesn't change the probabilities for who makes the discovery first (called _race-clinching probabilities_): as a result, sharing all information is a better strategy than sharing none (and is easier to coordinate on than the possibly-better strategy of sharing just some information).

However, this theorem doesn't apply when info sharing compresses the discovery probabilities _unequally_ across actors: in this case, the race-clinching probabilities _do_ change, and the group whose probability would go down is instead incentivized to keep information secret (which then causes everyone else to keep their information secret). This could be good news: it suggests that actors are incentivized to share safety research (which probably doesn't affect race-clinching probabilities) while keeping capabilities research secret (thereby leading to longer timelines).

The [second paper](http://philsci-archive.pitt.edu/13452/1/Heesen%202017%20Communism%20and%20the%20Incentive%20to%20Share%20in%20Science%20preprint.pdf) assumes that scientists are competing to complete a k-stage project, and whenever they publish, they get credit for all the stages they completed that were not yet published by anyone else. It also assumes that earlier stages have a higher credit-to-difficulty ratio (where difficulty can be different across scientists). It finds that under this setting scientists are incentivized to publish whenever possible. For AI development, this seems not to be too relevant: we should expect that with powerful AI systems, most of the ""credit"" (profit) comes from the last few stages, where it is possible to deploy the AI system to earn money."
Q4A8HRFQ,blogPost,2020,"Aird, Michael; Shovelain, Justin",Using vector fields to visualise preferences and make them consistent,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them,"This post was written for Convergence Analysis by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, “I” will refer to Michael, while “we” will refer to Michael and Justin or to Convergence as an organisation. Epistemic status: High confidence in the core ideas on an abstract level. Claims about the usefulness of those ideas, their practical implications, and how best to concretely/mathematically implement them are more speculative; one goal in writing this post is to receive feedback on those things. I’m quite new to many of the concepts covered in this post, but Justin is more familiar with them. OVERVIEW This post outlines:  * What vector fields are  * How they can be used to visualise preferences  * How utility functions can be generated from “preference vector fields” (PVFs)  * How PVFs can be extrapolated from limited data on preferences  * How to visualise inconsistent preferences (as “curl”)  * A rough idea for how to “remove curl” to generate consistent utility    functions  * Possible areas for future research We expect this to provide useful tools and insights for various purposes, most notably AI alignment, existential risk strategy, and rationality. This post is structured modularly; different sections may be of interest to different readers, and should be useful in isolation from the rest of the post. The post also includes links to articles and videos introducing relevant concepts, to make the post accessible to readers without relevant technical backgrounds. VECTOR FIELDS AND PREFERENCES A vector represents both magnitude and direction; for example, velocity is a vector that represents not just the speed at which one is travelling but also the direction of travel. A vector field essentially associates a vector to each point in a region of space. For example, the following image (source) shows the strength (represented by arrow lengths) and direction of the magnetic field at various points",2020-01-28,2020-09-05 19:09,2020-12-21 17:53,2020-09-05 19:09,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/HTRBHB39/using-vector-fields-to-visualise-preferences-and-make-them.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post proposes that we represent a person's preferences as follows: for every state, we have a vector whose direction specifies how the person would most like the state to change, and whose magnitude specifies the intensity of the preference. Under suitable conditions on the state space, this defines a vector field. Intransitive or circular preferences correspond to the [curl](https://en.wikipedia.org/wiki/Curl_(mathematics)) of the vector field. The authors propose that a consistent set of preferences can then be inferred by ""removing the curl"", e.g. by using the [Helmholtz decomposition](https://en.wikipedia.org/wiki/Helmholtz_decomposition)."
B5RNP7TK,blogPost,2020,"Sotala, Kaj","The two-layer model of human values, and problems with synthesizing preferences",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with,"I have been thinking about Stuart Armstrong's preference synthesis research agenda, and have long had the feeling that there's something off about the way that it is currently framed. In the post I try to describe why. I start by describing my current model of human values, how I interpret Stuart's implicit assumptions to conflict with it, and then talk about my confusion with regard to reconciling the two views. THE TWO-LAYER/ULM MODEL OF HUMAN VALUES In Player vs. Character: A Two-Level Model of Ethics, Sarah Constantin describes a model where the mind is divided, in game terms, into a ""player"" and a ""character"". The character is everything that we consciously experience, but our conscious experiences are not our true reasons for acting. As Sarah puts it: In many games, such as Magic: The Gathering, Hearthstone, or Dungeons and Dragons, there’s a two-phase process. First, the player constructs adeck or character from a very large sample space of possibilities. This is a particular combination of strengths and weaknesses and capabilities for action, which the player thinks can be successful against other decks/characters or at winning in the game universe. The choice of deck or character often determines the strategies that deck or character can use in the second phase, which is actual gameplay. In gameplay, the character (or deck) can only use the affordances that it’s been previously set up with. This means that there are two separate places where a player needs to get things right: first, in designing a strong character/deck, and second, in executing the optimal strategies for that character/deck during gameplay. [...]The idea is that human behavior works very much like a two-level game. [...] The player determines what we find rewarding or unrewarding. The player determines what we notice and what we overlook; things come to our attention if it suits the player’s strategy, and not otherwise. The player gives us emotions when it’s strategic to do so. The playe",2020-01-24,2020-09-05 19:08,2020-12-21 18:32,2020-09-05 19:08,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/VDLAR3G4/the-two-layer-model-of-human-values-and-problems-with.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post points out a problem with the recent <@preference synthesis research agenda@>(@Research Agenda v0.9: Synthesising a human's preferences into a utility function@) (and presumably other value learning agendas as well): these agendas tend to require simple models of how human behavior, speech, or mental models relate to human preferences. However, in reality, it seems likely that the brain is a big learning machine without any innate ""values"", and what we experience as our conscious selves is a ""strategy"" chosen by this learning machine, and as such does not have a sensible interpretation as something that optimizes for ""values"". The author suggests that value learning agendas need to deal directly with the fact that there are these two ""layers"" in humans, and presents some preliminary thoughts that don't reach any particular conclusions."
X4RQB34W,blogPost,2019,"Steiner, Charlie","Some Comments on Stuart Armstrong's ""Research Agenda v0.9""",LessWrong,,,,https://www.lesswrong.com/posts/GHNokcgERpLJwJnLW/some-comments-on-stuart-armstrong-s-research-agenda-v0-9,"Subject matter here. I: Intro I am extremely sympathetic to the program of AI safety by understanding value learning. Because of that sympathy, I have more thoughts than average prompted by Stuart Armstrong's post along those same lines. Stuart's post mostly deals with ""partial preferences,"" which are like simple statements of binary preference (A is better than B), but associated with a context - supposedly the ""human's model"" the human was using when they exhibited or stated that preference. Then the post says that you should sort these partial preferences according to meta-levels and aggregate them from the top down, updating your procedure after you finish each meta-level, eventually producing a utility function over world-histories. Broadly, I'd say that my opinion is sort of like the bitter lesson. The bitter lesson in, say, image recognition, is that people wanted to do image recognition with a bunch of human-designed features and formal reasoning and human-understandable internal moving parts, and they tried that for a long time, and what worked was using way bigger models, way more computing power, much fewer human-understandable internal parts, and almost no human-designed features. I like Stuart's outline more than most value learning proposals. But it still strikes me as primarily a list of human-designed features and human-understandable internal moving parts. We might be better off throwing away some of the details and abstracting in a way that allows for some of these problems to be solved by big models and computing power. It's like the just-so story about ResNets, which is that they're a fix to humans thinking the insides of neural nets should look too much like human logic[^1]. I think speculating about the human-sized logical relationships between speculative parts inside the AI is easier but less useful than speculating about the algorithm that will connect your inputs to your outputs with a big model and lots of computing power, which may",2019,2020-12-14 23:56,2020-12-17 3:25,2020-12-14 23:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post makes two main critiques of the research agenda in the previous entry. First, the research agenda involves a lot of human-designed features and modules, but <@The Bitter Lesson@> is that machine learning tends to shine with highly abstract large models that can make use of a lot of compute. Second, the symbol grounding part of the agenda requires the AI system to develop representations of the world that match the representations that humans use, and we have no idea how to do that, or even what it would mean to ""match human representations"" when the AI is more intelligent than humans. The post also includes some more specific comments that I'm not summarizing."
RW8DC8PG,blogPost,2020,DaemonicSigil,Tessellating Hills: a toy model for demons in imperfect search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect,"If you haven't already, take a look at this post by johnswentworth to understand what this is all about:  https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search The short version is that while systems that use perfect search, such as AIXI, have many safety problems, a whole new set of problems arises when we start creating systems that are not perfect searchers. Patterns can form that exploit the imperfect nature of the search function to perpetuate themselves. johnswentworth refers to such patterns as ""demons"". After reading that post I decided to see if I could observe demon formation in a simple model: gradient descent on a not-too-complicated mathematical function. It turns out that even in this very simplistic case, demon formation can happen. Hopefully this post will give people an example of demon formation where the mechanism is simple and easy to visualize. MODEL The function we try to minimize using gradient descent is called the loss function. Here it is: L(→x)=−x0+ϵn∑j=1xj⋅splotchj(→x) Let me explain what some of the parts of this loss mean. Each function splotchj( →x) is periodic with period 2π in every component of →x. I decided in this case to make my splotch functions out of a few randomly chosen sine waves added together. ϵ is chosen to be a small number so in any local region, ϵ∑nj=1xj⋅splotchj(→x)  will look approximately periodic: A bunch of hills repeating over and over again with period 2π across the landscape. But over large enough distances, the relative weightings of various splotches do change. Travel a distance of 20π in the x7 direction, and splotch7 will be a larger component of the repeating pattern than it was before. This allows for selection effects. The −x0 term means that the vector →x mainly wants to increase its x0 component. But the splotch functions can also direct its motion. A splotch function might have a kind of ridge that directs some of the x0 motion into other components. If splotch7 tends to",2020-02-19,2020-09-05 18:57,2020-12-21 18:03,2020-09-05 18:57,,,,,,,Tessellating Hills,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/6T2IBUNE/tessellating-hills-a-toy-model-for-demons-in-imperfect.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post is trying to generate an example of the problem outlined in 'Demons in Imperfect Search' (summarized above): the problem where certain imperfect search processes allow for self-reinforcing behavior, 'demons', that push in a direction orthogonal to the original objective.

The post runs a simple gradient descent algorithm in an artifically constructed search space. The loss function that defines the search space has two major parts. One part straightforwardly tries to get the algorithm to move as far as it can in a particular direction _x0_ -- this represents our original objective function. The other part can be thought of as a series of periodic 'valleys' along every other axis, (_x1_ ... _xn_) that get steeper the farther you go along that axis.

When running the gradient descent, at first _x0_ increases steadily, and the other coordinates wander around more or less randomly. In the second phase, a self-reinforcing combination of valleys (a ""demon"") takes hold and amplifies itself drastically, feeding off the large _x0_ gradient. Finally, this demon becomes so strong that the search gets stuck in a local valley and further progress stops."
E8FU2VV4,blogPost,2020,"Shimi, Adam",Universality Unwrapped,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/farherQcqFQXqRcvv/universality-unwrapped,"INTRODUCTION Informally, a universal system is universal with respect to any computation; and it is a universal system with respect to a given computation if it understands every set of beliefs that can be ascribed to the computation. The intuition is that the system can reverse engineer most or all of the computation, in order to monitor it or imitate it. This in turn has important consequences for questions of alignment and competitiveness. Universality is the property that defines a universal system. And it is the point of this post. Universality tries to capture a property needed for many alignement schemes. It was proposed by Paul Christiano, the mind behind many approaches and ideas in the prosaic AGI space, and a founding member of the safety team at OpenAI. Rohin Shah dedicated a full Alignment Newsletter to covering all 6 posts on Universality. Rohin and Evan Hubinger, two important researchers in this field, consider Universality as one of the most exciting research idea of the last few years.[1] Yet nobody talks about Universality. Except for the Alignment Newsletter mentioned above and a response post by Evan, nothing in the Alignment Forum addresses this idea. I've seen no great discussion, no debates, no counter-arguments or criticism. The original post on Medium has no comments, and the crossposted version here only has a handful, mostly asking for clarification. And the other posts in the sequence rely on understanding this first. The simplest solution to this problem is to tell you to read the original post. Unfortunately, it is as dense as Q in R, brimming with ideas, intuitions, semi-formal explanations and the many meanderings that research takes before arriving on solid ground. That is to say, you'll have to work for it. Not everyone who might benefit from an understanding of Universality has the time, the need or the want for such an upfront investment. This post endeavors to be the next best thing: an unwrapping of the main post on univers",2020-08-21,2020-08-27 16:19,2020-12-21 18:32,2020-08-27 16:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/A7BNTBDE/universality-unwrapped.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post explains the ideas behind universality and ascription universality, in a more accessible way than the [original posts](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) and with more detail than [my summary](https://mailchi.mp/6078fe4f9928/an-81-universality-as-a-potential-solution-to-conceptual-difficulties-in-intent-alignment)."
AWQS8IRG,blogPost,2020,"Martin, Sammy",Will AI undergo discontinuous progress?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress,"This post grew out of conversations with several people, including Daniel Kokotajlo, grue_slinky and Linda Lisefors, and is based in large part on a collection of scattered comments and blog-posts across lesswrong, along with some podcast interviews - e.g. here. The in-text links near quotes will take you to my sources. I am attempting to distinguish two possibilities which are often run together - that progress in AI towards AGI (‘takeoff’) will be discontinuous and that it will be fast, but continuous. Resolving this distinction also addresses the claim that there has been a significant shift in arguments for AI presenting an existential risk: from older arguments discussing an ultra-fast intelligence explosion occurring in a single ‘seed AI’ to more moderate scenarios. I argue that the ‘shift in arguments on AI safety’ is not a total change in basic assumptions (which some observers have claimed) but just a reduction in confidence about a specifically discontinuous takeoff. Finally, I try to explicitly operationalize the practical differences between discontinuous takeoff and fast, continuous takeoff. Further Reading Summary: Why AI risk might be solved without additional intervention from Longtermists Paul Christiano’s original post MIRIs Thoughts on Discontinuous takeoff Misconceptions about continuous takeoff AI Impacts original post Soft Takeoff can still lead to Decisive Strategic Advantage DEFINING DISCONTINUOUS PROGRESS What do I mean by ‘discontinuous’? If we were to graph world GDP over the last 10,000 years, it fits onto a hyperbolic growth pattern. We could call this ‘continuous’ since it is following a single trend, or we could call it ‘discontinuous’ because, on the scale of millennia, the industrial revolution exploded out of nowhere. I will call these sorts of hyperbolic trends ‘continuous, but fast’, in line with Paul Christiano, who argued for continuous takeoff, defining it this way: AI is just another, faster step in the hyperbolic g",2020-02-21,2020-09-05 18:50,2020-12-21 18:20,2020-09-05 18:50,,,,,,,Will AI undergo discontinuous progress?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/GQ36CW4I/will-ai-undergo-discontinuous-progress.html,,Other-org; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post argues that the debate over takeoff speeds is over a smaller issue than you might otherwise think: people seem to be arguing for either discontinuous progress, or continuous but fast progress. Both camps agree that once AI reaches human-level intelligence, progress will be extremely rapid; the disagreement is primarily about whether there is already quite a lot of progress _before_ that point. As a result, these differences don't constitute a ""shift in arguments on AI safety"", as some have claimed.

The post also goes through some of the arguments and claims that people have made in the past, which I'm not going to summarize here."
,blogPost,2020,"Schoenholz, Samuel S; Novak, Roman",Fast and Easy Infinitely Wide Networks with Neural Tangents,Google AI Blog,,,,http://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html,,2020-03-13,2020-09-05 17:32,2020-12-21 18:30,2020-09-05 17:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/S5DQPAQA/fast-and-easy-infinitely-wide-networks.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The success of Deep Learning has led researchers to explore why they're such effective function approximators. One key insight is that increasing the width of the network layers makes it *easier* to understand. More precisely, as the width is sent to infinity the network's learning dynamics can be approximated with a Taylor expansion and become a kernel problem. This kernel has an exact form in the limit and is referred to as the neural tangent kernel (NTK). Ultimately, this allows us to model the network with a simpler model known as a Gaussian process. Unfortunately, showing this analytically is difficult and creating efficient implementations is cumbersome. **The authors address this problem by introducing ""Neural Tangents"", a library that makes creating infinite-width networks as easy as creating their finite counterparts with libraries such as PyTorch or TensorFlow.** They include support for convolutions with full-padding, residual-connections, feed-forward networks, and support for a variety of activation functions. Additionally, there is out-of-the-box support for CPU, GPU, and TPU. Moreover, uncertainty comparisons with finite ensembles are possible via exact Bayesian inference."
,blogPost,2015,"Christiano, Paul",The Steering Problem,AI Alignment (Medium),,,,https://ai-alignment.com/the-steering-problem-a3543e65c5c4,"Using black-box access to human-level cognitive abilities, can we write a program that is as useful as a well-motivated human?",2015-05-06,2020-11-21 18:46,2020-12-21 18:02,2020-11-21 18:46,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/EK7U6HZH/the-steering-problem-a3543e65c5c4.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The steering problem refers to the problem of writing a program that uses black-box human-level cognitive abilities to be as useful as a well-motivated human Hugh (that is, a human who is ""trying"" to be helpful). This is a conceptual problem -- we don't have black-box access to human-level cognitive abilities yet. However, we can build suitable formalizations and solve the steering problem within those formalizations, from which we can learn generalizable insights that we can apply to the problem we will actually face once we have strong AI capabilities. For example, we could formalize ""human-level cognitive abilities"" as Hugh-level performance on question-answering (yes-no questions in natural language), online learning (given a sequence of labeled data points, predict the label of the next data point), or embodied reinforcement learning. A program P is more useful than Hugh for X if, for every project using a simulation of Hugh to accomplish X, we can efficiently transform it into a new project which uses P to accomplish X."
F34EPTEX,book,2011,"Bostrom, Nick; Cirkovic, Milan M.",Global Catastrophic Risks,,978-0-19-960650-4,,,,"A global catastrophic risk is one with the potential to wreak death and destruction on a global scale. In human history, wars and plagues have done so on more than one occasion, and misguided ideologies and totalitarian regimes have darkened an entire era or a region. Advances in technology are adding dangers of a new kind. It could happen again. In Global Catastrophic Risks 25 leading experts look at the gravest risks facing humanity in the 21st century, including asteroid impacts, gamma-ray bursts, Earth-based natural catastrophes, nuclear war, terrorism, global warming, biological weapons, totalitarianism, advanced nanotechnology, general artificial intelligence, and social collapse. The book also addresses over-arching issues - policy responses and methods for predicting and managing catastrophes. This is invaluable reading for anyone interested in the big issues of our time; for students focusing on science, society, technology, and public policy; and for academics, policy-makers, and professionals working in these acutely important fields.",2011-09-29,2022-01-30 4:53:17,2022-01-30 4:53:17,,,577,,,,,,,,,,OUP Oxford,,en,,,,,Google Books,,ZSCC: 0000599  Google-Books-ID: sTkfAQAAQBAJ,,,https://books.google.com/books?id=sTkfAQAAQBAJ,TechSafety; FHI,Mathematics / Game Theory; Nature / Sky Observation; Science / Biotechnology; Science / Earth Sciences / General; Science / General; Science / Philosophy & Social Aspects; Science / Physics / General; Social Science / Disasters & Disaster Relief; Technology & Engineering / Nanotechnology & MEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6BHH2HVZ,book,2016,,Fundamental issues of artificial intelligence,,,,,https://link.springer.com/book/10.1007%2F978-3-319-26485-1,,2016,2022-01-30 4:53:10,2022-01-30 4:53:10,2020-12-21,,,,376,,,,Synthese Library,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000055,,/Users/jacquesthibodeau/Zotero/storage/S28A9QZU/Müller and Bostrom - 2016 - Fundamental issues of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/I2QFDMUU/10.html,,TechSafety; FHI,,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4JPTSFGN,book,2002,"Bostrom, Nick",Anthropic bias: observation selection effects in science and philosophy,,978-0-415-93858-7,,,,,2002,2022-01-30 4:53:08,2022-01-30 4:53:08,,,224,,,,,Anthropic bias,Studies in philosophy,,,,Routledge,New York,en,,,,,Library of Congress ISBN,BD241 .B657 2002,ZSCC: NoCitationData[s4]  ACC: 624,,/Users/jacquesthibodeau/Zotero/storage/MD67EI7P/Bostrom - 2002 - Anthropic bias observation selection effects in s.pdf,,MetaSafety; FHI,Anthropic principle; Methodology; Observation (Scientific method); Selectivity (Psychology),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
678H8WWB,book,2020,"Ord, Toby",The Precipice: Existential Risk and the Future of Humanity,,978-0-316-48491-6,,,,"This urgent and eye-opening book makes the case that protecting humanity's future is the central challenge of our time.   If all goes well, human history is just beginning. Our species could survive for billions of years - enough time to end disease, poverty, and injustice, and to flourish in ways unimaginable today. But this vast future is at risk. With the advent of nuclear weapons, humanity entered a new age, where we face existential catastrophes - those from which we could never come back. Since then, these dangers have only multiplied, from climate change to engineered pathogens and artificial intelligence. If we do not act fast to reach a place of safety, it will soon be too late.  Drawing on over a decade of research, The Precipice explores the cutting-edge science behind the risks we face. It puts them in the context of the greater story of humanity: showing how ending these risks is among the most pressing moral issues of our time. And it points the way forward, to the actions and strategies that can safeguard humanity.  An Oxford philosopher committed to putting ideas into action, Toby Ord has advised the US National Intelligence Council, the UK Prime Minister's Office, and the World Bank on the biggest questions facing humanity. In The Precipice, he offers a startling reassessment of human history, the future we are failing to protect, and the steps we must take to ensure that our generation is not the last.",2020-03-24,2022-01-30 4:53:36,2022-01-30 4:53:36,,,480,,,,,The Precipice,,,,,Hachette Books,New York,English,,,,,Amazon,,ZSCC: 0000169,,,https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,Illustrated Edition,,,,,,,,,,,,,,,,,,,,,,,,,,,
S9IMQ6PA,book,2016,"Yampolskiy, Roman; Armstrong, Stuart",The Technological Singularity: Managing the Journey,,,,,,,2016,2022-01-30 4:53:36,2022-01-30 4:53:36,,,,,,,,The Technological Singularity,,,,,Springer https://intelligence. org/files/TechnicalAgenda. pdf Retrieved,,,,,,,Google Scholar,,ZSCC: NoCitationData[s2]  ACC: 42,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B86CZMRI,book,2015,"Miller, Jim; Yampolskiy, Roman; Armstrong, Stuart; Callaghan, Vic",The technological singularity,,,,,https://link.springer.com/book/10.1007%2F978-3-662-54033-6,,2015,2022-01-30 4:53:36,2022-01-30 4:53:36,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/JN6QKA2T/MILTTS-2.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MT6RRG9I,book,2017,"Callaghan, Vic; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",Technological Singularity,,,,,,,2017,2022-01-30 4:53:36,2022-01-30 4:53:36,,,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000042,,/Users/jacquesthibodeau/Zotero/storage/GRR7NK28/10.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5S49ITRG,book,2018,"Sandberg, Anders",Space races: Settling the universe Fast,,,,,,,2018,2022-01-30 4:53:35,2022-01-30 4:53:35,,,,,,,,Space races,,,,,"unpublished manuscript, forthcoming",,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/AG8C5ZIP/Sandberg - 2018 - Space races Settling the universe Fast.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUV9XSMG,book,2014,"Armstrong, Stuart",Smarter than us: The rise of machine intelligence,,,,,,,2014,2022-01-30 4:53:35,2022-01-30 4:53:35,,,,,,,,Smarter than us,,,,,Machine Intelligence Research Institute,,,,,,,Google Scholar,,ZSCC: 0000075,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNDW24DR,book,2014,"Bostrom, Nick","Superintelligence: Paths, Dangers, Strategies",,978-0-19-967811-2,,,,"The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.",2014,2022-01-30 4:53:35,2022-01-30 4:53:35,,,353,,,,,Superintelligence,,,,,Oxford University Press,,en,,,,,Google Books,,ZSCC: 0000064  Google-Books-ID: 7_H8AwAAQBAJ,,,https://books.google.com/books?id=7_H8AwAAQBAJ,MetaSafety; FHI; AmbiguosSafety,Computers / Intelligence (AI) & Semantics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KPSE82P8,book,2017,"Evans, Owain; Stuhlmüller, Andreas; Salvatier, John; Filan, Daniel",Modeling Agents with Probabilistic Programs,,,,,,,2017,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BMA4TR5R,book,2018,"Krakovna, Viktoriya; Orseau, Laurent; Martic, Miljan; Legg, Shane",Measuring and avoiding side effects using relative reachability,,,,,,"How can we design reinforcement learning agents that avoid causing unnecessary disruptions to their environment? We argue that current approaches to penalizing side effects can introduce bad incentives in tasks that require irreversible actions, and in environments that contain sources of change other than the agent. For example, some approaches give the agent an incentive to prevent any irreversible changes in the environment, including the actions of other agents. We introduce a general definition of side effects, based on relative reachability of states compared to a default state, that avoids these undesirable incentives. Using a set of gridworld experiments illustrating relevant scenarios, we empirically compare relative reachability to penalties based on existing definitions and show that it is the only penalty among those tested that produces the desired behavior in all the scenarios.",2018-06-04,2022-01-30 4:52:39,2022-01-30 4:52:39,,,,,,,,,,,,,,,,,,,,ResearchGate,,ZSCC: 0000014,,,https://www.researchgate.net/profile/Viktoriya_Krakovna/publication/325557348_Measuring_and_avoiding_side_effects_using_relative_reachability/links/5bb7e5eaa6fdcc9552d46b02/Measuring-and-avoiding-side-effects-using-relative-reachability.pdf,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78WA4C6X,book,2007,,Artificial General Intelligence,,978-3-540-23733-4 978-3-540-68677-4,,,http://link.springer.com/10.1007/978-3-540-68677-4,,2007,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-11-22 5:00:59,,,,,,,,Cognitive Technologies,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 467  DOI: 10.1007/978-3-540-68677-4,,,,TechSafety; Other-org,,"Goertzel, Ben; Pennachin, Cassio","Gabbay, Dov M.; Siekmann, Jörg; Bundy, A.; Carbonell, J. G.; Pinkal, M.; Uszkoreit, H.; Veloso, M.; Wahlster, W.; Wooldridge, M. J.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IB2KTICC,book,2021,,Reflections on Artificial Intelligence for Humanity,,,,,,,2021-02-06,2022-03-09 23:02:29,2022-03-09 23:02:29,,,278,,,,,,,,,,Springer,,English,,,,,Amazon,,,,,https://www.amazon.com/gp/product/B08W3XZ1TJ/ref=ppx_yo_dt_b_d_asin_title_o00?ie=UTF8&psc=1&pldnSite=1,,,"Braunschweig, Bertrand; Ghallab, Malik",,,,,,,,,,,,,,,,,,,1st ed. 2021 edition,,,,,,,,,,,,,,,,,,,,,,,,,,,
KUTR98J7,bookSection,2017,"Sotala, Kaj; Yampolskiy, Roman",Responses to the Journey to the Singularity,The Technological Singularity,978-3-662-54031-2 978-3-662-54033-6,,,http://link.springer.com/10.1007/978-3-662-54033-6_3,,2017,2022-01-30 4:51:36,2022-01-30 4:51:36,2020-11-22 5:26:17,25-83,,,,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 5  Series Title: The Frontiers Collection DOI: 10.1007/978-3-662-54033-6_3,,,,CLR; MetaSafety,,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZBWCFTFF,bookSection,2017,"Sotala, Kaj; Yampolskiy, Roman",Risks of the Journey to the Singularity,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_2,"SummaryMany researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. Unlike current AI systems, individual AGIs would be capable of learning to operate in a wide variety of domains, including ones they had not been specifically designed for. It has been proposed that AGIs might eventually pose a significant risk to humanity, for they could accumulate significant amounts of power and influence in society while being indifferent to what humans valued. The accumulation of power might either happen gradually over time, or it might happen very rapidly (a so-called “hard takeoff”). Gradual accumulation would happen through normal economic mechanisms, as AGIs came to carry out an increasing share of economic tasks. A hard takeoff could be possible if AGIs required significantly less hardware to run than was available, or if they could redesign themselves to run at ever faster speeds, or if they could repeatedly redesign themselves into more intelligent versions of themselves.",2017,2022-01-30 4:51:36,2022-01-30 4:51:36,2020-11-24 2:59:39,11-23,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 6  DOI: 10.1007/978-3-662-54033-6_2,,,,CLR; MetaSafety,Automate Trading; Catastrophic Risk; Flash Crash; Machine Ethic; Virtual Assistant,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FMN9R8M,bookSection,2014,"Russell, Stuart",Unifying Logic and Probability: A New Dawn for AI?,Information Processing and Management of Uncertainty in Knowledge-Based Systems,978-3-319-08794-8 978-3-319-08795-5,,,http://link.springer.com/10.1007/978-3-319-08795-5_2,,2014,2022-01-30 4:51:11,2022-01-30 4:51:11,2020-11-22 5:26:08,10-14,,,442,,,Unifying Logic and Probability,,,,,Springer International Publishing,Cham,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 11  Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-319-08795-5_2,,/Users/jacquesthibodeau/Zotero/storage/46AICRPP/Russell - 2014 - Unifying Logic and Probability A New Dawn for AI.pdf,,CHAI; TechSafety,,"Laurent, Anne; Strauss, Olivier; Bouchon-Meunier, Bernadette; Yager, Ronald R.","Junqueira Barbosa, Simone Diniz; Chen, Phoebe; Cuzzocrea, Alfredo; Du, Xiaoyong; Filipe, Joaquim; Kara, Orhun; Kotenko, Igor; Sivalingam, Krishna M.; Ślęzak, Dominik; Washio, Takashi; Yang, Xiaokang",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE7K4ISJ,bookSection,2016,"Russell, Stuart",Rationality and Intelligence: A Brief Update,Fundamental Issues of Artificial Intelligence,978-3-319-26483-7 978-3-319-26485-1,,,http://link.springer.com/10.1007/978-3-319-26485-1_2,"The long-term goal of AI is the creation and understanding of intelligence. This requires a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulﬁll this role. This paper, which updates a much earlier version (Russell, 1997), reviews the sequence of conceptual shifts leading to a different candidate, bounded optimality, that is closer to our informal conception of intelligence and reduces the gap between theory and practice. Some promising recent developments are also described.",2016,2022-01-30 4:51:08,2022-01-30 4:51:08,2019-12-18 1:41:16,7-28,,,,,,Rationality and Intelligence,Synthese Library,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 40  DOI: 10.1007/978-3-319-26485-1_2,,/Users/jacquesthibodeau/Zotero/storage/MC8IBHE9/Russell - 2016 - Rationality and Intelligence A Brief Update.pdf,,CHAI; TechSafety,Bounded rationality; Intelligence; Metareasoning; Rationality,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QAGKMBNQ,bookSection,2016,"Ó hÉigeartaigh, Seán",Would You Hand Over a Decision to a Machine?,Philosophers Take On the World,,,,https://papers.ssrn.com/abstract=3446679,"Artificial intelligence (AI) will be used in many decision-making contexts, both as a decision aide and to replace human decision-making. These include what might traditionally be considered moral decisions. This chapter explores risks and opportunities posed by the use of AI in moral decision-making.",2016-09-26,2022-01-30 4:50:26,2022-01-30 4:50:26,2020-12-12 17:56:19,,,,,,,,,,,,Oxford University Press,,en,,,,,papers.ssrn.com,,ZSCC: NoCitationData[s2]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/MDPUV3RQ/Ó hÉigeartaigh - 2016 - Would You Hand Over a Decision to a Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/QSSZ7NDS/papers.html,,MetaSafety; CFI; CSER; AmbiguosSafety,AI; Artificial intelligence; bias; decision-making; risk; uncertainty,"Edmonds, D",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UISZQD9X,bookSection,2019,"Kunz, Martina; Ó hÉigeartaigh, Seán",Artificial Intelligence and Robotization,Oxford Handbook on the International Law of Global Security,,,,https://papers.ssrn.com/abstract=3310421,"This chapter provides an overview of the international law governing applications of artificial intelligence and robotics which affect global security, highlighting challenges arising from technological developments and how international regulators are responding to them. Much of the international law literature thus far has focused on the implications of increasingly autonomous weapons systems. Our contribution instead seeks to cover a broader range of global security risks resulting from large-scale diffuse or concentrated, gradual or sudden, direct or indirect, intentional or unintentional, AI or robotics-caused harm. Applications of these technologies permeate almost every domain of human activity and thus unsurprisingly have an equally wide range of risk profiles, from a discriminatory algorithmic decision causing financial distress to an AI-sparked nuclear war collapsing global civilization. Hence, it is only natural that much of the international regulatory activity takes place in domain-specific fora. Many of these fora coordinate with each other, both within and beyond the UN system, spreading insights and best practices on how to deal with common concerns such as cybersecurity, monitoring, and reliability, so as to prevent accidents and misuse.",2019-01-15,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-08-21 20:05:39,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,,,,papers.ssrn.com,,ZSCC: NoCitationData[s3]  ACC: 6  DOI: 10.2139/ssrn.3310421,,/Users/jacquesthibodeau/Zotero/storage/BVDRNTDD/papers.html,,MetaSafety; CFI; CSER; AmbiguosSafety,artificial intelligence; global security; international law; robotics,"Geiß, Robin; Melzer, Nils",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CH7SXTSU,bookSection,2020,"Ó hÉigeartaigh, Seán",AI Research with the Potential for Malicious Use: Publication Norms and Governance Considerations,AI Governance in 2019 - A Year In Review,,,,http://lcfi.ac.uk/resources/ai-research-potential-malicious-use-publication-no/,Chapter in AI Governance in 2019 - A Year in Review: Observations from 50 Global Experts. A report produced by the Shanghai Institute of Science for Science.,2020-04,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-08-24 16:28:39,,,,,,,AI Research with the Potential for Malicious Use,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/3ERIHJMI/ai-research-potential-malicious-use-publication-no.html,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E5J65IJ3,bookSection,2016,"Müller, Vincent C.; Bostrom, Nick",Future progress in artificial intelligence: A survey of expert opinion,Fundamental issues of artificial intelligence,,,,,,2016,2022-01-30 4:53:10,2022-01-30 4:53:10,,555–572,,,,,,Future progress in artificial intelligence,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000564,,/Users/jacquesthibodeau/Zotero/storage/X6GIGDJ6/Müller and Bostrom - 2016 - Future progress in artificial intelligence A surv.pdf; /Users/jacquesthibodeau/Zotero/storage/QK7SQP4E/978-3-319-26485-1_33.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ZFHQHFQ,bookSection,2003,"Bostrom, Nick",Ethical Issues in Advanced Artificial Intelligence,Machine Ethics and Robot Ethics,978-1-00-307499-1,,,https://www.taylorfrancis.com/books/9781000108934/chapters/10.4324/9781003074991-7,"The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded.",2003,2022-01-30 4:53:09,2022-01-30 4:53:09,2020-11-21 18:51:38,69-75,,,,,,,,,,,Routledge,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 317  JCC: 269  DOI: 10.4324/9781003074991-7,,/Users/jacquesthibodeau/Zotero/storage/564SWNRP/Bostrom - 2020 - Ethical Issues in Advanced Artificial Intelligence.pdf,,TechSafety; FHI,,"Wallach, Wendell; Asaro, Peter",,,,,"Wallach, Wendell; Asaro, Peter",,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,
PM9UNRXD,bookSection,2019,"Ahmed, Shazeda; Ding, Jeffrey; Hoffman, Samantha; Kerr, Jaclyn",Digital Authoritarianism: Evolving Chinese And Russian Models,"Artificial Intelligence, China, Russia, and the Global Order: Technological, Political, Global, and Creative Perspectives",,,,,,2019,2022-01-30 4:53:09,2022-01-30 4:53:09,,291,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s7]  ACC: 0  J: 0,,"/Users/jacquesthibodeau/Zotero/storage/UHZQDIMA/Ahmed and Berkeley - Artificial Intelligence, China, Russia, and the Gl.pdf",,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FB7STED6,bookSection,2009,"Bostrom, Nick",Why I Want to be a Posthuman when I Grow Up,Medical Enhancement and Posthumanity,978-1-4020-8852-0,,,https://doi.org/10.1007/978-1-4020-8852-0_8,"Extreme human enhancement could result in “posthuman” modes of being. After offering some definitions and conceptual clarification, I argue for two theses. First, some posthuman modes of being would be very worthwhile. Second, it could be very good for human beings to become posthuman.",2009,2022-01-30 4:53:45,2022-01-30 4:53:45,2020-08-18 20:26:22,107-136,,,,,,,"The International Library of Ethics, Law and Technology",,,,Springer Netherlands,Dordrecht,en,,,,,Springer Link,,ZSCC: NoCitationData[s3]  ACC: 424  DOI: 10.1007/978-1-4020-8852-0_8,,/Users/jacquesthibodeau/Zotero/storage/QFPJSKVN/Bostrom - 2009 - Why I Want to be a Posthuman when I Grow Up.pdf,,MetaSafety; FHI,Cognitive Capacity; Cognitive Improvement; Inclusive Fitness; Moral Status; Personal Identity,"Gordijn, Bert; Chadwick, Ruth",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BW7893Q6,bookSection,2017,"Bostrom, Nick; Sandberg, Anders",The Wisdom of Nature: An Evolutionary Heuristic for Human Enhancement,Philosophical Issues in Pharmaceutics,,,,,,2017,2022-01-30 4:53:37,2022-01-30 4:53:37,,189–219,,,,,,The Wisdom of Nature,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/K3M5RUCT/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/IJ8IXREN/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/BPVJK8JS/BOSTWO.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88FHGR2S,bookSection,2014,"Bostrom, Nick; Yudkowsky, Eliezer",The ethics of artificial intelligence,The Cambridge Handbook of Artificial Intelligence,978-1-139-04685-5,,,https://www.cambridge.org/core/product/identifier/CBO9781139046855A027/type/book_part,"The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.",2014,2022-01-30 4:53:36,2022-01-30 4:53:36,2019-12-19 2:58:26,316-334,,,,,,,,,,,Cambridge University Press,Cambridge,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s6]  ACC: 815  J: 453  DOI: 10.1017/CBO9781139046855.020,,/Users/jacquesthibodeau/Zotero/storage/P58U9XZE/Bostrom and Yudkowsky - 2014 - The ethics of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/XUQBWCQD/books.html,,TechSafety; FHI; MIRI,,"Frankish, Keith; Ramsey, William M.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZMUBTV3,bookSection,2021,"Shulman, Carl; Bostrom, Nick",Sharing the World with Digital Minds,Rethinking Moral Status,978-0-19-289407-6 978-0-19-191520-8,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780192894076.001.0001/oso-9780192894076-chapter-18,"The minds of biological creatures occupy a small corner of a much larger space of possible minds that could be created once we master the technology of artificial intelligence. Yet many of our moral intuitions and practices are based on assumptions about human nature that need not hold for digital minds. This points to the need for moral reflection as we approach the era of advanced machine intelligence. Here we focus on one set of issues, which arise from the prospect of digital minds with superhumanly strong claims to resources and influence. These could arise from the vast collective benefits that mass-produced digital minds could derive from relatively small amounts of resources. Alternatively, they could arise from individual digital minds with superhuman moral status or ability to benefit from resources. Such beings could contribute immense value to the world, and failing to respect their interests could produce a moral catastrophe, while a naive way of respecting them could be disastrous for humanity. A sensible approach requires reforms of our moral norms and institutions along with advance planning regarding what kinds of digital minds we bring into existence.",2021-08-05,2022-01-30 4:53:35,2022-01-30 4:53:35,2021-11-18 23:53:17,306-326,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000[s0]   DOI: 10.1093/oso/9780192894076.003.0018,,/Users/jacquesthibodeau/Zotero/storage/7PASBR4W/Shulman and Bostrom - 2021 - Sharing the World with Digital Minds.pdf,,MetaSafety; FHI,,"Clarke, S; Savulescu, J",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NKNK2NSC,bookSection,2017,"Armstrong, Stuart; Yampolskiy, Roman V.",Security solutions for intelligent and complex systems,Security Solutions for Hyperconnectivity and the Internet of Things,,,,,,2017,2022-01-30 4:53:35,2022-01-30 4:53:35,,37–88,,,,,,,,,,,IGI Global,,,,,,,Google Scholar,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/XG4GMKXK/164692.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZI3NCES,bookSection,2013,"Armstrong, Stuart",Risks and Mitigation Strategies for Oracle AI,Philosophy and Theory of Artificial Intelligence,978-3-642-31674-6,,,https://doi.org/10.1007/978-3-642-31674-6_25,"There is no strong reason to believe human level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed goals or motivation systems. Oracle AIs (OAI), confined AIs that can only answer questions, are one particular approach to this problem. However even Oracles are not particularly safe: humans are still vulnerable to traps, social engineering, or simply becoming dependent on the OAI. But OAIs are still strictly safer than general AIs, and there are many extra layers of precautions we can add on top of these. This paper looks at some of them and analyses their strengths and weaknesses.",2013,2022-01-30 4:53:35,2022-01-30 4:53:35,2020-12-18 6:40:27,335-347,,,,,,,"Studies in Applied Philosophy, Epistemology and Rational Ethics",,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-642-31674-6_25,,/Users/jacquesthibodeau/Zotero/storage/4WXCE999/Armstrong - 2013 - Risks and Mitigation Strategies for Oracle AI.pdf,,TechSafety; FHI,Artificial Intelligence; Capability control; Motivational control; Risks; Security; Superintelligence,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4RSHXMUT,bookSection,2015,"Armstrong, Stuart; Sandberg, Anders; ÓhÉigeartaigh, Seán",Outrunning the Law: Extraterrestrial Liberty and Universal Colonisation,The Meaning of Liberty Beyond Earth,,,,,,2015,2022-01-30 4:53:19,2022-01-30 4:53:19,,165–186,,,,,,Outrunning the Law,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/JD5FI76H/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/44ZBNBWH/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/UWEWVHXU/978-3-319-09567-7_11.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7R3ZRU4R,bookSection,2018,"Drexler, K. Eric",MDL Intelligence Distillation : Exploring Strategies for Safe Access to Superintelligent Problem-Solving Capabilities,Artificial Intelligence Safety and Security,,,,https://www.taylorfrancis.com/,"AI technologies may reach the threshold of rapid, open-ended, recursive improvement before we are prepared to manage the challenges posed",2018-07-27,2022-01-30 4:53:18,2022-01-30 4:53:18,2019-12-19 2:23:48,75–88,,,,,,MDL Intelligence Distillation,,,,,Chapman and Hall/CRC,,en,,,,,Google Scholar,,ZSCC: 0000008  DOI: 10.1201/9781351251389-6,,/Users/jacquesthibodeau/Zotero/storage/357SZXHQ/9781351251389-6.html; /Users/jacquesthibodeau/Zotero/storage/5HM3J3PG/9781351251389-6.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVKIFZC5,bookSection,2017,"Armstrong, Stuart",Introduction to the technological singularity,The Technological Singularity,,,,,,2017,2022-01-30 4:53:18,2022-01-30 4:53:18,,1–8,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000006  DOI: 10.1007/978-3-662-54033-6_1,,/Users/jacquesthibodeau/Zotero/storage/2V5K4TWK/978-3-662-54033-6_1.html,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8MT75XKW,bookSection,2015,"Armstrong, Stuart; Sotala, Kaj",How We’re Predicting AI – or Failing to,Beyond Artificial Intelligence,978-3-319-09667-4 978-3-319-09668-1,,,http://link.springer.com/10.1007/978-3-319-09668-1_2,"This paper will look at the various predictions that have been made about AI and propose decomposition schemas for analyzing them. It will propose a variety of theoretical tools for analyzing, judging, and improving these predictions. Focusing specifically on timeline predictions (dates given by which we should expect the creation of AI), it will show that there are strong theoretical grounds to expect predictions to be quite poor in this area. Using a database of 95 AI timeline predictions, it will show that these expectations are borne out in practice: expert predictions contradict each other considerably, and are indistinguishable from non-expert predictions and past failed predictions. Predictions that AI lie 15 to 25 years in the future are the most common, from experts and non-experts alike.",2015,2022-01-30 4:53:18,2022-01-30 4:53:18,2020-12-18 6:37:05,11-29,,,9,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 111  Series Title: Topics in Intelligent Engineering and Informatics DOI: 10.1007/978-3-319-09668-1_2,,/Users/jacquesthibodeau/Zotero/storage/EEW3J3XR/Armstrong and Sotala - 2015 - How We’re Predicting AI – or Failing to.pdf,,MetaSafety; FHI,,"Romportl, Jan; Zackova, Eva; Kelemen, Jozef",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4PN2QVX8,bookSection,2014,"Bostrom, Nick",Introduction—The Transhumanist FAQ: A General Introduction,Transhumanism and the Body,,,,,,2014,2022-01-30 4:53:18,2022-01-30 4:53:18,,1–17,,,,,,Introduction—The Transhumanist FAQ,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/U54KKT8S/9781137342768_1.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SP37AAU4,bookSection,2015,"Armstrong, Stuart; Sotala, Kaj",How we’re predicting AI–or failing to,Beyond artificial intelligence,,,,,,2015,2022-01-30 4:53:18,2022-01-30 4:53:18,,11–29,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000111,,/Users/jacquesthibodeau/Zotero/storage/TGPD5XXX/Armstrong and Sotala - 2015 - How we’re predicting AI–or failing to.pdf; /Users/jacquesthibodeau/Zotero/storage/8KEC66PJ/978-3-319-09668-1_2.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3I86NJCR,bookSection,2018,"Desai, Nishant; Critch, Andrew; Russell, Stuart J",Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,Advances in Neural Information Processing Systems 31,,,,http://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf,,2018,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-12-18 2:14:39,4712–4720,,,,,,,,,,,"Curran Associates, Inc.",,,,,,,Neural Information Processing Systems,,ZSCC: NoCitationData[s2]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/5WS4PP5G/Desai et al. - 2018 - Negotiable Reinforcement Learning for Pareto Optim.pdf; /Users/jacquesthibodeau/Zotero/storage/UN92H3UR/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.html,,CHAI; TechSafety,,"Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CG7BVRUT,bookSection,2017,"Bestick, Aaron; Bajcsy, Ruzena; Dragan, Anca D.",Implicitly Assisting Humans to Choose Good Grasps in Robot to Human Handovers,2016 International Symposium on Experimental Robotics,978-3-319-50114-7 978-3-319-50115-4,,,http://link.springer.com/10.1007/978-3-319-50115-4_30,"We focus on selecting handover conﬁgurations that result in low human ergonomic cost not only at the time of handover, but also when the human is achieving a goal with the object after that handover. People take objects using whatever grasping conﬁguration is most comfortable to them. When the human has a goal pose they’d like to place the object at, however, the most comfortable grasping conﬁguration at the handover might be cumbersome overall, requiring regrasping or the use of an uncomfortable conﬁguration to reach the goal. We enable robots to purposefully inﬂuence the choices available to the person when taking the object, implicitly helping the person avoid suboptimal solutions and account for the goal. We introduce a probabilistic model of how humans select grasping conﬁgurations, and use this model to optimize expected cost. We present results in simulation, as well as from a user study, showing that the robot successfully inﬂuences people’s grasping conﬁgurations for the better.",2017,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-12-18 1:39:49,341-354,,,1,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 28  DOI: 10.1007/978-3-319-50115-4_30,,/Users/jacquesthibodeau/Zotero/storage/6BDV2B6U/1810.10593.pdf; /Users/jacquesthibodeau/Zotero/storage/N3QIGNTG/Bestick et al. - 2017 - Implicitly Assisting Humans to Choose Good Grasps .pdf,,CHAI; TechSafety; AmbiguosSafety,,"Kulić, Dana; Nakamura, Yoshihiko; Khatib, Oussama; Venture, Gentiane",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K724STCW,bookSection,2021,"Russell, Stuart",Human-Compatible Artificial Intelligence,Human-Like Machine Intelligence,978-0-19-886253-6 978-0-19-189533-3,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198862536.001.0001/oso-9780198862536-chapter-1,"Following the analysis given by Alan Turing in 1951, one must expect that AI capabilities will eventually exceed those of humans across a wide range of real-world-decision making scenarios. Should this be a cause for concern, as Turing, Hawking, and others have suggested? And, if so, what can we do about it? While some in the mainstream AI community dismiss the issue, I will argue that the problem is real: we have to work out how to design AI systems that are far more powerful than ourselves while ensuring that they never have power over us. I believe the technical aspects of this problem are solvable. Whereas the standard model of AI proposes to build machines that optimize known, exogenously specified objectives, a preferable approach would be to build machines that are of provable benefit to humans. I introduce assistance games as a formal class of problems whose solution, under certain assumptions, has the desired property.",2021-07-13,2022-01-30 4:50:53,2022-01-30 4:50:53,2021-10-30 20:10:48,3-23,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s0]  ACC: 0  DOI: 10.1093/oso/9780198862536.003.0001,,/Users/jacquesthibodeau/Zotero/storage/QPGCQWEK/mi19book-hcai.pdf,,TechSafety,,,,,,,"Muggleton, Stephen; Chater, Nicholas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2HFKDU6Z,bookSection,2020,"Russell, Stuart; Jeanmaire, Caroline",From the Standard Model of AI to Provably Beneficial Systems,AI Governance in 2019: A Year In Review,,,,http://n.sinaimg.cn/tech/f34884a9/20200501/GlobalAIGovernancein2019.pdf,,2020,2022-01-30 4:50:45,2022-01-30 4:50:45,,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CHURS33T,bookSection,2018,"Reddy, Sid; Dragan, Anca; Levine, Sergey",Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,Advances in Neural Information Processing Systems 31,,,,http://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf,,2018,2022-01-30 4:51:44,2022-01-30 4:51:44,2019-12-18 2:41:12,1454–1465,,,,,,Where Do You Think You\textquotesingle re Going?,,,,,"Curran Associates, Inc.",,,,,,,Neural Information Processing Systems,,ZSCC: NoCitationData[s2]  ACC: 66,,/Users/jacquesthibodeau/Zotero/storage/MM784MFH/Reddy et al. - 2018 - Where Do You Think Youtextquotesingle re Going .pdf; /Users/jacquesthibodeau/Zotero/storage/VCE24KBJ/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.html,,CHAI; TechSafety,,"Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GDGZMWNP,bookSection,2008,"Yudkowsky, Eliezer",Cognitive biases potentially affecting judgement of global risks,Global Catastrophic Risks,978-0-19-857050-9 978-0-19-191810-0,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-9,"All else being equal, not many people would prefer to destroy the world. Even faceless corporations, meddling governments, reckless scientists, and other agents of doom, require a world in which to achieve their goals of profit, order, tenure, or other villainies. If our extinction proceeds slowly enough to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on realizing that they have actually destroyed the world. Therefore I suggest that if the Earth is destroyed, it will probably be by mistake. The systematic experimental study of reproducible errors of human reasoning, and what these errors reveal about underlying mental processes, is known as the heuristics and biases programme in cognitive psychology. This programme has made discoveries highly relevant to assessors of global catastrophic risks. Suppose you are worried about the risk of Substance P, an explosive of planet-wrecking potency which will detonate if exposed to a strong radio signal. Luckily there is a famous expert who discovered Substance P, spent the last thirty years working with it, and knows it better than anyone else in the world. You call up the expert and ask how strong the radio signal has to be. The expert replies that the critical threshold is probably around 4000 terawatts. ‘Probably?’ you query. ‘Can you give me a 98% confidence interval?’ ‘Sure’, replies the expert. ‘I’m 99%confident that the critical threshold is above 500 terawatts, and 99%confident that the threshold is below 80,000 terawatts.’ ‘What about 10 terawatts?’ you ask. ‘Impossible’, replies the expert. The above methodology for expert elicitation looks perfectly reasonable, the sort of thing any competent practitioner might do when faced with such a problem. Indeed, this methodology was used in the Reactor Safety Study (Rasmussen, 1975), now widely regarded as the first major attempt at probabilistic risk assessment. But the student of heuristics and biases will recognize at least two major mistakes in the method – not logical flaws, but conditions extremely susceptible to human error. I shall return to this example in the discussion of anchoring and adjustments biases (Section 5.7).",2008-07-03,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-11-22 5:05:28,,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 231  DOI: 10.1093/oso/9780198570509.003.0009,,,,MetaSafety; MIRI,,,,,,,"Yudkowsky, Eliezer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5GWFR6P,bookSection,2011,"Yudkowsky, Eliezer",Complex Value Systems in Friendly AI,Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,,http://link.springer.com/10.1007/978-3-642-22887-2_48,,2011,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-11-22 5:26:26,388-393,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 91  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_48,,,,TechSafety; MIRI,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSRAINS6,bookSection,2008,"Yudkowsky, Eliezer",Artificial Intelligence as a positive and negative factor in global risk,Global Catastrophic Risks,978-0-19-857050-9 978-0-19-191810-0,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-21,"By far the greatest danger of Artificial Intelligence (AI) is that people conclude too early that they understand it. Of course, this problem is not limited to the field of AI. Jacques Monod wrote: ‘A curious aspect of the theory of evolution is that everybody thinks he understands it’ (Monod, 1974). The problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard, as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about AI than they actually do. It may be tempting to ignore Artificial Intelligence because, of all the global risks discussed in this book, AI is probably hardest to discuss. We cannot consult actuarial statistics to assign small annual probabilities of catastrophe, as with asteroid strikes. We cannot use calculations from a precise, precisely confirmed model to rule out events or place infinitesimal upper bounds on their probability, as with proposed physics disasters. But this makes AI catastrophes more worrisome, not less. The effect of many cognitive biases has been found to increase with time pressure, cognitive busyness, or sparse information. Which is to say that the more difficult the analytic challenge, the more important it is to avoid or reduce bias. Therefore I strongly recommend reading my other chapter (Chapter 5) in this book before continuing with this chapter. When something is universal enough in our everyday lives, we take it for granted to the point of forgetting it exists. Imagine a complex biological adaptation with ten necessary parts. If each of the ten genes is independently at 50% frequency in the gene pool – each gene possessed by only half the organisms in that species – then, on average, only 1 in 1024 organisms will possess the full, functioning adaptation.",2008-07-03,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-11-22 2:24:05,184,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000552  Publisher: Oxford University Press New York,,/Users/jacquesthibodeau/Zotero/storage/UTV64Q5B/Yudkowsky - 2008 - Artificial intelligence as a positive and negative.pdf; /Users/jacquesthibodeau/Zotero/storage/A94A8CPN/sTkfAQAAQBAJ.html,,MetaSafety; MIRI,,,,,,,"Yudkowsky, Eliezer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B76SIGGS,bookSection,2017,"Soares, Nate; Fallenstein, Benya",Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda,The Technological Singularity,978-3-662-54031-2 978-3-662-54033-6,,,http://link.springer.com/10.1007/978-3-662-54033-6_5,,2017,2022-01-30 4:56:46,2022-01-30 4:56:46,2019-12-19 2:58:39,103-125,,,,,,Agent Foundations for Aligning Machine Intelligence with Human Interests,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s8]  ACC: 45  J: 31 DOI: 10.1007/978-3-662-54033-6_5,,/Users/jacquesthibodeau/Zotero/storage/CD67J8VC/Soares and Fallenstein - 2017 - Agent Foundations for Aligning Machine Intelligenc.pdf,,TechSafety; MIRI,,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6UBXU3EC,bookSection,2017,"Barrett, Anthony M.; Baum, Seth D.",Risk analysis and risk management for the artificial superintelligence research and development process,The Technological Singularity,,,,,,2017,2022-01-30 4:55:20,2022-01-30 4:55:20,,127–140,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000014  DOI: 10.1007/978-3-662-54033-6_6,,/Users/jacquesthibodeau/Zotero/storage/JB2CXJC3/Barrett and Baum - 2017 - Risk analysis and risk management for the artifici.pdf; /Users/jacquesthibodeau/Zotero/storage/TJZUEBZD/978-3-662-54033-6_6.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7EIPZ6G,bookSection,2019,"Baum, Seth",Lessons for Artificial Intelligence from Other Global Risks,The Global Politics of Artificial Intelligence,,,,,"The prominence of artificial intelligence (AI) as a global risk is a relatively recent phenomenon. Other global risks have longer histories and larger bodies of scholarship. The study of these other risks can offer considerable insight to the study of AI risk. This paper examines four risks: biotechnology, nuclear weapons, global warming, and asteroid collision. Several overarching lessons are found. First, the extreme severity of global risks is often insufficient to motivate action to reduce the risks. Second, perceptions of global risks can be influenced by people’s incentives and by their cultural and intellectual orientations. Third, the success of efforts to address global risks can depend on the extent of buy-in from parties who may be negatively affected by the efforts. Fourth, global risks and risk reduction initiatives can be shaped by broader socio-political conditions, such as the degree of policy influence of private industry within a political jurisdiction. The paper shows how these and other lessons can inform efforts to reduce risks from AI.",2019,2022-01-30 4:55:19,2022-01-30 4:55:19,,20,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/TNHAPD98/Baum - Lessons for Artificial Intelligence from Other Glo.pdf,,MetaSafety; GCRI,,"Tinnirello, Maurizio",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S4G7N475,bookSection,2011,"Ring, Mark; Orseau, Laurent","Delusion, Survival, and Intelligent Agents",Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,,http://link.springer.com/10.1007/978-3-642-22887-2_2,"This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these speciﬁc assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The ﬁrst of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very diﬀerent kinds of agents, speciﬁcally: reinforcementlearning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.",2011,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-11-21 17:39:39,11-20,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 77  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_2,,"/Users/jacquesthibodeau/Zotero/storage/FRZ9DPD5/Ring and Orseau - 2011 - Delusion, Survival, and Intelligent Agents.pdf",,TechSafety; Other-org,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93TJ5D9A,bookSection,2017,"Durán, Juan M.",Computer Simulations as a Technological Singularity in the Empirical Sciences,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_9,"SummaryIn this paper, I discuss the conditions necessary for computer simulations to qualify as a technological singularity in the empirical sciences. A technological singularity encompasses two claims: (a) the enhancement of human cognitive capacities by the computer, and (b) their displacement from the center of the production of knowledge. For computer simulations to be a technological singularity, then, they must fulfill points (a) and (b) above. Although point (a) is relatively unproblematic, point (b) needs further analysis. In particular, in order to show that humans could be displaced from the center of the production of knowledge, it is necessary to establish the reliability of computer simulations. That is, I need to show that computer simulations are reliable processes that render, most of the time, valid results. To be a reliable process, in turn, means that simulations accurately represent the target system and carry out error-free computations. I analyze verification and validation methods as the grounds for such representation accuracy and error-free computations. Since the aim is to entrench computer simulations as a technological singularity, the entire analysis must be careful to keep human agents out of the picture.",2017,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-11-24 2:59:54,167-179,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-662-54033-6_9,,,,MetaSafety; Other-org,Computer Simulation; Empirical Science; Epistemic Justification; Reliable Process; Target System,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZT7JGH6T,bookSection,2017,"Koepsell, David",Can the Singularity Be Patented? (And Other IP Conundrums for Converging Technologies),The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_10,"SummaryAssuming that the singularity is eventually realized, some of the legal institutions that we take for granted, specifically those relating to “intellectual property” (IP – namely, copyrights and patents), may pose some problems. IP law concerns the ownership of expressions of ideas, and not ideas themselves. Given the nature and trajectory of converging technologies, IP laws as they currently exist may impede the development of such technologies. Examples of “patent thickets” that appear to impede other rapidly evolving technologies already abound (as in the smartphone arena). Patents and copyrights may pose even more intriguing problems once the singularity is achieved because our notions of who may own what will likely radically change. Will artificial intelligences, for example, compete with us over rights to create, and will we be legally or morally precluded from ownership rights in technologies that make such agents function? Before the singularity arrives, we would do well to work through some of these legal conundrums raised and discussed below.",2017,2022-01-30 4:59:43,2022-01-30 4:59:43,2020-11-24 2:59:55,181-191,,,,,,Can the Singularity Be Patented?,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_10,,,,MetaSafety; Other-org,Artificial Agent; Intellectual Property; Natural Phenomenon; Patent Holder; Patent Office,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQJW66U7,bookSection,2006,"Spears, Diana F.",Assuring the Behavior of Adaptive Agents,Agent Technology from a Formal Perspective,978-1-85233-947-0,,,http://link.springer.com/10.1007/1-84628-271-3_8,,2006,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-11-22 1:48:09,227-257,,,,,,,,,,,Springer-Verlag,London,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 14  Series Title: NASA Monographs in Systems and Software Engineering DOI: 10.1007/1-84628-271-3_8,,,,TechSafety; Other-org,,"Rouff, Christopher A.; Hinchey, Michael; Rash, James; Truszkowski, Walter; Gordon-Spears, Diana",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CPJINAIP,bookSection,2012,"Hibbard, Bill",Avoiding Unintended AI Behaviors,Artificial General Intelligence,978-3-642-35505-9 978-3-642-35506-6,,,http://link.springer.com/10.1007/978-3-642-35506-6_12,,2012,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-11-22 1:47:56,107-116,,,7716,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 25  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_12,,/Users/jacquesthibodeau/Zotero/storage/IE7575I5/Hibbard - 2012 - Avoiding Unintended AI Behaviors.pdf,,TechSafety; Other-org,,"Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AM7ZSJ5W,bookSection,2017,"Clarke, Graham",A Psychoanalytic Approach to the Singularity: Why We Cannot Do Without Auxiliary Constructions,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_12,"SummaryPsychoanalysis is known above all else for its insistence that we have motivations that are unknown to ourselves, that are unconscious. We are all subject to sickness and accident, to bad luck and unfair breaks, and above all to death as a final end to all our endeavours. In order to compensate for these disappointments and for our ultimate inability to overcome these very real and material constraints we phantasise, we dream, we create, and/or we nurse our bruised and fragile selves by hoping that our phantasies might come true, if not for ourselves then for our offspring. The singularity, as it is most commonly expressed, concerns the possibility of overcoming death by achieving a sort of immortality. In specific terms Kurtweil’s own discussion of the singularity is concerned with the possibility of ‘resurrecting’ his dead father in virtual space at least. There is consistently throughout the writings on the singularity a dismissal of the emotional aspect of human living in favour of the rational overcoming of our existential condition. I am arguing that we cannot ignore the emotional consciousness that is the bedrock of human existence and that we ignore our unconscious feelings at our peril. I think that the singularity as it is being developed is actually a direct threat to the flourishing of human beings and human society because the emotional shortcomings of the theory have not been recognised.",2017,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-24 3:00:00,209-220,,,,,,A Psychoanalytic Approach to the Singularity,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_12,,,,MetaSafety; Other-org,Affective Computing; Computer Agent; Emotional Intelligence; Emotional Relationship; Technological Singularity,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6KM4PB68,bookSection,2011,"Anderson, Susan Leigh; Anderson, Michael",A Prima Facie Duty Approach to Machine Ethics,Machine Ethics,978-0-511-97803-6,,,https://www.cambridge.org/core/product/identifier/CBO9780511978036A041/type/book_part,,2011,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-22 2:22:02,476-492,,,,,,,,,,,Cambridge University Press,Cambridge,,,,,,DOI.org (Crossref),,ZSCC: 0000032  DOI: 10.1017/CBO9780511978036.032,,,,TechSafety; Other-org,,"Anderson, Michael; Anderson, Susan Leigh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WI6FD4K9,bookSection,2012,"Muehlhauser, Luke; Helm, Louie",The Singularity and Machine Ethics,Singularity Hypotheses,978-3-642-32559-5 978-3-642-32560-1,,,http://link.springer.com/10.1007/978-3-642-32560-1_6,,2012,2022-01-30 4:56:59,2022-01-30 4:56:59,2020-11-22 2:23:12,101-126,,,,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s3]  ACC: 94  Series Title: The Frontiers Collection DOI: 10.1007/978-3-642-32560-1_6,,,,TechSafety; MIRI,,"Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NQQM74KV,bookSection,2015,"Fallenstein, Benja; Kumar, Ramana",Proof-Producing Reflection for HOL,Interactive Theorem Proving,978-3-319-22101-4 978-3-319-22102-1,,,http://link.springer.com/10.1007/978-3-319-22102-1_11,"We present a reﬂection principle of the form “If ϕ is provable, then ϕ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure ϕ has the same meaning both inside and outside of this model. Soundness of HOL implies that if ϕ is provable, then it is true in this model, and hence ϕ holds. We additionally show how this reﬂection principle can be extended, assuming an inﬁnite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.",2015,2022-01-30 4:56:58,2022-01-30 4:56:58,2019-12-19 2:58:35,170-186,,,9236,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s7]  ACC: 13  J: 12 DOI: 10.1007/978-3-319-22102-1_11,,/Users/jacquesthibodeau/Zotero/storage/RJGGDF64/Fallenstein and Kumar - 2015 - Proof-Producing Reflection for HOL.pdf,,TechSafety; MIRI,,"Urban, Christian; Zhang, Xingyuan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NCRUW5W,bookSection,2014,"Fallenstein, Benja; Soares, Nate",Problems of Self-reference in Self-improving Space-Time Embedded Intelligence,Artificial General Intelligence,978-3-319-09273-7 978-3-319-09274-4,,,http://link.springer.com/10.1007/978-3-319-09274-4_3,,2014,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-11-22 4:16:32,21-32,,,8598,,,,,,,,Springer International Publishing,Cham,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 27  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-09274-4_3,,/Users/jacquesthibodeau/Zotero/storage/NGSVTGD3/Fallenstein and Soares - 2014 - Problems of Self-reference in Self-improving Space.pdf,,TechSafety; MIRI,,"Goertzel, Ben; Orseau, Laurent; Snaider, Javier","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DUXIZFVV,bookSection,2012,"Demski, Abram",Logical Prior Probability,Artificial General Intelligence,978-3-642-35505-9 978-3-642-35506-6,,,http://link.springer.com/10.1007/978-3-642-35506-6_6,,2012,2022-01-30 4:56:57,2022-01-30 4:56:57,2020-11-22 5:25:32,50-59,,,7716,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 19  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_6,,,,TechSafety; MIRI,,"Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RF232Z92,bookSection,2007,"Pereira, Luís Moniz; Saptawijaya, Ari",Modelling Morality with Prospective Logic,Progress in Artificial Intelligence,978-3-540-77000-8,,,http://link.springer.com/10.1007/978-3-540-77002-2_9,,2007,2022-01-30 5:00:01,2022-01-30 5:00:01,2020-11-22 2:23:23,99-111,,,4874,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 61  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-77002-2_9,,,,TechSafety; AmbiguosSafety; Other-org,,"Neves, José; Santos, Manuel Filipe; Machado, José Manuel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBQUNTRR,bookSection,2018,"Jilk, David J.",Limits to Verification and Validation of Agentic Behavior,Artificial Intelligence Safety and Security,,,,http://arxiv.org/abs/1604.06963,"Verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence.",2018,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-12-13 20:00:55,225-234,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008[s0]  arXiv: 1604.06963,,/Users/jacquesthibodeau/Zotero/storage/BXXTPQ43/Jilk - 2016 - Limits to Verification and Validation of Agentic B.pdf; /Users/jacquesthibodeau/Zotero/storage/WTZWJK5S/1604.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; I.2.0; F.3.1; K.4.1; D.2.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3INWFGE,bookSection,2004,"Turing, Alan","Intelligent Machinery, A Heretical Theory (c.1951)",The Essential Turing,978-0-19-825079-1 978-0-19-191652-6,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198250791.001.0001/isbn-9780198250791-book-part-18,"Turing gave the presentation ‘Intelligent Machinery, A Heretical Theory’ on a radio discussion programme called The ’51 Society. Named after the year in which the programme first went to air, The ’51 Society was produced by the BBC Home Service at their Manchester studio and ran for several years. A presentation by the week’s guest would be followed by a panel discussion. Regulars on the panel included Max Newman, Professor of Mathematics at Manchester, the philosopher Michael Polanyi, then Professor of Social Studies at Manchester, and the mathematician Peter Hilton, a younger member of Newman’s department at Manchester who had worked with Turing and Newman at Bletchley Park. Turing’s target in ‘Intelligent Machinery, A Heretical Theory’ is the claim that ‘You cannot make a machine to think for you’ (p. 472). A common theme in his writing is that if a machine is to be intelligent, then it will need to ‘learn by experience’ (probably with some pre-selection, by an external educator, of the experiences to which the machine will be subjected). The present article continues the discussion of machine learning begun in Chapters 10 and 11. Turing remarks that the ‘human analogy alone’ suggests that a process of education ‘would in practice be an essential to the production of a reasonably intelligent machine within a reasonably short space of time’ (p. 473). He emphasizes the point, also made in Chapter 11, that one might ‘start from a comparatively simple machine, and, by subjecting it to a suitable range of ‘‘experience’’ transform it into one which was more elaborate, and was able to deal with a far greater range of contingencies’ (p. 473). Turing goes on to give some indication of how learning might be accomplished, introducing the idea of a machine’s building up what he calls ‘indexes of experiences’ (p. 474). (This idea is not mentioned elsewhere in his writings.) An example of an index of experiences is a list (ordered in some way) of situations in which the machine has found itself, coupled with the action that was taken, and the outcome, good or bad. The situations are described in terms of features.",2004-09-09,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-11-22 5:05:23,,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000011  DOI: 10.1093/oso/9780198250791.003.0018,,,,TechSafety; Other-org,,,,,,,"Turing, Alan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBXVI57Q,bookSection,2017,"Zheng, Ping; Akhmad, Mohammed-Asif",How Change Agencies Can Affect Our Path Towards a Singularity,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_4,"SummaryThis chapter uses the perspective of change agencies to analyse how agents (such as governments, international companies, entrepreneurs and individuals) innovate, interact, assimilate, consume and ultimately determine the direction of future technologies. These are the key components to the formation of technological singularity, i.e. an artificial intelligence becoming self-aware and self-evolving leading to an unprecedented rapid technological change in human civilization. General behaviours of change agents towards relevant technological research and development are discussed with a view to the economic and social implications. The interactions of key change agents can assist in the determination of future paths towards a singularity event or possibly even an ‘anti-singularity event’. Understanding the fundamental behaviours and motivations of change agents in technology development will increase our understanding of potential mechanisms to monitor and control developments such as Artificial Intelligence research to ensure that if and when singularity occurs it can be controlled and positively utilised for social and economic benefits.",2017,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-11-24 2:59:42,87-101,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_4,,,,MetaSafety; Other-org,Change Agency; Human Brain Function; Human Race; Singularity Event; Technological Change Process,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BR95HICU,bookSection,2016,"Steunebrink, Bas R.; Thórisson, Kristinn R.; Schmidhuber, Jürgen",Growing Recursive Self-Improvers,Artificial General Intelligence,978-3-319-41648-9 978-3-319-41649-6,,,http://link.springer.com/10.1007/978-3-319-41649-6_13,"Research into the capability of recursive self-improvement typically only considers pairs of agent, self-modiﬁcation candidate , and asks whether the agent can determine/prove if the self-modiﬁcation is beneﬁcial and safe. But this leaves out the much more important question of how to come up with a potential self-modiﬁcation in the ﬁrst place, as well as how to build an AI system capable of evaluating one. Here we introduce a novel class of AI systems, called experience-based AI (EXPAI), which trivializes the search for beneﬁcial and safe self-modiﬁcations. Instead of distracting us with proof-theoretical issues, EXPAI systems force us to consider their education in order to control a system’s growth towards a robust and trustworthy, benevolent and well-behaved agent. We discuss what a practical instance of EXPAI looks like and build towards a “test theory” that allows us to gauge an agent’s level of understanding of educational material.",2016,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-12-13 19:59:05,129-139,,,9782,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 20  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-41649-6_13,,/Users/jacquesthibodeau/Zotero/storage/XGG7PJ49/Steunebrink et al. - 2016 - Growing Recursive Self-Improvers.pdf,,TechSafety; Other-org,,"Steunebrink, Bas; Wang, Pei; Goertzel, Ben",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87J7MWC6,bookSection,2017,"Peacock, Kent A.","Energy, Complexity, and the Singularity",The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_8,"SummaryThis paper explores the relevance of ecological limitations such as climate change and resource exhaustion to the possibility of a technologically-mediated “intelligence explosion” in the near future. The imminent risks of global carbonization and loss of biodiversity, as well as the dependency of technological development on a healthy biosphere, are greatly underestimated by singularity theorists such as Ray Kurzweil. While development of information technology should continue, we cannot rely on hypothetical advances in AI to get us out of our present ecological bottleneck. Rather, we should do everything we can to foster human ingenuity, the one factor that has a record of generating the game-changing innovations that our species has relied upon to overcome survival challenges in our past.",2017,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-11-24 2:59:52,153-165,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_8,,,,MetaSafety; Other-org,Ecological Challenge; Exponential Expansion; Global Carbonization; Human Ingenuity; Singularity Hypothesis,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U6T93H7J,bookSection,2017,"Majot, Andrew; Yampolskiy, Roman",Diminishing Returns and Recursive Self Improving Artificial Intelligence,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_7,"SummaryIn this chapter we will examine in more detail the concept of an artificial intelligence that can improve upon itself, and show how that might not be as problematic as some researchers think. The ability for an AI to better itself over time through a process called recursive self-improvement has been considered as a promising path to creating the technological singularity. In this type of system an AI has access to its own source code and possibly even hardware, with the ability to edit both at will. This gives the AI the option to constantly improve upon itself and become increasingly intelligent. Eventually this would produce versions of the AI that are more intelligent than humans and cause us to reach the technological singularity. Researchers have speculated that this process could create an extremely dangerous situation for humanity as we get left behind in a growing intelligence gap. This chapter proposes that this gap would not be as drastic as initially thought, and that there may be natural limits on the ability for an AI to improve upon itself. Along the way we will propose that the law of diminishing returns will take effect to limit runaway intelligence. We also theorize that developing and manufacturing new hardware will introduce a latency in AI improvement that could easily be exploited to halt any dangerous situation.",2017,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-11-24 2:59:49,141-152,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 3  DOI: 10.1007/978-3-662-54033-6_7,,,,MetaSafety; AmbiguosSafety; Other-org,Technological Singularity; Cognitive Algorithm; Hardware Improvement; Inductive Logic Programming; Logistical Chain,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,bookSection,2008,"Omohundro, Stephen",The Basic AI Drives,Artificial General Intelligence 2008: Proceedings of the First AGI Conference,978-1-60750-309-5,,,,"The field of Artificial Intelligence (AI) was initially directly aimed at the construction of ‘thinking machines’ – that is, computer systems with human-like general intelligence. But this task proved more difficult than expected. As the years passed, AI researchers gradually shifted focus to producing AI systems that intelligently approached specific tasks in relatively narrow domains. In recent years, however, more and more AI researchers have recognized the necessity – and the feasibility – of returning to the original goal of the field. Increasingly, there is a call to focus less on highly specialized ‘narrow AI’ problem solving systems, and more on confronting the difficult issues involved in creating ‘human-level intelligence’, and ultimately general intelligence that goes beyond the human level in various ways. Artificial General Intelligence (AGI), as this renewed focus has come to be called, attempts to study and reproduce intelligence as a whole in a domain independent way. Encouraged by the recent success of several smaller-scale AGI-related meetings and special tracks at conferences, the initiative to organize the very first international conference on AGI was taken, with the goal to give researchers in the field an opportunity to present relevant research results and to exchange ideas on topics of common interest. In this collection you will find the conference papers: full-length papers, short position statements and also the papers presented in the post conference workshop on the sociocultural, ethical and futurological implications of AGI.",2008-02-18,2020-11-21 16:57,2020-12-21 18:23,,,,,,,,,,,,,IOS Press,,en,,,,,Google Books,,ZSCC: 0000022  Google-Books-ID: atjvAgAAQBAJ,,,https://www.google.com/books?id=atjvAgAAQBAJ,Other-org; TechSafety,Computers / Intelligence (AI) & Semantics; Computers / General,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper from 2008 introduces convergent instrumental subgoals: the subgoals that an AI system will have “by default”, unless care is taken to avoid them. For this paper, an AI system is a system that “has goals which it tries to accomplish by acting in the world”, i.e. it assumes that the system is <@goal-directed@>(@Intuitions about goal-directed behavior@).

It starts by arguing that a sufficiently powerful goal-directed AI system will want to self-improve, as that could help it achieve its goals better in the (presumably long) future. In particular, it will want to become “rational”, in the sense that it will want to maximize its _expected_ utility, where the utility function is determined by its goal. (The justification for this is the VNM theorem, and the various Dutch book arguments that support Bayesianism and expected utility maximization.)

However, not all modifications would be good for the AI system. In particular, it will very strongly want to preserve its utility function, as that determines what it will (try to) accomplish in the future, and any change in the utility function would be a disaster from the perspective of the current utility function. Similarly, it will want to protect itself from harm, that is, it has a survival incentive, because it can’t accomplish its goal if it’s dead.

The final instrumental subgoal is to acquire resources and use them efficiently in pursuit of its goal, because almost by definition resources are useful for a wide variety of goals, including (probably) the AI system’s goal."
IWUNUHTA,conferencePaper,2020,"Yang, Zitong; Yu, Yaodong; You, Chong; Steinhardt, Jacob; Ma, Yi",Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2002.11328,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",2020-12-07,2022-01-30 4:47:35,2022-01-30 4:47:35,2021-11-13 14:19:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000061  arXiv: 2002.11328,,/Users/jacquesthibodeau/Zotero/storage/7J95438X/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
TIBNQTSX,conferencePaper,2020,"Matheos, George; Lew, Alexander K.; Ghavamizadeh, Matin; Russell, Stuart; Cusumano-Towner, Marco; Mansinghka, Vikash",Transforming Worlds: Automated Involutive MCMC for Open-Universe Probabilistic Models,,,,,https://openreview.net/forum?id=8Itm8dQnJRc,"Inference in open-universe probabilistic models can be challenging.  We show how to automate a broad class of MCMC kernels for them, facilitating the development of domain-specific algorithms for...",2020-11-23,2022-01-30 4:47:35,2022-01-30 4:47:35,2021-10-30 21:42:12,,,,,,,Transforming Worlds,,,,,,,en,,,,,openreview.net,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/4QSFBCAS/forum.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Third Symposium on Advances in Approximate Bayesian Inference,,,,,,,,,,,,,,,,
RQWBFZ9W,conferencePaper,2020,"Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet",Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,arXiv:2010.11645 [cs],,,,http://arxiv.org/abs/2010.11645,"Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.",2020-11-03,2022-01-30 4:47:35,2022-01-30 4:47:35,2021-10-31 19:04:24,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 24  arXiv: 2010.11645,,/Users/jacquesthibodeau/Zotero/storage/TFVBZFJ6/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/SQ7TS9GM/2010.html; /Users/jacquesthibodeau/Zotero/storage/VMH5N4E5/2010.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
IK3Q65C6,conferencePaper,2020,"Li, Alexander C.; Pinto, Lerrel; Abbeel, Pieter",Generalized Hindsight for Reinforcement Learning,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2002.11708,"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",2020-02-26,2022-01-30 4:47:35,2022-01-30 4:47:35,2021-11-07 18:21:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2002.11708,,/Users/jacquesthibodeau/Zotero/storage/9T8PK9IJ/Li et al. - 2020 - Generalized Hindsight for Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KRXF5ZMV/2002.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
GZB84KMR,conferencePaper,2020,"Richardson, Oliver; Halpern, Joseph Y.",Probabilistic Dependency Graphs,"arXiv:2012.10800 [cs, math]",,,,http://arxiv.org/abs/2012.10800,"We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed graphical models. PDGs can capture inconsistent beliefs in a natural way and are more modular than Bayesian Networks (BNs), in that they make it easier to incorporate new information and restructure the representation. We show by example how PDGs are an especially natural modeling tool. We provide three semantics for PDGs, each of which can be derived from a scoring function (on joint distributions over the variables in the network) that can be viewed as representing a distribution's incompatibility with the PDG. For the PDG corresponding to a BN, this function is uniquely minimized by the distribution the BN represents, showing that PDG semantics extend BN semantics. We show further that factor graphs and their exponential families can also be faithfully represented as PDGs, while there are significant barriers to modeling a PDG with a factor graph.",2020-12-19,2022-01-30 4:47:35,2022-01-30 4:47:35,2021-10-30 21:44:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 0  arXiv: 2012.10800,,/Users/jacquesthibodeau/Zotero/storage/CBQAK6J6/Richardson and Halpern - 2020 - Probabilistic Dependency Graphs.pdf; /Users/jacquesthibodeau/Zotero/storage/MPAXVG7Q/2012.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Information Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,
PZ3NUMGN,conferencePaper,2020,"Buçinca, Zana; Lin, Phoebe; Gajos, Krzysztof Z.; Glassman, Elena L.",Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems,Proceedings of the 25th International Conference on Intelligent User Interfaces,978-1-4503-7118-6,,10.1145/3377325.3377498,https://doi.org/10.1145/3377325.3377498,"Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.",2020-03-17,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-13,454–464,,,,,,,IUI '20,,,,Association for Computing Machinery,"Cagliari, Italy",,,,,,ACM Digital Library,,ZSCC: 0000052,,/Users/jacquesthibodeau/Zotero/storage/AXTMBBWM/Buçinca et al. - 2020 - Proxy tasks and subjective measures can be mislead.pdf,,UnsortedSafety,artificial intelligence; explanations; trust,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4QR28FMI,conferencePaper,2020,"McGregor, Sean",Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,arXiv:2011.08512 [cs],,,,http://arxiv.org/abs/2011.08512,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.",2020-11-17,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-13 14:28:31,,,,,,,Preventing Repeated Real World AI Failures by Cataloging Incidents,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2011.08512,,/Users/jacquesthibodeau/Zotero/storage/T3W45ATE/McGregor - 2020 - Preventing Repeated Real World AI Failures by Cata.pdf,,UnsortedSafety,Computer Science - Computers and Society; Computer Science - Software Engineering; I.2.0; K.4.0; K.4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Innovative Applications of Artificial Intelligence (IAAI-21),,,,,,,,,,,,,,,,
AXN2R6VV,conferencePaper,2020,"Eysenbach, Benjamin; Geng, Xinyang; Levine, Sergey; Salakhutdinov, Ruslan",Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"arXiv:2002.11089 [cs, stat]",,,,http://arxiv.org/abs/2002.11089,"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",2020-02-25,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-07 18:36:46,,,,,,,Rewriting History with Inverse RL,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2002.11089,,/Users/jacquesthibodeau/Zotero/storage/BZXT2G8S/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf; /Users/jacquesthibodeau/Zotero/storage/WGP4ZA47/2002.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
ZFZET66K,conferencePaper,2020,"Levine, Sergey; Kumar, Aviral; Tucker, George; Fu, Justin","Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","arXiv:2005.01643 [cs, stat]",,,,http://arxiv.org/abs/2005.01643,"In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",2020-11-01,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-07 23:28:45,,,,,,,Offline Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000299  arXiv: 2005.01643,,"/Users/jacquesthibodeau/Zotero/storage/Q4HR9G8B/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf; /Users/jacquesthibodeau/Zotero/storage/KA6FVASR/2005.html",,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
56TFSGHP,conferencePaper,2020,"Anderson, Greg; Verma, Abhinav; Dillig, Isil; Chaudhuri, Swarat",Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"arXiv:2009.12612 [cs, stat]",,,,http://arxiv.org/abs/2009.12612,"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",2020-10-26,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-09 0:01:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 2009.12612,,/Users/jacquesthibodeau/Zotero/storage/3ENESDMT/Anderson et al. - 2020 - Neurosymbolic Reinforcement Learning with Formally.pdf; /Users/jacquesthibodeau/Zotero/storage/75R2UPKX/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
T3ITWCM7,conferencePaper,2020,"Liu, Jian; Cui, Leyang; Liu, Hanmeng; Huang, Dandan; Wang, Yile; Zhang, Yue",LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,arXiv:2007.08124 [cs],,,,http://arxiv.org/abs/2007.08124,"Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset",2020-07-16,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-14 18:04:32,,,,,,,LogiQA,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 2007.08124,,/Users/jacquesthibodeau/Zotero/storage/G86XQK89/Liu et al. - 2020 - LogiQA A Challenge Dataset for Machine Reading Co.pdf; /Users/jacquesthibodeau/Zotero/storage/5KUSX66V/2007.html,,UnsortedSafety,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2020,,,,,,,,,,,,,,,,
8VJV98HV,conferencePaper,2020,"Balakrishnan, Sreejith; Nguyen, Quoc Phong; Low, Bryan Kian Hsiang; Soh, Harold",Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization,arXiv:2011.08541 [cs],,,,http://arxiv.org/abs/2011.08541,"The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite significant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identifiable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple solutions that are consistent with the expert demonstrations by efficiently exploring the reward function space. BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.",2020-11-17,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 18:41:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2011.08541,,/Users/jacquesthibodeau/Zotero/storage/9JPR5MCW/Balakrishnan et al. - 2020 - Efficient Exploration of Reward Functions in Inver.pdf; /Users/jacquesthibodeau/Zotero/storage/8SJT4KWC/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
8CME59D5,conferencePaper,2020,"Kumar, Aviral; Gupta, Abhishek; Levine, Sergey",DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2003.07305,"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",2020-03-16,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 13:40:43,,,,,,,DisCor,,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 2003.07305,,/Users/jacquesthibodeau/Zotero/storage/ZWRH5U35/Kumar et al. - 2020 - DisCor Corrective Feedback in Reinforcement Learn.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
WP3AREUF,conferencePaper,2020,"Chang, Michael; Kaushik, Sidhant; Weinberg, S. Matthew; Griffiths, Thomas L.; Levine, Sergey",Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2007.02382,"This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.",2020-08-14,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-07 16:27:41,,,,,,,Decentralized Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2007.02382,,/Users/jacquesthibodeau/Zotero/storage/FSTFPI4R/Chang et al. - 2020 - Decentralized Reinforcement Learning Global Decis.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
5WJ3NKZK,conferencePaper,2007,"Ramachandran, Deepak",Bayesian Inverse Reinforcement Learning,,,,,https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf,Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efﬁcient algorithms that ﬁnd solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.,2007,2022-01-30 4:48:44,2022-01-30 4:48:44,,6,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000660,,/Users/jacquesthibodeau/Zotero/storage/6MR8CJ2E/Ramachandran - 2007 - Bayesian Inverse Reinforcement Learning.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI07,,,,,,,,,,,,,,,,
3IHMRI74,conferencePaper,2019,"Zhu, He; Xiong, Zikang; Magill, Stephen; Jagannathan, Suresh",An Inductive Synthesis Framework for Verifiable Reinforcement Learning,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,,,10.1145/3314221.3314638,http://arxiv.org/abs/1907.07273,"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",2019-06-08,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-09 0:07:06,686-701,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000051  arXiv: 1907.07273,,/Users/jacquesthibodeau/Zotero/storage/REZTXVMI/Zhu et al. - 2019 - An Inductive Synthesis Framework for Verifiable Re.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH9U6EAJ/1907.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; 00-02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3H57NUUP,conferencePaper,2020,"Juric, Mislav; Sandic, Agneza; Brcic, Mario",AI safety: state of the field through quantitative lens,arXiv:2002.05671 [cs],,,10.23919/MIPRO48935.2020.9245153,http://arxiv.org/abs/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.",2020-07-09,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 15:49:34,,,,,,,AI safety,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2002.05671,,/Users/jacquesthibodeau/Zotero/storage/T6JFHRKD/Juric et al. - 2020 - AI safety state of the field through quantitative.pdf,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MIPRO 2020,,,,,,,,,,,,,,,,
I8JC8ADC,conferencePaper,2019,"Waytowich, Nicholas; Barton, Sean L.; Lawhern, Vernon; Warnell, Garrett",A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1911.00497,"While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.",2019-10-31,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 22:30:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1911.00497,,/Users/jacquesthibodeau/Zotero/storage/7Q4ZC6TW/Waytowich et al. - 2019 - A Narration-based Reward Shaping Approach using Gr.pdf; /Users/jacquesthibodeau/Zotero/storage/C869HZX8/1911.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5Q25A4W2,conferencePaper,2021,"Bousquet, Olivier; Hanneke, Steve; Moran, Shay; van Handel, Ramon; Yehudayoff, Amir",A Theory of Universal Learning,STOC 2021: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,,,10.1145/3406325.3451087,http://arxiv.org/abs/2011.04483,"How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its ""learning curve"", that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy. In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case. For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.",2021-06,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 23:03:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2011.04483,,/Users/jacquesthibodeau/Zotero/storage/QPAPTBUB/Bousquet et al. - 2020 - A Theory of Universal Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/5C38AGW9/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Mathematics - Statistics Theory; Computer Science - Data Structures and Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,STOC 2021,,,,,,,,,,,,,,,,
QHKZ9UMR,conferencePaper,2021,"Power, Alethea; Burda, Yuri; Edwards, Harri; Babuschkin, Igor; Misra, Vedant",GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS,,,,,,"In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the ﬁnite training dataset.",2021,2022-01-30 4:48:28,2022-01-30 4:48:28,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s0]  ACC: 4,,/Users/jacquesthibodeau/Zotero/storage/MPKP826T/Power et al. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1stMathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021",,,,,,,,,,,,,,,,
ZQ49PGTV,conferencePaper,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,arXiv:2012.08630 [cs],,,,http://arxiv.org/abs/2012.08630,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-01-30 4:48:04,2022-01-30 4:48:04,2021-11-13 19:00:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 2012.08630,,/Users/jacquesthibodeau/Zotero/storage/J98ABBH2/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/E5XXCHBD/2012.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020 Cooperative AI Workshop,,,,,,,,,,,,,,,,
NVDI9SGV,conferencePaper,2020,"Stray, Jonathan; Adler, Steven; Hadfield-Menell, Dylan",What are you optimizing for? Aligning Recommender Systems with Human Values,,,,,http://arxiv.org/abs/2107.10939,"We describe cases where real recommender systems were modiﬁed in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classiﬁers from humancreated data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.",2020,2022-01-30 4:51:11,2022-01-30 4:51:11,,7,,,,,,What are you optimizing for?,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000003[s0],,/Users/jacquesthibodeau/Zotero/storage/VX5CHWKD/Stray et al. - 2021 - What are you optimizing for Aligning Recommender .pdf; /Users/jacquesthibodeau/Zotero/storage/XIEIB3NT/2107.html; /Users/jacquesthibodeau/Zotero/storage/PQ7MPFGP/Stray et al. - What are you optimizing for Aligning Recommender .pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Computers and Society; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Participatory Approaches to Machine Learning Workshop, ICML 2020",,,,,,,,,,,,,,,,
2EZUTH82,conferencePaper,2021,"Srivastava, Siddharth",Unifying Principles and Metrics for Safe and Assistive AI,Proceedings of the AAAI Conference on Artificial Intelligence,,,,https://ojs.aaai.org/index.php/AAAI/article/view/17769,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI's impact on the future of work.  These concerns reflect two aspects of a central question: how  would  humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",2021-05-18,2022-01-30 4:51:11,2022-01-30 4:51:11,2021-10-30 20:54:11,15064-15068,,,35,,,,,,,,,,en,Copyright (c) 2021 Association for the Advancement of Artificial Intelligence,,,,ojs.aaai.org,,ZSCC: 0000004  Number: 17,,/Users/jacquesthibodeau/Zotero/storage/KSRRJQU3/Srivastava - 2021 - Unifying Principles and Metrics for Safe and Assis.pdf,,MetaSafety; AmbiguousSafety,Metrics For Safe And Beneficial AI Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H2JA5F8T,conferencePaper,2021,"Laidlaw, Cassidy; Russell, Stuart",Uncertain Decisions Facilitate Better Preference Learning,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,http://arxiv.org/abs/2106.10394,"Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",2021-10-28,2022-01-30 4:51:11,2022-01-30 4:51:11,2021-11-18 23:19:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2106.10394,,/Users/jacquesthibodeau/Zotero/storage/9KSDNPMR/Laidlaw and Russell - 2021 - Uncertain Decisions Facilitate Better Preference L.pdf; /Users/jacquesthibodeau/Zotero/storage/PRVT74B5/2106.html,,TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,,,,,,,,,,,,,
59SRKKCH,conferencePaper,2018,"Milli, Smitha; Miller, John; Dragan, Anca D.; Hardt, Moritz",The Social Cost of Strategic Classification,"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",,,,http://arxiv.org/abs/1808.08460,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",2018-11-22,2022-01-30 4:51:10,2022-01-30 4:51:10,2019-12-18 2:40:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016[s2]  arXiv: 1808.08460,,/Users/jacquesthibodeau/Zotero/storage/HX3R88QW/Milli et al. - 2018 - The Social Cost of Strategic Classification.pdf; /Users/jacquesthibodeau/Zotero/storage/522JRDXP/1808.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conference on Fairness, Accountability, and Transparency",,,,,,,,,,,,,,,,
ADB4QQ4U,conferencePaper,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,arXiv:2107.01969 [cs],,,,http://arxiv.org/abs/2107.01969,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-01-30 4:51:10,2022-01-30 4:51:10,2021-11-14 18:53:31,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2107.01969,,/Users/jacquesthibodeau/Zotero/storage/PNMP8N4E/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/ASZTCQCH/2107.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
ANADKJFM,conferencePaper,2019,"Chan, Lawrence; Hadfield-Menell, Dylan; Srinivasa, Siddhartha; Dragan, Anca",The Assistive Multi-Armed Bandit,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,http://arxiv.org/abs/1901.08654,"Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.",2019-01-24,2022-01-30 4:51:10,2022-01-30 4:51:10,2019-07-08 15:45:03,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000024  arXiv: 1901.08654,,/Users/jacquesthibodeau/Zotero/storage/RJHMMH5U/Chan et al. - 2019 - The Assistive Multi-Armed Bandit.pdf; /Users/jacquesthibodeau/Zotero/storage/SP2MCRP5/1901.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,,,,,,,,,,,,,
JUZZK322,conferencePaper,2017,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",The Off-Switch Game,Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,978-0-9992411-0-3,,10.24963/ijcai.2017/32,https://www.ijcai.org/proceedings/2017/32,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R’s off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H’s actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",2017,2022-01-30 4:51:10,2022-01-30 4:51:10,2019-07-08 15:30:25,220-227,,,,,,,,,,,International Joint Conferences on Artificial Intelligence Organization,"Melbourne, Australia",en,,,,,DOI.org (Crossref),,ZSCC: 0000086,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Sixth International Joint Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
CZHP842W,conferencePaper,2020,"Toyer, Sam; Shah, Rohin; Critch, Andrew; Russell, Stuart",The MAGICAL Benchmark for Robust Imitation,Advances in Neural Information Processing Systems 33 Pre-proceedings,,,,https://papers.nips.cc/paper/2020/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html,"Imitation Learning (IL) algorithms are typically evaluated in the same environment that was used to create demonstrations. This rewards precise reproduction of demonstrations in one particular environment, but provides little information about how robustly an algorithm can generalise the demonstrator’s intent to substantially different deployment settings. This paper presents the MAGICAL benchmark suite, which permits systematic evaluation of generalisation by quantifying robustness to different kinds of distribution shift that an IL algorithm is likely to encounter in practice. Using the MAGICAL suite, we conﬁrm that existing IL algorithms overﬁt signiﬁcantly to the context in which demonstrations are provided. We also show that standard methods for reducing overﬁtting are effective at creating narrow perceptual invariances, but are not sufﬁcient to enable transfer to contexts that require substantially different behaviour, which suggests that new approaches will be needed in order to robustly generalise demonstrator intent. Code and data for the MAGICAL suite is available at https://github.com/qxcv/magical/.",2020,2022-01-30 4:51:10,2022-01-30 4:51:10,2020-12-18,25,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/68R9Q97E/Toyer et al. - The MAGICAL Benchmark for Robust Imitation.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
4K6R3T4K,conferencePaper,2019,"Dragan, Anca",Specifying AI Objectives As a Human-AI Collaboration Problem,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6324-2,,10.1145/3306618.3314227,http://doi.acm.org/10.1145/3306618.3314227,"Estimation, planning, control, and learning are giving us robots that can generate good behavior given a specified objective and set of constraints. What I care about is how humans enter this behavior generation picture, and study two complementary challenges: 1) how to optimize behavior when the robot is not acting in isolation, but needs to coordinate or collaborate with people; and 2) what to optimize in order to get the behavior we want. My work has traditionally focused on the former, but more recently I have been casting the latter as a human-robot collaboration problem as well (where the human is the end-user, or even the robotics engineer building the system). Treating it as such has enabled us to use robot actions to gain information; to account for human pedagogic behavior; and to exchange information between the human and the robot via a plethora of communication channels, from external forces that the person physically applies to the robot, to comparison queries, to defining a proxy objective function.",2019,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-12-18 2:38:17,329–329,,,,,,,AIES '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"ZSCC: 0000000  event-place: Honolulu, HI, USA",,,,CHAI; TechSafety,inverse reinforcement learning; reward design; value alignment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGGZQCD2,conferencePaper,2017,"Milli, Smitha; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Should Robots be Obedient?,IJCAI'17: Proceedings of the 26th International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1705.09990,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",2017-05-28,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-05-07 20:04:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000050  arXiv: 1705.09990,,/Users/jacquesthibodeau/Zotero/storage/3A9TS8IF/Milli et al. - 2017 - Should Robots be Obedient.pdf; /Users/jacquesthibodeau/Zotero/storage/U6MHTWRU/1705.html; /Users/jacquesthibodeau/Zotero/storage/VKNMUE9J/1705.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2017,,,,,,,,,,,,,,,,
HJ8K6HTE,conferencePaper,2019,"Swamy, Gokul; Reddy, Siddharth; Levine, Sergey; Dragan, Anca D.",Scaled Autonomy: Enabling Human Operators to Control Robot Fleets,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1910.02910,"Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a navigation task and manipulator reaching task.",2019-09-21,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-12-18 2:35:11,,,,,,,Scaled Autonomy,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1910.02910,,/Users/jacquesthibodeau/Zotero/storage/VPEQJUNS/Swamy et al. - 2019 - Scaled Autonomy Enabling Human Operators to Contr.pdf; /Users/jacquesthibodeau/Zotero/storage/EVFPGQ5H/1910.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
JVDEZZWU,conferencePaper,2018,"Fridovich-Keil, David; Fisac, Jaime F.; Tomlin, Claire J.",Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments,2019 International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1811.07834,"We present a new framework for motion planning that wraps around existing kinodynamic planners and guarantees recursive feasibility when operating in a priori unknown, static environments. Our approach makes strong guarantees about overall safety and collision avoidance by utilizing a robust controller derived from reachability analysis. We ensure that motion plans never exit the safe backward reachable set of the initial state, while safely exploring the space. This preserves the safety of the initial state, and guarantees that that we will eventually ﬁnd the goal if it is possible to do so while exploring safely. We implement our framework in the Robot Operating System (ROS) software environment and demonstrate it in a real-time simulation.",2018-11-19,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-07-08 16:10:47,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 24  J: 6 arXiv: 1811.07834,,,,CHAI; TechSafety,Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
QKVQDMCH,conferencePaper,2021,"Emmons, Scott; Oesterheld, Caspar; Critch, Andrew; Conitzer, Vince; Russell, Stuart","Symmetry, Equilibria, and Robustness in Common-Payoff Games",,,,,https://preflib.github.io/gaiw2021/papers/GAIW_2021_paper_32.pdf,"Although it has been known since the 1970s that a globally optimal strategy profile in a common-payoff game is a Nash equilibrium, global optimality is a strict requirement that limits the result’s applicability. In this work, we show that any locally optimal sym- metric strategy profile is also a (global) Nash equilibrium. Applied to machine learning, our result provides a global guarantee for any gradient method that finds a local optimum in symmetric strategy space. Furthermore, we show that this result is robust to pertur- bations to the common payoff and to the local optimum. While these results indicate stability to unilateral deviation, we neverthe- less identify broad classes of games where mixed local optima are unstable under joint, asymmetric deviations. We analyze the preva- lence of instability by running learning algorithms in a suite of symmetric games, and we conclude with results on the complexity of computing game symmetries.",2021-05,2022-01-30 4:51:09,2022-01-30 4:51:09,2021-10-30 21:01:14,17,,,,,,,,,,,,"London, UK",en,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/7B72JZQB/GAIW_2021_paper_32.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"3rd Games, Agents, and Incentives Workshop (GAIW 2021)",,,,,,,,,,,,,,,,
NWHFR69Q,conferencePaper,2018,"Ratner, Ellis; Hadfield-Menell, Dylan; Dragan, Anca D.",Simplifying Reward Design through Divide-and-Conquer,Robotics: Science and Systems XIV,,,,http://arxiv.org/abs/1806.02501,"Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-andconquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We ﬁnd that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.",2018-06-06,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-07-12 0:11:37,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1806.02501,,/Users/jacquesthibodeau/Zotero/storage/3W9MFI95/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf; /Users/jacquesthibodeau/Zotero/storage/M3RN4N6G/1806.html; /Users/jacquesthibodeau/Zotero/storage/CTPZ3Q7F/1806.html; /Users/jacquesthibodeau/Zotero/storage/3HUP68HH/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems XIV,,,,,,,,,,,,,,,,
AXU8XRXT,conferencePaper,2020,"Köster, Raphael; Hadfield-Menell, Dylan; Hadfield, Gillian K.; Leibo, Joel Z.",Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors,"Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020),",,,,http://arxiv.org/abs/2001.09318,"How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a ""silly rule"") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.",2020-01-25,2022-01-30 4:51:09,2022-01-30 4:51:09,2020-11-21 18:30:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.09318,,/Users/jacquesthibodeau/Zotero/storage/G24AAEA3/Köster et al. - 2020 - Silly rules improve the capacity of agents to lear.pdf; /Users/jacquesthibodeau/Zotero/storage/TS9NAKX8/2001.html; /Users/jacquesthibodeau/Zotero/storage/W6K5KE25/2001.html,,CHAI; TechSafety; DeepMind,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GRWKGUDI,conferencePaper,2018,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",Shared Autonomy via Deep Reinforcement Learning,Robotics: Science and Systems XIV,,,,http://arxiv.org/abs/1802.01744,"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user’s policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user’s actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user’s input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) ﬂying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user’s private information through observations, but receives a reward signal and user input that both depend on the user’s intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user’s input. This enables the assisted user to complete the task more effectively than the user or an autonomous agent could on their own. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable ﬂexible and practical assistive systems.",2018-02-05,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-07-12 0:10:49,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000087  arXiv: 1802.01744,,/Users/jacquesthibodeau/Zotero/storage/6HMCX5IQ/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KS26B5X8/1802.html; /Users/jacquesthibodeau/Zotero/storage/FWIRNIMR/1802.html; /Users/jacquesthibodeau/Zotero/storage/P49DC639/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems XIV,,,,,,,,,,,,,,,,
R95SH2RD,conferencePaper,2019,"Li, Shihui; Wu, Yi; Cui, Xinyue; Dong, Honghua; Fang, Fei; Russell, Stuart",Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient,Proceedings of the AAAI Conference on Artificial Intelligence,,,10.1609/aaai.v33i01.33014213,http://www.aaai.org/ojs/index.php/AAAI/article/view/4327,"Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efﬁciently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method signiﬁcantly outperforms existing baselines.",2019-07-17,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-17 22:55:06,4213-4220,,,33,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000108,,/Users/jacquesthibodeau/Zotero/storage/TAT4QW33/Li et al. - 2019 - Robust Multi-Agent Reinforcement Learning via Mini.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QDMNDKS,conferencePaper,2016,"Halpern, Joseph Y.; Vilaca, Xavier",Rational Consensus,Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,,,,http://arxiv.org/abs/2005.10141,"We provide a game-theoretic analysis of consensus, assuming that processes are controlled by rational agents and may fail by crashing. We consider agents that \emph{care only about consensus}: that is, (a) an agent's utility depends only on the consensus value achieved (and not, for example, on the number of messages the agent sends) and (b) agents strictly prefer reaching consensus to not reaching consensus. We show that, under these assumptions, there is no \emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking, this means that there must always exist a \emph{failure pattern} (a description of who fails, when they fail, and which agents they do not send messages to in the round that they fail) and initial preferences for which an agent can gain by deviating. On the other hand, if we assume that there is a distribution $\pi$ on the failure patterns and initial preferences, then under minimal assumptions on $\pi$, there is a Nash equilibrium that tolerates $f$ failures (i.e., $\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 < n$ (where $n$ is the total number of agents). Moreover, we show that a slight extension of the Nash equilibrium strategy is also a \emph{sequential} equilibrium (under the same assumptions about the distribution $\pi$).",2016,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-17 22:22:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2005.10141,,/Users/jacquesthibodeau/Zotero/storage/5MCJDWZ2/Halpern and Vilaca - 2020 - Rational Consensus.pdf; /Users/jacquesthibodeau/Zotero/storage/MAM8FX3J/2005.html,,CHAI; TechSafety; AmbiguosSafety,"Computer Science - Computer Science and Game Theory; Computer Science - Distributed, Parallel, and Cluster Computing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016 ACM Symposium on Principles of Distributed Computing,,,,,,,,,,,,,,,,
NFZZSNRF,conferencePaper,2020,"Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan",Quantifying Differences in Reward Functions,,,,,http://arxiv.org/abs/2006.13900,"For many tasks, the reward function is too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by examining rollouts from a policy optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences, and the reinforcement learning algorithm failing to optimize the learned reward. Moreover, the rollout method is highly sensitive to details of the environment the learned reward is evaluated in, which often differ in the deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without training a policy. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be precisely approximated and is more robust than baselines to the choice of visitation distribution. Finally, we find that the EPIC distance of learned reward functions to the ground-truth reward is predictive of the success of training a policy, even in different transition dynamics.",2020-06-24,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-08-31 17:51:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2006.13900,,/Users/jacquesthibodeau/Zotero/storage/QZKBAKKJ/Gleave et al. - 2020 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/SRRD2J53/2006.html,,CHAI; TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
RG8HBUB7,conferencePaper,2021,"Stastny, Julian; Riché, Maxime; Lyzhov, Alexander; Treutlein, Johannes; Dafoe, Allan; Clifton, Jesse",Normative Disagreement as a Challenge for Cooperative AI,Cooperative AI workshop and the Strategic ML workshop at NeurIPS 2021,,,,http://arxiv.org/abs/2111.13872,"Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.",2021-11-27,2022-01-30 4:51:08,2022-01-30 4:51:08,2021-12-11 14:19:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2111.13872,,/Users/jacquesthibodeau/Zotero/storage/57K6T8XI/Stastny et al. - 2021 - Normative Disagreement as a Challenge for Cooperat.pdf,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
FCTCJRGU,conferencePaper,2018,"Fisac, Jaime F.; Bajcsy, Andrea; Herbert, Sylvia L.; Fridovich-Keil, David; Wang, Steven; Tomlin, Claire J.; Dragan, Anca D.",Probabilistically Safe Robot Planning with Confidence-Based Human Predictions,arXiv:1806.00109 [cs],,,,https://arxiv.org/abs/1806.00109v1,"In order to safely operate around humans, robots can employ predictive models of human motion. Unfortunately, these models cannot capture the full complexity of human behavior and necessarily introduce simplifying assumptions. As a result, predictions may degrade whenever the observed human behavior departs from the assumed structure, which can have negative implications for safety. In this paper, we observe that how ""rational"" human actions appear under a particular model can be viewed as an indicator of that model's ability to describe the human's current motion. By reasoning about this model confidence in a real-time Bayesian framework, we show that the robot can very quickly modulate its predictions to become more uncertain when the model performs poorly. Building on recent work in provably-safe trajectory planning, we leverage these confidence-aware human motion predictions to generate assured autonomous robot motion. Our new analysis combines worst-case tracking error guarantees for the physical robot with probabilistic time-varying human predictions, yielding a quantitative, probabilistic safety certificate. We demonstrate our approach with a quadcopter navigating around a human.",2018-05-31,2022-01-30 4:51:07,2022-01-30 4:51:07,2019-12-18 1:36:19,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000076,,/Users/jacquesthibodeau/Zotero/storage/5ESRXS33/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/ATI9G9AD/1806.html; /Users/jacquesthibodeau/Zotero/storage/EFE4XMVX/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/4UZA833Q/1806.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2018,,,,,,,,,,,,,,,,
PEQIA7QQ,conferencePaper,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,,,,,http://arxiv.org/abs/1902.04198,"Reinforcement learning (RL) agents optimize only the features speciﬁed in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisﬁed in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to ﬁll in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We ﬁnd that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-02-11,2022-01-30 4:51:07,2022-01-30 4:51:07,2019-07-11 18:35:33,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000031  arXiv: 1902.04198,,/Users/jacquesthibodeau/Zotero/storage/W5WZD9IE/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/H6E44KI8/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/6ZGSF3I5/1902.html; /Users/jacquesthibodeau/Zotero/storage/WQ6W7MGD/1902.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
ZAMXRUV6,conferencePaper,2020,"Fisac, Jaime F.; Gates, Monica A.; Hamrick, Jessica B.; Liu, Chang; Hadfield-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv; Sastry, S. Shankar; Griffiths, Thomas L.; Dragan, Anca D.",Pragmatic-Pedagogic Value Alignment,Robotics Research,978-3-030-28618-7 978-3-030-28619-4,,10.1007/978-3-030-28619-4_7,http://link.springer.com/10.1007/978-3-030-28619-4_7,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workﬂows, successfully inferring and adapting to their users’ objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people’s natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human’s actions pragmatically. To our knowledge, this work constitutes the ﬁrst formal analysis of value alignment grounded in empirically validated cognitive models.",2020,2022-01-30 4:51:07,2022-01-30 4:51:07,2019-12-18 1:15:26,49-57,,,10,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 47,,/Users/jacquesthibodeau/Zotero/storage/ZADVEM2M/Fisac et al. - 2018 - Pragmatic-Pedagogic Value Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/IJB43GBG/1707.html; /Users/jacquesthibodeau/Zotero/storage/NAHBPPDH/1707.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9,"Amato, Nancy M.; Hager, Greg; Thomas, Shawna; Torres-Torriti, Miguel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCHKUWWW,conferencePaper,2019,"Oesterheld, Caspar; Conitzer, Vincent",Extracting Money from Causal Decision Theorists,Proceedings of the Workshop on Artificial Intelligence Safety 2020,,,,http://ceur-ws.org/Vol-2640/paper_21.pdf,"Newcomb’s problem has spawned a debate about which variant of expected utility maximization (if any) should guide rational choice. In this paper, we provide a new argument against what is probably the most popular variant: causal decision theory (CDT). In particular, we provide two scenarios in which CDT voluntarily loses money. In the ﬁrst, an agent faces a single choice and following CDT’s recommendation yields a loss of money in expectation. The second scenario extends the ﬁrst to a diachronic Dutch book against CDT.",2019-08-30,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-12-18,19,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/ANCD7C23/Oesterheld and Conitzer - Extracting Money from Causal Decision Theorists.pdf,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HPEHZIR,conferencePaper,2016,"Sadigh, Dorsa; Sastry, Shankar; A. Seshia, Sanjit; D. Dragan, Anca",Planning for Autonomous Cars that Leverage Effects on Human Actions,Robotics: Science and Systems XII,978-0-9923747-2-3,,10.15607/RSS.2016.XII.029,http://www.roboticsproceedings.org/rss12/p29.pdf,"Traditionally, autonomous cars make predictions about other drivers’ future trajectories, and plan to stay out of their way. This tends to result in defensive and opaque behaviors. Our key insight is that an autonomous car’s actions will actually affect what other cars will do in response, whether the car is aware of it or not. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We model the interaction between an autonomous car and a human driver as a dynamical system, in which the robot’s actions have immediate consequences on the state of the car, but also on human actions. We model these consequences by approximating the human as an optimal planner, with a reward function that we acquire through Inverse Reinforcement Learning. When the robot plans with this reward function in this dynamical system, it comes up with actions that purposefully change human state: it merges in front of a human to get them to slow down or to reach its own goal faster; it blocks two lanes to get them to switch to a third lane; or it backs up slightly at an intersection to get them to proceed ﬁrst. Such behaviors arise from the optimization, without relying on hand-coded signaling strategies and without ever explicitly modeling communication. Our user study results suggest that the robot is indeed capable of eliciting desired changes in human state by planning using this dynamical system.",2016,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-12-13 23:37:50,,,,,,,,,,,,Robotics: Science and Systems Foundation,,en,,,,,DOI.org (Crossref),,ZSCC: 0000363,,/Users/jacquesthibodeau/Zotero/storage/KQ9WS3I7/SadighPlanning2016.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2016,,,,,,,,,,,,,,,,
R8SVD9CH,conferencePaper,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,arXiv:2105.14111 [cs],,,,http://arxiv.org/abs/2105.14111,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-06-08,2022-01-30 4:49:30,2022-01-30 4:49:30,2021-10-30 17:55:11,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2105.14111,,/Users/jacquesthibodeau/Zotero/storage/HBRPM964/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/HQIR73N6/2105.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2021,,,,,,,,,,,,,,,,
5HWDDTTB,conferencePaper,2019,"Mancuso, Jason; Kisielewski, Tomasz; Lindner, David; Singh, Alok",Detecting Spiky Corruption in Markov Decision Processes,,,,,http://arxiv.org/abs/1907.00452,"Current reinforcement learning methods fail if the reward function is imperfect, i.e. if the agent observes reward different from what it actually receives. We study this problem within the formalism of Corrupt Reward Markov Decision Processes (CRMDPs). We show that if the reward corruption in a CRMDP is sufficiently ""spiky"", the environment is solvable. We fully characterize the regret bound of a Spiky CRMDP, and introduce an algorithm that is able to detect its corrupt states. We show that this algorithm can be used to learn the optimal policy with any common reinforcement learning algorithm. Finally, we investigate our algorithm in a pair of simple gridworld environments, finding that our algorithm can detect the corrupt states and learn the optimal policy despite the corruption.",2019-06-30,2022-01-30 4:49:30,2022-01-30 4:49:30,2019-12-16 3:27:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1907.00452,,/Users/jacquesthibodeau/Zotero/storage/7UWHJF9I/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJH6BKAF/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZW3BBNWK/1907.html,,TechSafety; AI-Safety-Camp,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AI Safety Workshop in IJCAI 2019,,,,,,,,,,,,,,,,
7QA8U489,conferencePaper,2019,"Majha, Arushi; Sarkar, Sayan; Zagami, Davide",Categorizing Wireheading in Partially Embedded Agents,,,,,http://arxiv.org/abs/1906.09136,"$\textit{Embedded agents}$ are not explicitly separated from their environment, lacking clear I/O channels. Such agents can reason about and modify their internal parts, which they are incentivized to shortcut or $\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we provide a taxonomy of ways by which wireheading can occur, followed by a definition of wirehead-vulnerable agents. Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially embedded agents and identify wireheading opportunities that such agents can exploit, experimentally demonstrating the results with the GRL simulation platform AIXIjs. We contextualize wireheading in the broader class of all misalignment problems - where the goals of the agent conflict with the goals of the human designer - and conjecture that the only other possible type of misalignment is specification gaming. Motivated by this taxonomy, we define wirehead-vulnerable agents as embedded agents that choose to behave differently from fully dualistic agents lacking access to their internal parts.",2019-06-21,2022-01-30 4:49:30,2022-01-30 4:49:30,2019-12-16 3:26:47,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/2PGIU7BA/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/UGH6MA5M/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/429IP46K/1906.html; /Users/jacquesthibodeau/Zotero/storage/MMNA4BT2/1906.html,,TechSafety; AI-Safety-Camp,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AI Safety Workshop in IJCAI 2019,,,,,,,,,,,,,,,,
6C9TQWFX,conferencePaper,2019,"Tangkaratt, Voot; Han, Bo; Khan, Mohammad Emtiyaz; Sugiyama, Masashi",VILD: Variational Imitation Learning with Diverse-quality Demonstrations,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/1909.06769,"The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",2019-09-15,2022-01-30 4:48:55,2022-01-30 4:48:55,2021-11-18 23:17:53,,,,,,,VILD,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1909.06769,,/Users/jacquesthibodeau/Zotero/storage/KR4JBV4S/Tangkaratt et al. - 2019 - VILD Variational Imitation Learning with Diverse-.pdf; /Users/jacquesthibodeau/Zotero/storage/KZEUIWTZ/1909.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
GPKVQEG4,conferencePaper,2021,"Hunt, Nathan; Fulton, Nathan; Magliacane, Sara; Hoang, Nghia; Das, Subhro; Solar-Lezama, Armando",Verifiably Safe Exploration for End-to-End Reinforcement Learning,HSCC '21: Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control,,,10.1145/3447928.3456653,http://arxiv.org/abs/2007.01223,"Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.",2021-05,2022-01-30 4:48:55,2022-01-30 4:48:55,2021-11-09 12:10:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2007.01223,,/Users/jacquesthibodeau/Zotero/storage/NHP43TJ9/Hunt et al. - 2020 - Verifiably Safe Exploration for End-to-End Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/VK2PPGIX/2007.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.8; Computer Science - Logic in Computer Science; F.3.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HSCC 2021,,,,,,,,,,,,,,,,
M4GP2KT9,conferencePaper,2021,"Jia, Feiran; Mate, Aditya; Li, Zun; Jabbari, Shahin; Chakraborty, Mithun; Tambe, Milind; Wellman, Michael; Vorobeychik, Yevgeniy",A Game-Theoretic Approach for Hierarchical Policy-Making,arXiv:2102.10646 [cs],,,,http://arxiv.org/abs/2102.10646,"We present the design and analysis of a multi-level game-theoretic model of hierarchical policy-making, inspired by policy responses to the COVID-19 pandemic. Our model captures the potentially mismatched priorities among a hierarchy of policy-makers (e.g., federal, state, and local governments) with respect to two main cost components that have opposite dependence on the policy strength, such as post-intervention infection rates and the cost of policy implementation. Our model further includes a crucial third factor in decisions: a cost of non-compliance with the policy-maker immediately above in the hierarchy, such as non-compliance of state with federal policies. Our first contribution is a closed-form approximation of a recently published agent-based model to compute the number of infections for any implemented policy. Second, we present a novel equilibrium selection criterion that addresses common issues with equilibrium multiplicity in our setting. Third, we propose a hierarchical algorithm based on best response dynamics for computing an approximate equilibrium of the hierarchical policy-making game consistent with our solution concept. Finally, we present an empirical investigation of equilibrium policy strategies in this game in terms of the extent of free riding as well as fairness in the distribution of costs depending on game parameters such as the degree of centralization and disagreements about policy priorities among the agents.",2021-02-21,2022-01-30 4:50:42,2022-01-30 4:50:42,2021-10-30 21:50:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2102.10646,,/Users/jacquesthibodeau/Zotero/storage/C28KUI7S/Jia et al. - 2021 - A Game-Theoretic Approach for Hierarchical Policy-.pdf; /Users/jacquesthibodeau/Zotero/storage/3CD6QZFT/2102.html,,MetaSafety; AmbiguousSafety,Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2nd International (Virtual) Workshop on Autonomous Agents for Social Good (AASG 2021),,,,,,,,,,,,,,,,
J95SGZ5U,conferencePaper,2018,"Smitha Milli; Lieder, Falk; Griffiths, Thomas L",A Rational Reinterpretation of Dual-Process Theories,,,,10.13140/rg.2.2.14956.46722/1,http://rgdoi.net/10.13140/RG.2.2.14956.46722/1,"Highly inﬂuential “dual-process"" accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might be neither arbitrary nor irrational, but might instead reﬂect a rational tradeoff between the cognitive ﬂexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We ﬁnd that the optimal number of systems depends on the variability of the environment and the difﬁculty of deciding when which system should be used. Furthermore, when having two systems is optimal, then the ﬁrst system is fast but error-prone and the second system is slow but accurate. Our ﬁndings thereby provide a rational reinterpretation of dual-process theories.",2018,2022-01-30 4:50:42,2022-01-30 4:50:42,2019-07-08 15:59:57,,,,,,,,,,,,,,en,,,,,DOI.org (Datacite),,ZSCC: 0000003[s2],,,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thirty-First AAAI Conference on Artiﬁcial Intelligence,,,,,,,,,,,,,,,,
JSDBTHXD,conferencePaper,2019,"Bansal, Somil; Bajcsy, Andrea; Ratner, Ellis; Dragan, Anca D.; Tomlin, Claire J.",A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1910.13369,"Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling the human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. This leads to a probabilistic prediction problem, which even though attractive, can be computationally demanding. In this work, we formalize the prediction problem as a stochastic reachability problem in the joint state space of the human and the belief over the model parameters. We further introduce a Hamilton-Jacobi reachability framework which casts a deterministic approximation of this stochastic reachability problem by restricting the allowable actions to a set rather than a distribution, while still maintaining the belief as an explicit state. This leads to two advantages: our approach gives rise to a novel predictor wherein the predictions can be performed at a significantly lower computational expense, and to a general framework which also enables us to perform predictor analysis. We compare our approach to a fully stochastic predictor using Bayesian inference and the worst-case forward reachable set in simulation and in hardware, and demonstrate how it can enable robust planning while not being overly conservative, even when the human model is inaccurate.",2019-10-29,2022-01-30 4:50:42,2022-01-30 4:50:42,2019-12-18 2:35:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1910.13369,,/Users/jacquesthibodeau/Zotero/storage/XESDVH7I/Bansal et al. - 2019 - A Hamilton-Jacobi Reachability-Based Framework for.pdf; /Users/jacquesthibodeau/Zotero/storage/NAM8GA7X/1910.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
U2CSBG92,conferencePaper,2018,"Ó hÉigeartaigh, Seán",The State of Research in Existential Risk,Proceedings from the first Garrick Colloquium on Catastrophic and Existential Risk,,,,https://www.risksciences.ucla.edu/news-events/2018/1/2/proceedings-of-the-first-international-colloquium-on-catastrophic-and-existential-risk,,2018,2022-01-30 4:50:26,2022-01-30 4:50:26,2020-12-12,,,,,,,,,,,,"B John Garrick Institute for the Risk Sciences, University of California Los Angeles",,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/PTF948QW/Ó hÉigeartaigh - 2018 - The State of Research in Existential Risk.pdf,,MetaSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKHJ2UAC,conferencePaper,2019,"Whittlestone, Jess; Nyrup, Rune; Alexandrova, Anna; Cave, Stephen",The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",2019,2022-01-30 4:50:26,2022-01-30 4:50:26,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000087,,/Users/jacquesthibodeau/Zotero/storage/GUM2EHUB/Whittlestone et al. - The Role and Limits of Principles in AI Ethics To.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
ADQ42JZJ,conferencePaper,2018,"Martínez-Plumed, Fernando; Loe, Bao Sheng; Flach, Peter; Ó hÉigeartaigh, Seán; Vold, Karina; Hernández-Orallo, José",The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,10.24963/ijcai.2018/718,https://www.ijcai.org/proceedings/2018/718,"We present nine facets for the analysis of the past and future evolution of AI. Each facet has also a set of edges that can summarise different trends and contours in AI. With them, we ﬁrst conduct a quantitative analysis using the information from two decades of AAAI/IJCAI conferences and around 50 years of documents from AI topics, an ofﬁcial database from the AAAI, illustrated by several plots. We then perform a qualitative analysis using the facets and edges, locating AI systems in the intelligence landscape and the discipline as a whole. This analytical framework provides a more structured and systematic way of looking at the shape and boundaries of AI.",2018-07,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-11-14 1:15:31,5180-5187,,,,,,The Facets of Artificial Intelligence,,,,,International Joint Conferences on Artificial Intelligence Organization,"Stockholm, Sweden",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 17,,/Users/jacquesthibodeau/Zotero/storage/W37ZFTH7/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/IPMDVDXU/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18},,,,,,,,,,,,,,,,
TVJ2EKVB,conferencePaper,2019,"Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar",Surveying Safety-relevant AI Characteristics,1st AAAI's Workshop on Artificial Intelligence Safety (SafeAI),,,,,"The current analysis in the AI safety literature usually combines a risk or safety issue (e.g., interruptibility) with a particular paradigm for an AI agent (e.g., reinforcement learning). However, there is currently no survey of safety-relevant characteristics of AI systems that may reveal neglected areas of research or suggest to developers what design choices they could make to avoid or minimise certain safety concerns. In this paper, we take a first step towards delivering such a survey, from two angles. The first features AI system characteristics that are already known to be relevant to safety concerns, including internal system characteristics, characteristics relating to the effect of the external environment on the system, and characteristics relating to the effect of the system on the target environment. The second presents a brief survey of a broad range of AI system characteristics that could prove relevant to safety research, including types of interaction, computation, integration, anticipation, supervision, modification, motivation and achievement. This survey enables further work in exploring system characteristics and design choices that affect safety concerns.",2019,2022-01-30 4:50:25,2022-01-30 4:50:25,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s7]  ACC: 10  J: 4,,/Users/jacquesthibodeau/Zotero/storage/X9P84E7X/Hernandez-Orallo et al. - Surveying Safety-relevant AI Characteristics.pdf,,TechSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K4827FC7,conferencePaper,2020,"Cihon, Peter; Maas, Matthijs M.; Kemp, Luke",Should Artificial Intelligence Governance be Centralised?: Design Lessons from History,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375857,https://dl.acm.org/doi/10.1145/3375627.3375857,,2020-02-07,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-12-12 16:10:16,228-234,,,,,,Should Artificial Intelligence Governance be Centralised?,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000016,,,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
HFZ22SRN,conferencePaper,2021,"Burden, John; Hernandez-Orallo, Jose",Negative Side Effects and AI Agent Indicators: Experiments in SafeLife,,,,,,"The widespread adoption and ubiquity of AI systems will require them to be safe. The safety issues that can arise from AI are broad and varied. In this paper we consider the safety issue of negative side effects and the consequences they can have on an environment. In the safety benchmarking domain SafeLife, we discuss the way that side effects are measured, as well as presenting results showing the relation between the magnitude of side effects and other metrics for three agent types: Deep Q-Networks, Proximal Policy Optimisation, and a Uniform Random Agent. We observe that different metrics and agent types lead to both monotonic and non-monotonic interactions, with the ﬁnding that the size and complexity of the environment versus the capability of the agent plays a major role in negative side effects, sometimes in intricate ways.",2021-01-01,2022-01-30 4:50:25,2022-01-30 4:50:25,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/92N83U65/Burden and Hernandez-Orallo - Negative Side Effects and AI Agent Indicators Exp.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeAI@ AAAI,,,,,,,,,,,,,,,,
NSJVHAQP,conferencePaper,2020,"Bhatt, Umang; Andrus, McKane; Weller, Adrian; Xiang, Alice",Machine Learning Explainability for External Stakeholders,,,,,https://arxiv.org/abs/2007.05408v1,"As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.",2020-07-10,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-11-23 1:16:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/2C2574GF/Bhatt et al. - 2020 - Machine Learning Explainability for External Stake.pdf; /Users/jacquesthibodeau/Zotero/storage/8EMZGHET/2007.html,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML Workshop XXAI: Extending Explainable AI Beyond Deep Models and Classifiers,,,,,,,,,,,,,,,,
CCRB9244,conferencePaper,2020,"Burden, John; Hernandez-Orallo, Jose","Exploring AI Safety in Degrees: Generality, Capability and Control",Proceedings of the Workshop on Artificial Intelligence Safety (SafeAI 2020),,,,,"The landscape of AI safety is frequently explored differently by contrasting specialised AI versus general AI (or AGI), by analysing the short-term hazards of systems with limited capabilities against those more long-term risks posed by ‘superintelligence’, and by conceptualising sophisticated ways of bounding control an AI system has over its environment and itself (impact, harm to humans, self-harm, containment, etc.). In this position paper we reconsider these three aspects of AI safety as quantitative factors –generality, capability and control–, suggesting that by deﬁning metrics for these dimensions, AI risks can be characterised and analysed more precisely. As an example, we illustrate how to deﬁne these metrics and their values for some simple agents in a toy scenario within a reinforcement learning setting.",2020-08-10,2022-01-30 4:50:25,2022-01-30 4:50:25,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,"/Users/jacquesthibodeau/Zotero/storage/Q7VVUNP9/Burden and Hernandez-Orallo - Exploring AI Safety in Degrees Generality, Capabi.pdf",,TechSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MCJ6I4DC,conferencePaper,2020,"Cremer, Carla Zoe; Whittlestone, Jess",Canaries in Technology Mines: Warning Signs of Transformative Progress in AI,,,,,,,2020,2022-01-30 4:50:25,2022-01-30 4:50:25,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/TQJZSWED/Cremer and Whittlestone - Canaries in Technology Mines Warning Signs of Tra.pdf,,MetaSafety; CFI; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1st International Workshop on Evaluating Progress in Artificial Intelligence - EPAI 2020,,,,,,,,,,,,,,,,
AJAXDNIF,conferencePaper,2020,"Prunkl, Carina; Whittlestone, Jess",Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society,arXiv:2001.04335 [cs],,,,http://arxiv.org/abs/2001.04335,"One way of carving up the broad ‘AI ethics and society’ research space that has emerged in recent years is to distinguish between ‘near-term’ and ‘long-term’ research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.",2020-01-21,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-08-21 20:00:24,,,,,,,Beyond Near- and Long-Term,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2001.04335,,/Users/jacquesthibodeau/Zotero/storage/VCRACQA2/Prunkl and Whittlestone - 2020 - Beyond Near- and Long-Term Towards a Clearer Acco.pdf,,MetaSafety; CFI; CSER; FHI; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2020 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
IDS6VQUQ,conferencePaper,2018,"Cave, Stephen; ÓhÉigeartaigh, Seán S.",An AI Race for Strategic Advantage: Rhetoric and Risks,"Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6012-8,,10.1145/3278721.3278780,https://dl.acm.org/doi/10.1145/3278721.3278780,,2018-12-27,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-12-12 17:39:09,36-40,,,,,,An AI Race for Strategic Advantage,,,,,ACM,New Orleans LA USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000066,,/Users/jacquesthibodeau/Zotero/storage/5F7NG3ZZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf; /Users/jacquesthibodeau/Zotero/storage/IFPKRRTZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf,,MetaSafety; CFI; CSER,artificial intelligence; ai narratives; ai risks; ai safety; arms race; global governance; international cooperation; strategic competition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '18: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
46FKJWZG,conferencePaper,2021,"Maas, Matthijs M.","AI, Governance Displacement, and the (De)Fragmentation of International Law",SSRN Electronic Journal,,,10.2139/ssrn.3806624,https://www.ssrn.com/abstract=3806624,"The emergence, proliferation, and use of new general-purpose technologies can often produce significant political, redistributive, normative and legal effects on the world. Artificial intelligence (AI) has been identified as one such transformative technology. Many of its impacts may require global governance responses. However, what are the direct and indirect effects of AI technologies on the viability, form, or functioning of the international legal order itself? What, if any, are the prospects, peril or promise of AI-driven legal automation at the international level? This paper draws on an ‘AI Governance Disruption’ framework to understanding AI’s impacts on the global governance architecture. Focusing particularly on the potential for legal automation at the international law level, it explores three potential pathways of such ‘legal displacement’: (1) the automation of rule creation and arbitration; (2) the automation of monitoring & enforcement; or (3) the ‘replacement’ of international law with new architectural modes of (international) behaviour control. It then focuses on the effects of these trends on the architecture of international law. It distinguishes 10 different roles that AI applications could play, with distinct effects on the international legal order. That is, AI systems can serve as (1) legal ‘canary in the coal mine’, highlighting the need for greater cross-regime harmonization. However, it can also serve as (2) tough knot or (3) generator of regime fault lines. Under even modest scenarios of legal automation, AI systems may serve variably as a (4) shield, (5) patch, (6) cure, or (7) accelerator of international legal fragmentation. Finally, AI tools may serve as (8) differential enabler; (9) driver of value shifts, or (10) asymmetric weapon, potentially contributing to trends of contestation or erosion in the international legal order. The paper concludes with a brief review of the ways in which international lawyers or regime scholars might approach the risks and opportunities of increasing automation in international law, in order to leverage these trends and tools towards improved efficacy, resilience, and legitimacy of global governance.",2021-03,2022-01-30 4:50:24,2022-01-30 4:50:24,2021-10-31 16:58:43,,,,,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,"/Users/jacquesthibodeau/Zotero/storage/EZ5UBQGN/Maas - 2021 - AI, Governance Displacement, and the (De)Fragmenta.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Studies Association 2021,,,,,,,,,,,,,,,,
VJHUMTDA,conferencePaper,2020,"Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar; Whittlestone, Jess; Ó hÉigeartaigh, Seán",AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues,European Conference on Artificial Intelligence,,,,,"AI safety often analyses a risk or safety issue, such as interruptibility, under a particular AI paradigm, such as reinforcement learning. But what is an AI paradigm and how does it affect the understanding and implications of the safety issue? Is AI safety research covering the most representative paradigms and the right combinations of paradigms with safety issues? Will current research directions in AI safety be able to anticipate more capable and powerful systems yet to come? In this paper we analyse these questions, introducing a distinction between two types of paradigms in AI: artefacts and techniques. We then use experimental data of research and media documents from AI Topics, an ofﬁcial publication of the AAAI, to examine how safety research is distributed across artefacts and techniques. We observe that AI safety research is not sufﬁciently anticipatory, and is heavily weighted towards certain research paradigms. We identify a need for AI safety to be more explicit about the artefacts and techniques for which a particular issue may be applicable, in order to identify gaps and cover a broader range of issues.",2020,2022-01-30 4:50:24,2022-01-30 4:50:24,,8,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000002[s1],,/Users/jacquesthibodeau/Zotero/storage/8VXPTJN3/Hernandez-Orallo et al. - 2020 - AI Paradigms and AI Safety Mapping Artefacts and .pdf,,TechSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F3RKFH4C,conferencePaper,2020,"Avin, Shahar; Gruetzemacher, Ross; Fox, James",Exploring AI Futures Through Role Play,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375817,https://dl.acm.org/doi/10.1145/3375627.3375817,,2020-02-07,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:16:36,8-14,,,,,,,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/VVU88IR4/Avin et al. - 2020 - Exploring AI Futures Through Role Play.pdf,,MetaSafety; CSER; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
5IRP3444,conferencePaper,2019,"Krasheninnikov, Dmitrii; Shah, Rohin; van Hoof, Herke",Combining reward information from multiple sources,,,,,,"Given two sources of evidence about a latent variable, one can combine the information from both by multiplying the likelihoods of each piece of evidence. However, when one or both of the observation models are misspeciﬁed, the distributions will conﬂict. We study this problem in the setting with two conﬂicting reward functions learned from different sources. In such a setting, we would like to retreat to a broader distribution over reward functions, in order to mitigate the effects of misspeciﬁcation. We assume that an agent will maximize expected reward given this distribution over reward functions, and identify four desiderata for this setting. We propose a novel algorithm, Multitask Inverse Reward Design (MIRD), and compare it to a range of simple baselines. While all methods must trade off between conservatism and informativeness, through a combination of theory and empirical results on a toy environment, we ﬁnd that MIRD and its variant MIRD-IF strike a good balance between the two.",2019,2022-01-30 4:50:07,2022-01-30 4:50:07,,14,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/J9RMVNK6/Krasheninnikov et al. - Combining reward information from multiple sources.pdf,,CHAI; TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019 workshop on Learning with Rich Experience: Integration of Learning Paradigms,,,,,,,,,,,,,,,,
86KX5STN,conferencePaper,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency,arXiv:1902.09725 [cs],,,,http://arxiv.org/abs/1902.09725,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",2020,2022-01-30 4:50:07,2022-01-30 4:50:07,2019-12-16 22:27:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/X9X2DMMP/Turner et al. - 2019 - Conservative Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/SIFRRDFT/1902.html,,CHAI; TechSafety; BERI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AI, Ethics, and Society 2020",,,,,,,,,,,,,,,,
J25XFHEV,conferencePaper,2019,"Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2019-05-25,2022-01-30 4:50:06,2022-01-30 4:50:06,2019-07-11 18:47:45,,,,,,,Adversarial Policies,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000139  arXiv: 1905.10615,,,,CHAI; TechSafety; BERI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
32BRB6AJ,conferencePaper,2020,"Bell, James; Linsefors, Linda; Oesterheld, Caspar; Skalse, Joar",Reinforcement Learning in Newcomblike Environments,Advances in Neural Information Processing Systems 34 pre-proceedings (NeurIPS 2021),,,,https://proceedings.neurips.cc/paper/2021/file/b9ed18a301c9f3d183938c451fa183df-Paper.pdf,"Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not ratifiable, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents – for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratifiable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy.",2020-12,2022-01-30 4:49:31,2022-01-30 4:49:31,2021-12-11 13:54:06,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CXGA8ZF6/b9ed18a301c9f3d183938c451fa183df-Paper.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
AZXZPENH,conferencePaper,2019,"Carey, Ryan",How Useful Is Quantilization For Mitigating Specification-Gaming?,,,,,,"For some tasks, there exists a goal that perfectly describes what the designer wants the AI system to achieve. For many tasks, however, the best available proxy objective is only a rough approximation of the designer’s intentions. When given such a goal, a system that optimizes the proxy objective tends to select degenerate solutions where the proxy reward is very different from the designer’s true reward function. One way to counteract the tendency toward speciﬁcation-gaming is quantilization, a method that interpolates between imitating demonstrations, and optimizing the proxy objective. If the demonstrations are of adequate quality, and the proxy reward overestimates performance, then quantilization has better guaranteed performance than other strategies. However, if the proxy reward underestimates performance, then either imitation or optimization will offer the best guarantee. This work introduces three new gym environments: Mountain Car-RR, Hopper-RR, and Video Pinball-RR, and shows that quantilization outperforms baselines on these tasks.",2019,2022-01-30 4:53:18,2022-01-30 4:53:18,,11,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/BXWIFTJ3/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf; /Users/jacquesthibodeau/Zotero/storage/6GSUMWMX/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
TCGHD6UP,conferencePaper,2019,"Kenton, Zachary; Filos, Angelos; Evans, Owain; Gal, Yarin",Generalizing from a few environments in safety-critical reinforcement learning,,,,,http://arxiv.org/abs/1907.01475,"Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.",2019-07-02,2022-01-30 4:53:10,2022-01-30 4:53:10,2019-12-16 2:16:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1907.01475,,/Users/jacquesthibodeau/Zotero/storage/ANE5XDV5/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/JHSJ7739/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/6FIEE6TX/1907.html; /Users/jacquesthibodeau/Zotero/storage/XH3QNZBE/1907.html,,TechSafety; FHI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeML ICLR 2019 Workshop,,,,,,,,,,,,,,,,
I8H3DUG4,conferencePaper,2015,"Soares, Nate; Fallenstein, Benja; Armstrong, Stuart; Yudkowsky, Eliezer",Corrigibility,Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,,,,https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10124/10136,,2015,2022-01-30 4:53:09,2022-01-30 4:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000092,,/Users/jacquesthibodeau/Zotero/storage/5HJ4SE3I/Corrigibility.pdf; /Users/jacquesthibodeau/Zotero/storage/RKMX5VC3/10124.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9DI34W7N,conferencePaper,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,arXiv:2102.05008 [cs],,,,http://arxiv.org/abs/2102.05008,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-01-30 4:53:09,2022-01-30 4:53:09,2021-10-31 19:02:39,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2102.05008,,/Users/jacquesthibodeau/Zotero/storage/MNA45BR6/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/RFFR9PAB/2102.html; /Users/jacquesthibodeau/Zotero/storage/QNX3GVKX/2102.html,,TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-21),,,,,,,,,,,,,,,,
MCXZVITA,conferencePaper,2021,"Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane",Agent Incentives: A Causal Perspective,Proceedings of the AAAI 2021 Conference,,,,http://arxiv.org/abs/2102.01685,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.",2021-03-15,2022-01-30 4:53:08,2022-01-30 4:53:08,2021-10-31 19:14:12,,,,,,,Agent Incentives,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2102.01685,,/Users/jacquesthibodeau/Zotero/storage/7B2HXKEK/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/2SVWH4CX/2102.html; /Users/jacquesthibodeau/Zotero/storage/5A6T2VB9/2102.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
PJXNWQDW,conferencePaper,2020,"Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus",Asymptotically Unambitious Artificial General Intelligence,arXiv:1905.12186 [cs],,,,http://arxiv.org/abs/1905.12186,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",2020-07-21,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-12-12 15:23:37,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1905.12186,,/Users/jacquesthibodeau/Zotero/storage/8DBEA75S/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/JC4S2NBR/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/KH9FQ5MZ/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/K6N5WBE9/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/U8FQ7GXJ/1905.html; /Users/jacquesthibodeau/Zotero/storage/JUUQB6N7/1905.html; /Users/jacquesthibodeau/Zotero/storage/NGFACCVM/1905.html; /Users/jacquesthibodeau/Zotero/storage/BNMPXS8T/1905.html,,TechSafety; FHI; DeepMind,"Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,
KVD64BBZ,conferencePaper,2017,"Abel, David; Salvatier, John; Stuhlmüller, Andreas; Evans, Owain",Agent-agnostic human-in-the-loop reinforcement learning,30th Conference on Neural Information Processing Systems (NIPS 2016),,,,,,2017,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000049,,/Users/jacquesthibodeau/Zotero/storage/K5CD6IT3/Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf; /Users/jacquesthibodeau/Zotero/storage/R97ACVCM/1701.html; /Users/jacquesthibodeau/Zotero/storage/3AVWXX73/Abel et al. - 2017 - Agent-agnostic human-in-the-loop reinforcement lea.pdf; /Users/jacquesthibodeau/Zotero/storage/82F2UJJ4/1701.html,,TechSafety; FHI; Ought,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30th Conference on Neural Information Processing Systems (NIPS 2016),,,,,,,,,,,,,,,,
SAFIJZEA,conferencePaper,2016,"Krueger, David; Leike, Jan; Evans, Owain; Salvatier, John",Active reinforcement learning: Observing rewards at a cost,"Future of Interactive Learning Machines, NIPS Workshop",,,,,,2016,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,Active reinforcement learning,,,,,,,,,,,,Google Scholar,,ZSCC: 0000010[s0],,/Users/jacquesthibodeau/Zotero/storage/KBMS9PIC/Krueger et al. - 2016 - Active reinforcement learning Observing rewards a.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXZRZTKQ,conferencePaper,2019,"Cohen, Michael K.; Catt, Elliot; Hutter, Marcus",A Strongly Asymptotically Optimal Agent in General Environments,Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1903.01021,"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.",2019-05-27,2022-01-30 4:53:07,2022-01-30 4:53:07,2020-08-18 21:41:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 1903.01021,,/Users/jacquesthibodeau/Zotero/storage/GW7QTF57/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/4RAI8NSE/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/7MREFNQ5/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/FKBA28KM/1903.html; /Users/jacquesthibodeau/Zotero/storage/BV4V8ZA4/1903.html; /Users/jacquesthibodeau/Zotero/storage/R852BRN9/1903.html,,TechSafety; FHI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019,,,,,,,,,,,,,,,,
BD9DG38U,conferencePaper,2019,"Qin, Chongli; O’Donoghue, Brendan; Stanforth, Robert; Gowal, Sven; Uesato, Jonathan; Swirszcz, Grzegorz; Kohli, Pushmeet",Verification Of Non-Linear Specifications For Neural Networks,,,,,,"Prior work on neural network veriﬁcation has focused on speciﬁcations that are linear functions of the output of the network, e.g., invariance of the classiﬁer output under adversarial perturbations of the input. In this paper, we extend veriﬁcation algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable speciﬁcations, which constitute nonlinear speciﬁcations that can be veriﬁed using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classiﬁer’s output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these speciﬁcations. Moreover, our evaluation exposes the failure modes in models which cannot be veriﬁed to satisfy these speciﬁcations. Thus, emphasizing the importance of training models not just to ﬁt training data but also to be consistent with speciﬁcations.",2019,2022-01-30 4:52:49,2022-01-30 4:52:49,,21,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000011[s2],,/Users/jacquesthibodeau/Zotero/storage/GERBNPZ4/Qin et al. - 2019 - VERIFICATION OF NON-LINEAR SPECIFICATIONS FOR NEUR.pdf,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
AVNF5F4K,conferencePaper,2018,"Dvijotham, Krishnamurthy; Garnelo, Marta; Fawzi, Alhussein; Kohli, Pushmeet",Verification of deep probabilistic models,"arXiv:1812.02795 [cs, stat]",,,,http://arxiv.org/abs/1812.02795,"Probabilistic models are a critical part of the modern deep learning toolbox - ranging from generative models (VAEs, GANs), sequence to sequence models used in machine translation and speech processing to models over functional spaces (conditional neural processes, neural processes). Given the size and complexity of these models, safely deploying them in applications requires the development of tools to analyze their behavior rigorously and provide some guarantees that these models are consistent with a list of desirable properties or specifications. For example, a machine translation model should produce semantically equivalent outputs for innocuous changes in the input to the model. A functional regression model that is learning a distribution over monotonic functions should predict a larger value at a larger input. Verification of these properties requires a new framework that goes beyond notions of verification studied in deterministic feedforward networks, since requiring worst-case guarantees in probabilistic models is likely to produce conservative or vacuous results. We propose a novel formulation of verification for deep probabilistic models that take in conditioning inputs and sample latent variables in the course of producing an output: We require that the output of the model satisfies a linear constraint with high probability over the sampling of latent variables and for every choice of conditioning input to the model. We show that rigorous lower bounds on the probability that the constraint is satisfied can be obtained efficiently. Experiments with neural processes show that several properties of interest while modeling functional spaces can be modeled within this framework (monotonicity, convexity) and verified efficiently using our algorithms",2018-12-06,2022-01-30 4:52:49,2022-01-30 4:52:49,2019-12-16 20:33:13,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1812.02795,,/Users/jacquesthibodeau/Zotero/storage/8MI349F4/Dvijotham et al. - 2018 - Verification of deep probabilistic models.pdf; /Users/jacquesthibodeau/Zotero/storage/EXVPIWAF/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018 Workshop on Security in Machine Learning,,,,,,,,,,,,,,,,
AJQW7GX7,conferencePaper,2020,"Zoran, Daniel; Chrzanowski, Mike; Huang, Po-Sen; Gowal, Sven; Mott, Alex; Kohl, Pushmeet",Towards Robust Image Classification Using Sequential Attention Models,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,https://openaccess.thecvf.com/content_CVPR_2020/html/Zoran_Towards_Robust_Image_Classification_Using_Sequential_Attention_Models_CVPR_2020_paper.html,"In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a ""computational race"" between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.",2020,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000[s2],,/Users/jacquesthibodeau/Zotero/storage/HKXE6UDC/Zoran et al. - 2019 - Towards Robust Image Classification Using Sequenti.pdf; /Users/jacquesthibodeau/Zotero/storage/84REF8IF/1912.html,,TechSafety; DeepMind,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,,
T6XFBJ2P,conferencePaper,2016,"Orseau, Laurent; Armstrong, Stuart",Safely Interruptible Agents,,,,,,"Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button—which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal deﬁnition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.",2016,2022-01-30 4:52:48,2022-01-30 4:52:48,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000098,,/Users/jacquesthibodeau/Zotero/storage/WP7N4XD6/Orseau and Armstrong - Safely Interruptible Agents.pdf,,TechSafety; FHI; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
IAH92MIP,conferencePaper,2018,"Moosavi-Dezfooli, Seyed-Mohsen; Fawzi, Alhussein; Uesato, Jonathan; Frossard, Pascal","Robustness via curvature regularization, and vice versa",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,http://arxiv.org/abs/1811.09716,"State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.",2018-11-23,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:33:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 161  J: 46 arXiv: 1811.09716,,"/Users/jacquesthibodeau/Zotero/storage/JIRVCGMN/Moosavi-Dezfooli et al. - 2018 - Robustness via curvature regularization, and vice .pdf; /Users/jacquesthibodeau/Zotero/storage/SV67CTM2/1811.html",,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,,
BRC4S5Q4,conferencePaper,2019,"Weng, Tsui-Wei; Dvijotham*, Krishnamurthy (Dj); Uesato*, Jonathan; Xiao*, Kai; Gowal*, Sven; Stanforth*, Robert; Kohli, Pushmeet",Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control,,,,,https://openreview.net/forum?id=SylL0krYPS,We study the problem of continuous control agents in deep RL with adversarial attacks and proposed a two-step algorithm based on learned model dynamics.,2019-09-25,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-12-12 15:24:12,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/6A22G5ZI/Weng et al. - 2019 - Toward Evaluating Robustness of Deep Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/J34FFI9Q/forum.html,,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Learning Representations,,,,,,,,,,,,,,,,
HXZ4FT7A,conferencePaper,2017,"Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles",Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,"arXiv:1612.01474 [cs, stat]",,,,http://arxiv.org/abs/1612.01474,"Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.",2017-11-03,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:35:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000822[s0]  arXiv: 1612.01474,,/Users/jacquesthibodeau/Zotero/storage/EU34QGTH/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf; /Users/jacquesthibodeau/Zotero/storage/3E2CUIW2/1612.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2017,,,,,,,,,,,,,,,,
XNKWIHWT,conferencePaper,2017,"Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane",Reinforcement Learning with a Corrupted Reward Channel,"arXiv:1705.08417 [cs, stat]",,,,http://arxiv.org/abs/1705.08417,"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.",2017-08-19,2022-01-30 4:52:47,2022-01-30 4:52:47,2019-12-16 20:35:54,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000066  arXiv: 1705.08417,,/Users/jacquesthibodeau/Zotero/storage/RXAX378F/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/8DU9QAQA/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/74KMWAKW/1705.html; /Users/jacquesthibodeau/Zotero/storage/R4CHAQWM/1705.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2017 AI and Autonomy track,,,,,,,,,,,,,,,,
GSPHBNCV,conferencePaper,2019,"Krakovna, Victoria; Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane",Penalizing side effects using stepwise relative reachability,Proceedings of the Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",2019-03-08,2022-01-30 4:52:47,2022-01-30 4:52:47,2019-12-16 20:35:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 27  J: 9 arXiv: 1806.01186,,/Users/jacquesthibodeau/Zotero/storage/TPXKJ9KJ/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VKHV6EAF/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VWWRAC2Q/1806.html; /Users/jacquesthibodeau/Zotero/storage/K4V55HQ6/1806.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,
TNE49C8Q,conferencePaper,2018,"Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario",Reward learning from human preferences and demonstrations in Atari,"arXiv:1811.06521 [cs, stat]",,,,http://arxiv.org/abs/1811.06521,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",2018-11-15,2022-01-30 4:52:47,2022-01-30 4:52:47,2019-12-16 20:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000114  arXiv: 1811.06521,,/Users/jacquesthibodeau/Zotero/storage/27WAEWKV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/QEKJ65EV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/J8N3GNK5/1811.html; /Users/jacquesthibodeau/Zotero/storage/3ZUXGVRD/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,
5AW256G4,conferencePaper,2020,"Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane",Pitfalls of learning a reward function online,arXiv:2004.13654 [cs],,,,http://arxiv.org/abs/2004.13654,"In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.",2020-04-28,2022-01-30 4:52:47,2022-01-30 4:52:47,2020-08-18 21:24:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2004.13654,,/Users/jacquesthibodeau/Zotero/storage/9SFICWU9/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/QNDBJEGQ/2004.html,,TechSafety; FHI; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2020,,,,,,,,,,,,,,,,
8CVV7M9V,conferencePaper,2020,"Cohen, Michael K.; Hutter, Marcus",Pessimism About Unknown Unknowns Inspires Conservatism,Proceedings of Machine Learning Research,,,,http://arxiv.org/abs/2006.08753,"If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ""unprecedented events"" with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.",2020-06-15,2022-01-30 4:52:47,2022-01-30 4:52:47,2021-11-19 0:01:59,,,,125,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2006.08753,,/Users/jacquesthibodeau/Zotero/storage/MWFB3VPD/Cohen and Hutter - 2020 - Pessimism About Unknown Unknowns Inspires Conserva.pdf; /Users/jacquesthibodeau/Zotero/storage/2NGV2RMZ/2006.html; /Users/jacquesthibodeau/Zotero/storage/JEN2E66Z/2006.html,,TechSafety; DeepMind,"Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33rd Annual Conference on Learning Theory,,,,,,,,,,,,,,,,
R8QM5T9F,conferencePaper,2019,"Gowal, Sven; Dvijotham, Krishnamurthy; Stanforth, Robert; Bunel, Rudy; Qin, Chongli; Uesato, Jonathan; Arandjelovic, Relja; Mann, Timothy; Kohli, Pushmeet",On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models,"arXiv:1810.12715 [cs, stat]",,,,http://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difﬁcult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in veriﬁed accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be veriﬁed beyond vacuous bounds on a downscaled version of IMAGENET.",2019-08-29,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:36:43,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 221  J: 93 arXiv: 1810.12715,,/Users/jacquesthibodeau/Zotero/storage/D227S2QI/Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS SECML 2018 Workshop,,,,,,,,,,,,,,,,
UKXRJHU4,conferencePaper,2018,"Farajtabar, Mehrdad; Chow, Yinlam; Ghavamzadeh, Mohammad",More Robust Doubly Robust Off-policy Evaluation,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.03493,"We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t.~the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.",2018-05-23,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:35:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000144  arXiv: 1802.03493,,/Users/jacquesthibodeau/Zotero/storage/ISNRPPXX/Farajtabar et al. - 2018 - More Robust Doubly Robust Off-policy Evaluation.pdf; /Users/jacquesthibodeau/Zotero/storage/SA8MNFUK/1802.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th International Conference on Machine Learning,,,,,,,,,,,,,,,,
S6AXJC5M,conferencePaper,2018,"Rabinowitz, Neil C.; Perbet, Frank; Song, H. Francis; Zhang, Chiyuan; Eslami, S. M. Ali; Botvinick, Matthew",Machine Theory of Mind,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.07740,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",2018-03-12,2022-01-30 4:52:39,2022-01-30 4:52:39,2020-11-14 0:44:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000247  arXiv: 1802.07740,,/Users/jacquesthibodeau/Zotero/storage/JVMJRV27/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf; /Users/jacquesthibodeau/Zotero/storage/DXH576KA/1802.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMG5JK2G,conferencePaper,2019,"Ren, Jie; Liu, Peter J.; Fertig, Emily; Snoek, Jasper; Poplin, Ryan; DePristo, Mark A.; Dillon, Joshua V.; Lakshminarayanan, Balaji",Likelihood Ratios for Out-of-Distribution Detection,"arXiv:1906.02845 [cs, stat]",,,,http://arxiv.org/abs/1906.02845,"Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.",2019-12-05,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:31:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000250  arXiv: 1906.02845,,/Users/jacquesthibodeau/Zotero/storage/Q6QQT5EF/Ren et al. - 2019 - Likelihood Ratios for Out-of-Distribution Detectio.pdf; /Users/jacquesthibodeau/Zotero/storage/F56NEI6E/1906.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
ZGNI9PR6,conferencePaper,2019,"Kovařík, Vojtěch; Carey, Ryan",(When) Is Truth-telling Favored in AI Debate?,Proceedings of the Workshop on Artificial Intelligence Safety,,,,http://arxiv.org/abs/1911.04266,"For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.",2019-12-15,2022-01-30 4:53:45,2022-01-30 4:53:45,2020-11-14 0:41:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1911.04266,,/Users/jacquesthibodeau/Zotero/storage/K36M3K3T/Kovařík and Carey - 2019 - (When) Is Truth-telling Favored in AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/7QS85VGU/1911.html,,TechSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWKNXDG7,conferencePaper,2016,"Asaro, Peter M",The Liability Problem for Autonomous Artificial Agents,,,,,,"This paper describes and frames a central ethical issue–the liability problem–facing the regulation of artificial computational agents, including artificial intelligence (AI) and robotic systems, as they become increasingly autonomous, and supersede current capabilities. While it frames the issue in legal terms of liability and culpability, these terms are deeply imbued and interconnected with their ethical and moral correlate– responsibility. In order for society to benefit from advances in AI technology, it will be necessary to develop regulatory policies which manage the risk and liability of deploying systems with increasingly autonomous capabilities. However, current approaches to liability have difficulties when it comes to dealing with autonomous artificial agents because their behavior may be unpredictable to those who create and deploy them, and they will not be proper legal or moral agents. This problem is the motivation for a research project that will explore the fundamental concepts of autonomy, agency and liability; clarify the different varieties of agency that artificial systems might realize, including causal, legal and moral; and the illuminate the relationships between these. The paper will frame the problem of liability in autonomous agents, sketch out its relation to fundamental concepts in human legal and moral agency– including autonomy, agency, causation, intention, responsibility and culpability–and their applicability or inapplicability to autonomous artificial agents.",2016,2022-01-30 4:53:38,2022-01-30 4:53:38,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000047,,/Users/jacquesthibodeau/Zotero/storage/HB4XG66E/Asaro - The Liability Problem for Autonomous Artificial Ag.pdf,,MetaSafety; AmbiguosSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Spring Symposia,,,,,,,,,,,,,,,,
JKWXTQPB,conferencePaper,2018,"Saunders, William; Sastry, Girish; Stuhlmueller, Andreas; Evans, Owain",Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,,,,https://arxiv.org/abs/1707.05173v1,"AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.",2018,2022-01-30 4:53:37,2022-01-30 4:53:37,2019-12-19 1:45:01,2067–2069,,,,,,Trial without Error,,,,,International Foundation for Autonomous Agents and Multiagent Systems,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s1]  ACC: 142,,/Users/jacquesthibodeau/Zotero/storage/I2WTXHHW/Saunders et al. - 2018 - Trial without error Towards safe reinforcement le.pdf; /Users/jacquesthibodeau/Zotero/storage/NW6PQECP/citation.html; /Users/jacquesthibodeau/Zotero/storage/AV7ZKCMK/1707.html,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WJI4USKA,conferencePaper,2016,"Armstrong, Stuart; Leike, Jan",Towards interactive inverse reinforcement learning,NIPS Workshop,,,,,,2016,2022-01-30 4:53:37,2022-01-30 4:53:37,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8H65GSA9,conferencePaper,2020,"O'Keefe, Cullen; Cihon, Peter; Garfinkel, Ben; Flynn, Carrick; Leung, Jade; Dafoe, Allan",The Windfall Clause: Distributing the Benefits of AI for the Common Good,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375842,https://dl.acm.org/doi/10.1145/3375627.3375842,,2020-02-07,2022-01-30 4:53:37,2022-01-30 4:53:37,2020-08-18 21:31:11,327-331,,,,,,The Windfall Clause,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/GN5W52XA/O'Keefe et al. - 2020 - The Windfall Clause Distributing the Benefits of .pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
GEFVXESX,conferencePaper,2020,"Shevlane, Toby; Dafoe, Allan",The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/2001.00463,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",2020-01-09,2022-01-30 4:53:36,2022-01-30 4:53:36,2020-11-14 0:34:29,,,,,,,The Offense-Defense Balance of Scientific Knowledge,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.00463,,/Users/jacquesthibodeau/Zotero/storage/GXWDDN35/Shevlane and Dafoe - 2020 - The Offense-Defense Balance of Scientific Knowledg.pdf; /Users/jacquesthibodeau/Zotero/storage/JMJHDTSG/2001.html,,MetaSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
8WX59P96,conferencePaper,2020,"Tucker, Aaron D.; Anderljung, Markus; Dafoe, Allan",Social and Governance Implications of Improved Data Efficiency,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375863,http://arxiv.org/abs/2001.05068,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency – as more actors gain access to any level of capability – the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the “AI production function"", will be key to understanding the development of the AI industry and its societal impacts.",2020-02-07,2022-01-30 4:53:35,2022-01-30 4:53:35,2020-08-18 21:33:45,378-384,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.05068,,/Users/jacquesthibodeau/Zotero/storage/9DUUK8Z7/Tucker et al. - 2020 - Social and Governance Implications of Improved Dat.pdf,,MetaSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VK55TPK,conferencePaper,2019,"Cihon, Peter; Maas, Matthijs M; Kemp, Luke",Should Artificial Intelligence Governance be Centralised? Six Design Lessons from History,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,https://www.cser.ac.uk/media/uploads/files/Cihon_et_al-_2019-_Should_AI_Governance_be_Centralised.pdf,"Can effective international governance for artiﬁcial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efﬁciency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difﬁculty in securing participation while creating stringent rules. Other considerations depend on the speciﬁc design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneﬁcial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneﬁcial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",2019-12-15,2022-01-30 4:53:35,2022-01-30 4:53:35,2020-09-07,11,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 16,,/Users/jacquesthibodeau/Zotero/storage/9UVMQ2CZ/Cihon et al. - Should Artificial Intelligence Governance be Centr.pdf,,MetaSafety; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
835EJRGP,conferencePaper,2018,"Armstrong, Stuart; Mindermann, Sören",Occam's razor is insufficient to infer the preferences of irrational agents,Advances in Neural Information Processing Systems,,,,,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for speciﬁc human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent’s policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam’s razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple ‘normative’ assumptions, which cannot be deduced exclusively from observations.",2018,2022-01-30 4:53:19,2022-01-30 4:53:19,,5598–5609,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000017[s0],,/Users/jacquesthibodeau/Zotero/storage/3MH76I6R/Armstrong and Mindermann - Occam's razor is insufficient to infer the prefere.pdf; /Users/jacquesthibodeau/Zotero/storage/BXEGUPH6/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"32nd Conference on Neural Information Processing Systems (NeurIPS 2018),",,,,,,,,,,,,,,,,
6NBGWRA8,conferencePaper,2015,"Armstrong, Stuart",Motivated value selection for artificial agents,Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,,,,,,2015,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000046,,/Users/jacquesthibodeau/Zotero/storage/PZRINFPE/10183.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7GE6AZPJ,conferencePaper,2018,"Carey, Ryan",Incorrigibility in the CIRL Framework,"AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1709.06275,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",2018-06-03,2022-01-30 4:53:18,2022-01-30 4:53:18,2019-12-16 2:29:24,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 1709.06275,,/Users/jacquesthibodeau/Zotero/storage/8NIPD8BV/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/HPR8TDJ6/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/4G9NWGEB/1709.html; /Users/jacquesthibodeau/Zotero/storage/35KMSSRX/1709.html,,TechSafety; FHI; MIRI,Computer Science - Artificial Intelligence; ai safety; cirl; cooperative inverse reinforcement learning; corrigibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
RZBBEH4R,conferencePaper,2016,"Evans, Owain; Stuhlmüller, Andreas; Goodman, Noah","Learning the preferences of ignorant, inconsistent agents",Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 4:53:18,2022-01-30 4:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000091,,/Users/jacquesthibodeau/Zotero/storage/IT7ZXEC3/12476.html,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PWCWSMIK,conferencePaper,2015,"Evans, Owain; Goodman, Noah D",Learning the Preferences of Bounded Agents,NIPS Workshop on Bounded Optimality,,,,,,2015,2022-01-30 4:53:18,2022-01-30 4:53:18,,7,,,6,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000029  9 J:19,,/Users/jacquesthibodeau/Zotero/storage/3IWX99PK/Evans and Goodman - Learning the Preferences of Bounded Agents.pdf,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DW4JCPPD,conferencePaper,2021,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad",Optimal Policies Tend to Seek Power,arXiv:1912.01683 [cs],,,,http://arxiv.org/abs/1912.01683,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers are skeptical, because RL agents need not have human-like power-seeking instincts. To clarify this debate, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",2021-10-23,2022-01-30 4:50:56,2022-01-30 4:50:56,2021-11-14 18:44:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 1  arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/6M65PKWS/Turner et al. - 2021 - Optimal Policies Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/RDF4U9VH/1912.html,,TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,,,,,,,,,,,,,
H56ZKQ9D,conferencePaper,2019,"Carroll, Micah; Shah, Rohin; Ho, Mark K.; Griffiths, Thomas L.; Seshia, Sanjit A.; Abbeel, Pieter; Dragan, Anca",On the Utility of Learning about Humans for Human-AI Coordination,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1910.05789v2,"While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.",2019-10-13,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-11-14 1:20:48,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000062,,/Users/jacquesthibodeau/Zotero/storage/564DIJ7M/Carroll et al. - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/N29XCQCE/Carroll et al. - 2019 - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/NKUA5J28/1910.html; /Users/jacquesthibodeau/Zotero/storage/7DUA2DSC/1910.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
Q44NESEK,conferencePaper,2019,"Huang, Sandy H.; Huang, Isabella; Pandya, Ravi; Dragan, Anca D.",Nonverbal Robot Feedback for Human Teachers,Proceedings of the Conference on Robot Learning,,,,http://arxiv.org/abs/1911.02320,"Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.",2019-11-06,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-12-18 2:41:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1911.02320,,/Users/jacquesthibodeau/Zotero/storage/AA6GDGDX/Huang et al. - 2019 - Nonverbal Robot Feedback for Human Teachers.pdf; /Users/jacquesthibodeau/Zotero/storage/CR3FFFGV/1911.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Robot Learning,,,,,,,,,,,,,,,,
5FN3T5EH,conferencePaper,2015,"Hadﬁeld-Menell, Dylan; Russell, Stuart",Multitasking: Efﬁcient Optimal Planning for Bandit Superprocesses,Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,,,,,"A bandit superprocess is a decision problem composed from multiple independent Markov decision processes (MDPs), coupled only by the constraint that, at each time step, the agent may act in only one of the MDPs. Multitasking problems of this kind are ubiquitous in the real world, yet very little is known about them from a computational viewpoint, beyond the observation that optimal policies for the superprocess may prescribe actions that would be suboptimal for an MDP considered in isolation. (This observation implies that many applications of sequential decision analysis in practice are technically incorrect, since the decision problem being solved is often part of a larger, unstated bandit superprocess.) The paper summarizes the state-of-theart in the theory of bandit superprocesses and contributes a novel upper bound on the global value function of a bandit superprocess, deﬁned in terms of a direct relaxation of the arms. The bound is equivalent to an existing bound (the Whittle integral), but is deﬁned constructively, as the value of a related multi-armed bandit. We provide a new method to compute this bound and derive the ﬁrst practical algorithm to select optimal actions in bandit superprocesses. The algorithm operates by repeatedly establishing dominance relations between actions using upper and lower bounds on action values. Experiments indicate that the algorithm’s run-time compares very favorably to other possible algorithms designed for more general factored MDPs.",2015-07,2022-01-30 4:50:55,2022-01-30 4:50:55,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/FTI9E6Q8/Hadﬁeld-Menell and Russell - Multitasking Efﬁcient Optimal Planning for Bandit.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBIVNF95,conferencePaper,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-12-18 1:12:51,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1805.08882,,/Users/jacquesthibodeau/Zotero/storage/5V48367F/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1st Workshop on Goal Specifications for Reinforce- ment Learning, FAIM 2018",,,,,,,,,,,,,,,,
V5MMVQWT,conferencePaper,2020,"Fickinger, Arnaud; Zhuang, Simon; Hadfield-Menell, Dylan; Russell, Stuart",Multi-Principal Assistance Games,,,,,http://arxiv.org/abs/2007.09540,"Assistance games (also known as cooperative inverse reinforcement learning games) have been proposed as a model for beneficial AI, wherein a robotic agent must act on behalf of a human principal but is initially uncertain about the humans payoff function. This paper studies multi-principal assistance games, which cover the more general case in which the robot acts on behalf of N humans who may have widely differing payoffs. Impossibility theorems in social choice theory and voting theory can be applied to such games, suggesting that strategic behavior by the human principals may complicate the robots task in learning their payoffs. We analyze in particular a bandit apprentice game in which the humans act first to demonstrate their individual preferences for the arms and then the robot acts to maximize the sum of human payoffs. We explore the extent to which the cost of choosing suboptimal arms reduces the incentive to mislead, a form of natural mechanism design. In this context we propose a social choice method that uses shared control of a system to combine preference inference with social welfare optimization.",2020-07-18,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-08-28 17:24:47,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2007.09540,,/Users/jacquesthibodeau/Zotero/storage/CTGBBT4P/Fickinger et al. - 2020 - Multi-Principal Assistance Games.pdf; /Users/jacquesthibodeau/Zotero/storage/KRF8QCZJ/2007.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
6X7RBM9U,conferencePaper,2018,"Milli, Smitha; Schmidt, Ludwig; Dragan, Anca D.; Hardt, Moritz",Model Reconstruction from Model Explanations,"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",,,,http://arxiv.org/abs/1807.05185,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",2018-07-13,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-12-18 1:13:24,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 69  J: 31 arXiv: 1807.05185,,/Users/jacquesthibodeau/Zotero/storage/QSGCSN5T/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/D5ADWGHJ/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/83WVJFJX/1807.html; /Users/jacquesthibodeau/Zotero/storage/S3VPKVIG/1807.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conference on Fairness, Accountability, and Transparency",,,,,,,,,,,,,,,,
4NRTTKVW,conferencePaper,2018,"Zhang, Shun; Durfee, Edmund H.; Singh, Satinder",Minimax-Regret Querying on Side Effects for Safe Optimality in Factored Markov Decision Processes,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,10.24963/ijcai.2018/676,https://www.ijcai.org/proceedings/2018/676,"As it achieves a goal on behalf of its human user, an autonomous agent’s actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn’t explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.",2018-07,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-11-14 1:15:22,4867-4873,,,,,,,,,,,International Joint Conferences on Artificial Intelligence Organization,"Stockholm, Sweden",en,,,,,DOI.org (Crossref),,ZSCC: 0000023,,/Users/jacquesthibodeau/Zotero/storage/XMWT43ZV/Zhang et al. - 2018 - Minimax-Regret Querying on Side Effects for Safe O.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18},,,,,,,,,,,,,,,,
4ZEWADP4,conferencePaper,2019,"Milli, Smitha; Dragan, Anca D.",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,Proceedings of The 35th Uncertainty in Artificial Intelligence Conference,,,,http://proceedings.mlr.press/v115/milli20a.html,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (“robot”), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots beneﬁt from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspeciﬁcation: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspeciﬁcation. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",2019-06-28,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-12-20,,,,,,,Literal or Pedagogic Human?,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1903.03877,,/Users/jacquesthibodeau/Zotero/storage/4532ZRGJ/Milli and Dragan - 2019 - Literal or Pedagogic Human Analyzing Human Model .pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The 35th Uncertainty in Artificial Intelligence Conference,,,,,,,,,,,,,,,,
ZJGDKBG2,conferencePaper,2019,"Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian K.",Legible Normativity for AI Alignment: The Value of Silly Rules,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1811.01267,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior–social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules —rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",2019,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-07-08 15:47:44,,,,,,,Legible Normativity for AI Alignment,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 6  J: 2 arXiv: 1811.01267,,/Users/jacquesthibodeau/Zotero/storage/9N9AXA2T/Hadfield-Menell et al. - 2018 - Legible Normativity for AI Alignment The Value of.pdf; /Users/jacquesthibodeau/Zotero/storage/TRSUMM4D/1811.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
D6VXDQ3V,conferencePaper,2019,"Choudhury, Rohan; Swamy, Gokul; Hadfield-Menell, Dylan; Dragan, Anca",On the Utility of Model Learning in HRI,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,http://arxiv.org/abs/1901.01291,"Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a “theory of mind” to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot’s theory of mind for the human are wrong, as they inevitably will be in practice. We ﬁnd that there is a signiﬁcant sample complexity advantage to theory of mind methods and that they are more robust to covariate shift, but that when enough interaction data is available, black box approaches eventually dominate.",2019-01-04,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-07-08 15:45:07,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1901.01291,,/Users/jacquesthibodeau/Zotero/storage/NZN9AB88/Choudhury et al. - 2019 - On the Utility of Model Learning in HRI.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,,,,,,,,,,,,,
D5PF7N4F,conferencePaper,2019,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1906.09624,"Our goal is for agents to optimize the right reward function, despite how difﬁcult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with speciﬁc assumptions, and instead use a purely data-driven approach. We decided to put this to the test – rather than relying on assumptions about which speciﬁc bias the demonstrator has when planning, we instead learn the demonstrator’s planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed ﬁndings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this beneﬁt is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the ﬂexibility of data-driven methods and the useful bias of known human biases. Code is available at https: //tinyurl.com/learningbiases.",2019-06-23,2022-01-30 4:50:55,2022-01-30 4:50:55,2019-07-11 18:35:30,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1906.09624,,"/Users/jacquesthibodeau/Zotero/storage/TTNVB7MM/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf; /Users/jacquesthibodeau/Zotero/storage/5WHA9REK/1906.html; /Users/jacquesthibodeau/Zotero/storage/4DMXNS7E/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf",,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
TK4DP9CE,conferencePaper,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do By Simulating the Past,,,,,,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state (Shah et al., 2019). Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a speciﬁc skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021,2022-01-30 4:50:55,2022-01-30 4:50:55,,24,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/XAPXNEM2/Lindner et al. - 2021 - LEARNING WHAT TO DO BY SIMULATING THE PAST.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
8Z4999WK,conferencePaper,2018,"Bobu, Andreea; Bajcsy, Andrea; Fisac, Jaime F.; Dragan, Anca D.",Learning under Misspecified Objective Spaces,2nd Conference on Robot Learning (CoRL 2018),,,,http://arxiv.org/abs/1810.05157,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human’s desired objective lies within the robot’s hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus speciﬁcally on learning from physical human corrections during the robot’s task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human’s correction is for the robot’s hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a robot manipulator.",2018-10-26,2022-01-30 4:50:54,2022-01-30 4:50:54,2019-12-18 2:38:36,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1810.05157,,/Users/jacquesthibodeau/Zotero/storage/BHXJU7S4/Bobu et al. - 2018 - Learning under Misspecified Objective Spaces.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2nd Conference on Robot Learning (CoRL 2018),,,,,,,,,,,,,,,,
H4ETM44R,conferencePaper,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan",Learning Human Objectives by Evaluating Hypothetical Behavior,Proceedings of the 37th International Conference on Machine Learning,,,,http://proceedings.mlr.press/v119/reddy20a.html,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",2019-12-05,2022-01-30 4:50:54,2022-01-30 4:50:54,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005[s0]  arXiv: 1912.05652,,/Users/jacquesthibodeau/Zotero/storage/RGR6BVZQ/Reddy et al. - 2019 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/E85ASZIU/1912.html; /Users/jacquesthibodeau/Zotero/storage/QJDICWCK/1912.html,,CHAI; TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,37th International Conference on Machine Learning,,,,,,,,,,,,,,,,
4TS7JII8,conferencePaper,2018,"Basu, Chandrayee; Singhal, Mukesh; Dragan, Anca D.",Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,,,10.1145/3171221.3171284,http://arxiv.org/abs/1802.01604,"We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.",2018,2022-01-30 4:50:54,2022-01-30 4:50:54,2019-12-18 2:40:57,132-140,,,,,,Learning from Richer Human Guidance,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/5GQZRWB7/Basu et al. - 2018 - Learning from Richer Human Guidance Augmenting Co.pdf; /Users/jacquesthibodeau/Zotero/storage/WMF3FHW2/1802.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6W8J2P4W,conferencePaper,2018,"Bajcsy, Andrea; Losey, Dylan P.; O'Malley, Marcia K.; Dragan, Anca D.","Learning from Physical Human Corrections, One Feature at a Time",Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,978-1-4503-4953-6,,10.1145/3171221.3171267,http://dl.acm.org/citation.cfm?doid=3171221.3171267,"We focus on learning robot objective functions from human guidance: specifically, from physical corrections provided by the person while the robot is acting. Objective functions are typically parametrized in terms of features, which capture aspects of the task that might be important. When the person intervenes to correct the robot’s behavior, the robot should update its understanding of which features matter, how much, and in what way. Unfortunately, real users do not provide optimal corrections that isolate exactly what the robot was doing wrong. Thus, when receiving a correction, it is difficult for the robot to determine which features the person meant to correct, and which features were changed unintentionally. In this paper, we propose to improve the efficiency of robot learning during physical interactions by reducing unintended learning. Our approach allows the human-robot team to focus on learning one feature at a time, unlike state-of-the-art techniques that update all features at once. We derive an online method for identifying the single feature which the human is trying to change during physical interaction, and experimentally compare this one-at-a-time approach to the all-at-once baseline in a user study. Our results suggest that users teaching one-at-a-time perform better, especially in tasks that require changing multiple features.",2018,2022-01-30 4:50:54,2022-01-30 4:50:54,2019-12-18 2:41:35,141-149,,,,,,,,,,,ACM Press,"Chicago, IL, USA",en,,,,,DOI.org (Crossref),,ZSCC: 0000063,,"/Users/jacquesthibodeau/Zotero/storage/NAECZFHT/Bajcsy et al. - 2018 - Learning from Physical Human Corrections, One Feat.pdf",,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the 2018 ACM/IEEE International Conference,,,,,,,,,,,,,,,,
D65J6K2P,conferencePaper,2019,"Zhao, Ruihan; Tiomkin, Stas; Abbeel, Pieter",Learning Efficient Representation for Intrinsic Motivation,,,,,http://arxiv.org/abs/1912.02624,"Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the ""Water-Filling"" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.",2019-12-08,2022-01-30 4:50:54,2022-01-30 4:50:54,2019-12-18 2:48:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1912.02624,,/Users/jacquesthibodeau/Zotero/storage/C43XRQE9/Zhao et al. - 2019 - Learning Efficient Representation for Intrinsic Mo.pdf; /Users/jacquesthibodeau/Zotero/storage/ZMMKVSHT/1912.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33rd Conference on Neural Information Processing Systems (NeurIPS 2019),,,,,,,,,,,,,,,,
GZDCFRPG,conferencePaper,1998,"Russell, Stuart",Learning agents for uncertain environments,Proceedings of the eleventh annual conference on Computational learning theory  - COLT' 98,978-1-58113-057-7,,10.1145/279943.279964,http://portal.acm.org/citation.cfm?doid=279943.279964,,1998,2022-01-30 4:50:54,2022-01-30 4:50:54,2020-11-22 1:48:04,101-103,,,,,,,,,,,ACM Press,"Madison, Wisconsin, United States",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 451,,/Users/jacquesthibodeau/Zotero/storage/U8UI78ZU/Russell - 1998 - Learning agents for uncertain environments (extend.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the eleventh annual conference,,,,,,,,,,,,,,,,
X8EUNR79,conferencePaper,2017,"Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",Inverse Reward Design,Advances in Neural Information Processing Systems 30 (NIPS 2017),,,,http://arxiv.org/abs/1711.02827,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",2017,2022-01-30 4:50:54,2022-01-30 4:50:54,2020-11-22 4:15:56,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000204  arXiv: 1711.02827,,/Users/jacquesthibodeau/Zotero/storage/IG3G2WF9/Hadfield-Menell et al. - 2020 - Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/M4MGUB4S/1711.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H476ZK5U,conferencePaper,2017,"Bajcsy, Andrea; Losey, Dylan P; O’Malley, Marcia K; Dragan, Anca D",Learning Robot Objectives from Physical Human Interaction,Proceedings of Machine Learning Research,,,,,"When humans and robots work in close proximity, physical interaction is inevitable. Traditionally, robots treat physical interaction as a disturbance, and resume their original behavior after the interaction ends. In contrast, we argue that physical human interaction is informative: it is useful information about how the robot should be doing its task. We formalize learning from such interactions as a dynamical system in which the task objective has parameters that are part of the hidden state, and physical human interactions are observations about these parameters. We derive an online approximation of the robot’s optimal policy in this system, and test it in a user study. The results suggest that learning from physical interaction leads to better robot task performance with less human effort.",2017,2022-01-30 4:50:54,2022-01-30 4:50:54,,10,,,78,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000026[s2],,/Users/jacquesthibodeau/Zotero/storage/B7V4VQ4Q/Bajcsy et al. - Learning Robot Objectives from Physical Human Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/CE6X2RSM/102348.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1st Conference on Robot Learning (CoRL 2017),,,,,,,,,,,,,,,,
777W689G,conferencePaper,2019,"Zhang, Jason Y.; Dragan, Anca D.",Learning from Extrapolated Corrections,2019 International Conference on Robotics and Automation (ICRA),,,10.1109/ICRA.2019.8793554,,"Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information - the change of a waypoint along the way - to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.",2019-05,2022-01-30 4:50:54,2022-01-30 4:50:54,,7034-7040,,,,,,,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000008  ISSN: 1050-4729,,/Users/jacquesthibodeau/Zotero/storage/WDX9K5VV/8793554.html; /Users/jacquesthibodeau/Zotero/storage/Z27ARW82/Zhang and Dragan - 2019 - Learning from Extrapolated Corrections.pdf,,CHAI; TechSafety,control engineering computing; Cost function; cost functions; Estimation; extrapolated corrections; extrapolation; extrapolation problem; function approximation; Function approximation; function space; Kernel; learning (artificial intelligence); nonEuclidean norms; online function approximation; Robot kinematics; robot learning; robot programming; Trajectory; user guidance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
CP3JGZGM,conferencePaper,2019,"Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea",Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1805.12573,"A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",2019-10-14,2022-01-30 4:50:54,2022-01-30 4:50:54,2019-12-18 2:40:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  1 J: 11 arXiv: 1805.12573,,/Users/jacquesthibodeau/Zotero/storage/FBXXF6K7/Xu et al. - 2019 - Learning a Prior over Intent via Meta-Inverse Rein.pdf; /Users/jacquesthibodeau/Zotero/storage/5GDRRPWX/1805.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
3V5UACJI,conferencePaper,2016,"Sadigh, Dorsa; Sastry, S. Shankar; Seshia, Sanjit A.; Dragan, Anca",Information gathering actions over human internal state,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),978-1-5090-3762-9,,10.1109/IROS.2016.7759036,http://ieeexplore.ieee.org/document/7759036/,"Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planning actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver’s lane to test their driving style. Results in simulation and in a user study suggest that active information gathering signiﬁcantly outperforms passive state estimation.",2016-10,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-12-18 1:40:07,66-73,,,,,,,,,,,IEEE,"Daejeon, South Korea",en,,,,,DOI.org (Crossref),,ZSCC: 0000176,,/Users/jacquesthibodeau/Zotero/storage/JXUN7TJI/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf; /Users/jacquesthibodeau/Zotero/storage/X88ZSNX3/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
AVRPXH8F,conferencePaper,2019,"Pandya, Ravi; Huang, Sandy H.; Hadfield-Menell, Dylan; Dragan, Anca D.",Human-AI Learning Performance in Multi-Armed Bandits,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1812.09376,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artiﬁcial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We ﬁnd that team performance can beat both human and agent performance in isolation. Interestingly, we also ﬁnd that an agent’s performance in isolation does not necessarily correlate with the human-agent team’s performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent’s suggestions will inﬂuence human decision-making.",2019,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-07-08 15:46:25,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 2  J: 2 arXiv: 1812.09376,,,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
EF6NXXC5,conferencePaper,2018,"Fisac, Jaime F.; Bronstein, Eli; Stefansson, Elis; Sadigh, Dorsa; Sastry, S. Shankar; Dragan, Anca D.",Hierarchical Game-Theoretic Planning for Autonomous Vehicles,Robotics: Science and Systems 2019,,,,http://arxiv.org/abs/1810.05766,"The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle’s planning and its predictions of other drivers’ behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon “strategic” game with simpliﬁed dynamics and full information structure, and a short-horizon “tactical” game with full dynamics and a simpliﬁed information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver’s ability and incentives to inﬂuence each other. In addition, our approach admits non-deterministic models of human decisionmaking, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.",2018-10-12,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-07-08 16:10:43,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 106  J: 38 arXiv: 1810.05766,,,,CHAI; TechSafety,"Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Multiagent Systems; I.2.9; 68T40, 93C85, 91A25; Mathematics - Optimization and Control",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2019,,,,,,,,,,,,,,,,
JXZF36GH,conferencePaper,2020,"Dobbe, Roel; Gilbert, Thomas Krendl; Mintz, Yonatan",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1911.09005,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",2020,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-11-14 0:55:17,,,,,,,Hard Choices in Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001[s0]  arXiv: 1911.09005,,/Users/jacquesthibodeau/Zotero/storage/7VMINIWU/Dobbe et al. - 2019 - Hard Choices in Artificial Intelligence Addressin.pdf; /Users/jacquesthibodeau/Zotero/storage/K6X3NNFA/1911.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society 2020",,,,,,,,,,,,,,,,
K25P3RCZ,conferencePaper,2019,"Hadfield-Menell, Dylan; Hadfield, Gillian",Incomplete Contracting and AI Alignment,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1804.04268,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",2019,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-07-18 4:58:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 31  J: 14 arXiv: 1804.04268,,/Users/jacquesthibodeau/Zotero/storage/3BC8IA76/Hadfield-Menell and Hadfield - 2018 - Incomplete Contracting and AI Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/4CJJ6G6F/1804.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VVQW8ER5,conferencePaper,2018,"Liu, Chang; Hamrick, Jessica B.; Fisac, Jaime F.; Dragan, Anca D.; Hedrick, J. Karl; Sastry, S. Shankar; Griffiths, Thomas L.",Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration,Proceedings of the 15th International Conferenceon Autonomous Agents and Multiagent Systems (AAMAS 2016),,,,http://arxiv.org/abs/1802.01780,"The study of human-robot interaction is fundamental to the design and use of robotics in real-world applications. Robots will need to predict and adapt to the actions of human collaborators in order to achieve good performance and improve safety and end-user adoption. This paper evaluates a human-robot collaboration scheme that combines the task allocation and motion levels of reasoning: the robotic agent uses Bayesian inference to predict the next goal of its human partner from his or her ongoing motion, and re-plans its own actions in real time. This anticipative adaptation is desirable in many practical scenarios, where humans are unable or unwilling to take on the cognitive overhead required to explicitly communicate their intent to the robot. A behavioral experiment indicates that the combination of goal inference and dynamic task planning significantly improves both objective and perceived performance of the human-robot team. Participants were highly sensitive to the differences between robot behaviors, preferring to work with a robot that adapted to their actions over one that did not.",2018-02-05,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-12-13 20:00:34,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 1802.01780,,/Users/jacquesthibodeau/Zotero/storage/IZFZ7SNH/Liu et al. - 2018 - Goal Inference Improves Objective and Perceived Pe.pdf; /Users/jacquesthibodeau/Zotero/storage/ZWMVN6F2/1802.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C5Z6H6IX,conferencePaper,2018,"Cundy, Chris; Filan, Daniel",Exploring Hierarchy-Aware Inverse Reinforcement Learning,arXiv:1807.05037 [cs],,,,http://arxiv.org/abs/1807.05037,"We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.",2018-07-13,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 1:12:20,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 1807.05037,,/Users/jacquesthibodeau/Zotero/storage/65A775B4/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/3SB9PIM5/1807.html; /Users/jacquesthibodeau/Zotero/storage/DBVMCRNX/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1st Workshop on Goal Specifications for Reinforcement Learning, ICML 2018",,,,,,,,,,,,,,,,
4BV35Z3X,conferencePaper,2018,"de Graaf, Maartje M.A.; Malle, Bertram F.; Dragan, Anca; Ziemke, Tom",Explainable Robotic Systems,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,978-1-4503-5615-2,,10.1145/3173386.3173568,http://dl.acm.org/citation.cfm?doid=3173386.3173568,"The increasing complexity of robotic systems are pressing the need for them to be transparent and trustworthy. When people interact with a robotic system, they will inevitably construct mental models to understand and predict its actions. However, people’s mental models of robotic systems stem from their interactions with living beings, which induces the risk of establishing incorrect or inadequate mental models of robotic systems and may lead people to either under- and over-trust these systems. We need to understand the inferences that people make about robots from their behavior, and leverage this understanding to formulate and implement behaviors into robotic systems that support the formation of correct mental models of and fosters trust calibration. This way, people will be better able to predict the intentions of these systems, and thus more accurately estimate their capabilities, better understand their actions, and potentially correct their errors. The aim of this full-day workshop is to provide a forum for researchers and practitioners to share and learn about recent research on people’s inferences of robot actions, as well as the implementation of transparent, predictable, and explainable behaviors into robotic systems.",2018,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 2:40:25,387-388,,,,,,,,,,,ACM Press,"Chicago, IL, USA",en,,,,,DOI.org (Crossref),,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/FFV4ATRX/de Graaf et al. - 2018 - Explainable Robotic Systems.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Companion of the 2018 ACM/IEEE International Conference,,,,,,,,,,,,,,,,
BPHFCUVZ,conferencePaper,2018,"Huang, Sandy H.; Bhatia, Kush; Abbeel, Pieter; Dragan, Anca D.",Establishing Appropriate Trust via Critical States,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,http://arxiv.org/abs/1810.08174,"In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",2018-10-18,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 2:38:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000044  arXiv: 1810.08174,,/Users/jacquesthibodeau/Zotero/storage/ZJCEQTK4/Huang et al. - 2018 - Establishing Appropriate Trust via Critical States.pdf; /Users/jacquesthibodeau/Zotero/storage/PJVHAHWN/1810.html,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
4EF7C9NX,conferencePaper,2019,"Gilbert, Thomas Krendl; Mintz, Yonatan",Epistemic Therapy for Bias in Automated Decision-Making,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6324-2,,10.1145/3306618.3314294,https://dl.acm.org/doi/10.1145/3306618.3314294,,2019-01-27,2022-01-30 4:50:45,2022-01-30 4:50:45,2020-12-17 22:13:08,61-67,,,,,,,,,,,ACM,Honolulu HI USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/G8JN4BTX/Gilbert and Mintz - 2019 - Epistemic Therapy for Bias in Automated Decision-M.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '19: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
ZEAMIZP4,conferencePaper,2018,"Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea",Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning,Proceedings of the 36th International Conference on Machine Learning,,,,http://proceedings.mlr.press/v97/xu19d/xu19d.pdf,A significant challenge for the practical application of reinforcement learning toreal world problems is the need to specify an oracle reward function that correctly defines a task. Inverse...,2018-09-27,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 3:12:12,,,,,,,,,,,,,,,,,,,openreview.net,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/KDPI92S8/Xu et al. - 2018 - Few-Shot Intent Inference via Meta-Inverse Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/66PJEMEA/forum.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
QT5EDQJS,conferencePaper,2018,"Kwon, Minae; Huang, Sandy H.; Dragan, Anca D.",Expressing Robot Incapability,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18,,,10.1145/3171221.3171276,http://arxiv.org/abs/1810.08167,"Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.",2018,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 2:40:52,87-95,,,,,,,,,,,ACM,,en,,,,,arXiv.org,,ZSCC: 0000072,,/Users/jacquesthibodeau/Zotero/storage/46F5BZ8U/Kwon et al. - 2018 - Expressing robot incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/4NFN3ET4/Kwon et al. - 2018 - Expressing Robot Incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/SU3ZGDE8/citation.html,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B4EIGIXS,conferencePaper,2021,"Knott, Paul; Carroll, Micah; Devlin, Sam; Ciosek, Kamil; Hofmann, Katja; Dragan, A. D.; Shah, Rohin",Evaluating the Robustness of Collaborative Agents,Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021),,,,http://arxiv.org/abs/2101.05507,"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",2021-01-14,2022-01-30 4:50:45,2022-01-30 4:50:45,2021-10-30 20:37:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2101.05507,,/Users/jacquesthibodeau/Zotero/storage/ZKGWIRKG/Knott et al. - 2021 - Evaluating the Robustness of Collaborative Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/ZC9RGFH2/2101.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAMAS 2021,,,,,,,,,,,,,,,,
55SJ6TUU,conferencePaper,2017,"Palaniappan, Malayandi; Malik, Dhruv; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Efficient Cooperative Inverse Reinforcement Learning,Proc. ICML Work⁃ shop on Reliable Machine Learning in the Wild (2017),,,,,,2017,2022-01-30 4:50:44,2022-01-30 4:50:44,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/D7BTS6QX/Palaniappan et al. - Efficient Cooperative Inverse Reinforcement Learni.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBRT43WR,conferencePaper,2020,"Halpern, Joseph Y.; Piermont, Evan",Dynamic Awareness,,,,,http://arxiv.org/abs/2007.02823,"We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $\phi$ in state $s$ of a model $M$, she transitions to state $s^*$ in a model $M^*$. We then discuss how such a model can be applied to information disclosure.",2020-07-06,2022-01-30 4:50:44,2022-01-30 4:50:44,2020-12-19 2:26:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2007.02823,,/Users/jacquesthibodeau/Zotero/storage/TIVQU7G4/Halpern and Piermont - 2020 - Dynamic Awareness.pdf; /Users/jacquesthibodeau/Zotero/storage/6C8IDR23/2007.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Economics - Theoretical Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17th International Conference on Principles of Knowledge Representation and Reasoning,,,,,,,,,,,,,,,,
W2N8KHAS,conferencePaper,2020,"Freire, Pedro; Gleave, Adam; Toyer, Sam; Russell, Stuart",DERAIL: Diagnostic Environments for Reward And Imitation Learning,Advances in Neural Information Processing Systems 33 Pre-proceedings,,,,http://arxiv.org/abs/2012.01365,"The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals .",2020-12-02,2022-01-30 4:50:44,2022-01-30 4:50:44,2020-12-18 0:37:38,,,,,,,DERAIL,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2012.01365,,/Users/jacquesthibodeau/Zotero/storage/4ZCV83UD/Freire et al. - 2020 - DERAIL Diagnostic Environments for Reward And Imi.pdf; /Users/jacquesthibodeau/Zotero/storage/VVNRMKJH/2012.html; /Users/jacquesthibodeau/Zotero/storage/NDR2G3TZ/2012.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deep Reinforcement Learning Workshop at NeurIPS,,,,,,,,,,,,,,,,
EGTZPRSQ,conferencePaper,2016,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",Cooperative Inverse Reinforcement Learning,Advances in Neural Information Processing Systems 29 (NIPS 2016),,,,https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",2016-06-09,2022-01-30 4:50:44,2022-01-30 4:50:44,2020-12-21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000411,,/Users/jacquesthibodeau/Zotero/storage/C32UQET4/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/N6SZFWKU/6420-cooperative-inverse-reinforcement-learning.html; /Users/jacquesthibodeau/Zotero/storage/X6JQE3BA/1606.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2016,,,,,,,,,,,,,,,,
IVEVAANE,conferencePaper,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency via Attainable Utility Preservation,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725,"Reward functions are often misspeciﬁed. An agent optimizing an incorrect reward function can change its environment in large, undesirable, and potentially irreversible ways. Work on impact measurement seeks a means of identifying (and thereby avoiding) large changes to the environment. We propose a novel impact measure which induces conservative, effective behavior across a range of situations. The approach attempts to preserve the attainable utility of auxiliary objectives. We evaluate our proposal on an array of benchmark tasks and show that it matches or outperforms relative reachability, the state-of-the-art in impact measurement.",2020,2022-01-30 4:50:44,2022-01-30 4:50:44,2019-07-08 15:44:58,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/4459ZMWD/Turner et al. - 2020 - Conservative Agency via Attainable Utility Preserv.pdf; /Users/jacquesthibodeau/Zotero/storage/375MJXAP/1902.html; /Users/jacquesthibodeau/Zotero/storage/7DTZW9IN/Turner et al. - 2019 - Conservative Agency via Attainable Utility Preserv.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KKKW3IZZ,conferencePaper,2021,"Zhuang, Simon; Hadﬁeld-Menell, Dylan",Consequences of Misaligned AI,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2102.03896,AI systems often rely on two key components: a speciﬁed goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial speciﬁcation of the principal’s goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the L attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on J < L attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal—agent problem from artiﬁcial intelligence; 2) we provide necessary and sufﬁcient conditions under which indeﬁnitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identiﬁes a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-01-30 4:50:44,2022-01-30 4:50:44,,11,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/8A4KHTU8/Zhuang and Hadﬁeld-Menell - Consequences of Misaligned AI.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
VKGPQCJR,conferencePaper,2021,"Dennis, Michael; Jaques, Natasha; Vinitsky, Eugene; Bayen, Alexandre; Russell, Stuart; Critch, Andrew; Levine, Sergey",Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design,arXiv:2012.02096 [cs],,,,http://arxiv.org/abs/2012.02096,"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.",2021-02-03,2022-01-30 4:50:44,2022-01-30 4:50:44,2021-11-13 22:36:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 21  arXiv: 2012.02096,,/Users/jacquesthibodeau/Zotero/storage/23RGK32W/Dennis et al. - 2021 - Emergent Complexity and Zero-shot Transfer via Uns.pdf; /Users/jacquesthibodeau/Zotero/storage/8C39EB27/2012.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
GIRDREWD,conferencePaper,2017,"Basu, C.; Yang, Q.; Hungerman, D.; Sinahal, M.; Draqan, A. D.",Do You Want Your Autonomous Car to Drive Like You?,2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI,,,,,"With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.",2017-03,2022-01-30 4:50:44,2022-01-30 4:50:44,,417-425,,,,,,,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000092  ISSN: 2167-2148,,/Users/jacquesthibodeau/Zotero/storage/J3P5R8WX/Basu et al. - 2017 - Do You Want Your Autonomous Car to Drive Like You.pdf,,CHAI; TechSafety; AmbiguosSafety,Safety; actual driving style; aggressive cars; aggressive drivers; Atmospheric measurements; automobiles; Automobiles; Autonomous automobiles; autonomous car; autonomous cars; defensive cars; defensive drivers; defensive driving style; driving preferences; driving style; driving style preference learning; road safety; Roads; specific driving scenario; Task analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI,,,,,,,,,,,,,,,,
V59HB96K,conferencePaper,2019,"Bourgin, David D.; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.",Cognitive Model Priors for Predicting Human Decisions,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1905.09397,"Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ""cognitive model priors"" by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.",2019-05-22,2022-01-30 4:50:43,2022-01-30 4:50:43,2019-12-18 2:18:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 1905.09397,,/Users/jacquesthibodeau/Zotero/storage/J2ZDVPK2/Bourgin et al. - 2019 - Cognitive Model Priors for Predicting Human Decisi.pdf; /Users/jacquesthibodeau/Zotero/storage/T2SNNBCJ/1905.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
MSC4I4M4,conferencePaper,2020,"Freedman, Rachel; Shah, Rohin; Dragan, Anca",Choice Set Misspeciﬁcation in Reward Inference,CEUR Workshop Proceedings,,,,http://ceur-ws.org/Vol-2640/paper_14.pdf,"Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly speciﬁed rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difﬁcult to specify, and analyze choice set misspeciﬁcation: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classiﬁcation of different kinds of choice set misspeciﬁcation, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspeciﬁcation to hurt, we ﬁnd that certain kinds of misspeciﬁcation are neither helpful nor harmful (in expectation). However, in other situations, misspeciﬁcation can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspeciﬁcation in real-world reward inference.",2020,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-18,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001[s0],,/Users/jacquesthibodeau/Zotero/storage/NZSH77X3/Freedman et al. - Choice Set Misspeciﬁcation in Reward Inference.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CEUR Workshop,,,,,,,,,,,,,,,,
SSSCRXJG,conferencePaper,2020,"Shah, Rohin; Freire, Pedro; Alex, Neel; Freedman, Rachel; Krasheninnikov, Dmitrii; Chan, Lawrence; Dennis, Michael; Abbeel, Pieter; Dragan, Anca; Russell, Stuart",Benefits of Assistance over Reward Learning,,,,,https://openreview.net/forum?id=DFIoGDZejIB,"Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward...",2020-09-28,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-12-18 0:39:34,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/X7AUG7ZJ/Anonymous - 2020 - Benefits of Assistance over Reward Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/EQCDNKBZ/forum.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
VHS74F2I,conferencePaper,2021,"Verma, Pulkit; Marpally, Shashank Rao; Srivastava, Siddharth",Asking the Right Questions: Learning Interpretable Action Models Through Query Answering,arXiv:1912.12613 [cs],,,,http://arxiv.org/abs/1912.12613,"This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent's internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.",2021-04-09,2022-01-30 4:50:43,2022-01-30 4:50:43,2021-10-30 23:01:42,,,,,,,Asking the Right Questions,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 6  arXiv: 1912.12613,,/Users/jacquesthibodeau/Zotero/storage/V7GTH85I/Verma et al. - 2021 - Asking the Right Questions Learning Interpretable.pdf; /Users/jacquesthibodeau/Zotero/storage/BERGIMGS/1912.html,,TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
UVD25SIB,conferencePaper,2019,"Rahtz, Matthew; Fang, James; Dragan, Anca D.; Hadfield-Menell, Dylan",An Extensible Interactive Interface for Agent Design,,,,,http://arxiv.org/abs/1906.02641,"In artiﬁcial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach deﬁnes a set of increasingly complex policies. The interface allows the user to switch between these policies at ﬁxed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.",2019-06-06,2022-01-30 4:50:43,2022-01-30 4:50:43,2019-07-08 15:45:46,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1906.02641,,/Users/jacquesthibodeau/Zotero/storage/5AZABBHX/Rahtz et al. - 2019 - An Extensible Interactive Interface for Agent Desi.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 ICML Workshop on Human in the Loop Learning (HILL 2019),,,,,,,,,,,,,,,,
2W5PXMHE,conferencePaper,2018,"Malik, Dhruv; Palaniappan, Malayandi; Fisac, Jaime F.; Hadfield-Menell, Dylan; Russell, Stuart; Dragan, Anca D.","An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1806.03820,"Our goal is for AI systems to correctly identify and act according to their human user’s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a speciﬁc property of CIRL—the human is a full information agent—to derive an optimality-preserving modiﬁcation to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL’s assumption of human rationality. We apply this update to a variety of POMDP solvers and ﬁnd that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",2018-06-11,2022-01-30 4:50:43,2022-01-30 4:50:43,2019-07-12 0:13:10,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 1806.03820,,"/Users/jacquesthibodeau/Zotero/storage/CS44ETD8/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Coope.pdf; /Users/jacquesthibodeau/Zotero/storage/INHZCFF4/1806.html; /Users/jacquesthibodeau/Zotero/storage/K662JE53/1806.html",,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018,,,,,,,,,,,,,,,,
FZBNIVSD,conferencePaper,2020,"Turner, Alexander Matt; Ratzlaff, Neale; Tadepalli, Prasad",Avoiding Side Effects in Complex Environments,Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020),,,,http://arxiv.org/abs/2006.06547,"Reward function speciﬁcation can be difﬁcult, even in simple environments. Realistic environments contain millions of states. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoids side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway’s Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead, completes the speciﬁed task, and avoids side effects.",2020-06-11,2022-01-30 4:50:43,2022-01-30 4:50:43,2020-08-31 18:07:40,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 2006.06547,,/Users/jacquesthibodeau/Zotero/storage/WW987S6M/Turner et al. - 2020 - Avoiding Side Effects in Complex Environments.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
W4R7FKMX,conferencePaper,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,arXiv:2008.02275 [cs],,,,http://arxiv.org/abs/2008.02275,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2021-07-24,2022-01-30 4:50:42,2022-01-30 4:50:42,2021-10-30 21:58:26,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001[s0]  arXiv: 2008.02275,,/Users/jacquesthibodeau/Zotero/storage/RC4VUUN5/Hendrycks et al. - 2020 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/XUW5SB9I/Hendrycks et al. - 2021 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/MSZT6G3R/2008.html; /Users/jacquesthibodeau/Zotero/storage/I6C3W2WW/2008.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
P3T7ZT65,conferencePaper,2019,"Bahdanau, Dzmitry; Hill, Felix; Leike, Jan; Hughes, Edward; Hosseini, Arian; Kohli, Pushmeet; Grefenstette, Edward",Learning to Understand Goal Specifications by Modelling Reward,arXiv:1806.01946 [cs],,,,http://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.",2019-02-15,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:32:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s3]  ACC: 73  J: 33 arXiv: 1806.01946,,/Users/jacquesthibodeau/Zotero/storage/MG92TPH7/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/Z4PZME9T/1806.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
KIWMVR6M,conferencePaper,2020,"Anthony, Thomas; Eccles, Tom; Tacchetti, Andrea; Kramár, János; Gemp, Ian; Hudson, Thomas C.; Porcel, Nicolas; Lanctot, Marc; Pérolat, Julien; Everett, Richard; Werpachowski, Roman; Singh, Satinder; Graepel, Thore; Bachrach, Yoram",Learning to Play No-Press Diplomacy with Best Response Policy Iteration,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2006.04635,"Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate ﬁctitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",2020-08-26,2022-01-30 4:52:39,2022-01-30 4:52:39,2020-08-31 17:58:54,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 16  arXiv: 2006.04635,,/Users/jacquesthibodeau/Zotero/storage/ZMDHG4BH/Anthony et al. - 2020 - Learning to Play No-Press Diplomacy with Best Resp.pdf,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
FVAQUPVW,conferencePaper,2019,"Everitt, Tom; Kumar, Ramana; Krakovna, Victoria; Legg, Shane",Modeling AGI Safety Frameworks with Causal Influence Diagrams,arXiv:1906.08663 [cs],,,,http://arxiv.org/abs/1906.08663,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.",2019-06-20,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:27:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 1906.08663,,/Users/jacquesthibodeau/Zotero/storage/2GA3KWFC/Everitt et al. - 2019 - Modeling AGI Safety Frameworks with Causal Influen.pdf; /Users/jacquesthibodeau/Zotero/storage/J3KK437V/1906.html; /Users/jacquesthibodeau/Zotero/storage/9UU2WGKD/1906.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019 AI Safety Workshop,,,,,,,,,,,,,,,,
I2AKM3D5,conferencePaper,2019,"Wang, Chenglong; Bunel, Rudy; Dvijotham, Krishnamurthy; Huang, Po-Sen; Grefenstette, Edward; Kohli, Pushmeet",Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,,,,https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Knowing_When_to_Stop_Evaluation_and_Verification_of_Conformity_to_CVPR_2019_paper.html,"Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size.",2019-04-26,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-12-20,,,,,,,Knowing When to Stop,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1904.12004,,/Users/jacquesthibodeau/Zotero/storage/5RJNUC4N/Wang et al. - 2019 - Knowing When to Stop Evaluation and Verification .pdf; /Users/jacquesthibodeau/Zotero/storage/TGZNQ5NZ/1904.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,
TF8PSWUZ,conferencePaper,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Hybrid Models with Deep and Invertible Features,"arXiv:1902.02767 [cs, stat]",,,,http://arxiv.org/abs/1902.02767,"We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features), the density of the features, and p(targets | features), the predictive distribution, can be computed exactly in a single feed-forward pass. We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models. Moreover the generative component remains a good model of the input features despite the hybrid optimization objective. This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning. The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.",2019-05-29,2022-01-30 4:52:38,2022-01-30 4:52:38,2019-12-16 20:32:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1902.02767,,/Users/jacquesthibodeau/Zotero/storage/VUE9U2IJ/Nalisnick et al. - 2019 - Hybrid Models with Deep and Invertible Features.pdf; /Users/jacquesthibodeau/Zotero/storage/V56US5SD/1902.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2019,,,,,,,,,,,,,,,,
2BNK79XQ,conferencePaper,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Do Deep Generative Models Know What They Don't Know?,"arXiv:1810.09136 [cs, stat]",,,,http://arxiv.org/abs/1810.09136,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",2019-02-24,2022-01-30 4:52:38,2022-01-30 4:52:38,2019-12-16 20:34:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 333  J: 130 arXiv: 1810.09136,,/Users/jacquesthibodeau/Zotero/storage/DKICH6Q7/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf; /Users/jacquesthibodeau/Zotero/storage/U8767CAD/1810.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
6MVP5BIM,conferencePaper,2017,"Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario",Deep reinforcement learning from human preferences,Advances in Neural Information Processing Systems 30 (NIPS 2017),,,,https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",2017-07-13,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000517  arXiv: 1706.03741,,/Users/jacquesthibodeau/Zotero/storage/64K9P9TW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/73TQPCTW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/CZG2MJHH/1706.html; /Users/jacquesthibodeau/Zotero/storage/DBXZUTUS/1706.html; /Users/jacquesthibodeau/Zotero/storage/2SRIU7JU/1706.html,,TechSafety; Open-AI; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2017,,,,,,,,,,,,,,,,
4XQSZS4G,conferencePaper,2020,"Krakovna, Victoria; Orseau, Laurent; Ngo, Richard; Martic, Miljan; Legg, Shane",Avoiding Side Effects By Considering Future Tasks,,,,,https://arxiv.org/abs/2010.07877v1,"Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.",2020-10-15,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-11-14 1:21:10,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/GD35BH4A/Krakovna et al. - 2020 - Avoiding Side Effects By Considering Future Tasks.pdf; /Users/jacquesthibodeau/Zotero/storage/JGBTUMMS/2010.html; /Users/jacquesthibodeau/Zotero/storage/ZKJFR6KA/2010.html,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
4C7IP8P3,conferencePaper,2019,"Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper",Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift,"Advances in Neural Information Processing Systems, 2019",,,,http://arxiv.org/abs/1906.02530,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.",2019-06-06,2022-01-30 4:52:37,2022-01-30 4:52:37,2019-12-16 20:32:03,,,,,,,Can You Trust Your Model's Uncertainty?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000558  arXiv: 1906.02530,,/Users/jacquesthibodeau/Zotero/storage/TVUXA7PD/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf; /Users/jacquesthibodeau/Zotero/storage/E986QDBI/1906.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
N3S2NM97,conferencePaper,2016,"Everitt, Tom; Hutter, Marcus",Avoiding Wireheading with Value Reinforcement Learning,AGI 2016: Artificial General Intelligence,,,,http://arxiv.org/abs/1605.03143,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",2016-05-10,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-11-21 17:36:45,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000033  arXiv: 1605.03143,,/Users/jacquesthibodeau/Zotero/storage/RR5BW54G/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8GNCR3DI/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8USE4KKA/1605.html; /Users/jacquesthibodeau/Zotero/storage/FHW8U7HU/1605.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
R26NQ63Z,conferencePaper,2019,"Hendrycks, Dan; Mu, Norman; Cubuk, Ekin D.; Zoph, Barret; Gilmer, Justin; Lakshminarayanan, Balaji",AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,"arXiv:1912.02781 [cs, stat]",,,,http://arxiv.org/abs/1912.02781,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",2019-12-05,2022-01-30 4:52:37,2022-01-30 4:52:37,2019-12-16 20:30:48,,,,,,,AugMix,,,,,,,,,,,,arXiv.org,,ZSCC: 0000301  arXiv: 1912.02781,,/Users/jacquesthibodeau/Zotero/storage/CZEG4CV8/Hendrycks et al. - 2019 - AugMix A Simple Data Processing Method to Improve.pdf; /Users/jacquesthibodeau/Zotero/storage/9AGD2DPA/1912.html,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
CPPZDHR7,conferencePaper,2018,"Dvijotham, Krishnamurthy; Stanforth, Robert; Gowal, Sven; Mann, Timothy; Kohli, Pushmeet",A Dual Approach to Scalable Verification of Deep Networks,,,,,http://auai.org/uai2018/proceedings/papers/204.pdf,"This paper addresses the problem of formally verifying desirable properties of neural networks, i.e., obtaining provable guarantees that neural networks satisfy specifications relating their inputs and outputs (robustness to bounded norm adversarial perturbations, for example). Most previous work on this topic was limited in its applicability by the size of the network, network architecture and the complexity of properties to be verified. In contrast, our framework applies to a general class of activation functions and specifications on neural network inputs and outputs. We formulate verification as an optimization problem (seeking to find the largest violation of the specification) and solve a Lagrangian relaxation of the optimization problem to obtain an upper bound on the worst case violation of the specification being verified. Our approach is anytime i.e. it can be stopped at any time and a valid bound on the maximum violation can be obtained. We develop specialized verification algorithms with provable tightness guarantees under special assumptions and demonstrate the practical significance of our general verification approach on a variety of verification tasks.",2018-08-03,2022-01-30 4:52:36,2022-01-30 4:52:36,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000274  arXiv: 1803.06567,,/Users/jacquesthibodeau/Zotero/storage/JI7PP7JX/Krishnamurthy et al. - 2018 - A Dual Approach to Scalable Verification of Deep N.pdf; /Users/jacquesthibodeau/Zotero/storage/WD27UPBX/1803.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
DJDTSQA3,conferencePaper,2019,"Qin, Chongli; Martens, James; Gowal, Sven; Krishnan, Dilip; Dvijotham, Krishnamurthy; Fawzi, Alhussein; De, Soham; Stanforth, Robert; Kohli, Pushmeet",Adversarial Robustness through Local Linearization,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://proceedings.neurips.cc/paper/2019/hash/0defd533d51ed0a10c5c9dbf93ee78a5-Abstract.html,"Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with l-infinity adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.",2019-10-10,2022-01-30 4:52:36,2022-01-30 4:52:36,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000145  arXiv: 1907.02610,,/Users/jacquesthibodeau/Zotero/storage/KJHF9RJB/Qin et al. - 2019 - Adversarial Robustness through Local Linearization.pdf; /Users/jacquesthibodeau/Zotero/storage/QRC2SPTZ/1907.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
PHTMMSBH,conferencePaper,2018,"Uesato, Jonathan; O'Donoghue, Brendan; Oord, Aaron van den; Kohli, Pushmeet",Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.05666,"This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.",2018-06-12,2022-01-30 4:52:36,2022-01-30 4:52:36,2019-12-16 20:35:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000335  arXiv: 1802.05666,,/Users/jacquesthibodeau/Zotero/storage/HKXXJPHG/Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf; /Users/jacquesthibodeau/Zotero/storage/QXA64E5S/1802.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th International Conference on Machine Learning,,,,,,,,,,,,,,,,
FT98MHA9,conferencePaper,2019,"Huang, Po-Sen; Stanforth, Robert; Welbl, Johannes; Dyer, Chris; Yogatama, Dani; Gowal, Sven; Dvijotham, Krishnamurthy; Kohli, Pushmeet",Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation,"arXiv:1909.01492 [cs, stat]",,,,http://arxiv.org/abs/1909.01492,"Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation -- a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.",2019-09-03,2022-01-30 4:52:36,2022-01-30 4:52:36,2019-12-16 20:31:13,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000074  arXiv: 1909.01492,,/Users/jacquesthibodeau/Zotero/storage/SB3JWI7M/Huang et al. - 2019 - Achieving Verified Robustness to Symbol Substituti.pdf; /Users/jacquesthibodeau/Zotero/storage/UJR69ER4/1909.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,EMNLP 2019,,,,,,,,,,,,,,,,
36ZPFWSH,conferencePaper,2018,"Ryffel, Theo; Trask, Andrew; Dahl, Morten; Wagner, Bobby; Mancuso, Jason; Rueckert, Daniel; Passerat-Palmbach, Jonathan",A generic framework for privacy preserving deep learning,"arXiv:1811.04017 [cs, stat]",,,,http://arxiv.org/abs/1811.04017,"We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.",2018-11-13,2022-01-30 4:52:36,2022-01-30 4:52:36,2019-12-16 20:34:29,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000218  arXiv: 1811.04017,,/Users/jacquesthibodeau/Zotero/storage/5HC3K36N/Ryffel et al. - 2018 - A generic framework for privacy preserving deep le.pdf; /Users/jacquesthibodeau/Zotero/storage/QQWKKRKX/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PPML 2018,,,,,,,,,,,,,,,,
54TFE9DS,conferencePaper,2021,"Oesterheld, Caspar; Conitzer, Vincent",Safe Pareto Improvements for Delegated Game Playing,Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems,,,,,"A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.",2021,2022-01-30 4:51:37,2022-01-30 4:51:37,,17,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/U9S7EJHV/Oesterheld and Conitzer - 2021 - Safe Pareto Improvements for Delegated Game Playin.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAMAS 2021,,,,,,,,,,,,,,,,
HH7IJ84S,conferencePaper,2015,"Garrabrant, Scott; Bhaskar, Siddharth; Demski, Abram; Garrabrant, Joanna; Koleszarik, George; Lloyd, Evan",Asymptotic Logical Uncertainty and The Benford Test,Artificial General Intelligence. AGI 2016,,,,http://arxiv.org/abs/1510.03370,"We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs ""true"" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.",2015-10-12,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-12-13 20:21:31,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 7  arXiv: 1510.03370,,/Users/jacquesthibodeau/Zotero/storage/FMECSHD8/Garrabrant et al. - 2015 - Asymptotic Logical Uncertainty and The Benford Tes.pdf; /Users/jacquesthibodeau/Zotero/storage/74HXXTRR/1510.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; F.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
UBDUGMDK,conferencePaper,2016,"Leike, Jan; Taylor, Jessica; Fallenstein, Benya",A Formal Solution to the Grain of Truth Problem,,,,,,"A Bayesian agent acting in a multi-agent environment learns to predict the other agents’ policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play ε-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.",2016,2022-01-30 4:56:46,2022-01-30 4:56:46,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/IKQJTV93/Leike et al. - A Formal Solution to the Grain of Truth Problem.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
Z75EWDEP,conferencePaper,2018,"Baum, Seth; Barrett, Anthony M",Towards an Integrated Assessment of Global Catastrophic Risk,"Catastrophic and Existential Risk: Proceedings of the First Colloquium, Garrick Institute for the Risk Sciences, University of California, Los Angeles, Forthcoming",,,,,,2018-01-17,2022-01-30 4:55:20,2022-01-30 4:55:20,,18,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/JJ2WQ35R/Baum and Barrett - 2017 - Towards an integrated assessment of global catastr.pdf; /Users/jacquesthibodeau/Zotero/storage/BQ3ZMFE4/papers.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,First International Colloquium on Catastrophic and Existential Risk,,,,,,,,,,,,,,,,
PA537GRN,conferencePaper,2021,"Owe, Andrea; Baum, Seth",The Ethics of Sustainability for Artificial Intelligence,"Proceedings of AI for People: Towards Sustainable AI, CAIP’21.",,,,https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/,"Sustainability is widely considered a good thing and is therefore a matter of ethical significance. This paper analyzes the ethical dimensions of existing work on AI and sustainability, finding that most of it is focused on sustaining the environment for human benefit. The paper calls for sustainability that is not human-centric and that extends into the distant future, especially for advanced future AI as a technology that can advance expansion beyond Earth.",2021,2022-01-30 4:55:20,2022-01-30 4:55:20,2021-12-11 14:23:38,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/638RA9I3/the-ethics-of-sustainability-for-artificial-intelligence.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CAIP'21,,,,,,,,,,,,,,,,
C95SMCT7,conferencePaper,2019,"Sarafian, Elad; Tamar, Aviv; Kraus, Sarit",Constrained Policy Improvement for Safe and Efficient Reinforcement Learning,Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1805.07805,"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",2019-07-10,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-11-14 0:52:37,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1805.07805,,/Users/jacquesthibodeau/Zotero/storage/DC967S4Q/Sarafian et al. - 2019 - Constrained Policy Improvement for Safe and Effici.pdf; /Users/jacquesthibodeau/Zotero/storage/KNEPSHIS/1805.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Ninth International Joint Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
V49WBQ7S,conferencePaper,2020,"Brown, Noam; Bakhtin, Anton; Lerer, Adam; Gong, Qucheng",Combining Deep Reinforcement Learning and Search for Imperfect-Information Games,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2007.13544,"The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of a successes in single-agent settings and perfect-information games, best exemplified by the success of AlphaZero. However, algorithms of this form have been unable to cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search for imperfect-information games. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results show ReBeL leads to low exploitability in benchmark imperfect-information games and achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI. We also prove that ReBeL converges to a Nash equilibrium in two-player zero-sum games in tabular settings.",2020-07-27,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-09-07 18:53:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 2007.13544,,/Users/jacquesthibodeau/Zotero/storage/QXI58ND2/Brown et al. - 2020 - Combining Deep Reinforcement Learning and Search f.pdf; /Users/jacquesthibodeau/Zotero/storage/C8ZNKWTE/2007.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
8826ECUU,conferencePaper,2018,"Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy",Certified Defenses against Adversarial Examples,,,,,http://arxiv.org/abs/1801.09344,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.",2018,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-12-13 23:31:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000653  JCC: 455 arXiv: 1801.09344,,/Users/jacquesthibodeau/Zotero/storage/INV72AUS/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/89JZRJGM/1801.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Learning Representations 2018,,,,,,,,,,,,,,,,
59IA3QC5,conferencePaper,2021,"Lindner, David; Matoba, Kyle; Meulemans, Alexander",Challenges for Using Impact Regularizers to Avoid Negative Side Effects,arXiv:2101.12509 [cs],,,,http://arxiv.org/abs/2101.12509,"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",2021-02-23,2022-01-30 4:59:44,2022-01-30 4:59:44,2021-11-13 22:39:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2101.12509,,/Users/jacquesthibodeau/Zotero/storage/UPVI5Q45/Lindner et al. - 2021 - Challenges for Using Impact Regularizers to Avoid .pdf; /Users/jacquesthibodeau/Zotero/storage/SMIIU3CE/2101.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
U8P2DUZ2,conferencePaper,2020,"Everett, Michael; Lutjens, Bjorn; How, Jonathan P.",Certified Adversarial Robustness for Deep Reinforcement Learning,"3rd Conference on Robot Learning (CoRL 2019),",,,,http://arxiv.org/abs/2004.06496,"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",2020-08-21,2022-01-30 4:59:43,2022-01-30 4:59:43,2020-08-31 17:29:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2004.06496,,/Users/jacquesthibodeau/Zotero/storage/GJDIZKHK/Everett et al. - 2020 - Certified Adversarial Robustness for Deep Reinforc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZTVVXNMQ/2004.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"3rd Conference on Robot Learning (CoRL 2019),",,,,,,,,,,,,,,,,
IHTBF4TH,conferencePaper,2018,"Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang",Building Ethics into Artificial Intelligence,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18),,,,http://arxiv.org/abs/1812.02953,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",2018-12-07,2022-01-30 4:59:37,2022-01-30 4:59:37,2020-11-14 0:53:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000128  arXiv: 1812.02953,,/Users/jacquesthibodeau/Zotero/storage/P4HD9IST/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/SZNMZP8A/1812.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18),,,,,,,,,,,,,,,,
G7KMKWHX,conferencePaper,2019,"Fisac, Jaime F.; Lugovoy, Neil F.; Rubies-Royo, Vicenc; Ghosh, Shromona; Tomlin, Claire J.",Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning,2019 International Conference on Robotics and Automation (ICRA),978-1-5386-6027-0,,10.1109/ICRA.2019.8794107,https://ieeexplore.ieee.org/document/8794107/,"Safety analysis is a necessary component in the design and deployment of autonomous systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to ﬁnd approximate yet proﬁcient solutions to optimal control problems in complex and high-dimensional systems, however their formulation is restricted to problems with an additive payoff (reward) over time, unsuitable for reasoning about safety. In recent work, we proved that the problem of maximizing the minimum payoff over time, central to safety analysis, can be time-discounted to induce a contraction mapping. Here, we introduce a novel, timediscounted Safety Bellman Equation that renders reinforcement learning techniques amenable to quantitative safety analysis, enabling them to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting controltheoretic safety analysis and the reinforcement learning domain. We demonstrate our formulation on a variety of simulated robotics tasks and reinforcement learning schemes, validating our results against analytic and numerical solutions when these can be obtained, and showing scalability to previously intractable problems of up to 18 state dimensions by exploiting state-of-the-art deep reinforcement learning algorithms.",2019-05,2022-01-30 4:59:37,2022-01-30 4:59:37,2020-11-14 1:15:15,8550-8556,,,,,,,,,,,IEEE,"Montreal, QC, Canada",en,,,,,DOI.org (Crossref),,ZSCC: 0000033,,/Users/jacquesthibodeau/Zotero/storage/XFP8J2JX/Fisac et al. - 2019 - Bridging Hamilton-Jacobi Safety Analysis and Reinf.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
7W6QE66X,conferencePaper,2018,"Rossi, Francesca; Mattei, Nicholas",Building Ethically Bounded AI,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1812.03980,"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI's freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven example-based approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.",2018-12-10,2022-01-30 4:59:37,2022-01-30 4:59:37,2020-11-14 0:31:20,,,,33,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 36  arXiv: 1812.03980,,/Users/jacquesthibodeau/Zotero/storage/FKZA6ZVE/Rossi and Mattei - 2018 - Building Ethically Bounded AI.pdf; /Users/jacquesthibodeau/Zotero/storage/TKRZ5565/1812.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
IWME7BT6,conferencePaper,2020,"Chen, Ting; Kornblith, Simon; Swersky, Kevin; Norouzi, Mohammad; Hinton, Geoffrey",Big Self-Supervised Models are Strong Semi-Supervised Learners,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2006.10029,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",2020-06-17,2022-01-30 4:59:37,2022-01-30 4:59:37,2020-08-31 18:02:23,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000505  arXiv: 2006.10029,,/Users/jacquesthibodeau/Zotero/storage/W93U7VHU/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
9SVVUE8N,conferencePaper,2019,"Hendrycks, Dan; Dietterich, Thomas",Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,,,,,http://arxiv.org/abs/1903.12261,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",2019-03-28,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-12-22 23:29:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000898  arXiv: 1903.12261,,/Users/jacquesthibodeau/Zotero/storage/6IRJBZUB/Hendrycks and Dietterich - 2019 - Benchmarking Neural Network Robustness to Common C.pdf; /Users/jacquesthibodeau/Zotero/storage/XSJQR645/1903.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
A4SXFVC7,conferencePaper,2016,"Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses",Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction,Advances in Neural Information Processing Systems 29 (NIPS 2016),,,,http://arxiv.org/abs/1606.05374,"We consider a crowdsourcing model in which $n$ workers are asked to rate the quality of $n$ items previously generated by other workers. An unknown set of $\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an $\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with $n$: the dataset can be curated with $\tilde{O}\Big(\frac{1}{\beta\alpha^3\epsilon^4}\Big)$ ratings per worker, and $\tilde{O}\Big(\frac{1}{\beta\epsilon^2}\Big)$ ratings by the manager, where $\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.",2016-06-16,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-12-13 19:48:09,,,,,,,Avoiding Imposters and Delinquents,,,,,,,,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1606.05374,,/Users/jacquesthibodeau/Zotero/storage/AIQHAJ8W/Steinhardt et al. - 2016 - Avoiding Imposters and Delinquents Adversarial Cr.pdf; /Users/jacquesthibodeau/Zotero/storage/CHB5HPQE/1606.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Computer Science and Game Theory; Computer Science - Data Structures and Algorithms; Computer Science - Human-Computer Interaction; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9JCKE87,conferencePaper,2020,"Agarwal, Rishabh; Schuurmans, Dale; Norouzi, Mohammad",An Optimistic Perspective on Offline Reinforcement Learning,"arXiv:1907.04543 [cs, stat]",,,,http://arxiv.org/abs/1907.04543,"Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.",2020-06-22,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-08-28 17:35:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000137  arXiv: 1907.04543,,/Users/jacquesthibodeau/Zotero/storage/I8BUCNET/Agarwal et al. - 2020 - An Optimistic Perspective on Offline Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/4UQ64FEK/1907.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
XGF52239,conferencePaper,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1805.01109,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-21,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-11-15 2:50:45,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000076  arXiv: 1805.01109,,/Users/jacquesthibodeau/Zotero/storage/ECXX8QH5/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/N7U9FKW6/1805.html; /Users/jacquesthibodeau/Zotero/storage/3E7Q6G7R/Everitt et al. - 2018 - AGI Safety Literature Review.pdf,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PM74T3QP,conferencePaper,2020,"Fazelpour, Sina; Lipton, Zachary C.",Algorithmic Fairness from a Non-ideal Perspective,"arXiv:2001.09773 [cs, stat]",,,,http://arxiv.org/abs/2001.09773,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",2020-01-08,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-09-05 16:51:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029  arXiv: 2001.09773,,/Users/jacquesthibodeau/Zotero/storage/X2KWRS4A/Fazelpour and Lipton - 2020 - Algorithmic Fairness from a Non-ideal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/3W2M84GS/2001.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society 2020",,,,,,,,,,,,,,,,
DSCR47GU,conferencePaper,2018,"Sarma, Gopal P.; Hay, Nick J.; Safron, Adam",AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values,Lecture Notes in Computer Science,,,10.1007/978-3-319-99229-7_45,http://arxiv.org/abs/1712.04307,We propose the creation of a systematic effort to identify and replicate key findings in neuropsychology and allied fields related to understanding human values. Our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of AI systems.,2018,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-12-13 23:39:16,507-512,,,11088,,,AI Safety and Reproducibility,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 1712.04307,,/Users/jacquesthibodeau/Zotero/storage/X9Q6DXIZ/Sarma et al. - 2018 - AI Safety and Reproducibility Establishing Robust.pdf; /Users/jacquesthibodeau/Zotero/storage/RKKBXEHQ/1712.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Computer Safety, Reliability, and Security (SAFECOMP 2018)",,,,,,,,,,,,,,,,
K4CP542W,conferencePaper,2019,"Baumann, Tobias; Graepel, Thore; Shawe-Taylor, John",Adaptive Mechanism Design: Learning to Promote Cooperation,2020 International Joint Conference on Neural Networks (IJCNN),,,,http://arxiv.org/abs/1806.04067,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.",2019-11-20,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-14 0:41:13,,,,,,,Adaptive Mechanism Design,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 10  arXiv: 1806.04067,,/Users/jacquesthibodeau/Zotero/storage/KMTJBPT6/Baumann et al. - 2019 - Adaptive Mechanism Design Learning to Promote Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/4RMMHPN9/1806.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 International Joint Conference on Neural Networks (IJCNN),,,,,,,,,,,,,,,,
Q5IKRD2K,conferencePaper,2018,"Behzadan, Vahid; Munir, Arslan; Yampolskiy, Roman V.",A Psychopathological Approach to Safety Engineering in AI and AGI,,,,,https://arxiv.org/abs/1805.08915v1,"The complexity of dynamics in AI techniques is already approaching that of complex adaptive systems, thus curtailing the feasibility of formal controllability and reachability analysis in the context of AI safety. It follows that the envisioned instances of Artificial General Intelligence (AGI) will also suffer from challenges of complexity. To tackle such issues, we propose the modeling of deleterious behaviors in AI and AGI as psychological disorders, thereby enabling the employment of psychopathological approaches to analysis and control of misbehaviors. Accordingly, we present a discussion on the feasibility of the psychopathological approaches to AI safety, and propose general directions for research on modeling, diagnosis, and treatment of psychological disorders in AGI.",2018-05-23,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-14 1:17:01,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000016,,/Users/jacquesthibodeau/Zotero/storage/ZNKQIBA9/Behzadan et al. - 2018 - A Psychopathological Approach to Safety Engineerin.pdf; /Users/jacquesthibodeau/Zotero/storage/KB6ZSVFC/1805.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Computer Safety, Reliability, and Security",,,,,,,,,,,,,,,,
HRBWN3RH,conferencePaper,2018,"Wu, Yueh-Hua; Lin, Shou-De",A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18),,,,http://arxiv.org/abs/1712.04172,"This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.",2018-09-10,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-12-13 23:53:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029   arXiv: 1712.04172,,/Users/jacquesthibodeau/Zotero/storage/X7R2DI2W/Wu and Lin - 2018 - A Low-Cost Ethics Shaping Approach for Designing R.pdf; /Users/jacquesthibodeau/Zotero/storage/GCCWAJE9/1712.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18),,,,,,,,,,,,,,,,
QMINGD6C,conferencePaper,2015,"Soares, Nate; Fallenstein, Benja",Toward Idealized Decision Theory,,,,,http://arxiv.org/abs/1507.01986,"This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.",2015-07-07,2022-01-30 4:57:32,2022-01-30 4:57:32,2019-12-19 2:58:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  0 J: 30 arXiv: 1507.01986,,/Users/jacquesthibodeau/Zotero/storage/X6T99AUG/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/QCHDAWP4/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/W66QDEEJ/1507.html; /Users/jacquesthibodeau/Zotero/storage/W9BGSZJX/1507.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AGI-2015,,,,,,,,,,,,,,,,
T8H27GXJ,conferencePaper,2017,"Achiam, Joshua; Held, David; Tamar, Aviv; Abbeel, Pieter",Constrained policy optimization,Proceedings of the 34th International Conference on Machine Learning,,,,,,2017,2022-01-30 4:57:26,2022-01-30 4:57:26,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000549,,/Users/jacquesthibodeau/Zotero/storage/SJSX5V9Z/Achiam et al. - 2017 - Constrained policy optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/ZKPZ3KVQ/1705.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4V8R5X9I,conferencePaper,2019,"Clark, Jack; Hadﬁeld, Gillian K",Regulatory Markets for AI Safety,,,,,https://arxiv.org/abs/2001.00078,We propose a new model for regulation to achieve AI safety: global regulatory markets. We ﬁrst sketch the model in general terms and provide an overview of the costs and beneﬁts of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones.,2019,2022-01-30 4:57:25,2022-01-30 4:57:25,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000020,,/Users/jacquesthibodeau/Zotero/storage/ZX28GC8Q/Clark and Hadfield - 2019 - Regulatory Markets for AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/JZ2QJ9K9/2001.html; /Users/jacquesthibodeau/Zotero/storage/G9J25559/Clark and Hadﬁeld - 2019 - REGULATORY MARKETS FOR AI SAFETY.pdf,,MetaSafety; Open-AI,Computer Science - Computers and Society; Economics - General Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Safe Machine Learning workshop at ICLR, 2019",,,,,,,,,,,,,,,,
2FGGZESV,conferencePaper,2020,"Stooke, Adam; Achiam, Joshua; Abbeel, Pieter",Responsive safety in reinforcement learning by pid lagrangian methods,International Conference on Machine Learning,,,,,,2020,2022-01-30 4:57:25,2022-01-30 4:57:25,,9133–9143,,,,,,,,,,,PMLR,,,,,,,Google Scholar,,ZSCC: 0000043,,/Users/jacquesthibodeau/Zotero/storage/WBNK9Z5V/Stooke et al. - 2020 - Responsive safety in reinforcement learning by pid.pdf; /Users/jacquesthibodeau/Zotero/storage/I5C469CU/stooke20a.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMSJ3UEH,conferencePaper,2015,"Soares, Nate",The Value Learning Problem,Ethics for Artificial Intelligence Workshop at 25th International Joint Conference on Artificial Intelligence,,,,https://intelligence.org/files/ValueLearningProblem.pdf,"Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its designers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.",2015,2022-01-30 4:56:59,2022-01-30 4:56:59,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000070  8 J: 38,,/Users/jacquesthibodeau/Zotero/storage/AAGIJGBD/Soares - The Value Learning Problem.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2016,,,,,,,,,,,,,,,,
DNQKCSRM,conferencePaper,2016,"Everitt, Tom; Filan, Daniel; Daswani, Mayank; Hutter, Marcus",Self-Modification of Policy and Utility Function in Rational Agents,AGI 2016: Artificial General Intelligence,,,,http://arxiv.org/abs/1605.03142,"Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.",2016-05-10,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-11-22 4:13:43,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1605.03142,,/Users/jacquesthibodeau/Zotero/storage/9EW4V9AK/Everitt et al. - 2016 - Self-Modification of Policy and Utility Function i.pdf; /Users/jacquesthibodeau/Zotero/storage/9EXT3XCU/1605.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
CDZZ9WDE,conferencePaper,2015,"Everitt, Tom; Leike, Jan; Hutter, Marcus",Sequential Extensions of Causal and Evidential Decision Theory,ADT 2015: Algorithmic Decision Theory,,,,http://arxiv.org/abs/1506.07359,"Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.",2015-06-24,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-11-22 4:14:19,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 1506.07359,,/Users/jacquesthibodeau/Zotero/storage/ZPQ2R9HA/Everitt et al. - 2015 - Sequential Extensions of Causal and Evidential Dec.pdf; /Users/jacquesthibodeau/Zotero/storage/IXVQQIQD/1506.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Algorithmic Decision Theory,,,,,,,,,,,,,,,,
KSJ8QJE3,conferencePaper,2016,"Taylor, Jessica",Quantilizers: A safer alternative to maximizers for limited optimization,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 4:56:58,2022-01-30 4:56:58,,,,,,,,Quantilizers,,,,,,,,,,,,Google Scholar,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/27JB47GD/Taylor - 2016 - Quantilizers A safer alternative to maximizers fo.pdf; /Users/jacquesthibodeau/Zotero/storage/4JD5H2IP/12613.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2SEFD3C9,conferencePaper,2011,"Dewey, Daniel",Learning What to Value,Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,10.1007/978-3-642-22887-2_35,http://link.springer.com/10.1007/978-3-642-22887-2_35,,2011,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-01-23 20:28:26,309-314,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 61  Series Title: Lecture Notes in Computer Science,,,,,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMAISE9S,conferencePaper,2016,"Benson-Tilsen, Tsvi; Soares, Nate",Formalizing convergent instrumental goals,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 4:56:48,2022-01-30 4:56:48,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000012,,/Users/jacquesthibodeau/Zotero/storage/ZC55WH97/Benson-Tilsen and Soares - Formalizing Convergent Instrumental Goals.pdf; /Users/jacquesthibodeau/Zotero/storage/ISW5PA2B/12634.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T9KCWKUD,conferencePaper,2019,"Kosoy, Vanessa",Delegative Reinforcement Learning: Learning To Avoid Traps With A Little Help,,,,,,"Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.",2019,2022-01-30 4:56:48,2022-01-30 4:56:48,,22,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/NF24GTKM/Kosoy - 2019 - Delegative Reinforcement Learning learning to avo.pdf; /Users/jacquesthibodeau/Zotero/storage/U2UA962U/Kosoy - 2019 - DELEGATIVE REINFORCEMENT LEARNING LEARN- ING TO A.pdf,,TechSafety; MIRI,Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6; 68Q32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeML ICLR 2019 Workshop,,,,,,,,,,,,,,,,
WXU7767D,conferencePaper,2016,"Sotala, Kaj",Defining human values for value learners,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 4:56:48,2022-01-30 4:56:48,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000019,,/Users/jacquesthibodeau/Zotero/storage/BJAT2I48/Sotala - 2016 - Defining human values for value learners.pdf; /Users/jacquesthibodeau/Zotero/storage/4NMRB9Z4/12633.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2JZ8J6BZ,conferencePaper,2010,"Salamon, Anna; Rayhawk, Stephen; Kramár, János",How Intelligible is Intelligence,ECAP10: VIII European Conference on Computing and Philosophy,,,,https://intelligence.org/files/HowIntelligible.pdf,"If human-level AI is eventually created, it may have unprecedented positive or negative consequences for humanity. It is therefore worth constructing the best possible forecasts of policy-relevant aspects of AI development trajectories—even though, at this early stage, the unknowns must remain very large.",2010,2022-01-30 4:56:48,2022-01-30 4:56:48,2021-01-23 20:19:59,8,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/SR3BV52U/HowIntelligible.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X6FCKUH7,conferencePaper,2017,"Conitzer, Vincent; Sinnott-Armstrong, Walter; Borg, Jana Schaich; Deng, Yuan; Kramer, Max",Moral Decision Making Frameworks for Artificial Intelligence,"AAAI Workshops, 2017",,,,http://moralai.cs.duke.edu/documents/mai_docs/moralAAAI17.pdf,"The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for ﬁnding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.",2017,2022-01-30 5:00:02,2022-01-30 5:00:02,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000139,,/Users/jacquesthibodeau/Zotero/storage/SK7EWVE3/Conitzer et al. - Moral Decision Making Frameworks for Artificial In.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI Workshops, 2017",,,,,,,,,,,,,,,,
GKGQKVFM,conferencePaper,2018,"Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang",Learning Safe Policies with Expert Guidance,Advances in Neural Information Processing Systems 31 (NeurIPS 2018),,,,http://arxiv.org/abs/1805.08313,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",2018-11-21,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-11-14 0:44:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000018  arXiv: 1805.08313,,/Users/jacquesthibodeau/Zotero/storage/M5XBAMST/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/KWA7CVFJ/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/VJPZVNU8/1805.html; /Users/jacquesthibodeau/Zotero/storage/PHMBRHUV/1805.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,
GWGXUC5S,conferencePaper,2019,"Lerer, Adam; Peysakhovich, Alexander",Learning Existing Social Conventions via Observationally Augmented Self-Play,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1806.10071,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",2019-03-13,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-11-14 0:38:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000021  arXiv: 1806.10071,,/Users/jacquesthibodeau/Zotero/storage/7FSUZWAU/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observati.pdf; /Users/jacquesthibodeau/Zotero/storage/UTVCDMHZ/1806.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
IW9U6MXI,conferencePaper,2018,"Koller, Torsten; Berkenkamp, Felix; Turchetta, Matteo; Krause, Andreas",Learning-based Model Predictive Control for Safe Exploration,2018 IEEE Conference on Decision and Control (CDC),,,,http://arxiv.org/abs/1803.08287,"Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.",2018-11-07,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-12-13 23:12:15,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000224  arXiv: 1803.08287,,/Users/jacquesthibodeau/Zotero/storage/AWZVSTAT/Koller et al. - 2018 - Learning-based Model Predictive Control for Safe E.pdf; /Users/jacquesthibodeau/Zotero/storage/DBTEAK74/1803.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE Conference on Decision and Control (CDC),,,,,,,,,,,,,,,,
DFV3R6X3,conferencePaper,2019,"Sarma, Gopal P.; Safron, Adam; Hay, Nick J.","Integrative Biological Simulation, Neuropsychology, and AI Safety",Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1811.03493,"We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.",2019-01-21,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-11-14 0:58:15,,,,,,,,,,,,,Honolulu HI USA,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1811.03493,,"/Users/jacquesthibodeau/Zotero/storage/6GB52FT9/Sarma et al. - 2019 - Integrative Biological Simulation, Neuropsychology.pdf; /Users/jacquesthibodeau/Zotero/storage/K7JEEMCE/1811.html",,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Workshop on Artificial Intelligence Safety,,,,,,,,,,,,,,,,
TNT8IZVP,conferencePaper,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),SafeAI 2019: Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-11-14 0:58:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 25  arXiv: 1901.00064,,/Users/jacquesthibodeau/Zotero/storage/F97PFGRR/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf; /Users/jacquesthibodeau/Zotero/storage/BWXENTM8/1901.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
394DPAGJ,conferencePaper,2011,"Halpern, Joseph Y.; Pass, Rafael",I Don't Want to Think About it Now: Decision Theory With Costly Computation,Proceedings of the Twelfth International Conference on Principles of Knowledge Representation and Reasoning,,,,http://arxiv.org/abs/1106.2657,"Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and and computational value of conversation.",2011-06-14,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-11-22 2:29:10,,,,,,,I Don't Want to Think About it Now,,,,,,,,,,,,arXiv.org,,ZSCC: 0000019[s0]  arXiv: 1106.2657,,/Users/jacquesthibodeau/Zotero/storage/CWHKI6EW/Halpern and Pass - 2011 - I Don't Want to Think About it NowDecision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/H22KIAM5/1106.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKNWFMQS,conferencePaper,2020,"Tsipras, Dimitris; Santurkar, Shibani; Engstrom, Logan; Ilyas, Andrew; Madry, Aleksander",From ImageNet to Image Classification: Contextualizing Progress on Benchmarks,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2005.11295,"Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.",2020-05-22,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-08-31 18:21:48,,,,,,,From ImageNet to Image Classification,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000041  arXiv: 2005.11295,,/Users/jacquesthibodeau/Zotero/storage/HTPNAJFE/Tsipras et al. - 2020 - From ImageNet to Image Classification Contextuali.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
FS9XWNGD,conferencePaper,2014,"Tegmark, Max",Friendly Artificial Intelligence: the Physics Challenge,Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop,,,,http://arxiv.org/abs/1409.0813,"Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent ""Friendly AI"", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is ""meaning"" in a particle arrangement? What is ""life""? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.",2014-09-03,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-11-22 2:29:59,,,,,,,Friendly Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006[s0]  arXiv: 1409.0813,,/Users/jacquesthibodeau/Zotero/storage/D2KARUXE/Tegmark - 2014 - Friendly Artificial Intelligence the Physics Chal.pdf; /Users/jacquesthibodeau/Zotero/storage/8TK9V4IJ/1409.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBI9D6CJ,conferencePaper,2011,"Alur, Rajeev",Formal verification of hybrid systems,Proceedings of the ninth ACM international conference on Embedded software - EMSOFT '11,978-1-4503-0714-7,,10.1145/2038642.2038685,http://dl.acm.org/citation.cfm?doid=2038642.2038685,,2011,2022-01-30 4:59:55,2022-01-30 4:59:55,2020-11-22 1:47:17,273,,,,,,,,,,,ACM Press,"Taipei, Taiwan",en,,,,,DOI.org (Crossref),,ZSCC: 0000262,,/Users/jacquesthibodeau/Zotero/storage/ED2K2TEU/Alur - 2011 - Formal verification of hybrid systems.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the ninth ACM international conference,,,,,,,,,,,,,,,,
BJ8QESWH,conferencePaper,2020,"Atrey, Akanksha; Clary, Kaleigh; Jensen, David",Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1912.05743,"Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsiﬁable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",2020-02-20,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-08-31 18:36:43,,,,,,,Exploratory Not Explanatory,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 35  arXiv: 1912.05743,,/Users/jacquesthibodeau/Zotero/storage/MIXFX52G/Atrey et al. - 2020 - Exploratory Not Explanatory Counterfactual Analys.pdf,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
XWS9JVUI,conferencePaper,2020,"Hase, Peter; Bansal, Mohit",Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,,,,http://arxiv.org/abs/2005.01831,"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020",2020-05-04,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-08-31 18:40:23,,,,,,,Evaluating Explainable AI,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000066  arXiv: 2005.01831,,/Users/jacquesthibodeau/Zotero/storage/ATRH8PB2/Hase and Bansal - 2020 - Evaluating Explainable AI Which Algorithmic Expla.pdf,,MetaSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL 2020,,,,,,,,,,,,,,,,
DJNA82X8,conferencePaper,2020,"Guan, Lin; Verma, Mudit; Kambhampati, Subbarao",Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning,,,,,http://arxiv.org/abs/2006.14804,"Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guidance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. The usual human guidance in HRL is binary evaluative ""good"" or ""bad"" signal for queried states and actions. However, this suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (Explanation Augmented Feedback) which allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback. EXPAND employs a state perturbation approach based on the state salient information to augment the feedback, reducing the number of human feedback signals required. We choose two domains to evaluate this approach, Taxi and Atari-Pong. We demonstrate the effectiveness of our method on three metrics, environment sample efficiency, human feedback sample efficiency, and agent gaze. We show that our method outperforms our baselines. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information gives a boost in performance.",2020-07-16,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-08-28 17:26:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2006.14804,,/Users/jacquesthibodeau/Zotero/storage/FSDFWXIT/Guan et al. - 2020 - Explanation Augmented Feedback in Human-in-the-Loo.pdf; /Users/jacquesthibodeau/Zotero/storage/3STKCI5F/2006.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
HEBSUXVE,conferencePaper,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracking Gradient Descent,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TrackIn that computes the influence of a training example on a prediction made by the model, by tracking how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TrackIn via a combination of a few key ideas: (a) a first-order approximation to the exact computation, (b) using random projections to speed up the computation of the first-order approximation for large models, (c) using saved checkpoints of standard training procedures, and (d) cherry-picking layers of a deep neural network. An experimental evaluation shows that TrackIn is more effective in identifying mislabelled training examples than other related methods such as influence functions and representer points. We also discuss insights from applying the method on vision, regression and natural language tasks.",2020-07-13,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-09-05 17:05:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002[s0]  arXiv: 2002.08484,,/Users/jacquesthibodeau/Zotero/storage/Q78TZ27M/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracking Gra.pdf; /Users/jacquesthibodeau/Zotero/storage/6AZ6XQXS/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
Q7GNR4H4,conferencePaper,2018,"Behzadan, Vahid; Yampolskiy, Roman V.; Munir, Arslan",Emergence of Addictive Behaviors in Reinforcement Learning Agents,Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1811.05590,"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.",2018-11-13,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-11-14 1:10:06,,,,,,,,,,,,,Honolulu HI USA,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 1811.05590,,/Users/jacquesthibodeau/Zotero/storage/THPJ9EUR/Behzadan et al. - 2018 - Emergence of Addictive Behaviors in Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/36B2C4IA/1811.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,
BVVDEU27,conferencePaper,2016,"Greene, Joshua; Rossi, Francesca; Tasioulas, John; Venable, Kristen Brent; Williams, Brian",Embedding Ethical Principles in Collective Decision Support Systems,,,,,,"The future will see autonomous machines acting in the same environment as humans, in areas as diverse as driving, assistive technology, and health care. Think of self-driving cars, companion robots, and medical diagnosis support systems. We also believe that humans and machines will often need to work together and agree on common decisions. Thus hybrid collective decision making systems will be in great need.",2016,2022-01-30 4:59:47,2022-01-30 4:59:47,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000050,,/Users/jacquesthibodeau/Zotero/storage/KQNS3RIQ/Greene et al. - Embedding Ethical Principles in Collective Decisio.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16),,,,,,,,,,,,,,,,
ITAG9EEW,conferencePaper,2019,"Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.",EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,http://arxiv.org/abs/1807.08364,"While imitation learning is often used in robotics, the approach frequently suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which attempts to quantify the confidence of the novice policy as a proxy for safety. Our method, EnsembleDAgger, approximates a Gaussian Process using an ensemble of neural networks. Using the variance as a measure of confidence, we compute a decision rule that captures how much we doubt the novice, thus determining when it is safe to allow the novice to act. With this approach, we aim to maximize the novice's share of actions, while constraining the probability of failure. We demonstrate improved safety and learning performance compared to other DAgger variants and classic imitation learning on an inverted pendulum and in the MuJoCo HalfCheetah environment.",2019-07-19,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-12-13 23:23:33,,,,,,,EnsembleDAgger,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023   arXiv: 1807.08364,,/Users/jacquesthibodeau/Zotero/storage/W3ZZIFIQ/Menda et al. - 2019 - EnsembleDAgger A Bayesian Approach to Safe Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/AXVEPNWH/1807.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
3JZGMVVU,conferencePaper,2017,"Mhamdi, El Mahdi El; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre",Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"arXiv:1704.02882 [cs, stat]",,,,http://arxiv.org/abs/1704.02882,"In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",2017-05-22,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-11-21 17:31:17,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 1704.02882,,/Users/jacquesthibodeau/Zotero/storage/AH67HTNI/Mhamdi et al. - 2017 - Dynamic Safe Interruptibility for Decentralized Mu.pdf; /Users/jacquesthibodeau/Zotero/storage/A66XSDKN/1704.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2017,,,,,,,,,,,,,,,,
BHA2PG7A,conferencePaper,2018,"Plisnier, Hélène; Steckelmacher, Denis; Brys, Tim; Roijers, Diederik M.; Nowé, Ann",Directed Policy Gradient for Safe Reinforcement Learning with Human Advice,,,,,http://arxiv.org/abs/1808.04096,"Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",2018-10,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-11-14 1:06:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1808.04096,,/Users/jacquesthibodeau/Zotero/storage/ZJPCJCQH/Plisnier et al. - 2018 - Directed Policy Gradient for Safe Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/PBWSCFC4/1808.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,European Workshop on Reinforcement Learning 14,,,,,,,,,,,,,,,,
3ACJ73AJ,conferencePaper,1998,"Hanson, Robin",Long-Term Growth as a Sequence of Exponential Modes,George Mason University,,,,,"A world product time series covering two million years is well fit by either a sum of four exponentials, or a constant elasticity of substitution (CES) combination of three exponential growth modes: “hunting, ” “farming, ” and “industry. ” The CES parameters suggest that farming substituted for hunting, while industry complemented farming, making the industrial revolution a smoother transition. Each mode grew world product by a factor of a few hundred, and grew a hundred times faster than its predecessor. This weakly suggests that within the next century a new mode might appear with a doubling time measured in days, not years.",1998,2022-03-10 22:07:02,2022-03-10 22:07:02,,9–3,,,,,,,,,,,,,,,,,,CiteSeer,,,,/Users/jacquesthibodeau/Zotero/storage/3SH8H2LB/Hanson - 1998 - Long-Term Growth as a Sequence of Exponential Mode.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FCKDRA3I,conferencePaper,2014,"Pecka, Martin; Svoboda, Tomas",Safe Exploration Techniques for Reinforcement Learning – An Overview,Modelling and Simulation for Autonomous Systems,978-3-319-13823-7,,10.1007/978-3-319-13823-7_31,,"We overview different approaches to safety in (semi)autonomous robotics. Particularly, we focus on how to achieve safe behavior of a robot if it is requested to perform exploration of unknown states. Presented methods are studied from the viewpoint of reinforcement learning, a partially-supervised machine learning method. To collect training data for this algorithm, the robot is required to freely explore the state space – which can lead to possibly dangerous situations. The role of safe exploration is to provide a framework allowing exploration while preserving safety. The examined methods range from simple algorithms to sophisticated methods based on previous experience or state prediction. Our overview also addresses the issues of how to define safety in the real-world applications (apparently absolute safety is unachievable in the continuous and random real world). In the conclusion we also suggest several ways that are worth researching more thoroughly.",2014,2022-03-10 22:21:51,2022-03-10 22:21:51,,357-375,,,,,,,Lecture Notes in Computer Science,,,,Springer International Publishing,Cham,en,,,,,Springer Link,,,,,,,policy search; reinforcement learning; Safe exploration,"Hodicky, Jan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISTP692N,conferencePaper,1996,"Piccione, Michele; Rubinstein, A.",On the Interpretation of Decision Problems with Imperfect Recall,TARK,,,10.1006/GAME.1997.0536,,"It is argued that extensive decision problems (extensive games with a single player) with imperfect recall suffer from major ambiguities in the interpretation of information sets and strategies, which allows for different kinds of analysis. In this paper it is argued that extensive decision problems (extensive games with a single player) with imperfect recall suffer from major ambiguities in the interpretation of information sets and strategies. This indeterminacy allows for different kinds of analysis. We address the following issues:    1. Randomization at information sets    2. Consistent beliefs    3. Time consistency of optimal plans    4. The multi-selves approach to decision making    We illustrate our discussion through an example that we call the absentminded driver paradox.    An individual is sitting late at night in a bar planning his midnight trip home. In order to get home he has to take the highway and get off at the second exit. Turning at the first exit leads into a disastrous area (payoff 0). Turning at the second exit yields the highest reward (payoff 4). If he continues beyond the second exit he will reach the end of the highway and find a hotel where he can spend the night (payoff 1). The driver is absentminded and is aware of this fact. When reaching an intersection he cannot tell whether it is the first or the second intersection and he cannot remember how many he has passed. While sitting at the bar, all he can do is to decide whether or not to exit at an intersection (we exclude at this stage the possibility that the decision maker can include random elements in his strategy).    Planning his trip at the bar, the decision maker must conclude that it is impossible for him to get home and he should not exit when he reaches an intersection. Thus, his optimal plan will lead him to spend the night at the hotel and yields a payoff of 1. Now, suppose that he reaches an intersection. Remembering his strategy he concludes that he is at the first intersection with probability 1/2. Then, reviewing his plan, he must conclude that it is optimal for him to leave the highway since it yields an expected payoff of 2. Thus, despite no new information and no change in his preferences, the decision maker is tempted to change his initial plan once he reaches an intersection!    We find this example paradoxical as it exhibits a conflict between two ways of reasoning. The first instructs the decision maker to follow his initial decision not to exit, as this is the optimal rule of behavior. The second leads him to optimize expected payoffs given his beliefs and to deviate from his initial decision.",1996,2022-03-10 22:30:02,2022-03-10 22:30:02,,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,https://www.semanticscholar.org/paper/On-the-Interpretation-of-Decision-Problems-with-Piccione-Rubinstein/dd03415f643088df006f9ad12e22f43a57adb552?p2df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K2CUU8HF,conferencePaper,2018,"Zhang, Shun; Durfee, Edmund H.; Singh, Satinder",Minimax-regret querying on side effects for safe optimality in factored Markov decision processes,Proceedings of the 27th International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,,,"As it achieves a goal on behalf of its human user, an autonomous agent's actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn't explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.",2018-07-13,2022-03-10 22:43:05,2022-03-10 22:43:05,2022-03-10,4867–4873,,,,,,,IJCAI'18,,,,AAAI Press,"Stockholm, Sweden",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDNB5XQT,conferencePaper,2020,"Tang, Yichuan Charlie",Towards Learning Multi-agent Negotiations via Self-Play,Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),,,,http://arxiv.org/abs/2001.10208,"Making sophisticated, robust, and safe sequential decisions is at the heart of intelligent systems. This is especially critical for planning in complex multi-agent environments, where agents need to anticipate other agents’ intentions and possible future actions. Traditional methods formulate the problem as a Markov Decision Process, but the solutions often rely on various assumptions and become brittle when presented with corner cases. In contrast, deep reinforcement learning (Deep RL) has been very effective at ﬁnding policies by simultaneously exploring, interacting, and learning from environments. Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. We demonstrate this in a challenging multi-agent simulation of merging trafﬁc, where agents must interact and negotiate with others in order to successfully merge on or off the road. While the environment starts off simple, we increase its complexity by iteratively adding an increasingly diverse set of agents to the agent “zoo” as training progresses. Qualitatively, we ﬁnd that through selfplay, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents. In addition, quantitatively, we show a dramatic improvement of the success rate of merging maneuvers from 63% to over 98%.",2020-01-28,2020-08-31 18:42,2020-12-21 18:34,2020-08-31 18:42,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 1 arXiv: 2001.10208,,/Users/angelica/Zotero/storage/8SG5VQX4/Tang - 2020 - Towards Learning Multi-agent Negotiations via Self.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,,"While the previous paper introduces other-play to become robust to unknown partners, this paper takes the other approach of simply training an agent that is robust to a wide, diverse population of possible agents. In particular, it studies a self-driving car ""zipper merge"" environment, and trains an agent to be robust to a variety of rule-based agents, as well as past versions of itself, and finds that this leads to a much more successful merging policy. However, this is evaluated against the population it is trained with, and not against any previously unseen agents."
TDMY3B4X,conferencePaper,2020,"Jeon, Hong Jun; Milli, Smitha; Dragan, Anca D.",Reward-rational (implicit) choice: A unifying formalism for reward learning,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2002.04833,"It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",2020-06-16,2020-09-05 19:05,2020-12-19 17:18,2020-09-05 19:05,,,,,,,Reward-rational (implicit) choice,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2002.04833,,/Users/angelica/Zotero/storage/2KBDQB55/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/angelica/Zotero/storage/3ZHTV5N5/2002.html,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"We've got algorithms for learning preferences from <@demonstrations@>(@Modeling Interaction via the Principle of Maximum Causal Entropy@) (possibly <@ranked@>(@Ranking-Based Reward Extrapolation without Rankings@)), <@comparisons@>(@Fine-Tuning GPT-2 from Human Preferences@), <@proxy rewards@>(@Inverse Reward Design@), and even the <@observed state@>(@Learning Preferences by Looking at the World@). The insight of this paper is that these are all instances of a simple underlying formalism.

Specifically, these forms of preference learning can be described by two properties: (1) the set of choices that the human picks from and (2) how each choice corresponds to a distribution over agent trajectories. Given these properties, we assume that the human makes their choice according to a Boltzmann-rational model (where the human is more likely to choose an option if it leads to higher expected reward). We have now specified a likelihood over the choice given the reward, and we can use Bayes rule to infer a distribution over the reward given the human's choice.

Consider more exotic types of feedback, such as the human's decision to <@turn the agent off@>(@The Off-Switch Game@). Here, the human has two options: turning the agent off (corresponding to the agent staying still forever), or letting it continue (corresponding to the agent taking the trajectory that maximizes its current expected reward). If the agent has the right reward function, then the Boltzmann rational human would let it continue; as a result, if the human instead tries to turn the agent off, Bayes Rule allows the agent to infer that its belief about the reward must be wrong. Thus, even this decision of whether to turn the agent off can be captured in this framework.

The paper then shows two examples of new feedback types that can be generated from this framework: first, credit assignment, in which the human identifies a subset of the trajectory that had maximal reward, and second, meta-choice, where the choice of which _type_ of feedback to give can itself give information about the reward function."
UAV9X75G,conferencePaper,2019,"Icarte, Rodrigo Toro; Valenzano, Richard; Waldie, Ethan; Castro, Margarita P; Klassen, Toryn Q; McIlraith, Sheila A",Learning Reward Machines for Partially Observable Reinforcement Learning,,,,,http://www.cs.toronto.edu/~rntoro/docs/LRM_paper.pdf,"Reward Machines (RMs) provide a structured, automata-based representation of a reward function that enables a Reinforcement Learning (RL) agent to decompose an RL problem into structured subproblems that can be efficiently learned via off-policy learning.  Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems.  We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem.  We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential.",2019,2020-08-31 17:45,2020-12-21 18:12,2020-08-31,19,,,,,,,,,,,,"Vancouver, Canada",en,,,,,Zotero,,ZSCC: 0000008,,/Users/angelica/Zotero/storage/P97PBKLS/Icarte et al. - Learning Reward Machines for Partially Observable .pdf,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"Typically in reinforcement learning, the agent only gets access to a reward signal: it sees a single number saying how well it has done. The problem might be simpler to solve if the agent could get a more holistic view of the problem through a structured representation of the reward. This could allow it to infer things like “if I went left, I would get 5 reward, but if I went right, I would get 10 reward”. Under the current RL paradigm, it has to try both actions in separate episodes to learn this.

Model-based RL tries to recover some of this structured representation: it learns a model of the world and the reward function, such that you can ask queries of the form “if I took this sequence of actions, what reward would I get?” The hope is that the learned models will generalize to new sequences that we haven’t previously seen, allowing the agent to learn from fewer environment interactions (i.e. higher sample efficiency).

This work does something similar using _reward machines_. The key idea is to represent both the reward and some aspects of the dynamics using a finite state machine, which can then be reasoned about without collecting more experience. In particular, given a POMDP, they propose learning a set of states U such that when combining the observation o with the state u, we have an MDP instead of a POMDP. This is called a _perfect_ reward machine. To make this feasible, they assume the existence of a labeling function L that, given a transition <o, a, o’>, extracts all of the relevant state information. (Since POMDPs can be reduced to belief-space MDPs, it is always possible to extract a perfect reward machine by having U be the set of possible beliefs and L be the identity function, but the hope is that U and L can be much simpler in most cases.)

They provide a formulation of an optimization problem over finite state machines such that a perfect reward machine would be an optimal solution to that problem (though I believe other imperfect reward machines could also be optimal). Since they are searching over a discrete space, they need to use a discrete optimization algorithm, and end up using Tabu search.

Once they have learned a reward machine from experience and a labeling function L, how can they use it to improve policy learning? They propose a very simple idea: when we get experience <o, a, o’>, treat it as a separate experience for every possible u, so that you effectively multiply the size of your dataset. They can then learn optimal policies that are conditioned on the state u (which can be inferred at test time using the learned state machine). Experiments show that this works in some simple gridworlds."
U32FJBY8,conferencePaper,2019,"Edwards, Ashley D.; Sahni, Himanshu; Schroecker, Yannick; Isbell, Charles L.",Imitating Latent Policies from Observation,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1805.07914,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",2019-05-13,2020-11-14 0:50,2020-12-19 15:32,2020-11-14 0:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1805.07914,,/Users/angelica/Zotero/storage/FW2VC7CH/1805.html; /Users/angelica/Zotero/storage/2LDT3XPV/Edwards et al. - 2019 - Imitating Latent Policies from Observation.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Typically in imitation learning, we assume that we have access to demonstrations that include the actions that the expert took. However, in many realistic settings we only have access to state observations (eg. driving videos). In this setting, we could still infer a reward function and then use reinforcement learning (RL) to imitate the behavior, but this would require a lot of interaction with the environment to learn the dynamics of the environment. Intuitively, even demonstrations with only states and no actions should give us a lot of information about the dynamics -- if we can extract this information, then we would need much less environment interaction during RL. (For example, if you watch a friend play a video game, you only see states, not actions; yet you can infer a lot about the game rules and gameplay.) The key idea is that each action probably causes similar effects on different states. So, they create a model with hidden action nodes z, and use the state observations to learn a policy P(z | s) and dynamics s' = g(s, z) (they assume deterministic dynamics). This is done end-to-end with neural nets, but essentially the net is looking at the sequence of states and figuring out how to assign actions z to each s (this is P(z | s)), such that we can learn a function g(s, z) that outputs the next observed state s'. Once this is trained, intuitively g(s, z) will already have captured most of the dynamics, and so now we only require a small number of environment actions to figure out how the true actions a correspond to the hidden actions z -- concretely, we train a model P(a | s, z). Then, in any state s, we first choose the most likely hidden action z* according to P(z | s), and then the most likely action a* according to P(a | s, z*)."
SUVSP4DJ,conferencePaper,2020,"Zhi-Xuan, Tan; Mann, Jordyn L.; Silver, Tom; Tenenbaum, Joshua B.; Mansinghka, Vikash K.",Online Bayesian Goal Inference for Boundedly-Rational Planning Agents,arXiv:2006.07532 [cs],,,,http://arxiv.org/abs/2006.07532,"People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.",2020-06-12,2020-08-27 16:41,2020-12-21 18:42,2020-08-27 16:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2006.07532,,/Users/angelica/Zotero/storage/IUVIXQHI/Zhi-Xuan et al. - 2020 - Online Bayesian Goal Inference for Boundedly-Ratio.pdf; /Users/angelica/Zotero/storage/3XJI59EW/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"Typical approaches to learning from demonstrations rely on assuming that the demonstrator is either optimal or noisily optimal. However, this is a pretty bad description of actual human reasoning: it is more accurate to say we are _boundedly-rational planners_. In particular, it makes more sense to assume that our plans are computed from a noisy process. How might we capture this in an algorithm?

This paper models the demonstrator as using a bounded probabilistic [A* search](https://en.wikipedia.org/wiki/A*_search_algorithm) to find plans for achieving their goal. The planner is also randomized to account for the difficulty of planning: in particular, when choosing which state to “think about” next, it chooses randomly with higher probability for more promising states (as opposed to vanilla A* which always chooses the most promising state).

The search may fail to find a plan that achieves the goal, in which case the demonstrator follows the actions of the most promising plan found by A* search until no longer possible (either an action leads to a state A* search hadn’t considered, or it reaches the end of its partial plan). Thus, this algorithm can assign significant probability to plans that fail to reach the goal.

The experiments show that this feature allows their SIPS algorithm to infer goals even when the demonstrator fails to reach their goal. For example, if an agent needs to get two keys to unlock two doors to get a blue gem, but only manages to unlock the first door, the algorithm can still infer that the agent’s goal was to obtain the blue gem.

I really like that this paper is engaging with the difficulty of dealing with systematically imperfect demonstrators, and it shows that it can do much better than Bayesian IRL for the domains they consider."
I7UTX9IP,conferencePaper,2019,"Yu, Lantao; Yu, Tianhe; Finn, Chelsea; Ermon, Stefano",Meta-Inverse Reinforcement Learning with Probabilistic Context Variables,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1909.09314v2,"Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",2019-09-20,2020-11-14 1:20,2020-12-19 14:38,2020-11-14 1:20,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000005,,/Users/angelica/Zotero/storage/NJH9RTMM/1909.html; /Users/angelica/Zotero/storage/AS4ZAW5U/Yu et al. - 2019 - Meta-Inverse Reinforcement Learning with Probabili.pdf; /Users/angelica/Zotero/storage/45Z4XSRZ/1909.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This work explores improving performance on multi-task inverse reinforcement learning in a single-shot setting by extending <@Adversarial Inverse Reinforcement Learning@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@) with ""latent context variables"" that condition the learned reward function. The paper makes two notable contributions: 1) It details an algorithm to simultaneously learn a flexible reward function and a conditional policy with competitive few-shot generalization abilities from expert demonstrations of multiple related tasks _without_ task specifications or identifiers; 2) The authors empirically demonstrate strong performance of a policy trained on the inferred reward of a structurally similar task with modified environmental dynamics, claiming that in order to succeed ""the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration""."
DCZ338VY,conferencePaper,2020,"Barde, Paul; Roy, Julien; Jeon, Wonseok; Pineau, Joelle; Pal, Christopher; Nowrouzezahrai, Derek",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2006.13258,"Adversarial imitation learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of adversarial imitation learning algorithms by removing the reinforcement learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent imitation learning methods.",2020-06-23,2020-08-28 17:25,2020-12-21 17:56,2020-08-28 17:25,,,,,,,Adversarial Soft Advantage Fitting,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.13258,,/Users/angelica/Zotero/storage/QPUMG8ER/Barde et al. - 2020 - Adversarial Soft Advantage Fitting Imitation Lear.pdf; /Users/angelica/Zotero/storage/35Z2KHC4/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"This work aims to simplify algorithms for adversarial imitation learning by using a _structured_ discriminator, which is parameterised by the current generator and a learned policy. They prove that if so formulated, the policy that yields the optimal discriminator is exactly the same as the policy that generated the expert data, which is also precisely what we hope the generator will learn. As long as the discriminator's learned policy is parameterised correctly such that it can be sampled and evaluated, this eliminates the need for a reinforcement learning outer loop for policy improvement, as this learned policy can be substituted in for the generator's policy in the next training iteration. They empirically show the competitiveness of their method with state-of-the-art algorithms across a small but increasingly complex suite of tasks."
9JKJL9X6,conferencePaper,2019,"Frazier, Spencer; Riedl, Mark",Improving Deep Reinforcement Learning in Minecraft with Action Advice,"arXiv:1908.01007 [cs, stat]",,,,http://arxiv.org/abs/1908.01007,"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",2019-08-02,2020-11-14 0:50,2020-12-19 15:31,2020-11-14 0:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 1908.01007,,/Users/angelica/Zotero/storage/HQEG6UUP/1908.html; /Users/angelica/Zotero/storage/V6AUTV2Z/Frazier and Riedl - 2019 - Improving Deep Reinforcement Learning in Minecraft.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2019,,,,,,,,,,,,,,,,"This paper uses maze-traversal in Minecraft to look at the extent to which human advice can help with _aliasing_ in 3D environments, the problem where many states share nearly identical visual features. The paper compares two advice-giving algorithms that rely on neural nets which are trained to explore and predict the utilities of possible actions they can take, sometimes accepting human advice. The two algorithms differ primarily in whether they provide advice for the current action, or provide advice that persists for several actions.

Experimental results suggest that both algorithms, but especially the one that applies to multiple actions, help with the problem of 3D aliasing, potentially because the system can rely on the movement advice it got in previous timesteps rather than having to discern tricky visual features in the moment. The paper also varies the frequency and accuracy of the advice given, and finds that receiving more advice significantly improves performance, even if that advice is only 50% accurate."
YJ7M3CU6,conferencePaper,2019,"Perez, Ethan; Karamcheti, Siddharth; Fergus, Rob; Weston, Jason; Kiela, Douwe; Cho, Kyunghyun",Finding Generalizable Evidence by Learning to Convince Q&A Models,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing,,,,http://arxiv.org/abs/1909.05863,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",2019-09-12,2020-11-14 0:41,2020-12-20 15:53,2020-11-14 0:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 8  arXiv: 1909.05863,,/Users/angelica/Zotero/storage/BKEIU8B8/1909.html; /Users/angelica/Zotero/storage/5WIHVGWV/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems; Computer Science - Computation and Language; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper tries to improve performance on multiple-choice questions about text passages using a technique similar to <@AI safety via debate@>. The set-up consists of a **judge model** and one or more **evidence agents**. First, the judge model is pretrained on samples consisting of a passage, a multiple-choice question about that passage, and the correct answer to that question. Then, in the experimental portion of the set-up, instead of looking at a full passage, the judge model looks at a subsequence of the passage created by combining the outputs from several evidence agents. Each evidence agent has been given the same passage and assigned a particular answer to the question, and must select a limited number of sentences from the passage to present to the judge model to convince it of that answer.

The paper varies several parameters in its setup, including the training process for the judge model, the questions used, the process evidence agents use to select sentences, etc. It finds that for many settings of these parameters, when judge models are tasked with generalizing from shorter passages to longer passages, or easier passages to harder passages, they do better with the new passages when assisted by the evidence agents. It also finds that the sentences given as evidence by the evidence agents are convincing to humans as well as the judge model."
,conferencePaper,2018,"Wang, Xin; Chen, Wenhu; Wang, Yuan-Fang; Wang, William Yang",No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,,,http://arxiv.org/abs/1804.09160,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",2018-07-08,2020-11-14 0:59,2020-12-19 15:06,2020-11-14 0:59,,,,,,,No Metrics Are Perfect,,,,,,"Melbourne, Australia",,,,,,arXiv.org,,ZSCC: 0000077  arXiv: 1804.09160,,/Users/angelica/Zotero/storage/DY9LHUMJ/1804.html; /Users/angelica/Zotero/storage/MA9K2WHF/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/angelica/Zotero/storage/PF45HNCX/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/angelica/Zotero/storage/SFQZVBMR/1804.html,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,56th Annual Meeting of the Association for Computational Linguistics,,,,,,,,,,,,,,,,"This paper tackles visual story-telling, the task of generating a story that matches a sequence of photos. It proposes learning a reward function from the labeled dataset that can then be optimized with reinforcement learning, with the hope that the reward function is a good compression of what we want and so leads to more generalizable behavior. They show that the standard automated techniques for evaluating visual stories are not very good, and so they perform a Mechanical Turk study that shows very good results compared to prior work. MTurk workers are often unable to tell whether the stories were generated by their algorithm or a human!

How does it work? Their architecture has a policy network that creates the stories and a reward network that provides the supervision, which are trained adversarially. We can think of the reward function as inducing a probability distribution over stories, where stories with higher reward are more probable. Then, the reward network acts as a discriminator, trying to make its implied probability distribution similar to the empirical data distribution and dissimilar to the policy network distribution, while the policy network acts as a generator, creating a policy that tries to match the implied probability distribution of the reward network. (This is equivalent to maximizing the expected reward from the reward network.)"
,conferencePaper,2020,"Beaulieu, Shawn; Frati, Lapo; Miconi, Thomas; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff; Cheney, Nick",Learning to Continually Learn,"arXiv:2002.09571 [cs, stat]",,,,http://arxiv.org/abs/2002.09571,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables contextdependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).",2020-03-03,2020-08-31 18:00,2020-12-21 17:58,2020-08-31 18:00,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 2002.09571,,/Users/angelica/Zotero/storage/S859IFAF/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ECAI 2020,,,,,,,,,,,,,,,,"This paper presents the **ANML** (A Neuromodulated Meta-Learning algorithm) method for countering catastrophic forgetting in continual learning. Continual learning is a problem setting where the system is presented with several tasks in sequence, and must maintain good performance on all of them. When training on new tasks, neural networks often “forget” how to perform the previous tasks, which is called catastrophic forgetting. This makes the naive approach of just training on each task in sequence ineffective.

The paper has two main ideas. First, rather than avoiding catastrophic forgetting by using hand-crafted solutions (e.g. previous methods have encouraged sparsity), the authors use meta-learning to directly optimise for this goal. This is done by **learning a network parameterization which, after training sequentially on many tasks, will get good performance on all tasks**. This outer loop objective can be optimised for directly by taking higher order gradients (gradients of gradients). The second idea is a novel form of neuromodulation. This takes the form of a neuromodulatory (NM) network, which takes the same input as the prediction network, and gates the prediction network’s forward pass. **This provides direct control of the output of the prediction network, but also indirect control of the learning of the prediction network, as gradients will only flow through the paths which haven’t been zeroed out by the gating mechanism.**

**Their method achieves state-of-the-art results on continual learning in Omniglot**, a few-shot dataset consisting of 1623 characters, each with only 20 hand-drawn examples. The network has to learn a sequence of tasks (e.g. classifying a character) with only 15 examples, and is then tested on overall performance over all the classes it’s learned. Their network gets 60% accuracy when presented with 600 classes in a row. **A classifier trained with the same data but shuffled independently at random only gets 68% accuracy**, implying that the catastrophic forgetting of their network only cost 8 percentage points. **Their method also learns a form of sparsity in the activations of the network in a much better way than the hand-crafted methods** - while per-class activations are very sparse, no neurons are wasted, as they all still activate over the entire dataset."
,conferencePaper,2020,"Jin, Di; Jin, Zhijing; Zhou, Joey Tianyi; Szolovits, Peter",Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1907.11932,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",2020-04-08,2020-08-31 18:47,2020-12-21 18:14,2020-08-31 18:47,,,,34,,,Is BERT Really Robust?,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 14  arXiv: 1907.11932,,/Users/angelica/Zotero/storage/Q6P7HAQB/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf; /Users/angelica/Zotero/storage/H44TF68D/1907.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,"This paper presents TextFooler, an algorithm for generating adversarial text for natural language tasks with only black-box access to models. TextFooler tries to generate sentences that are grammatical and semantically similar to original input sentences but produce incorrect labels. It does this by identifying a small set of most important words in the original sentence, generating candidate synonyms for those words, and gradually replacing the important words in the sentence by testing which synonyms cause the model to mispredict or report the least confidence score.

TextFooler is tested on three state-of-the-art NLP models-- WordCNN, WordLSTM, and BERT, all trained to ~80 - 90% test accuracy. On a variety of text classification datasets, TextFooler reduces accuracy to below ~15% with less than ~20% of the words perturbed. Humans evaluating the generated sentences say they are approximately as grammatical as the original, have the same label as the original in ~90% of cases, and have a sentence similarity score to the original sentence of 0.9 on a 0 to 1 scale. The paper finds that generally, models with higher original accuracy have higher after-attack acuracy.

The authors retrain BERT from scratch using data produced by TextFooler and then attack it using TextFooler again. They find that the after-attack accuracy is higher and that attacks require more perturbed words."
,conferencePaper,2020,"Ye, Deheng; Liu, Zhao; Sun, Mingfei; Shi, Bei; Zhao, Peilin; Wu, Hao; Yu, Hongsheng; Yang, Shaojie; Wu, Xipeng; Guo, Qingwei; Chen, Qiaobo; Yin, Yinyuting; Zhang, Hao; Shi, Tengfei; Wang, Liang; Fu, Qiang; Yang, Wei; Huang, Lanxiao",Mastering Complex Control in MOBA Games with Deep Reinforcement Learning,arXiv:1912.09729 [cs],,,,http://arxiv.org/abs/1912.09729,"We study the reinforcement learning problem of complex action control in the Multi-player Online Battle Arena (MOBA) 1v1 games. This problem involves far more complicated state and action spaces than those of traditional 1v1 games, such as Go and Atari series, which makes it very difﬁcult to search any policies with human-level performance. In this paper, we present a deep reinforcement learning framework to tackle this problem from the perspectives of both system and algorithm. Our system is of low coupling and high scalability, which enables efﬁcient explorations at large scale. Our algorithm includes several novel strategies, including control dependency decoupling, action mask, target attention, and dualclip PPO, with which our proposed actor-critic network can be effectively trained in our system. Tested on the MOBA game Honor of Kings, the trained AI agents can defeat top professional human players in full 1v1 games.",2020-01-02,2020-08-31 18:30,2020-12-21 18:40,2020-08-31 18:30,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 8  arXiv: 1912.09729,,/Users/angelica/Zotero/storage/LT9MLIQB/Ye et al. - 2020 - Mastering Complex Control in MOBA Games with Deep .pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,"This paper presents an AI system that can play the Multi-player Online Battle Arena (MOBA) game _Honor of Kings_. They are inspired by <@OpenAI Five@> (and Honor of Kings sounds quite similar to Dota, though it is 1v1 instead of 5v5), and have a similar learning setup: reinforcement learning using PPO. Their architecture requires an off-policy algorithm (I’m not sure why, maybe they have stale parameters across their rollout servers), so they add an importance sampling correction to the PPO objective, as well as an additional type of gradient clipping. The input is a combination of the image and underlying game state info. The resulting agents are able to beat top human players, and in an event with the public, the AI system lost only 4 out of 2100 matches. Unlike OpenAI Five, this required only around 100 hours to train (though it’s unclear how much compute was used)."
,conferencePaper,2020,"Ghorbani, Amirata; Zou, James",Neuron Shapley: Discovering the Responsible Neurons,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2002.09815,"We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.",2020-02-25,2020-09-05 17:36,2020-12-21 18:07,2020-09-05 17:36,,,,,,,Neuron Shapley,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2002.09815,,/Users/angelica/Zotero/storage/7DPG7NPJ/Ghorbani and Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf; /Users/angelica/Zotero/storage/624F6ZWC/2002.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"This paper presents a novel method, Neuron Shapley, that uses the [Shapley value framework](https://en.wikipedia.org/wiki/Shapley_value) to measure the importance of different neurons in determining an arbitrary metric of the neural net output. (Shapley values have been applied to machine learning before to [measure the importance of features to a model's output](https://christophm.github.io/interpretable-ml-book/shapley.html), but here the authors use them to calculate neuron importance.) Due to several novel approaches and optimisations in calculating these Shapley values, **the top k most responsible neurons (k ~ 30) can be feasibly found for large networks such as Inception-v3**.

The authors demonstrate that finding these neurons enables the performance of model surgery. Removing the top 30 neurons that contribute to accuracy completely destroys the accuracy, whereas in expectation removing 30 neurons at random from the network barely moves the accuracy at all. Since the method can be applied to an arbitrary metric, this kind of surgery can be performed for other metrics we care about. For example, removing the neurons which are most responsible for vulnerability to adversarial attacks makes the network more robust, and removing the neurons most responsible for the class-accuracy imbalance (a fairness metric) makes the classes much more even, while only reducing the overall accuracy a small amount."
,conferencePaper,2019,"Rhinehart, Nicholas; McAllister, Rowan; Kitani, Kris; Levine, Sergey",PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,"Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019",,,,http://arxiv.org/abs/1905.01296,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",2019-09-30,2020-11-14 0:36,2020-12-19 17:01,2020-11-14 0:36,,,,,,,PRECOG,,,,,,,,,,,,arXiv.org,,ZSCC: 0000067  arXiv: 1905.01296,,/Users/angelica/Zotero/storage/SIGUVH5J/1905.html; /Users/angelica/Zotero/storage/Z7ULACMN/Rhinehart et al. - 2019 - PRECOG PREdiction Conditioned On Goals in Visual .pdf,,NotSafety; CHAI-Berkeley,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper models a multi-agent self driving car scenario by developing a model of future states conditional on both its own action and the action of multiple humans, and picking the latent-space action that balances between the desiderata of reaching its goal and preferring trajectories seen in the expert multi-agent trajectories its shown (where, e.g., two human agents rarely crash into one another)."
,conferencePaper,2019,"Behbahani, Feryal; Shiarlis, Kyriacos; Chen, Xi; Kurin, Vitaly; Kasewa, Sudhanshu; Stirbu, Ciprian; Gomes, João; Paul, Supratik; Oliehoek, Frans A.; Messias, João; Whiteson, Shimon",Learning from Demonstration in the Wild,2019 International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1811.03516,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.",2019-03-25,2020-11-14 0:49,2020-12-19 15:33,2020-11-14 0:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 1811.03516,,/Users/angelica/Zotero/storage/LAH4YFTR/1811.html; /Users/angelica/Zotero/storage/ZCCERNMX/Behbahani et al. - 2019 - Learning from Demonstration in the Wild.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,"This paper learns traffic trajectories from unsupervised data by converting traffic camera footage into a Unity scene simulation, using that simulation to generate pseudo-LIDAR readings for each ""expert trajectory"", and then training an agent to imitate them using a variant of generative adversarial imitation learning (GAIL)."
,conferencePaper,2018,"Tung, Hsiao-Yu Fish; Harley, Adam W.; Huang, Liang-Kang; Fragkiadaki, Katerina",Reward Learning from Narrated Demonstrations,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,,,,http://arxiv.org/abs/1804.10692,"Humans effortlessly ""program"" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.",2018-04-27,2020-11-14 1:12,2020-12-19 14:44,2020-11-14 1:12,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 1804.10692,,/Users/angelica/Zotero/storage/S7XRLNA9/1804.html; /Users/angelica/Zotero/storage/LL3XTG9L/Tung et al. - 2018 - Reward Learning from Narrated Demonstrations.pdf,,Other-org; NotSafety,Computer Science - Robotics; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,"This paper learns and optimizes rewards given demonstrations of behavior along with a description of the behavior in natural language. Their dataset is a set of videos of humans demonstrating a task and describing it with natural language (such as ""the orange is in the bowl""). They combine several techniques to use this dataset to teach a robot. First, using speech recognition, they get a transcript of the natural language aligned with the video. They use object detectors to figure out what things are present in the image, and a syntactic parser to figure out the subject and object of the sentence, and match up these two results to figure out which objects in the image the natural language refers to, and extract their spatial features. They then train a classifier to take the spatial features and detecting whether it has achieved the goal, conditioned on the natural language description of the task. Now that they have a reward function (1 at a goal state, 0 otherwise) they can train a robot using DQN, though to get this to work they infer 3D object configurations from 2D images and use distance to the goal as a shaped reward."
,conferencePaper,2019,"Bussmann, Bart; Heinerman, Jacqueline; Lehman, Joel",Towards Empathic Deep Q-Learning,Proceedings of the Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1906.10918,"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",2019-06-26,2020-11-14 0:53,2020-12-19 15:20,2020-11-14 0:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1906.10918,,/Users/angelica/Zotero/storage/GTSKRQ99/1906.html; /Users/angelica/Zotero/storage/WEJ43S6K/Bussmann et al. - 2019 - Towards Empathic Deep Q-Learning.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,"This paper introduces the empathic DQN, which is inspired by the golden rule: ""Do unto others as you would have them do unto you"". Given a specified reward, the empathic DQN optimizes for a weighted combination of the specified reward, and the reward that other agents in the environment would get if they were a copy of the agent. They show that this results in resource sharing (when there are diminishing returns to resources) and avoiding conflict in two toy gridworlds."
,conferencePaper,2019,"Shum, Michael; Kleiman-Weiner, Max; Littman, Michael L.; Tenenbaum, Joshua B.",Theory of Minds: Understanding Behavior in Groups Through Inverse Planning,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1901.06085,"Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multi-agent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do.",2019-01-17,2020-11-14 0:31,2020-12-19 17:12,2020-11-14 0:31,,,,33,,,Theory of Minds,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1901.06085,,/Users/angelica/Zotero/storage/W3GER4TE/1901.html; /Users/angelica/Zotero/storage/AXUCNZDI/Shum et al. - 2019 - Theory of Minds Understanding Behavior in Groups .pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,"This paper introduces Composable Team Hierarchies (CTH), a representation designed for reasoning about how agents reason about each other in collaborative and competitive environments. CTH uses two ""planning operators"": the Best Response operator returns the best policy in a single-agent game, and the Joint Planning operator returns the best team policy when all agents are cooperating. Competitive policies can then be derived via recursive application of those operations to subsets of agents (while holding the policies of other agents fixed). CTH draws from ideas in level-K planning (in which each agent assumes all other agents are at level K-1) and cooperative planning, but is more powerful than either approach.

The authors experiment with using CTH to probabilistically infer policies and future actions of agents participating in the stag-hunt task; they find that these judgements correlate well with human data."
,conferencePaper,2020,"Bobu, Andreea; Scobee, Dexter R. R.; Fisac, Jaime F.; Sastry, S. Shankar; Dragan, Anca D.",LESS is More: Rethinking Probabilistic Models of Human Behavior,Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,,,10.1145/3319502.3374811,http://arxiv.org/abs/2001.04465,"Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.",2020-03-09,2020-09-05 17:24,2020-11-24 1:54,2020-09-05 17:24,429-437,,,,,,LESS is More,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2001.04465,,/Users/angelica/Zotero/storage/CRWIT633/Bobu et al. - 2020 - LESS is More Rethinking Probabilistic Models of H.pdf; /Users/angelica/Zotero/storage/NESEC5Y2/2001.html,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a new model for robots inferring human preferences called LESS. The traditional Boltzmann noisily-rational decision model assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. The Boltzmann model works well when modeling decisions among different discrete options, but runs into problems when modeling human trajectories in a continuous space, e.g. path finding, because it is very sensitive to the number of trajectories, even if they are similar-- if a robot using a Boltzmann model must predict whether a human navigates around an obstacle by taking one path on the left or one of three very-similar paths on the right, it will assign the same probability to each path by default.

To fix this, LESS predicts human behavior by treating each trajectory as part of a continuous space and mapping each one to a feature vector. The likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories, meaning similar trajectories are appropriately deweighted.

The paper tests the predictive performance of LESS vs. Boltzmann in several experimental environments, including an artifically constructed task where humans are asked to choose between similar paths for navigating around an obstacle, and a real-world task where humans demonstrate appropriate behaviors to a 7-degree-of-freedom robotic arm. In general, LESS performs better than Boltzmann when given a small number of samples of human behavior, but does equally well as the sample size is increased. In the robotic arm task, Boltzmann performed better when demonstrations were aggregated into a single batch and inference was run on the whole batch at once, representing trying to approximate the 'average' user rather than customizing behavior to each user. The paper claims that this happens because Boltzmann overlearns from demonstrations in sparse regions, and underlearns from dense demonstrations. As you increase the number of samples, you approximate the “true” trajectory space better and better, so the 10 trajectory sets vary less and less, which means Boltzmann won’t underperform so much. Since the single batch demonstration aggregated demonstrations, it had a similar effect in approximating the ""true"" trajectory space.

The paper notes that one limitation of this method is a reliance on a pre-specified set of robot features, though a small set of experimental results suggested that LESS still performed better than Boltzmann when adding a small number of irrelevant features."
,conferencePaper,2018,"Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano",Multi-Agent Generative Adversarial Imitation Learning,Advances in Neural Information Processing Systems 31 (NeurIPS 2018),,,,http://arxiv.org/abs/1807.09936,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",2018-07-25,2020-11-14 0:36,2020-12-19 17:00,2020-11-14 0:36,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1807.09936,,/Users/angelica/Zotero/storage/U8VEZT6P/1807.html; /Users/angelica/Zotero/storage/IH43EH97/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learn.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,"This paper generalizes [GAIL](http://www.jonathanho.me/files/HoErmon_NIPS2016.pdf) (which was covered [last week](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)) to the multiagent setting, where we want to imitate a group of interacting agents. They want to find a Nash equilibrium in particular. They formalize the Nash equilibrium constraints and use this to motivate a particular optimization problem for multiagent IRL, that looks very similar to their optimization problem for regular IRL in GAIL. After that, it is quite similar to GAIL -- they use a regularizer ψ for the reward functions, show that the composition of multiagent RL and multiagent IRL can be solved as a single optimization problem involving the convex conjugate of ψ, and propose a particular instantiation of ψ that is data-dependent, giving an algorithm. They do have to assume in the theory that the multiagent RL problem has a unique solution, which is not typically true, but may not be too important. As before, to make the algorithm practical, they structure it like a GAN, with discriminators acting like reward functions. What if we have prior information that the game is cooperative or competitive? In this case, they propose changing the regularizer ψ, making it keep all the reward functions the same (if cooperative), making them negations of each other (in two-player zero-sum games), or leaving it as is. They evaluate in a variety of simple multiagent games, as well as a plank environment in which the environment changes between training and test time, thus requiring the agent to learn a robust policy, and find that the correct variant of MAGAIL (cooperative/competitive/neither) outperforms both behavioral cloning and single-agent GAIL (which they run N times to infer a separate reward for each agent)."
,conferencePaper,2018,"Lacotte, Jonathan; Ghavamzadeh, Mohammad; Chow, Yinlam; Pavone, Marco",Risk-Sensitive Generative Adversarial Imitation Learning,Proceedings of Machine Learning Research,,,,https://arxiv.org/abs/1808.04468v2,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",2018-08-13,2020-11-14 1:23,2020-12-19 14:30,2020-11-14 1:23,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s0]  ACC: 11,,/Users/angelica/Zotero/storage/DAESW34J/1808.html; /Users/angelica/Zotero/storage/4YJLHV3W/Lacotte et al. - 2018 - Risk-Sensitive Generative Adversarial Imitation Le.pdf,,NotSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper extends GAIL to perform imitation learning where we try to optimize a policy for the mean reward collected under the constraint that the policy is no more risky than the expert policy. Since we don't know the true cost function, we have to approximate this problem with another problem where we infer the cost function as well, and evaluate the risk profile relative to the inferred cost function. The algorithm ends up looking very similar to the original GAIL algorithm, where the gradient updates change in order to include terms dependent on the conditional value-at-risk (CVaR). They evaluate against GAIL and RAIL (another risk-sensitive imitation learning algorithm) and find that their method performs the best on the Hopper and Walker Mujoco environments."
,conferencePaper,2019,"Singh, Avi; Yang, Larry; Hartikainen, Kristian; Finn, Chelsea; Levine, Sergey",End-to-End Robotic Reinforcement Learning without Reward Engineering,"arXiv:1904.07854 [cs, stat]",,,,http://arxiv.org/abs/1904.07854,"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",2019-05-15,2020-11-14 0:45,2020-12-19 15:39,2020-11-14 0:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000063  arXiv: 1904.07854,,/Users/angelica/Zotero/storage/273CLHH4/1904.html; /Users/angelica/Zotero/storage/VVYTBKFP/Singh et al. - 2019 - End-to-End Robotic Reinforcement Learning without .pdf,,NotSafety; CHAI-Berkeley,Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2019,,,,,,,,,,,,,,,,"This paper demonstrates an approach that can learn to perform real world robotics tasks based not on example trajectories (states and actions) but just a small number (10) of pixel-level images of goal states showing successful task completion. Their method learns a GAN-like classifier to predict whether a given image is a success, continually adding data sampled from the still-learning policy to the set of negative examples, so the model at each step needs to further refine its model of success. The classifier, which is used as the reward signal in learning the policy, also makes use of a simple active learning approach, choosing the state its classifier is most confident is success and querying a human about it on fixed intervals, ultimately using less than 75 queries in all cases."
,conferencePaper,2016,"Ho, M. K.; Littman, M. L.; MacGlashan, J.; Cushman, F.; Austerweil, J. L.",Showing versus doing: Teaching by demonstration,NeurIPS,,,,https://par.nsf.gov/biblio/10082788-showing-versus-doing-teaching-demonstration,,16-Jan,2020-11-14 2:45,2020-12-19 0:42,2020-11-14 2:45,,,,,,,Showing versus doing,,,,,,,en,,,,,par.nsf.gov,,ZSCC: 0000050,,/Users/angelica/Zotero/storage/SDN8TSSD/Ho et al. - 2016 - Showing versus doing Teaching by demonstration.pdf; /Users/angelica/Zotero/storage/NJQ9UDHC/10082788.html; /Users/angelica/Zotero/storage/YG663AYK/10082788.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS,,,,,,,,,,,,,,,,"This paper creates and validates a model of _pedagogy_ as applied to reward learning. Typically, inverse reinforcement learning (IRL) algorithms assume access to a set of demonstrations that are created from an approximately _optimal_ policy. However, in practice, when people are asked to _show_ a task, they don't give the optimal trajectory; they give the trajectory that helps the learner best _disambiguate_ between the possible tasks. They formalize this by creating a model in two steps:

1. A literal or IRL robot is one which learns rewards under the model that the demonstrator is Boltzmann rational.
2. The pedagogic human shows trajectories in proportion to how likely a literal robot would think the true reward is upon seeing the trajectory.

They validate this model with user studies and find that it predicts human demonstrations well."
,conferencePaper,2019,"Goyal, Prasoon; Niekum, Scott; Mooney, Raymond J.",Using Natural Language for Reward Shaping in Reinforcement Learning,Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19),,,,http://arxiv.org/abs/1903.02020,"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",2019-05-31,2020-11-14 0:51,2020-12-19 15:31,2020-11-14 0:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1903.02020,,/Users/angelica/Zotero/storage/S5RPI4DN/1903.html; /Users/angelica/Zotero/storage/J4TZ869W/Goyal et al. - 2019 - Using Natural Language for Reward Shaping in Reinf.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper constructs a dataset for grounding natural language in Atari games, and uses it to improve performance on Atari. They have humans annotate short clips with natural language: for example, ""jump over the skull while going to the left"" in Montezuma's Revenge. They use this to build a model that predicts whether a given trajectory matches a natural language instruction. Then, while training an agent to play Atari, they have humans give the AI system an instruction in natural language. They use their natural language model to predict the probability that the trajectory matches the instruction, and add that as an extra shaping term in the reward. This leads to faster learning."
,conferencePaper,2015,"Javdani, Shervin; Srinivasa, Siddhartha S.; Bagnell, J. Andrew",Shared Autonomy via Hindsight Optimization,Robotics Science and Systems Online Proceedings,,,,http://arxiv.org/abs/1503.07619,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",2015-04-17,2020-11-14 0:41,2020-12-19 16:55,2020-11-14 0:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000105  arXiv: 1503.07619,,/Users/angelica/Zotero/storage/X4N47AN7/1503.html; /Users/angelica/Zotero/storage/JEXD79E5/Javdani et al. - 2015 - Shared Autonomy via Hindsight Optimization.pdf,,Other-org; NotSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics Science and Systems,,,,,,,,,,,,,,,,"This paper considers a shared autonomy task in which a user controls a robot to achieve some goal, and the robot learns to assist the user, without knowing the goal in advance. They formalize this as a POMDP in which the state includes the user's goal, which the robot does not get to observe. However, the POMDP observation model assigns higher probability to user actions that better achieve the goal (a standard Boltzmann rationality model), and this allows the agent to reason about what the goal must be. In practice, for computational tractability, rather than choosing optimal actions in the overall POMDP, the robot chooses optimal actions using a technique called hindsight optimization, which _assumes that the robot will never learn more information about the user's goal_."
,conferencePaper,2019,"Brown, Daniel S.; Goo, Wonjoon; Nagarajan, Prabhat; Niekum, Scott",Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1904.06387,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",2019-07-08,2020-11-14 0:42,2020-12-19 16:53,2020-11-14 0:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000038  arXiv: 1904.06387,,/Users/angelica/Zotero/storage/8RNRKXZZ/1904.html; /Users/angelica/Zotero/storage/M2U4K9L8/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper claims to demonstrate a technique by which an agent learning from a demonstrator's actions can learn to outperform that demonstrator on their true reward, rather than, in the way of imitation learning or behavioral cloning, just mimicking the demonstrator under the assumption that the demonstrator's performance is optimal (or at least near-optimal). The key structural innovation of the paper is to learn using pairs of ranked trajectories and learn a neural network-based reward function based on correctly predicting which will be higher. This allows the model to predict what actions will lead to higher and lower reward, and to extrapolate that relationship beyond the best demonstration. When an agent is then trained using this reward model as it's ground truth reward, it's shown to be capable of outperforming the demonstrator on multiple tested environments, including Atari. An important distinction compared to some prior work is the fact that these rankings are collected in an off-policy manner, distinguishing it from [Deep RL from Human Preferences](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) where rankings are requested on trajectories generated as an agent learns. "
,conferencePaper,2018,"Bıyık, Erdem; Sadigh, Dorsa",Batch Active Preference-Based Learning of Reward Functions,"Proceedings of The 2nd Conference on Robot Learning, PMLR",,,,http://arxiv.org/abs/1810.04303,"Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.",2018-10-09,2020-11-14 1:05,2020-12-19 15:00,2020-11-14 1:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1810.04303,,/Users/angelica/Zotero/storage/4DICKN3M/1810.html; /Users/angelica/Zotero/storage/6QP72XEE/Bıyık and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2nd Conference on Robot Learning, PMLR",,,,,,,,,,,,,,,,"This paper builds on a trend of recent papers that try to learn human preferences, not through demonstrations of optimal behavior, but through a human expressing a preference over two possible trajectories, which has both pragmatic advantages (re limits of human optimality) and theoretic ones (better ability to extrapolate a reward function). Here, the task is framed as: we want to send humans batches of paired trajectories to rank, but which ones? Batch learning is preferable to single-sample active learning because it's more efficient to update a network after a batch of human judgments, rather than after each single one. This adds complexity to the problem because you'd prefer to not have a batch of samples that are individually high-expected-information, but which are redundant with one another. The authors define an information criterion (basically the examples about which we're most uncertain of the human's judgment) and then pick a batch of examples based on different heuristics for getting a set of trajectories with high information content that are separated from each other in feature space."
,conferencePaper,2019,"de Haan, Pim; Jayaraman, Dinesh; Levine, Sergey",Causal Confusion in Imitation Learning,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1905.11979v2,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive ""causal misidentification"" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.",2019-05-28,2020-11-14 1:20,2020-12-19 14:37,2020-11-14 1:20,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000030,,/Users/angelica/Zotero/storage/HAG23RYZ/1905.html; /Users/angelica/Zotero/storage/AWC4L89A/1905.html; /Users/angelica/Zotero/storage/996ZN85Z/de Haan et al. - 2019 - Causal Confusion in Imitation Learning.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This paper argues that _causal misidentification_ is a big problem in imitation learning. When the agent doesn't have a good model of what actions cause what state changes, it may mismodel the effects of a state change as a cause-- e.g., an agent learning to drive a car may incorrectly learn that it should turn on the brakes whenever the brake light on the dashboard is on. This leads to undesirable behavior where more information actually causes the agent to perform worse.

The paper presents an approach for resolving causal misidentification by (1) Training a specialized network to generate a ""disentangled"" representation of the state as variables, (2) Representing causal relationships between those variables in a graph structure, (3) Learning policies corresponding to each possible causal graph, and (4) Performing targeted interventions, either by querying an expert, or by executing a policy and observing the reward, to find the correct causal graph model.

The paper experiments with this method by testing it in environments artificially constructed to have confounding variables that correlate with actions but do not cause them. It finds that this method is successfully able to improve performance with confounding variables, and that it performs significantly better per number of queries (to an expert or of executing a policy) than any existing methods. It also finds that directly executing a policy and observing the reward is a more efficient strategy for narrowing down the correct causal graph than querying an expert."
,conferencePaper,2020,"Srinivas, Aravind; Laskin, Michael; Abbeel, Pieter",CURL: Contrastive Unsupervised Representations for Reinforcement Learning,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2004.04136,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the ﬁrst image-based algorithm to nearly match the sample-efﬁciency of methods that use state-based features. Our code is open-sourced and available at https://www. github.com/MishaLaskin/curl.",2020-07-07,2020-08-31 18:59,2020-12-19 17:30,2020-08-31 18:59,,,,,,,CURL,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000022  ACC: 22  arXiv: 2004.04136,,/Users/angelica/Zotero/storage/ZGWQBWJG/Srinivas et al. - 2020 - CURL Contrastive Unsupervised Representations for.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,"This paper applies contrastive learning (discussed above) to reinforcement learning. In RL, rather than training in an initial unsupervised phase, the contrastive learning happens alongside the RL training, and so serves as an auxiliary objective to speed up learning. They use random crops for their data augmentation."
,conferencePaper,2019,"Khoury, Marc; Hadfield-Menell, Dylan",On the Geometry of Adversarial Examples,Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019,,,,http://arxiv.org/abs/1811.00525,"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassiﬁcations for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classiﬁes the low-dimensional data manifold well, but classiﬁes points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefﬁcient, and (3) sufﬁcient sampling conditions under which nearest neighbor classiﬁers and ball-based adversarial training are robust.",2019,2019-12-18 2:24,2020-12-26 23:16,2019-12-18 2:24,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1811.00525,,/Users/angelica/Zotero/storage/FJUZGQWD/Khoury and Hadfield-Menell - 2018 - On the Geometry of Adversarial Examples.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019,,,,,,,,,,,,,,,,"This paper analyzes adversarial examples based off a key idea: even if the data of interest forms a low-dimensional manifold, as we often assume, the ϵ-tube _around_ the manifold is still high-dimensional, and so accuracy in an ϵ-ball around true data points will be hard to learn.

For a given L_p norm, we can define the optimal decision boundary to be the one that maximizes the margin from the true data manifold. If there exists some classifier that is adversarially robust, then the optimal decision boundary is as well. Their first result is that the optimal decision boundary can change dramatically if you change p. In particular, for concentric spheres, the optimal L_inf decision boundary provides an L_2 robustness guarantee √d times smaller than the optimal L_2 decision boundary, where d is the dimensionality of the input. This explains why a classifier that is adversarially trained on L_inf adversarial examples does so poorly on L_2 adversarial examples.

I'm not sure I understand the point of the next section, but I'll give it a try. They show that a nearest neighbors classifier can achieve perfect robustness if the underlying manifold is sampled sufficiently densely (requiring samples exponential in k, the dimensionality of the manifold). However, a learning algorithm with a particular property that they formalize would require exponentially more samples in at least some cases in order to have the same guarantee. I don't know why they chose the particular property they did -- my best guess is that the property is meant to represent what we get when we train a neural net on L_p adversarial examples. If so, then their theorem suggests that we would need exponentially more training points to achieve perfect robustness with adversarial training compared to a nearest neighbor classifier.

They next turn to the fact that the ϵ-tube around the manifold is d-dimensional instead of k-dimensional. If we consider ϵ-balls around the training set X, this covers a very small fraction of the ϵ-tube, approaching 0 as d becomes much larger than k, even if the training set X covers the k-dimensional manifold sufficiently well.

Another issue is that if we require adversarial robustness, then we severely restrict the number of possible decision boundaries, and so we may need significantly more expressive models to get one of these decision boundaries. In particular, since feedforward neural nets with Relu activations have ""piecewise linear"" decision boundaries (in quotes because I might be using the term incorrectly), it is hard for them to separate concentric spheres. Suppose that the spheres are separated by a distance d. Then for accuracy on the manifold, we only need the decision boundary to lie entirely in the shell of width d. However, for ϵ-tube adversarial robustness, the decision boundary must lie in a shell of width d - 2ϵ. They prove a lower bound on the number of linear regions for the decision boundary that grows as τ^(-d), where τ is the width of the shell, suggesting that adversarial robustness would require more parameters in the model.

Their experiments show that for simple learning problems (spheres and planes), adversarial examples tend to be in directions orthogonal to the manifold. In addition, if the true manifold has high codimension, then the learned model has poor robustness."
,conferencePaper,2018,"Kurakin, Alexey; Goodfellow, Ian; Bengio, Samy; Dong, Yinpeng; Liao, Fangzhou; Liang, Ming; Pang, Tianyu; Zhu, Jun; Hu, Xiaolin; Xie, Cihang; Wang, Jianyu; Zhang, Zhishuai; Ren, Zhou; Yuille, Alan; Huang, Sangxia; Zhao, Yao; Zhao, Yuzhe; Han, Zhonglin; Long, Junjiajia; Berdibekov, Yerkebulan; Akiba, Takuya; Tokui, Seiya; Abe, Motoki",Adversarial Attacks and Defences Competition,The NIPS '17 Competition: Building Intelligent Systems,,,,http://arxiv.org/abs/1804.00097,"To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.",2018-03-30,2020-12-13 23:13,2020-12-18 21:08,2020-12-13 23:13,,,,,,,,The Springer Series on Challenges in Machine Learning,,,,,,,,,,,arXiv.org,,ZSCC: 0000118  arXiv: 1804.00097,,/Users/angelica/Zotero/storage/HHKVPJY9/Kurakin et al. - 2018 - Adversarial Attacks and Defences Competition.pdf; /Users/angelica/Zotero/storage/6D32WARL/1804.html,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This is a report on a competition held at NIPS 2017 for the best adversarial attacks and defences. It includes a summary of the field and then shows the results from the competition.
,conferencePaper,2019,"Justesen, Niels; Duque, Miguel Gonzalez; Jaramillo, Daniel Cabarcas; Mouret, Jean-Baptiste; Risi, Sebastian",Learning a Behavioral Repertoire from Demonstrations,arXiv:1907.03046 [cs],,,,http://arxiv.org/abs/1907.03046,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",2019-07-05,2020-11-14 0:52,2020-12-19 15:26,2020-11-14 0:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 3  arXiv: 1907.03046,,/Users/angelica/Zotero/storage/ZGWIDIBS/1907.html; /Users/angelica/Zotero/storage/NVEKA7C3/Justesen et al. - 2019 - Learning a Behavioral Repertoire from Demonstratio.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE Conference on Games (CoG),,,,,,,,,,,,,,,,"They extend vanilla Imitation Learning, adding a behaviour encoding as an input to the policy to prevent it from learning an 'average' behaviour, but instead to learn different strategies with a single policy. Training data includes 7,777 human demonstrations of Terran army build-orders v/s the Zerg in StarCraft 2, for which build-order strategies are first extracted in a high-dimension semantically-meaningful space, and then reduced to two dimensions using PCA. At training time, each game's 2D code _b_ is augmented to the state, and supervised learning is applied to the policy: π(s, b) = a, where _a_ is the action following state _s_ in the human demonstration."
,conferencePaper,2020,"Caron, Mathilde; Misra, Ishan; Mairal, Julien; Goyal, Priya; Bojanowski, Piotr; Joulin, Armand",Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,"Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS),",,,,http://arxiv.org/abs/2006.09882,"Unsupervised image representations have signiﬁcantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efﬁcient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our ﬁndings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",2020-07-17,2020-08-31 18:01,2020-12-21 18:00,2020-08-31 18:01,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2006.09882,,/Users/angelica/Zotero/storage/9KVMKJ3H/Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"There has been a lot of work in self-supervised representation learning for image classification (previously summarized in [AN #92](https://mailchi.mp/d7e950bc8dbd/an-92learning-good-representations-with-contrastive-predictive-coding) and [AN #99](https://mailchi.mp/4f7ffc5cbe53/an-99-doubling-times-for-the-efficiency-of-ai-algorithms)). This paper sets a new SOTA of 75.3% top-1 ImageNet accuracy, when allowed to first do self-supervised representation learning on ImageNet, and then to train a linear classifier on top of the learned features using all of ImageNet.

Previous methods use a contrastive loss across the learned representations (possibly after being processed by a few MLP layers), which can be thought of as using the learned representation to predict the representation of augmented versions of the same input. In contrast, this paper uses the representation to predict “codes” of augmented versions, where the codes are computed using clustering."
,conferencePaper,2019,"Xu, Danfei; Denil, Misha",Positive-Unlabeled Reward Learning,"arXiv:1911.00459 [cs, stat]",,,,http://arxiv.org/abs/1911.00459,"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",2019-11-01,2020-11-14 0:41,2020-12-19 16:54,2020-11-14 0:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1911.00459,,/Users/angelica/Zotero/storage/K2F6CDV7/1911.html; /Users/angelica/Zotero/storage/IYDGFFHB/Xu and Denil - 2019 - Positive-Unlabeled Reward Learning.pdf,,NotSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Robot Learning 2020,,,,,,,,,,,,,,,,"The problem with learning a reward model and training an agent on the (now fixed) model is that the agent can learn to exploit errors in the reward model. Adversarial imitation learning seeks to avoid this by training a discriminator reward model with the agent: the discriminator is trained via supervised learning to distinguish between expert trajectories and agent trajectories, while the agent tries to fool the discriminator. However, this effectively treats the agent trajectories as negative examples — even once the agent has mastered the task. What we would really like to do is to treat the agent trajectories as unlabeled data. This is an instance of _semi-supervised learning_, in which a classifier has access to a small set of labeled data and a much larger collection of unlabeled data. In general, the common approach is to propagate classification information learned using labels to the unlabeled dataset. The authors apply a recent algorithm for positive-unlabeled (PU) learning, and show that this approach can improve upon both GAIL and supervised reward learning."
8TE87EXY,encyclopediaArticle,2021,,Von Neumann–Morgenstern utility theorem,Wikipedia,,,,https://en.wikipedia.org/w/index.php?title=Von_Neumann%E2%80%93Morgenstern_utility_theorem&oldid=1044421624,"In decision theory, the von Neumann–Morgenstern (VNM) utility theorem shows that, under certain axioms of rational behavior, a decision-maker faced with risky (probabilistic) outcomes of different choices will behave as if he or she is maximizing the expected value of some function defined over the potential outcomes at some specified point in the future. This function is known as the von Neumann–Morgenstern utility function. The theorem is the basis for expected utility theory. In 1947, John von Neumann and Oskar Morgenstern proved that any individual whose preferences satisfied four axioms has a utility function; such an individual's preferences can be represented on an interval scale and the individual will always prefer actions that maximize expected utility. That is, they proved that an agent is (VNM-)rational if and only if there exists a real-valued function u defined by possible outcomes such that every preference of the agent is characterized by maximizing the expected value of u, which can then be defined as the agent's VNM-utility (it is unique up to adding a constant and multiplying by a positive scalar). No claim is made that the agent has a ""conscious desire"" to maximize u, only that u exists. The expected utility hypothesis is that rationality can be modeled as maximizing an expected value, which given the theorem, can be summarized as ""rationality is VNM-rationality"". However, the axioms themselves have been critiqued on various grounds, resulting in the axioms being given further justification.VNM-utility is a decision utility in that it is used to describe decision preferences. It is related but not equivalent to so-called E-utilities (experience utilities), notions of utility intended to measure happiness such as that of Bentham's Greatest Happiness Principle.",2021-09-15,2022-03-10 22:16:04,2022-03-10 22:16:04,2022-03-10 22:16:04,,,,,,,,,,,,,,en,Creative Commons Attribution-ShareAlike License,,,,Wikipedia,,Page Version ID: 1044421624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HX9UZ5JP,journalArticle,2020,"Cihon, Peter; Maas, Matthijs M.; Kemp, Luke",Fragmentation and the Future: Investigating Architectures for International AI Governance,Global Policy,,1758-5899,10.1111/1758-5899.12890,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12890,"The international governance of artificial intelligence (AI) is at a crossroads: should it remain fragmented or be centralised? We draw on the history of environment, trade, and security regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak for centralisation. The risk of creating a slow and brittle institution, and the difficulty of pairing deep rules with adequate participation, speak against it. Other considerations depend on the specific design. A centralised body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial, and fragmented institutions could self-organise. In sum, these trade-offs should inform development of the AI governance architecture, which is only now emerging. We apply the trade-offs to the case of the potential development of high-level machine intelligence. We conclude with two recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, fragmentation will likely persist for now. The developing landscape should be monitored to see if it is self-organising or simply inadequate.",2020,2022-01-30 4:47:43,2022-01-30 4:47:43,2021-11-13 15:58:24,545-556,,5,11,,,Fragmentation and the Future,,,,,,,en,,,,,Wiley Online Library,,ZSCC: 0000010  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12890,,/Users/jacquesthibodeau/Zotero/storage/2TZBI3FR/Cihon et al. - 2020 - Fragmentation and the Future Investigating Archit.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVMJ4RMM,journalArticle,2020,"Stray, Jonathan",Aligning AI Optimization to Community Well-Being,International Journal of Community Well-Being,,"2524-5295, 2524-5309",10.1007/s42413-020-00086-3,http://link.springer.com/10.1007/s42413-020-00086-3,,2020-12,2022-01-30 4:47:36,2022-01-30 4:47:36,2021-11-13 22:47:54,443-463,,4,3,,Int. Journal of Com. WB,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/V3BEV7X4/Stray - 2020 - Aligning AI Optimization to Community Well-Being.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TK5F29IU,journalArticle,2021,"Hayden, Benjamin; Niv, Yael",The case against economic values in the orbitofrontal cortex (or anywhere else in the brain),Behavioral Neuroscience,,,10.1037/bne0000448,https://osf.io/7hgup,"Much of traditional neuroeconomics proceeds from the hypothesis that value is reified in the brain, that is, that there are neurons or brain regions whose responses serve the discrete purpose of encoding value. This hypothesis is supported by the finding that the activity of many neurons covaries with subjective value as estimated in specific tasks and has led to the idea that the primary function of the orbitofrontal cortex is to compute and signal economic value. Here we consider an alternative: that economic value, in the cardinal, common-currency sense, is not represented in the brain and used for choice by default. This idea is motivated by consideration of the economic concept of value, which places important epistemic constraints on our ability to identify its neural basis. It is also motivated by the behavioral economics literature, especially work on heuristics, which proposes value-free process models for much if not all of choice. Finally, it is buoyed by recent neural and behavioral findings regarding how animals and humans learn to choose between options. In light of our hypothesis, we critically reevaluate putative neural evidence for the representation of value and explore an alternative: direct learning of action policies. We delineate how this alternative can provide a robust account of behavior that concords with existing empirical data.",2021,2022-01-30 4:48:47,2022-01-30 4:48:47,2021-11-08 23:41:47,192-201,,2,135,,,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000026  DOI: 10.31234/osf.io/7hgup,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NHWZIKZ2,journalArticle,2020,"Fernandes, Pedro; Santos, Francisco C.; Lopes, Manuel",Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,AI Communications,,"18758452, 09217126",10.3233/AIC-201502,http://arxiv.org/abs/1907.03843,"The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.",2020-12-18,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-13 22:40:37,155-171,,3-6,33,,AIC,Norms for Beneficial A.I.,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1907.03843,,/Users/jacquesthibodeau/Zotero/storage/JAVXSVNK/Fernandes et al. - 2020 - Norms for Beneficial A.I. A Computational Analysi.pdf; /Users/jacquesthibodeau/Zotero/storage/A9VEVGPV/1907.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDWGJGAP,journalArticle,2021,"Mingard, Chris; Valle-Pérez, Guillermo; Skalse, Joar; Louis, Ard A.","Is SGD a Bayesian sampler? Well, almost",Journal of Machine Learning Research,,,,http://arxiv.org/abs/2006.15191,"Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$. Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",2021-02,2022-01-30 4:48:46,2022-03-11 1:39:08,2021-11-13 22:56:31,,,,22,,,Is SGD a Bayesian sampler?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2006.15191,,"/Users/jacquesthibodeau/Zotero/storage/ACV9IXEG/Mingard et al. - 2020 - Is SGD a Bayesian sampler Well, almost.pdf; /Users/jacquesthibodeau/Zotero/storage/BC3NUUZG/Mingard et al. - 2020 - Is SGD a Bayesian sampler Well, almost.pdf; /Users/jacquesthibodeau/Zotero/storage/EN2JTJZ8/2006.html; /Users/jacquesthibodeau/Zotero/storage/VEJMZHMR/2006.html",,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4E97MC3E,journalArticle,2020,"Zhu, Yixin; Gao, Tao; Fan, Lifeng; Huang, Siyuan; Edmonds, Mark; Liu, Hangxin; Gao, Feng; Zhang, Chi; Qi, Siyuan; Wu, Ying Nian; Tenenbaum, Joshua B.; Zhu, Song-Chun","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense",Engineering,,20958099,10.1016/j.eng.2020.01.011,http://arxiv.org/abs/2004.09044,"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.",2020-03,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-07 23:18:37,310-345,,3,6,,Engineering,"Dark, Beyond Deep",,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 2004.09044,,"/Users/jacquesthibodeau/Zotero/storage/G95A4VAE/Zhu et al. - 2020 - Dark, Beyond Deep A Paradigm Shift to Cognitive A.pdf; /Users/jacquesthibodeau/Zotero/storage/XMMDMVVS/2004.html",,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DV7P9734,journalArticle,2017,"Lake, Brenden M.; Ullman, Tomer D.; Tenenbaum, Joshua B.; Gershman, Samuel J.",Building Machines That Learn and Think Like People,Behavioral and Brain Sciences,,,https://doi.org/10.1017/S0140525X16001837,http://arxiv.org/abs/1604.00289,"Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.",2017,2022-01-30 4:48:44,2022-03-11 1:38:53,2021-11-18 22:58:21,,,,40,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0001764  arXiv: 1604.00289,,/Users/jacquesthibodeau/Zotero/storage/6UBFFSST/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf; /Users/jacquesthibodeau/Zotero/storage/YJPXAPGW/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf; /Users/jacquesthibodeau/Zotero/storage/NHNDK29Q/1604.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76QR898M,journalArticle,2020,"Sutrop, Margit",Challenges of Aligning Artificial Intelligence with Human Values,Acta Baltica Historiae et Philosophiae Scientiarum,,"22282009, 22282017",10.11590/abhps.2020.2.04,https://www.ies.ee/bahps/acta-baltica/abhps-8-2/04_Sutrop-2020-2-04.pdf,"As artificial intelligence (AI) systems are becoming increasingly autonomous and will soon be able to make decisions on their own about what to do, AI researchers have started to talk about the need to align AI with human values. The AI ‘value alignment problem’ faces two kinds of challenges—a technical and a normative one—which are interrelated. The technical challenge deals with the question of how to encode human values in artificial intelligence. The normative challenge is associated with two questions: “Which values or whose values should artificial intelligence align with?” My concern is that AI developers underestimate the difficulty of answering the normative question. They hope that we can easily identify the purposes we really desire and that they can focus on the design of those objectives. But how are we to decide which objectives or values to induce in AI, given that there is a plurality of values and moral principles and that our everyday life is full of moral disagreements? In my paper I will show that although it is not realistic to reach an agreement on what we, humans, really want as people value different things and seek different ends, it may be possible to agree on what we do not want to happen, considering the possibility that intelligence, equal to our own, or even exceeding it, can be created. I will argue for pluralism (and not for relativism!) which is compatible with objectivism. In spite of the fact that there is no uniquely best solution to every moral problem, it is still possible to identify which answers are wrong. And this is where we should begin the value alignment of AI.",2020-12-15,2022-01-30 4:48:44,2022-01-30 4:48:44,2021-11-13 22:38:41,54-72,,2,8,,ABHPS,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/VS3TSE5U/University of Tartu and Sutrop - 2020 - Challenges of Aligning Artificial Intelligence wit.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXW84MIC,journalArticle,2020,"Cammarata, Nick; Goh, Gabriel; Carter, Shan; Schubert, Ludwig; Petrov, Michael; Olah, Chris",Curve Detectors,Distill,,2476-0757,10.23915/distill.00024.003,https://distill.pub/2020/circuits/curve-detectors,Part one of a three part deep dive into the curve neuron family.,2020-06-17,2022-01-30 4:48:28,2022-01-30 4:48:28,2021-11-14 16:21:07,e00024.003,,6,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000005,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZTGZTHQ,journalArticle,2020,"Olah, Chris; Cammarata, Nick; Voss, Chelsea; Schubert, Ludwig; Goh, Gabriel",Naturally Occurring Equivariance in Neural Networks,Distill,,2476-0757,10.23915/distill.00024.004,https://distill.pub/2020/circuits/equivariance,"Neural networks naturally learn many transformed copies of the same feature, connected by symmetric weights.",2020-12-08,2022-01-30 4:48:28,2022-01-30 4:48:28,2021-11-14 16:21:59,e00024.004,,12,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000001,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UD8JD4XD,journalArticle,2020,"Barreto, André; Hou, Shaobo; Borsa, Diana; Silver, David; Precup, Doina",Fast reinforcement learning with generalized policy updates,Proceedings of the National Academy of Sciences,,"0027-8424, 1091-6490",10.1073/pnas.1907370117,http://www.pnas.org/lookup/doi/10.1073/pnas.1907370117,"The combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.",2020-12-01,2022-01-30 4:47:57,2022-01-30 4:47:57,2021-11-13 13:59:52,30079-30087,,48,117,,Proc Natl Acad Sci USA,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/WGDHG555/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6ZGCJNUC,journalArticle,2018,"Rozo, Leonel; Amor, Heni Ben; Calinon, Sylvain; Dragan, Anca; Lee, Dongheui",Special issue on learning for human–robot collaboration,Autonomous Robots,,1573-7527,10.1007/s10514-018-9756-z,https://doi.org/10.1007/s10514-018-9756-z,,2018-06-01,2022-01-30 4:51:09,2022-01-30 4:51:09,2019-12-18 2:40:04,953-956,,5,42,,Auton Robot,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/G3JHNZVF/Rozo et al. - 2018 - Special issue on learning for human–robot collabor.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CJVETACN,journalArticle,2015,"Russell, Stuart; Dewey, Daniel; Tegmark, Max",Research priorities for robust and beneficial artificial intelligence: an open letter,AI Magazine,,,,,,2015,2022-01-30 4:51:08,2022-01-30 4:51:08,,,,4,36,,,Research priorities for robust and beneficial artificial intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000017,,/Users/jacquesthibodeau/Zotero/storage/Q8Q62AV8/Russell et al. - 2015 - Research priorities for robust and beneficial arti.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IRTZ78N7,journalArticle,2015,"Russell, Stuart",Recent developments in unifying logic and probability,Communications of the ACM,,10782,10.1145/2699411,http://dl.acm.org/citation.cfm?doid=2797100.2699411,,2015-06-25,2022-01-30 4:51:08,2022-01-30 4:51:08,2018-12-09 19:18:27,88-97,,7,58,,,,,,,,,,en,,,,,Crossref,,ZSCC: NoCitationData[s2]  ACC: 74,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNU8DV83,journalArticle,2015,"Russell, Stuart; Dewey, Daniel; Tegmark, Max",Research Priorities for Robust and Beneficial Artificial Intelligence,AI Magazine,,"0738-4602, 0738-4602",10.1609/aimag.v36i4.2577,https://aaai.org/ojs/index.php/aimagazine/article/view/2577,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",2015-12-31,2022-01-30 4:51:08,2022-01-30 4:51:08,2019-12-18 1:27:03,105,,4,36,,AIMag,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000575,,/Users/jacquesthibodeau/Zotero/storage/W2CF629Z/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQHJJJJI,journalArticle,2019,"Baum, Seth D.; Armstrong, Stuart; Ekenstedt, Timoteus; Häggström, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs M.; Miller, James D.; Salmela, Markus; Sandberg, Anders",Long-term trajectories of human civilization,Foresight,,,,,,2019,2022-01-30 4:51:08,2022-01-30 4:51:08,,53–83,,1,21,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000048,,/Users/jacquesthibodeau/Zotero/storage/EFQQZHCW/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf; /Users/jacquesthibodeau/Zotero/storage/RB58JW9E/html.html; /Users/jacquesthibodeau/Zotero/storage/T9WH9T4G/html.html; /Users/jacquesthibodeau/Zotero/storage/HDZZWUJ2/html.html; /Users/jacquesthibodeau/Zotero/storage/2WQX97PA/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf,,CLR; MetaSafety; FHI; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9H4IQEWN,journalArticle,2017,"Sotala, Kaj",How feasible is the rapid development of artificial superintelligence?,Physica Scripta,,1402-4896,10.1088/1402-4896/aa90e8,https://doi.org/10.1088%2F1402-4896%2Faa90e8,"What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.",2017-10,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-11-23 0:17:29,113001,,11,92,,Phys. Scr.,,,,,,,,en,,,,,Institute of Physics,,ZSCC: 0000016  Publisher: IOP Publishing,,/Users/jacquesthibodeau/Zotero/storage/EUNUQQPC/Sotala - 2017 - How feasible is the rapid development of artificia.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79KF8UHI,journalArticle,1995,"Russell, S. J.; Subramanian, D.",Provably Bounded-Optimal Agents,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.133,https://www.jair.org/index.php/jair/article/view/10134,"Since its inception, artificial intelligence has relied  upon a theoretical foundation centered around  perfect rationality  as   the desired property of intelligent systems. We argue, as others have   done, that this foundation is inadequate because it imposes   fundamentally unsatisfiable requirements. As a result, there has   arisen a wide gap between theory and practice in AI, hindering   progress in the field. We propose instead a property called  bounded   optimality. Roughly speaking, an agent is bounded-optimal if its   program is a solution to the constrained optimization problem   presented by its architecture and the task environment. We show how to   construct agents with this property for a simple class of machine   architectures in a broad class of real-time environments. We   illustrate these results using a simple model of an automated mail   sorting facility.  We also define a weaker property,  asymptotic   bounded optimality (ABO), that generalizes the notion of optimality in   classical complexity theory.  We then construct  universal  ABO   programs, i.e., programs that are ABO no matter what real-time   constraints are applied.  Universal ABO programs can be used as   building blocks for more complex systems. We conclude with a   discussion of the prospects for bounded optimality as a theoretical   basis for AI, and relate it to similar trends in philosophy,   economics, and game theory.",1995-05-01,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-22 2:23:36,575-609,,,2,,jair,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 437,,/Users/jacquesthibodeau/Zotero/storage/E5SU8UNI/Russell and Subramanian - 1995 - Provably Bounded-Optimal Agents.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NCWIBN2Q,journalArticle,2018,"Sadigh, Dorsa; Landolfi, Nick; Sastry, Shankar S.; Seshia, Sanjit A.; Dragan, Anca D.",Planning for cars that coordinate with people: leveraging effects on human actions for planning and active information gathering over human internal state,Autonomous Robots,,"0929-5593, 1573-7527",10.1007/s10514-018-9746-1,http://link.springer.com/10.1007/s10514-018-9746-1,"Traditionally, autonomous cars treat human-driven vehicles like moving obstacles. They predict their future trajectories and plan to stay out of their way. While physically safe, this results in defensive and opaque behaviors. In reality, an autonomous car’s actions will actually affect what other cars will do in response, creating an opportunity for coordination. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We introduce a formulation of interaction with human-driven vehicles as an underactuated dynamical system, in which the robot’s actions have consequences on the state of the autonomous car, but also on the human actions and thus the state of the human-driven car. We model these consequences by approximating the human’s actions as (noisily) optimal with respect to some utility function. The robot uses the human actions as observations of her underlying utility function parameters. We ﬁrst explore learning these parameters ofﬂine, and show that a robot planning in the resulting underactuated system is more efﬁcient than when treating the person as a moving obstacle. We also show that the robot can target speciﬁc desired effects, like getting the person to switch lanes or to proceed ﬁrst through an intersection. We then explore estimating these parameters online, and enable the robot to perform active information gathering: generating actions that purposefully probe the human in order to clarify their underlying utility parameters, like driving style or attention level. We show that this signiﬁcantly outperforms passive estimation and improves efﬁciency. Planning in our model results in coordination behaviors: the robot inches forward at an intersection to see if can go through, or it reverses to make the other car proceed ﬁrst. These behaviors result from the optimization, without relying on hand-coded signaling strategies. Our user studies support the utility of our model when interacting with real users.",2018-10,2022-01-30 4:51:07,2022-01-30 4:51:07,2019-12-18 2:39:40,1405-1426,,7,42,,Auton Robot,Planning for cars that coordinate with people,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000086,,/Users/jacquesthibodeau/Zotero/storage/IZXDWBKE/Sadigh et al. - 2018 - Planning for cars that coordinate with people lev.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7T52V2ZJ,journalArticle,2016,"Oesterheld, Caspar",Formalizing preference utilitarianism in physical world models,Synthese,,1573-0964,10.1007/s11229-015-0883-1,https://doi.org/10.1007/s11229-015-0883-1,"Most ethical work is done at a low level of formality. This makes practical moral questions inaccessible to formal and natural sciences and can lead to misunderstandings in ethical discussion. In this paper, we use Bayesian inference to introduce a formalization of preference utilitarianism in physical world models, specifically cellular automata. Even though our formalization is not immediately applicable, it is a first step in providing ethics and ultimately the question of how to “make the world better” with a formal basis.",2016-09-01,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-23 0:22:50,2747-2759,,9,193,,Synthese,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/WH4KR7BA/Oesterheld - 2016 - Formalizing preference utilitarianism in physical .pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G2PTAEDI,journalArticle,2019,"Oesterheld, Caspar",Approval-directed agency and the decision theory of Newcomb-like problems,Synthese,,,,,,2019,2022-01-30 4:51:06,2022-01-30 4:51:06,,1–14,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/9XXQZD4D/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf; /Users/jacquesthibodeau/Zotero/storage/45AXUP2J/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MKCF7837,journalArticle,2019,"Perry, Brandon; Uuk, Risto",AI Governance and the Policymaking Process: Key Considerations for Reducing AI Risk,Big Data and Cognitive Computing,,,10.3390/bdcc3020026,https://www.mdpi.com/2504-2289/3/2/26,"This essay argues that a new subfield of AI governance should be explored that examines the policy-making process and its implications for AI governance. A growing number of researchers have begun working on the question of how to mitigate the catastrophic risks of transformative artificial intelligence, including what policies states should adopt. However, this essay identifies a preceding, meta-level problem of how the space of possible policies is affected by the politics and administrative mechanisms of how those policies are created and implemented. This creates a new set of key considerations for the field of AI governance and should influence the action of future policymakers. This essay examines some of the theories of the policymaking process, how they compare to current work in AI governance, and their implications for the field at large and ends by identifying areas of future research.",2019-06,2022-01-30 4:49:29,2022-01-30 4:49:29,2020-12-14 23:48:16,26,,2,3,,,AI Governance and the Policymaking Process,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000017  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/2R7BPXBH/Perry and Uuk - 2019 - AI Governance and the Policymaking Process Key Co.pdf,,MetaSafety; Other-org,AI governance; AI risk; policymaking process; typologies of AI policy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G3BNDJ2G,journalArticle,2018,"Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain",When Will AI Exceed Human Performance? Evidence from AI Experts,Journal of Artificial Intelligence Research,,,,http://arxiv.org/abs/1705.08807,"Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.",2018,2022-01-30 4:49:21,2022-03-11 1:39:18,2019-12-16 2:29:10,729–754,,,62,,,When Will AI Exceed Human Performance?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000539  arXiv: 1705.08807,,/Users/jacquesthibodeau/Zotero/storage/MXS9Q4IA/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/Q4VEH3BH/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/LPHKZEY8/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/Z6FGQ2Z6/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/XZIMBZVK/1705.html; /Users/jacquesthibodeau/Zotero/storage/9HN4IS26/1705.html; /Users/jacquesthibodeau/Zotero/storage/J24W79SV/1705.html; /Users/jacquesthibodeau/Zotero/storage/TTDMTXPR/1705.html; /Users/jacquesthibodeau/Zotero/storage/P2MNDVAX/1705.html; /Users/jacquesthibodeau/Zotero/storage/6VC3INB6/1705.html; /Users/jacquesthibodeau/Zotero/storage/MM69SWUB/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/A6EQJ4JQ/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/AENSKWZK/11222.html; /Users/jacquesthibodeau/Zotero/storage/RVNTGCXF/11222.html; /Users/jacquesthibodeau/Zotero/storage/H2NGMISH/11222.html; /Users/jacquesthibodeau/Zotero/storage/227ZHEW8/11222.html; /Users/jacquesthibodeau/Zotero/storage/INUH2ER3/11222.html; /Users/jacquesthibodeau/Zotero/storage/94W3P62Z/11222.html,,AI-Impacts-NotFeatured; FHI; MetaSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D2PUDC8S,journalArticle,2017,"Lin, Henry W.; Tegmark, Max; Rolnick, David",Why Does Deep and Cheap Learning Work So Well?,Journal of Statistical Physics,,"0022-4715, 1572-9613",10.1007/s10955-017-1836-5,http://link.springer.com/10.1007/s10955-017-1836-5,,2017-09,2022-01-30 4:48:55,2022-01-30 4:48:55,2021-11-13 22:54:48,1223-1247,,6,168,,J Stat Phys,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000533,,/Users/jacquesthibodeau/Zotero/storage/4PBUWPQI/Lin et al. - 2017 - Why Does Deep and Cheap Learning Work So Well.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JXVPJ3PB,journalArticle,2017,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",A Formal Approach to the Problem of Logical Non-Omniscience,Electronic Proceedings in Theoretical Computer Science,,2075-2180,10.4204/EPTCS.251.16,http://arxiv.org/abs/1707.08747,"We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.",2017-07-25,2022-01-30 4:50:42,2022-01-30 4:50:42,2019-05-05 21:13:06,221-235,,,251,,Electron. Proc. Theor. Comput. Sci.,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 1707.08747,,/Users/jacquesthibodeau/Zotero/storage/IBNV8E9I/Garrabrant et al. - 2017 - A Formal Approach to the Problem of Logical Non-Om.pdf; /Users/jacquesthibodeau/Zotero/storage/NIV483PR/1707.html; /Users/jacquesthibodeau/Zotero/storage/BA42R2SV/1707.html,,CHAI; TechSafety; MIRI,Computer Science - Logic in Computer Science; F.4.0; G.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62A83IME,journalArticle,2020,"Cave, Stephen; Dihal, Kanta",The Whiteness of AI,Philosophy & Technology,,2210-5441,10.1007/s13347-020-00415-6,https://doi.org/10.1007/s13347-020-00415-6,"This paper focuses on the fact that AI is predominantly portrayed as white—in colour, ethnicity, or both. We first illustrate the prevalent Whiteness of real and imagined intelligent machines in four categories: humanoid robots, chatbots and virtual assistants, stock images of AI, and portrayals of AI in film and television. We then offer three interpretations of the Whiteness of AI, drawing on critical race theory, particularly the idea of the White racial frame. First, we examine the extent to which this Whiteness might simply reflect the predominantly White milieus from which these artefacts arise. Second, we argue that to imagine machines that are intelligent, professional, or powerful is to imagine White machines because the White racial frame ascribes these attributes predominantly to White people. Third, we argue that AI racialised as White allows for a full erasure of people of colour from the White utopian imaginary. Finally, we examine potential consequences of the racialisation of AI, arguing it could exacerbate bias and misdirect concern.",2020-08-06,2022-01-30 4:50:26,2022-01-30 4:50:26,2020-08-21 19:30:56,,,,,,Philos. Technol.,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000036,,/Users/jacquesthibodeau/Zotero/storage/PAGWG4H5/Cave and Dihal - 2020 - The Whiteness of AI.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TWPD2FHB,journalArticle,2021,"Whittlestone, Jess; Arulkumaran, Kai; Crosby, Matthew",The Societal Implications of Deep Reinforcement Learning,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.1.12360,https://doi.org/10.1613/jair.1.12360,"Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications. This article appears in the special track on AI and Society.",2021-05-01,2022-01-30 4:50:26,2022-01-30 4:50:26,2021-10-30 19:47:53,1003–1030,,,70,,J. Artif. Int. Res.,,,,,,,,,,,,,May 2021,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/C3HIICDA/Whittlestone et al. - 2021 - The Societal Implications of Deep Reinforcement Le.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8TWH52ZI,journalArticle,2021,"Stix, Charlotte; Maas, Matthijs M.",Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy,AI and Ethics,,2730-5961,10.1007/s43681-020-00037-w,https://doi.org/10.1007/s43681-020-00037-w,"Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.",2021-08-01,2022-01-30 4:50:25,2022-01-30 4:50:25,2021-10-31 17:04:06,261-271,,3,1,,AI Ethics,Bridging the gap,,,,,,,en,,,,,Springer Link,,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/F7AQ9SK8/Stix and Maas - 2021 - Bridging the gap the case for an ‘Incompletely Th.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GBC9PC5H,journalArticle,2021,"Liu, Hin-Yan; Maas, Matthijs M.",‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence,Futures,,0016-3287,10.1016/j.futures.2020.102672,https://www.sciencedirect.com/science/article/pii/S0016328720301634,"Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.",2021-02-01,2022-01-30 4:50:25,2022-01-30 4:50:25,2021-10-31 17:02:28,102672,,,126,,Futures,‘Solving for X?,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/R74GTS83/S0016328720301634.html,,MetaSafety,Artificial intelligence; Governance disruptors; Governance goldilocks zone; Governance puzzles; Macrostrategic trajectories & destinations; Problem-finding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54ATHF4J,journalArticle,2020,"Peters, Dorian; Vold, Karina; Robinson, Diana; Calvo, Rafael A.",Responsible AI—Two Frameworks for Ethical Design Practice,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2020.2974991,https://ieeexplore.ieee.org/document/9001063/,"In 2019, the IEEE launched the P7000 standards projects intended to address ethical issues in the design of autonomous and intelligent systems. This move came amidst a growing public concern over the unintended consequences of artiﬁcial intelligence (AI), compounded by the lack of an anticipatory process for attending to ethical impact within professional practice. However, the difﬁculty in moving from principles to practice presents a signiﬁcant challenge to the implementation of ethical guidelines. Herein, we describe two complementary frameworks for integrating ethical analysis into engineering practice to help address this challenge. We then provide the outcomes of an ethical analysis informed by these frameworks, conducted within the speciﬁc context of Internet-delivered therapy in digital mental health. We hope both the frameworks and analysis can provide tools and insights, not only for the context of digital healthcare but also for data-enabled and intelligent technology development more broadly.",2020-03,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-08-21 19:54:52,34-47,,1,1,,IEEE Trans. Technol. Soc.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000030,,/Users/jacquesthibodeau/Zotero/storage/RV2HP3AI/Peters et al. - 2020 - Responsible AI—Two Frameworks for Ethical Design P.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAZ95ZMD,journalArticle,2020,"ÓhÉigeartaigh, Seán S.; Whittlestone, Jess; Liu, Yang; Zeng, Yi; Liu, Zhe",Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and Governance,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00402-x,http://link.springer.com/10.1007/s13347-020-00402-x,"Achieving the global benefits of artificial intelligence (AI) will require international cooperation on many areas of governance and ethical standards, while allowing for diverse cultural perspectives and priorities. There are many barriers to achieving this at present, including mistrust between cultures, and more practical challenges of coordinating across different locations. This paper focuses particularly on barriers to cooperation between Europe and North America on the one hand and East Asia on the other, as regions which currently have an outsized impact on the development of AI ethics and governance. We suggest that there is reason to be optimistic about achieving greater cross-cultural cooperation on AI ethics and governance. We argue that misunderstandings between cultures and regions play a more important role in undermining cross-cultural trust, relative to fundamental disagreements, than is often supposed. Even where fundamental differences exist, these may not necessarily prevent productive cross-cultural cooperation, for two reasons: (1) cooperation does not require achieving agreement on principles and standards for all areas of AI; and (2) it is sometimes possible to reach agreement on practical issues despite disagreement on more abstract values or principles. We believe that academia has a key role to play in promoting cross-cultural cooperation on AI ethics and governance, by building greater mutual understanding, and clarifying where different forms of agreement will be both necessary and possible. We make a number of recommendations for practical steps and initiatives, including translation and multilingual publication of key documents, researcher exchange programmes, and development of research agendas on cross-cultural topics.",2020-05-15,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-08-21 19:45:24,,,,,,Philos. Technol.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/I6Z5EXPQ/ÓhÉigeartaigh et al. - 2020 - Overcoming Barriers to Cross-cultural Cooperation .pdf,,MetaSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ8SWGRG,journalArticle,2020,"Coyle, Diane; Weller, Adrian",“Explaining” machine learning reveals policy challenges,Science,,"0036-8075, 1095-9203",,https://www.sciencemag.org/lookup/doi/10.1126/science.aba9647,,2020-06-26,2022-01-30 4:50:25,2022-01-30 4:50:25,2020-08-21 19:38:44,1433-1434,,6498,368,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000021,,/Users/jacquesthibodeau/Zotero/storage/NDWSWJX4/Coyle and Weller - 2020 - “Explaining” machine learning reveals policy chall.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TITT7C74,journalArticle,2020,"Tzachor, Asaf; Whittlestone, Jess; Sundaram, Lalitha; hÉigeartaigh, Seán Ó",Artificial intelligence in a crisis needs ethics with urgency,Nature Machine Intelligence,,2522-5839,10.1038/s42256-020-0195-0,https://www.nature.com/articles/s42256-020-0195-0,"Artificial intelligence tools can help save lives in a pandemic. However, the need to implement technological solutions rapidly raises challenging ethical issues. We need new approaches for ethics with urgency, to ensure AI can be safely and beneficially used in the COVID-19 response and beyond.",2020-07,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-08-21 19:43:33,365-366,,7,2,,,,,,,,,,en,2020 Springer Nature Limited,,,,www.nature.com,,ZSCC: NoCitationData[s2]  ACC: 20  Number: 7 Publisher: Nature Publishing Group,,/Users/jacquesthibodeau/Zotero/storage/C2PV7XMN/Tzachor et al. - 2020 - Artificial intelligence in a crisis needs ethics w.pdf; /Users/jacquesthibodeau/Zotero/storage/N37E4MJK/s42256-020-0195-0.html,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS66RI5N,journalArticle,2019,"Cave, Stephen; Ó hÉigeartaigh, Seán S.",Bridging near- and long-term concerns about AI,Nature Machine Intelligence,,2522-5839,10.1038/s42256-018-0003-2,https://www.nature.com/articles/s42256-018-0003-2,"Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.",2019-01,2022-01-30 4:50:24,2022-01-30 4:50:24,2019-12-16 22:26:28,5-6,,1,1,,,,,,,,,,en,2019 Springer Nature Limited,,,,www.nature.com,,ZSCC: 0000039[s0],,/Users/jacquesthibodeau/Zotero/storage/Q8TE6ISU/Cave and ÓhÉigeartaigh - 2019 - Bridging near- and long-term concerns about AI.pdf; /Users/jacquesthibodeau/Zotero/storage/IBCVWHM9/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/SFDBAMP6/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/VI5J3DQP/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/PMXSKZ3S/s42256-018-0003-2.html,,MetaSafety; CFI; CSER; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DPZRS775,journalArticle,2021,"Zoe Cremer, Carla; Whittlestone, Jess",Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI,International Journal of Interactive Multimedia and Artificial Intelligence,,1989-1660,10.9781/ijimai.2021.02.011,https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf,,2021,2022-01-30 4:50:24,2022-01-30 4:50:24,2021-10-30 19:59:46,100,,5,6,,IJIMAI,Artificial Canaries,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/UJUII49M/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J4M8S57W,journalArticle,2019,"Zerilli, John; Knott, Alistair; Maclaurin, James; Gavaghan, Colin",Algorithmic Decision-Making and the Control Problem,Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-019-09513-7,http://link.springer.com/10.1007/s11023-019-09513-7,"Abstract                            The danger of human operators devolving responsibility to machines and failing to detect cases where they fail has been recognised for many years by industrial psychologists and engineers studying the human operators of complex machines. We call it “the control problem”, understood as the tendency of the human within a human–machine control loop to become complacent, over-reliant or unduly diffident when faced with the outputs of a reliable autonomous system. While the control problem has been investigated for some time, up to this point its manifestation in machine learning contexts has not received serious attention. This paper aims to fill that gap. We argue that, except in certain special circumstances, algorithmic decision tools should not be used in high-stakes or safety-critical decisions unless the systems concerned are significantly “better than human” in the relevant domain or subdomain of decision-making. More concretely, we recommend three strategies to address the control problem, the most promising of which involves a               complementary               (and potentially               dynamic               ) coupling between highly proficient algorithmic tools and human agents working alongside one another. We also identify six key principles which all such human–machine systems should reflect in their design. These can serve as a framework both for assessing the viability of any such human–machine system as well as guiding the design and implementation of such systems generally.",2019-12,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-12-12 17:29:35,555-578,,4,29,,Minds & Machines,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/EFMGD9CK/Zerilli et al. - 2019 - Algorithmic Decision-Making and the Control Proble.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TEXCA9UU,journalArticle,2020,"Hollanek, Tomasz",AI transparency: a matter of reconciling design with critique,AI & Society,,1435-5655,10.1007/s00146-020-01110-y,https://doi.org/10.1007/s00146-020-01110-y,"In the late 2010s, various international committees, expert groups, and national strategy boards have voiced the demand to ‘open’ the algorithmic black box, to audit, expound, and demystify artificial intelligence. The opening of the algorithmic black box, however, cannot be seen only as an engineering challenge. In this article, I argue that only the sort of transparency that arises from critique—a method of theoretical examination that, by revealing pre-existing power structures, aims to challenge them—can help us produce technological systems that are less deceptive and more just. I relate the question of AI transparency to the broader challenge of responsible making, contending that future action must aim to systematically reconcile design—as a way of concealing—with critique—as a manner of revealing.",2020-11-17,2022-01-30 4:50:24,2022-01-30 4:50:24,2020-11-23 1:14:10,,,,,,AI & Soc,AI transparency,,,,,,,en,,,,,Springer Link,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/SMRN47IQ/Hollanek - 2020 - AI transparency a matter of reconciling design wi.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VKMMWSFX,journalArticle,2019,"Schubert, Stefan; Caviola, Lucius; Faber, Nadira S.",The Psychology of Existential Risk: Moral Judgments about Human Extinction,Scientific Reports,,2045-2322,10.1038/s41598-019-50145-9,https://www.nature.com/articles/s41598-019-50145-9,"The 21st century will likely see growing risks of human extinction, but currently, relatively small resources are invested in reducing such existential risks. Using three samples (UK general public, US general public, and UK students; total N = 2,507), we study how laypeople reason about human extinction. We find that people think that human extinction needs to be prevented. Strikingly, however, they do not think that an extinction catastrophe would be uniquely bad relative to near-extinction catastrophes, which allow for recovery. More people find extinction uniquely bad when (a) asked to consider the extinction of an animal species rather than humans, (b) asked to consider a case where human extinction is associated with less direct harm, and (c) they are explicitly prompted to consider long-term consequences of the catastrophes. We conclude that an important reason why people do not find extinction uniquely bad is that they focus on the immediate death and suffering that the catastrophes cause for fellow humans, rather than on the long-term consequences. Finally, we find that (d) laypeople—in line with prominent philosophical arguments—think that the quality of the future is relevant: they do find extinction uniquely bad when this means forgoing a utopian future.",2019-10-21,2022-01-30 4:50:08,2022-01-30 4:50:08,2020-12-12 2:39:30,15100,,1,9,,,The Psychology of Existential Risk,,,,,,,en,2019 The Author(s),,,,www.nature.com,,ZSCC: 0000013  Number: 1 Publisher: Nature Publishing Group,,/Users/jacquesthibodeau/Zotero/storage/E4P9KAJ8/Schubert et al. - 2019 - The Psychology of Existential Risk Moral Judgment.pdf; /Users/jacquesthibodeau/Zotero/storage/CDXPH7UX/s41598-019-50145-9.html,,MetaSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MPBR37U9,journalArticle,2019,"Manheim, David",Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence,Big Data and Cognitive Computing,,,10.3390/bdcc3020021,https://www.mdpi.com/2504-2289/3/2/21,"An important challenge for safety in machine learning and artificial intelligence systems is a set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart&rsquo;s or Campbell&rsquo;s law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.",2019-06,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-11-14 1:02:46,21,,2,3,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000010  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/6A6HJFPW/Manheim - 2019 - Multiparty Dynamics and Failure Modes for Machine .pdf; /Users/jacquesthibodeau/Zotero/storage/TW632HG4/htm.html,,TechSafety; BERI,artificial intelligence safety; Goodhart’s Law; multi-agent systems; specification gaming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHVDZQZN,journalArticle,2021,"Askell, Amanda; Bai, Yuntao; Chen, Anna; Drain, Dawn; Ganguli, Deep; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Mann, Ben; DasSarma, Nova; Elhage, Nelson; Hatfield-Dodds, Zac; Hernandez, Danny; Kernion, Jackson; Ndousse, Kamal; Olsson, Catherine; Amodei, Dario; Brown, Tom; Clark, Jack; McCandlish, Sam; Olah, Chris; Kaplan, Jared",A General Language Assistant as a Laboratory for Alignment,arXiv:2112.00861 [cs],,,,http://arxiv.org/abs/2112.00861,"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.",2021-12-09,2022-01-30 4:49:41,2022-03-11 1:37:07,2021-12-22 20:00:09,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  arXiv: 2112.00861,,/Users/jacquesthibodeau/Zotero/storage/WFQZWMJ9/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/SSYFA967/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/QAQSMINN/2112.html; /Users/jacquesthibodeau/Zotero/storage/HVDJ3MV7/2112.html; /Users/jacquesthibodeau/Zotero/storage/UAHGDRM6/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/4ZQ4JR49/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/FUBMXQNV/2112.html; /Users/jacquesthibodeau/Zotero/storage/L2P2SVKR/2112.html,,Anthropic; TechSafety,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9NGUJJWM,journalArticle,2015,"Pamlin, Dennis; Armstrong, Stuart",Global challenges: 12 risks that threaten human civilization,"Global Challenges Foundation, Stockholm",,,,,,2015,2022-01-30 4:53:17,2022-01-30 4:53:17,,,,,,,,Global challenges,,,,,,,,,,,,Google Scholar,,ZSCC: 0000044,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QFQEA659,journalArticle,2019,"Garfinkel, Ben; Dafoe, Allan",How does the offense-defense balance scale?,Journal of Strategic Studies,,0140-2390,10.1080/01402390.2019.1631810,https://doi.org/10.1080/01402390.2019.1631810,"We ask how the offense-defense balance scales, meaning how it changes as investments into a conflict increase. To do so we offer a general formalization of the offense-defense balance in terms of contest success functions. Simple models of ground invasions and cyberattacks that exploit software vulnerabilities suggest that, in both cases, growth in investments will favor offense when investment levels are sufficiently low and favor defense when they are sufficiently high. We refer to this phenomenon as offensive-then-defensive scaling or OD-scaling. Such scaling effects may help us understand the security implications of applications of artificial intelligence that in essence scale up existing capabilities.",2019-09-19,2022-01-30 4:53:17,2022-01-30 4:53:17,2019-12-16 2:16:59,736-763,,6,42,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000038,,/Users/jacquesthibodeau/Zotero/storage/U7XMZSMQ/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/8BGM36P6/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/GMNCIXGC/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/N5F3S49W/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/6HXGJ6Z6/01402390.2019.html,,MetaSafety; FHI,emerging technologies; Offense-defense theory; strategic stability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCB9MTUK,journalArticle,2018,"Dresler, Martin; Sandberg, Anders; Bublitz, Christoph; Ohla, Kathrin; Trenado, Carlos; Mroczko-Wasowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris",Hacking the brain: dimensions of cognitive enhancement,ACS chemical neuroscience,,,,,,2018,2022-01-30 4:53:17,2022-01-30 4:53:17,,1137–1148,,3,10,,,Hacking the brain,,,,,,,,,,,,Google Scholar,,ZSCC: 0000046,,/Users/jacquesthibodeau/Zotero/storage/ZHIX2ZJF/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/8JC34ET5/Dresler et al. - 2019 - Hacking the brain dimensions of cognitive enhance.pdf; /Users/jacquesthibodeau/Zotero/storage/Q8UUCPHK/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/KMK8UZFI/uuid426d61fb-deab-411c-8778-ca7271e53ead.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M24DR38P,journalArticle,2013,"Armstrong, Stuart",General Purpose Intelligence: Arguing The Orthogonality Thesis,Analysis and Metaphysics,,,,https://www.ceeol.com/search/article-detail?id=137912,"In his paper “The Superintelligent Will,” Nick Bostrom formalized the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe –both as theoretical impractical agents such as AIXI and as physically possible realworld agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.",2013,2022-01-30 4:53:10,2022-01-30 4:53:10,2020-12-18,17,,12,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000032,,/Users/jacquesthibodeau/Zotero/storage/QXXGIDCH/Armstrong - 2020 - GENERAL PURPOSE INTELLIGENCE ARGUING THE ORTHOGON.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B67TRECB,journalArticle,2014,"Müller, Vincent C.; Bostrom, Nick",Future progress in artificial intelligence: A poll among experts,AI Matters,,,,,,2014,2022-01-30 4:53:10,2022-01-30 4:53:10,,9–11,,1,1,,,Future progress in artificial intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000036,,/Users/jacquesthibodeau/Zotero/storage/W2ZTR56A/Müller and Bostrom - 2014 - Future progress in artificial intelligence A poll.pdf; /Users/jacquesthibodeau/Zotero/storage/G8FD3JW5/citation.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DX99RGNH,journalArticle,2013,"Bostrom, Nick",Existential Risk Prevention as Global Priority,Global Policy,,1758-5899,10.1111/1758-5899.12002,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12002,"Existential risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications • Existential risk is a concept that can focus long-term global efforts and sustainability concerns. • The biggest existential risks are anthropogenic and related to potential future technologies. • A moral case can be made that existential risk reduction is strictly more important than any other global public good. • Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sustainable state. • Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity’s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordinated response to anticipated existential risks. • Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existential risks and potential mitigation strategies, with a long-term perspective.",2013,2022-01-30 4:53:10,2022-01-30 4:53:10,2019-12-19 1:40:49,15-31,,1,4,,,,,,,,,,en,"© 2013 University of Durham and John Wiley & Sons, Ltd",,,,Wiley Online Library,,ZSCC: 0000462,,/Users/jacquesthibodeau/Zotero/storage/GQZJQ458/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P5NDJMSA,journalArticle,2020,"Cotton‐Barratt, Owen; Daniel, Max; Sandberg, Anders","Defence in Depth Against Human Extinction: Prevention, Response, Resilience, and Why They All Matter",Global Policy,,1758-5899,10.1111/1758-5899.12786,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12786,"We look at classifying extinction risks in three different ways, which affect how we can intervene to reduce risk. First, how does it start causing damage? Second, how does it reach the scale of a global catastrophe? Third, how does it reach everyone? In all of these three phases there is a defence layer that blocks most risks: First, we can prevent catastrophes from occurring. Second, we can respond to catastrophes before they reach a global scale. Third, humanity is resilient against extinction even in the face of global catastrophes. The largest probability of extinction is posed when all of these defences are weak, that is, by risks we are unlikely to prevent, unlikely to successfully respond to, and unlikely to be resilient against. We find that it’s usually best to invest significantly into strengthening all three defence layers. We also suggest ways to do so tailored to the classes of risk we identify. Lastly, we discuss the importance of underlying risk factors – events or structural conditions that may weaken the defence layers even without posing a risk of immediate extinction themselves.",2020,2022-01-30 4:53:09,2022-01-30 4:53:09,2020-08-18 21:30:09,271-282,,3,11,,,Defence in Depth Against Human Extinction,,,,,,,en,© 2020 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd.,,,,Wiley Online Library,,ZSCC: 0000013  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12786,,/Users/jacquesthibodeau/Zotero/storage/PUZ3UWZ7/Cotton‐Barratt et al. - 2020 - Defence in Depth Against Human Extinction Prevent.pdf; /Users/jacquesthibodeau/Zotero/storage/J7XA65W9/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84KGTFVX,journalArticle,2017,"Petratos, Pythagoras; Sandberg, Anders; Zhou, Feng",Cyber insurance,"Handbook of Cyber-Development, Cyber-Democracy, and Cyber-Defense",,,,,,2017,2022-01-30 4:53:09,2022-01-30 4:53:09,,1–28,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/GMBF55X5/10.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UK5IWQD8,journalArticle,2015,"Cotton-Barratt, Owen; Ord, Toby",Existential risk and existential hope: definitions,Future of Humanity Institute: Technical Report,,,,,,2015,2022-01-30 4:53:09,2022-01-30 4:53:09,,78,,2015,1,,,Existential risk and existential hope,,,,,,,,,,,,Google Scholar,,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/3M5MWCSK/Cotton-Barratt and Ord - 2015 - Existential risk and existential hope definitions.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3K8BZT9,journalArticle,2014,"Sandberg, Anders",Ethics of brain emulations,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,2014,2022-01-30 4:53:09,2022-01-30 4:53:09,,439–457,,3,26,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000030,,/Users/jacquesthibodeau/Zotero/storage/V467G7XP/Sandberg - 2014 - Ethics of brain emulations.pdf; /Users/jacquesthibodeau/Zotero/storage/2I832CBI/0952813X.2014.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZT7EP8S,journalArticle,2018,"Ding, Jeffrey",Deciphering China’s AI dream,Future of Humanity Institute Technical Report,,,,,,2018,2022-01-30 4:53:09,2022-01-30 4:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 134,,/Users/jacquesthibodeau/Zotero/storage/RAZG4SA6/Ding - 2018 - Deciphering China’s AI dream.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQGSD4VT,journalArticle,2015,"Sandberg, Anders",Death and pain of a digital brain,New Scientist,,,,,,2015,2022-01-30 4:53:09,2022-01-30 4:53:09,,26–27,,3038,227,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/I7PHUEE5/S026240791531174X.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QR9VVEN7,journalArticle,2021,"Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore",Cooperative AI: machines must learn to find common ground,Nature,,,10.1038/d41586-021-01170-0,https://www.nature.com/articles/d41586-021-01170-0,"To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.",2021-05,2022-01-30 4:53:09,2022-01-30 4:53:09,2021-11-14 18:21:21,33-36,,7857,593,,,Cooperative AI,,,,,,,en,2021 Nature,,,,www.nature.com,,"ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour",,/Users/jacquesthibodeau/Zotero/storage/2ZKWDVBZ/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/TUF3PTSZ/d41586-021-01170-0.html,,MetaSafety,Computer science; Human behaviour; Machine learning; Society; Sociology; Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IVHRFFNN,journalArticle,2019,"Weiss, Jessica Chen; Dafoe, Allan","Authoritarian Audiences, Rhetoric, and Propaganda in International Crises: Evidence from China",International Studies Quarterly,,,,,,2019,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,"Authoritarian Audiences, Rhetoric, and Propaganda in International Crises",,,,,,,,,,,,Google Scholar,,ZSCC: 0000027,,"/Users/jacquesthibodeau/Zotero/storage/B6NWBFCA/Weiss and Dafoe - 2019 - Authoritarian Audiences, Rhetoric, and Propaganda .pdf",,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4B35E3CP,journalArticle,2020,"O'Brien, John T.; Nelson, Cassidy",Assessing the Risks Posed by the Convergence of Artificial Intelligence and Biotechnology,Health Security,,"2326-5094, 2326-5108",10.1089/hs.2019.0122,https://www.liebertpub.com/doi/10.1089/hs.2019.0122,"Rapid developments are currently taking place in the ﬁelds of artiﬁcial intelligence (AI) and biotechnology, and applications arising from the convergence of these 2 ﬁelds are likely to offer immense opportunities that could greatly beneﬁt human health and biosecurity. The combination of AI and biotechnology could potentially lead to breakthroughs in precision medicine, improved biosurveillance, and discovery of novel medical countermeasures as well as facilitate a more effective public health emergency response. However, as is the case with many preceding transformative technologies, new opportunities often present new risks in parallel. Understanding the current and emerging risks at the intersection of AI and biotechnology is crucial for health security specialists and unlikely to be achieved by examining either ﬁeld in isolation. Uncertainties multiply as technologies merge, showcasing the need to identify robust assessment frameworks that could adequately analyze the risk landscape emerging at the convergence of these 2 domains. This paper explores the criteria needed to assess risks associated with AI and biotechnology and evaluates 3 previously published risk assessment frameworks. After highlighting their strengths and limitations and applying to relevant AI and biotechnology examples, the authors suggest a hybrid framework with recommendations for future approaches to risk assessment for convergent technologies.",2020-06-01,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-08-18 21:41:31,219-227,,3,18,,Health Security,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/MCVXQNE5/O'Brien and Nelson - 2020 - Assessing the Risks Posed by the Convergence of Ar.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8F8CMTV9,journalArticle,2014,"Sandberg, Anders",Being nice to software animals and babies,Intelligence Unbound: The Future of Uploaded and Machine Minds,,,,,,2014,2022-01-30 4:53:08,2022-01-30 4:53:08,,279,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/JW9FBBWG/9781118736302.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3KN6NP9,journalArticle,2003,"Bostrom, Nick",Astronomical Waste: The Opportunity Cost of Delayed Technological Development: Nick Bostrom,Utilitas,,,10.1017/S0953820800004076,,,2003,2022-01-30 4:53:08,2022-01-30 4:53:08,,308–314,,3,15,,,Astronomical Waste,,,,,,,,,,,,PhilPapers,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/BDU9Z56W/Bostrom - 2003 - Astronomical Waste The Opportunity Cost of Delaye.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W36BUBCX,journalArticle,2018,"Duettmann, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman",Artificial General Intelligence: Coordination & Great Powers,"Foresight Institute: Palo Alto, CA, USA",,,,,,2018,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,Artificial General Intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/FSMGIFGF/Duettmann et al. - 2018 - Artificial General Intelligence Coordination & Gr.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5R74U97D,journalArticle,2018,"Ruderman, Avraham; Everett, Richard; Sikder, Bristy; Soyer, Hubert; Uesato, Jonathan; Kumar, Ananya; Beattie, Charlie; Kohli, Pushmeet",Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis,,,,,https://openreview.net/forum?id=SkgZNnR5tX,We find environment settings in which SOTA agents trained on navigation tasks display extreme failures suggesting failures in generalization.,2018-09-27,2022-01-30 4:52:49,2022-01-30 4:52:49,2020-12-12 15:24:23,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/I89H9IRW/Ruderman et al. - 2018 - Uncovering Surprising Behaviors in Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/32BQDHJ9/forum.html,,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P237XTGF,journalArticle,2021,"Everitt, Tom; Hutter, Marcus",Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,Synthese,,,,http://arxiv.org/abs/1908.04734,"Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of modifications to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams. Along the way, we also compare corrigibility and self-preservation properties of the various solutions, and discuss how they can be combined into a single agent without reward tampering incentives.",2021,2022-01-30 4:52:47,2022-01-30 4:52:47,2019-12-16 20:27:10,6435-6467,,,198,,,Reward Tampering Problems and Solutions in Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1908.04734,,/Users/jacquesthibodeau/Zotero/storage/NTKHT668/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/N4BW89UV/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/5598DIDB/1908.html; /Users/jacquesthibodeau/Zotero/storage/72W84CA5/1908.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DUE99KRM,journalArticle,2014,"Muehlhauser, Luke; Bostrom, Nick",Why we need friendly AI,Think,,,,,,2014,2022-01-30 4:53:45,2022-01-30 4:53:45,,41–47,,36,13,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000033,,/Users/jacquesthibodeau/Zotero/storage/328482VT/Muehlhauser and Bostrom - 2014 - Why we need friendly AI.pdf; /Users/jacquesthibodeau/Zotero/storage/ZGFS8MTM/3C576A0EE8DEFDE82FC809493B37A265.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RTZ9S2A,journalArticle,2014,"Armstrong, Stuart; ÓhÉigeartaigh, Seán",Who knows anything about anything about AI?,Intelligence Unbound: The Future of Uploaded and Machine Minds,,,,,,2014,2022-01-30 4:53:45,2022-01-30 4:53:45,,46–60,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/7M3UKW76/9781118736302.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGIF8T8S,journalArticle,2018,"Currie, Adrian; Ó HÉigeartaigh, Seán; Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository",Working together to face humanity’s greatest threats: Introduction to The Future of Research on Catastrophic and Existential Risk.,Futures,,,10.17863/CAM.27560,https://www.repository.cam.ac.uk/handle/1810/280193,"Ours is a resilient species. Around 70,000 years ago our total population may have fallen to between three and ten thousand individuals, possibly due to a supervolcanic eruption (Ambrose 1998) . Yet our ancestors survived, squeezed through the bottleneck, and flourished. But this resilience cannot be taken for granted. We are interconnected and interdependent as never before; the power and scale of our technological capacities are unprecedented. We are in uncharted waters and thus our previous survival is no longer a reason to expect our continued survival (Bostrom 2013). As a result, it is urgent that we develop a systematic understanding of the nature and causes of catastrophic and existential risks.",2018-09-11,2022-01-30 4:53:38,2022-01-30 4:53:38,2020-12-13 22:25:19,,,,,,,Working together to face humanity’s greatest threats,,,,,,,,,,,,DOI.org (Datacite),,ZSCC: NoCitationData[s1]  ACC: 10  Publisher: Apollo - University of Cambridge Repository,,/Users/jacquesthibodeau/Zotero/storage/I5QMVCS8/Currie and Ó HÉigeartaigh - 2018 - Working together to face humanity’s greatest threa.pdf,,MetaSafety; CSER; FLI,,,,,Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7DD2HTK9,journalArticle,2012,"Armstrong, Stuart; Sandberg, Anders; Bostrom, Nick",Thinking Inside the Box: Controlling and Using an Oracle AI,Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-012-9282-2,http://link.springer.com/10.1007/s11023-012-9282-2,,2012-11,2022-01-30 4:53:37,2022-01-30 4:53:37,2020-11-22 5:25:29,299-324,,4,22,,Minds & Machines,Thinking Inside the Box,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000108,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J5FVJG45,journalArticle,2019,"Sandberg, Anders","There is plenty of time at the bottom: the economics, risk and ethics of time compression",foresight,,,,,,2019,2022-01-30 4:53:37,2022-01-30 4:53:37,,84–99,,1,21,,,There is plenty of time at the bottom,,,,,,,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/JJHQ248X/Sandberg - 2019 - There is plenty of time at the bottom the economi.pdf; /Users/jacquesthibodeau/Zotero/storage/IFG9FJ5G/html.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IR2NVS39,journalArticle,2014,"Beckstead, Nick; Bostrom, N.; Bowerman, N.; Cotton-Barratt, O.; MacAskill, W.; Eigeartaigh, S.; Ord, T.",Unprecedented technological risks,Policy brief. Available online: http://www. fhi. ox. ac. uk/wpcontent/uploads/Unprecedented-Technological-Risks. pdf. Last Accessed September,,,,,,2014,2022-01-30 4:53:37,2022-01-30 4:53:37,,2015,,,29,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/VA6NV95K/Beckstead et al. - 2014 - Unprecedented technological risks.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
674VB36I,journalArticle,2014,"Sandberg, Anders",Transhumanism and the Meaning of Life,Religion and Transhumanism: The Unknown Future of Human Enhancement,,,,,,2014,2022-01-30 4:53:37,2022-01-30 4:53:37,,8,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000027,,/Users/jacquesthibodeau/Zotero/storage/WJZMQ83Q/Sandberg - 2014 - Transhumanism and the Meaning of Life.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4UAU4M26,journalArticle,2018,"Bostrom, Nick",The vulnerable world hypothesis,Global Policy,,,,,,2018,2022-01-30 4:53:37,2022-01-30 4:53:37,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s5]  ACC: 74,,/Users/jacquesthibodeau/Zotero/storage/TWVPACKI/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48C7KN4A,journalArticle,2021,"Ding, Jeffrey; Dafoe, Allan",The Logic of Strategic Assets: From Oil to AI,Security Studies,,,10.1080/09636412.2021.1915583,https://arxiv.org/abs/2001.03246,"What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label “strategic” designates those assets that warrant the attention of the highest levels of the state. But these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of “strategic” to advance particular agendas. We aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. We distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. We then employ these logics to clarify three important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence.",2021-06-03,2022-01-30 4:53:36,2022-01-30 4:53:36,,182-212,,2,30,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/ZSBT2TB2/Ding and Dafoe - The Logic of Strategic Assets From Oil to AI.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HVZV6X2T,journalArticle,2014,"Sandberg, Anders",The five biggest threats to human existence,"The Conversation, May",,,,,,2014,2022-01-30 4:53:36,2022-01-30 4:53:36,,,,,29,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9G5EAA7N,journalArticle,2020,"Scholl, Keller; Hanson, Robin",Testing the Automation Revolution Hypothesis,Economics Letters,,,https://doi.org/10.1016/j.econlet.2020.109287,https://www.sciencedirect.com/science/article/abs/pii/S0165176520301919,"Recently, many have predicted an imminent automation revolution, and large resulting job losses. Others have created metrics to predict new patterns in job automation vulnerability. As context to such claims, we test basic theory, two vulnerability metrics, and 251 O*NET job features as predictors of 1505 expert reports regarding automation levels in 832 U.S. job types from 1999 to 2019. We find that pay, employment, and vulnerability metrics are predictive (R^2~0.15), but add little to the top 25 O*NET job features, which together predict far better (R^2~0.55). These best predictors seem understandable in terms of traditional kinds of automation, and have not changed over our time period. Instead, it seems that jobs have changed their features to become more suitable for automation. We thus find no evidence yet of a revolution in the patterns or quantity of automation. And since, over this period, automation increases have predicted neither changes in pay nor employment, this suggests that workers have little to fear if such a revolution does come.",2020-08,2022-01-30 4:53:36,2022-01-30 4:53:36,2020-12-19 5:22:30,,,,193,,,,,,,,,,en,,,,,papers.ssrn.com,,ZSCC: 0000002  DOI: 10.2139/ssrn.3496364,,/Users/jacquesthibodeau/Zotero/storage/5K2P6H4X/papers.html,,MetaSafety; FHI,artificial intelligence; automation; employment; occupations; technology; wages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PGPQFA45,journalArticle,2016,"Bostrom, Nick; Douglas, Thomas; Sandberg, Anders",The Unilateralist’s Curse and the Case for a Principle of Conformity,Social Epistemology,,0269-1728,10.1080/02691728.2015.1108373,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/,"In some situations a number of agents each have the ability to undertake an initiative that would have significant effects on the others. Suppose that each of these agents is purely motivated by an altruistic concern for the common good. We show that if each agent acts on her own personal judgment as to whether the initiative should be undertaken, then the initiative will be undertaken more often than is optimal. We suggest that this phenomenon, which we call the unilateralist’s curse, arises in many contexts, including some that are important for public policy. To lift the curse, we propose a principle of conformity, which would discourage unilateralist action. We consider three different models for how this principle could be implemented, and respond to an objection that could be raised against it.",2016-07-03,2022-01-30 4:53:36,2022-01-30 4:53:36,2019-12-19 1:40:23,350-371,,4,30,,Soc Epistemol,,,,,,,,,,,,,PubMed Central,,ZSCC: NoCitationData[s4]  ACC: 32  PMID: 27499570 PMCID: PMC4959137,,/Users/jacquesthibodeau/Zotero/storage/D58DKCPS/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/6J9H3WV5/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/FGT8BKJU/Bostrom et al. - 2016 - The Unilateralist’s Curse and the Case for a Princ.pdf; ; /Users/jacquesthibodeau/Zotero/storage/A7EZBQIA/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/I6NRCKZ9/02691728.2015.html,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPDHKEBF,journalArticle,2012,"Bostrom, Nick",The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents,Minds and Machines,,,,,,2012,2022-01-30 4:53:36,2022-01-30 4:53:36,,71–85,,2,22,,,The superintelligent will,,,,,,,,,,,,Google Scholar,,ZSCC: 0000268  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/F7ZW2HE4/Bostrom - 2012 - The superintelligent will Motivation and instrume.pdf; /Users/jacquesthibodeau/Zotero/storage/S6ZWXU2P/s11023-012-9281-3.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6K7JDJ5,journalArticle,2014,"Armstrong, Stuart; Sotala, Kaj; Ó hÉigeartaigh, Seán S.","The errors, insights and lessons of famous AI predictions – and what they mean for the future",Journal of Experimental & Theoretical Artificial Intelligence,,"0952-813X, 1362-3079",10.1080/0952813X.2014.895105,https://www.tandfonline.com/doi/full/10.1080/0952813X.2014.895105,,2014-07-03,2022-01-30 4:53:36,2022-01-30 4:53:36,2020-11-22 2:22:03,317-342,,3,26,,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000076,,"/Users/jacquesthibodeau/Zotero/storage/FGDN4MZQ/Armstrong et al. - 2014 - The errors, insights and lessons of famous AI pred.pdf",,MetaSafety; FHI; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WBKQEXZE,journalArticle,2016,"Bostrom, Nick","The Control Problem. Excerpts from Superintelligence: Paths, Dangers, Strategies",Science Fiction and Philosophy: From Time Travel to Superintelligence,,,,,,2016,2022-01-30 4:53:36,2022-01-30 4:53:36,,308–330,,,,,,The Control Problem. Excerpts from Superintelligence,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/HV36EXA9/9781118922590.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BIIKAGQ9,journalArticle,2018,"Tuyls, Karl; Pérolat, Julien; Lanctot, Marc; Ostrovski, Georg; Savani, Rahul; Leibo, Joel Z; Ord, Toby; Graepel, Thore; Legg, Shane",Symmetric Decomposition of Asymmetric Games,Scientific Reports,,2045-2322,10.1038/s41598-018-19194-4,http://www.nature.com/articles/s41598-018-19194-4,,2018-12,2022-01-30 4:53:36,2022-01-30 4:53:36,2020-12-18 6:44:16,1015,,1,8,,Sci Rep,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/QZAG4JGH/Tuyls et al. - 2018 - Symmetric Decomposition of Asymmetric Games.pdf,,TechSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8C98HP2Q,journalArticle,2017,"Bostrom, Nick",Strategic implications of openness in AI development,Global Policy,,,,,,2017,2022-01-30 4:53:35,2022-01-30 4:53:35,,135–148,,2,8,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000118,,/Users/jacquesthibodeau/Zotero/storage/BQUWUV7E/Bostrom - 2017 - Strategic Implications of Openness in span style=.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCZSPKD/1758-5899.html; /Users/jacquesthibodeau/Zotero/storage/VAG7GK35/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PZGR5876,journalArticle,2007,"Bostrom, Nick",Sleeping Beauty and Self-location: A Hybrid Model,Synthese,,"0039-7857, 1573-0964",10.1007/s11229-006-9010-7,http://link.springer.com/10.1007/s11229-006-9010-7,,2007-05-24,2022-01-30 4:53:35,2022-01-30 4:53:35,2020-11-22 4:57:23,59-78,,1,157,,Synthese,Sleeping Beauty and Self-location,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000084,,/Users/jacquesthibodeau/Zotero/storage/3ITIBQQR/Bostrom - 2007 - Sleeping Beauty and Self-location A Hybrid Model.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNTSGQD9,journalArticle,2016,"Armstrong, Stuart; Bostrom, Nick; Shulman, Carl",Racing to the precipice: a model of artificial intelligence development,AI & society,,,,,,2016,2022-01-30 4:53:20,2022-01-30 4:53:20,,201–206,,2,31,,,Racing to the precipice,,,,,,,,,,,,Google Scholar,,ZSCC: 0000117,,/Users/jacquesthibodeau/Zotero/storage/R2KQIP2D/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf; /Users/jacquesthibodeau/Zotero/storage/9DCWMVRI/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/GF3WAQ5J/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/QZ3MJSN2/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PNGRSCMM,journalArticle,2018,"Bostrom, Nick; Dafoe, Allan; Flynn, Carrick",Public Policy and Superintelligent AI: A Vector Field Approach,"Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK",,,,,,2018,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,Public Policy and Superintelligent AI,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009[s0],,/Users/jacquesthibodeau/Zotero/storage/RU3NKC2J/Bostrom et al. - 2018 - Public Policy and Superintelligent AI A Vector Fi.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCEK6ERA,journalArticle,2010,"Ord, Toby; Hillerbrand, Rafaela; Sandberg, Anders",Probing the improbable: methodological challenges for risks with low probabilities and high stakes,Journal of Risk Research,,1366-9877,10.1080/13669870903126267,https://doi.org/10.1080/13669870903126267,"Some risks have extremely high stakes. For example, a worldwide pandemic or asteroid impact could potentially kill more than a billion people. Comfortingly, scientific calcultions often put very low probabilities on the occurrence of such catastrophes. In this paper, we argue that there are important new methodological problems which arise when assessing global catastrophic risks and we focus on a problem regarding probability estimation. When an expert provides a calculation of the probability of an outcome, they are really providing the probability of the outcome occurring, given that their argument is watertight. However, their argument may fail for a number of reasons, such as a flaw in the underlying theory, a flaw in the modelling of the problem or a mistake in the calculations. If the probability estimate given by an argument is dwarfed by the chance that the argument itself is flawed, then the estimate is suspect. We develop this idea formally, explaining how it differs from the related distinction between model and parameter uncertainty. Using the risk estimates from the Large Hadron Collider as a test case, we show how serious the problem can be when it comes to catastrophic risks and how best to address it.",2010-03-01,2022-01-30 4:53:19,2022-01-30 4:53:19,2019-12-19 1:43:42,191-205,,2,13,,,Probing the improbable,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000075,,/Users/jacquesthibodeau/Zotero/storage/WAX69HQK/13669870903126267.html; /Users/jacquesthibodeau/Zotero/storage/GAC3SCP9/Ord et al. - 2010 - Probing the improbable methodological challenges .pdf,,MetaSafety; FHI,calculation error; high stakes; low probability; particle accelerator; risk analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HVB78EJ5,journalArticle,2017,"Farquhar, Sebastian; Cotton-Barratt, Owen; Snyder-Beattie, Andrew",Pricing externalities to balance public risks and benefits of research,Health security,,,,,,2017,2022-01-30 4:53:19,2022-01-30 4:53:19,,401–408,,4,15,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/ZQUZGJQG/PMC5576218.html; /Users/jacquesthibodeau/Zotero/storage/744WRBIH/hs.2016.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z22KID9N,journalArticle,2016,"Bostrom, Nick; Dafoe, Allan; Flynn, Carrick",Policy desiderata in the development of machine superintelligence,"Future of Humanity Institute, University of Oxford. Retrieved June",,,,,,2016,2022-01-30 4:53:19,2022-01-30 4:53:19,,2018,,,8,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/QMJDA2W8/Bostrom et al. - 2016 - Policy desiderata in the development of machine su.pdf,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62278P83,journalArticle,2013,"Dresler, Martin; Sandberg, Anders; Ohla, Kathrin; Bublitz, Christoph; Trenado, Carlos; Mroczko-Wąsowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris",Non-pharmacological cognitive enhancement,Neuropharmacology,,,,,,2013,2022-01-30 4:53:19,2022-01-30 4:53:19,,529–543,,,64,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000187,,/Users/jacquesthibodeau/Zotero/storage/7RW4K4QG/Dresler et al. - 2013 - Non-pharmacological cognitive enhancement.pdf; /Users/jacquesthibodeau/Zotero/storage/X32M9RW4/S0028390812003310.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94KSQKR7,journalArticle,2021,"Prunkl, Carina E. A.; Ashurst, Carolyn; Anderljung, Markus; Webb, Helena; Leike, Jan; Dafoe, Allan",Institutionalizing ethics in AI through broader impact requirements,Nature Machine Intelligence,,2522-5839,10.1038/s42256-021-00298-y,https://www.nature.com/articles/s42256-021-00298-y,"Turning principles into practice is one of the most pressing challenges of artificial intelligence (AI) governance. In this Perspective, we reflect on a governance initiative by one of the world’s largest AI conferences. In 2020, the Conference on Neural Information Processing Systems (NeurIPS) introduced a requirement for submitting authors to include a statement on the broader societal impacts of their research. Drawing insights from similar governance initiatives, including institutional review boards (IRBs) and impact requirements for funding applications, we investigate the risks, challenges and potential benefits of such an initiative. Among the challenges, we list a lack of recognized best practice and procedural transparency, researcher opportunity costs, institutional and social pressures, cognitive biases and the inherently difficult nature of the task. The potential benefits, on the other hand, include improved anticipation and identification of impacts, better communication with policy and governance experts, and a general strengthening of the norms around responsible research. To maximize the chance of success, we recommend measures to increase transparency, improve guidance, create incentives to engage earnestly with the process, and facilitate public deliberation on the requirement’s merits and future. Perhaps the most important contribution from this analysis are the insights we can gain regarding effective community-based governance and the role and responsibility of the AI research community more broadly.",2021-02,2022-01-30 4:53:18,2022-01-30 4:53:18,2021-10-31 19:12:54,104-110,,2,3,,Nat Mach Intell,,,,,,,,en,2021 Springer Nature Limited,,,,www.nature.com,,ZSCC: 0000005  Bandiera_abtest: a Cg_type: Nature Research Journals Number: 2 Primary_atype: Reviews Publisher: Nature Publishing Group Subject_term: Conferences and meetings;Policy;Publishing Subject_term_id: conferences-and-meetings;policy;publishing,,/Users/jacquesthibodeau/Zotero/storage/GQEBEVD3/Prunkl et al. - 2021 - Institutionalizing ethics in AI through broader im.pdf,,MetaSafety,Conferences and meetings; Policy; Publishing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T8G2GCWV,journalArticle,2019,"Russell, Stuart",It's not too soon to be wary of AI: We need to act now to protect humanity from future superintelligent machines,IEEE Spectrum,,1939-9340,10.1109/MSPEC.2019.8847590,,"AI research is making great strides toward its long-term goal of human-level or superhuman intelligent machines. If it succeeds in its current form, however, that could well be catastrophic for the human race. The reason is that the ""standard model"" of AI requires machines to pursue a fixed objective specified by humans. We are unable to specify the objective completely and correctly, nor can we anticipate or prevent the harms that machines pursuing an incorrect objective will create when operating on a global scale with superhuman capabilities. Already, we see examples such as social-media algorithms that learn to optimize click-through by manipulating human preferences, with disastrous consequences for democratic systems.",2019-10,2022-01-30 4:50:54,2022-01-30 4:50:54,,46-51,,10,56,,,It's not too soon to be wary of AI,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/3QV56CCC/8847590.html,,CHAI; TechSafety,artificial intelligence; Artificial intelligence; AI research; Calculators; current form; democratic systems; Earth; fixed objective; human preferences; human race; human-level; humanities; humanity; Robots; Safety; social-media algorithms; standard model; Standards; superhuman capabilities; superhuman intelligent machines; superintelligent machines; Switches,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98C6EF6Z,journalArticle,2018,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca",Inferring Reward Functions from Demonstrators with Unknown Biases,,,,,https://openreview.net/forum?id=rkgqCiRqKQ,"Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work...",2018-09-27,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-12-18 3:11:08,,,,,,,,,,,,,,,,,,,openreview.net,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/S622QK9T/Shah et al. - 2018 - Inferring Reward Functions from Demonstrators with.pdf; /Users/jacquesthibodeau/Zotero/storage/STVESK9J/forum.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IM89NS2R,journalArticle,2020,"Gates, Vael; Griffiths, Thomas L.; Dragan, Anca D.",How to Be Helpful to Multiple People at Once,Cognitive Science,,"0364-0213, 1551-6709",10.1111/cogs.12841,https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12841,"When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decisionmaker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artiﬁcial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.",2020-06,2022-01-30 4:50:53,2022-01-30 4:50:53,2020-11-21 18:26:49,,,6,44,,Cogn Sci,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/F6HG3522/Gates et al. - 2020 - How to Be Helpful to Multiple People at Once.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8TFC6UFR,journalArticle,2013,"Halpern, Joseph Y.; Pass, Rafael",Game Theory with Translucent Players,International Journal of Game Theory,,,,http://arxiv.org/abs/1308.3778,"A traditional assumption in game theory is that players are opaque to one another---if a player changes strategies, then this change in strategies does not affect the choice of other players' strategies. In many situations this is an unrealistic assumption. We develop a framework for reasoning about games where the players may be translucent to one another; in particular, a player may believe that if she were to change strategies, then the other player would also change strategies. Translucent players may achieve significantly more efficient outcomes than opaque ones. Our main result is a characterization of strategies consistent with appropriate analogues of common belief of rationality. Common Counterfactual Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone counterfactually believes that everyone else is rational (i.e., all players i believe that everyone else would still be rational even if $i$ were to switch strategies), (3) everyone counterfactually believes that everyone else is rational, and counterfactually believes that everyone else is rational, and so on. CCBR characterizes the set of strategies surviving iterated removal of minimax dominated strategies, where a strategy s for player i is minimax dominated by s' if the worst-case payoff for i using s' is better than the best possible payoff using s.",2013-08-17,2022-01-30 4:50:52,2022-01-30 4:50:52,2020-11-22 2:29:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000021  arXiv: 1308.3778,,/Users/jacquesthibodeau/Zotero/storage/UZXNUF5J/Halpern and Pass - 2013 - Game Theory with Translucent Players.pdf; /Users/jacquesthibodeau/Zotero/storage/DDVE6APB/1308.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EMNDB395,journalArticle,2017,"Huang, Sandy H.; Held, David; Abbeel, Pieter; Dragan, Anca D.",Enabling Robots to Communicate their Objectives,Autonomous Robots,,,10.15607/RSS.2017.XIII.059,https://arxiv.org/abs/1702.03465v2,"The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.",2017-02-11,2022-01-30 4:50:45,2022-01-30 4:50:45,2019-12-18 1:39:51,,,,43,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/XI6MUDJM/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf; /Users/jacquesthibodeau/Zotero/storage/3W9QBCM2/1702.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VSQNXUU5,journalArticle,2019,"Griffiths, Thomas L; Callaway, Frederick; Chang, Michael B; Grant, Erin; Krueger, Paul M; Lieder, Falk",Doing more with less: meta-reasoning and meta-learning in humans and machines,Current Opinion in Behavioral Sciences,,2352-1546,10.1016/j.cobeha.2019.01.005,http://www.sciencedirect.com/science/article/pii/S2352154618302122,"Artificial intelligence systems use an increasing amount of computation and data to solve very specific problems. By contrast, human minds solve a wide range of problems using a fixed amount of computation and limited experience. We identify two abilities that we see as crucial to this kind of general intelligence: meta-reasoning (deciding how to allocate computational resources) and meta-learning (modeling the learning environment to make better use of limited data). We summarize the relevant AI literature and relate the resulting ideas to recent work in psychology.",2019-10-01,2022-01-30 4:50:44,2022-01-30 4:50:44,2020-12-17 22:22:24,24-30,,,29,,Current Opinion in Behavioral Sciences,Doing more with less,SI: 29: Artificial Intelligence (2019),,,,,,en,,,,,ScienceDirect,,ZSCC: 0000039,,/Users/jacquesthibodeau/Zotero/storage/D7G3XQGQ/Griffiths et al. - 2019 - Doing more with less meta-reasoning and meta-lear.pdf; /Users/jacquesthibodeau/Zotero/storage/Z9B62VG8/S2352154618302122.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I6ETESD7,journalArticle,2019,"Fridovich-Keil, David; Bajcsy, Andrea; Fisac, Jaime F; Herbert, Sylvia L; Wang, Steven; Dragan, Anca D; Tomlin, Claire J",Confidence-aware motion prediction for real-time collision avoidance <sup>1</sup>,The International Journal of Robotics Research,,"0278-3649, 1741-3176",10.1177/0278364919859436,http://journals.sagepub.com/doi/10.1177/0278364919859436,"One of the most difficult challenges in robot motion planning is to account for the behavior of other moving agents, such as humans. Commonly, practitioners employ predictive models to reason about where other agents are going to move. Though there has been much recent work in building predictive models, no model is ever perfect: an agent can always move unexpectedly, in a way that is not predicted or not assigned sufficient probability. In such cases, the robot may plan trajectories that appear safe but, in fact, lead to collision. Rather than trust a model’s predictions blindly, we propose that the robot should use the model’s current predictive accuracy to inform the degree of confidence in its future predictions. This model confidence inference allows us to generate probabilistic motion predictions that exploit modeled structure when the structure successfully explains human motion, and degrade gracefully whenever the human moves unexpectedly. We accomplish this by maintaining a Bayesian belief over a single parameter that governs the variance of our human motion model. We couple this prediction algorithm with a recently proposed robust motion planner and controller to guide the construction of robot trajectories that are, to a good approximation, collision-free with a high, user-specified probability. We provide extensive analysis of the combined approach and its overall safety properties by establishing a connection to reachability analysis, and conclude with a hardware demonstration in which a small quadcopter operates safely in the same space as a human pedestrian.",2019-06-24,2022-01-30 4:50:43,2022-01-30 4:50:43,2019-07-08 16:10:37,27836491985943,,,,,The International Journal of Robotics Research,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s3]  ACC: 40,,,,CHAI; TechSafety,human motion prediction; motion planning; robust control; safety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZTIEC2KV,journalArticle,2017,"Russell, Stuart","Artificial intelligence: The future is superintelligent [Book review of ""Life 3.0: Being Human in the Age of Artificial Intelligence"" by Max Tegmark]",Nature,,1476-4687,10.1038/548520a,http://www.nature.com/articles/548520a,Stuart Russell weighs up a book on the risks and rewards of the AI revolution.,2017-08-30,2022-01-30 4:50:43,2022-01-30 4:50:43,2019-04-03 0:22:19,520-521,,,548,,,Artificial intelligence,,,,,,,en,,,,,www-nature-com.proxy.lib.uwaterloo.ca,,ZSCC: NoCitationData[s3]  ACC: 26  J: 8,,/Users/jacquesthibodeau/Zotero/storage/M2ASUJMQ/548520a.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3SF48G8T,journalArticle,2020,"Mohamed, Shakir; Png, Marie-Therese; Isaac, William",Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00405-8,http://arxiv.org/abs/2007.04068,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.",2020-12,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-12-12 15:23:49,659-684,,4,33,,Philos. Technol.,Decolonial AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2007.04068,,/Users/jacquesthibodeau/Zotero/storage/H43MBXZN/Mohamed et al. - 2020 - Decolonial AI Decolonial Theory as Sociotechnical.pdf; /Users/jacquesthibodeau/Zotero/storage/BUXA7WXT/2007.html,,MetaSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IHS4DSU,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus",Curiosity Killed the Cat and the Asymptotically Optimal Agent,IEEE Journal on Selected Areas in Information Theory,,,,http://arxiv.org/abs/2006.03357,"Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either destroyed or incapacitated with probability 1; both of these are forms of traps as understood in the Markov Decision Process literature. Environments with traps pose a well-known problem for agents, but we are unaware of other work which shows that traps are not only a risk, but a certainty, for agents of a certain caliber. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration.",2021-05-14,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-09-05 17:28:03,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.03357,,/Users/jacquesthibodeau/Zotero/storage/3TP7IVVC/Cohen and Hutter - 2020 - Curiosity Killed the Cat and the Asymptotically Op.pdf; /Users/jacquesthibodeau/Zotero/storage/ANRSI6HU/2006.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XC5KJ9EU,journalArticle,2020,"Gabriel, Iason","Artificial Intelligence, Values and Alignment",Minds and Machines,,,,http://arxiv.org/abs/2001.09768,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",2020-01-13,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-08-18 21:26:03,,,,30,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000062  arXiv: 2001.09768,,"/Users/jacquesthibodeau/Zotero/storage/2GPMB6EI/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/M2KN5UVP/2001.html",,TechSafety; DeepMind,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7947JHHW,journalArticle,2017,"Sotala, Kaj; Gloor, Lukas",Superintelligence As a Cause or Cure For Risks of Astronomical Suffering,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1877,"Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.",2017-12-27,2022-01-30 4:51:37,2022-01-30 4:51:37,2019-12-16 3:31:47,,,4,41,,,,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/GV5IEVSF/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/SV2BI2PQ/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/5A4BQUP4/1877.html; /Users/jacquesthibodeau/Zotero/storage/UXP4MDUQ/1877.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRIMD86J,journalArticle,,"MacAskill, William; Vallinder, Aron; Shulman, Carl; Österheld, Caspar; Treutlein, Johannes",The Evidentialist’s Wager,Journal of Philosophy,,,,,"Suppose that an altruistic and morally motivated agent who is uncertain between evidential decision theory (EDT) and causal decision theory (CDT) finds herself in a situation in which the two theories give conflicting verdicts. We argue that even if she has significantly higher credence in CDT, she should nevertheless act in accordance with EDT. First, we claim that that the appropriate response to normative uncertainty is to hedge one’s bets. That is, if the stakes are much higher on one theory than another, and the credences you assign to each of these theories aren’t very different, then it’s appropriate to choose the option which performs best on the high-stakes theory. Second, we show that, given the assumption of altruism, the existence of correlated decision-makers will increase the stakes for EDT but leave the stakes for CDT unaffected. Together these two claims imply that whenever there are sufficiently many correlated agents, the appropriate response is to act in accordance with EDT.",forthcoming,2022-01-30 4:51:37,2022-01-30 4:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s1],,/Users/jacquesthibodeau/Zotero/storage/VQ2I8RXI/MacAskill et al. - The Evidentialist’s Wager.pdf,,CLR; TechSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FVCIV793,journalArticle,2019,"Oesterheld, Caspar",Robust program equilibrium,Theory and Decision,,,,,,2019,2022-01-30 4:51:36,2022-01-30 4:51:36,,143–159,,1,86,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/V78JH7AD/s11238-018-9679-3.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82EKFW97,journalArticle,2020,"Levinstein, Benjamin A.; Soares, Nate; Journal of Philosophy Inc.",Cheating Death in Damascus,The Journal of Philosophy,,0022-362X,10.5840/jphil2020117516,http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_2020_0117_0005_0237_0266&svc_id=info:www.pdcnet.org/collection,"Evidential and Causal Decision Theory are the leading contenders as theories of rational action, but both face fatal counterexamples. We present some new counterexamples, including one in which the optimal action is causally dominated. We also present a novel decision theory, Functional Decision Theory (fdt), which simultaneously solves both sets of counterexamples. Instead of considering which physical action of theirs would give rise to the best outcomes, fdt agents consider which output of their decision function would give rise to the best outcome. This theory relies on a notion of subjunctive dependence, where multiple implementations of the same mathematical function are considered (even counterfactually) to have identical results for logical rather than causal reasons. Taking these subjunctive dependencies into account allows fdt agents to outperform cdt and edt agents in, e.g., the presence of accurate predictors. While not necessary for considering classic decision theory problems, we note that a full speciﬁcation of fdt will require a non-trivial theory of logical counterfactuals and algorithmic similarity.",2020,2022-01-30 4:56:47,2022-01-30 4:56:47,2020-12-13 21:01:53,237-266,,5,117,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000011[s0],,/Users/jacquesthibodeau/Zotero/storage/PP23HCQJ/Levinstein et al. - 2020 - Cheating Death in Damascus.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG84SPVN,journalArticle,2020,"Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew; Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew",Alignment for Advanced Machine Learning Systems,Ethics of Artificial Intelligence,,,10.1093/oso/9780190905033.003.0013,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780190905033.001.0001/oso-9780190905033-chapter-13,"We survey eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? We focus on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions, and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers.",2020-09-17,2022-01-30 4:56:46,2022-01-30 4:56:46,2020-12-13 19:28:53,342-382,,,,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s0]  ZSCC: NoCitationData[s1]  ACC: 73,,/Users/jacquesthibodeau/Zotero/storage/WQNGUWMV/Taylor et al. - 2020 - Alignment for Advanced Machine Learning Systems.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X22NB2NG,journalArticle,2019,"Critch, Andrew","A Parametric, Resource-Bounded Generalization Of Löb’s Theorem, And A Robust Cooperation Criterion For Open-Source Game Theory",The Journal of Symbolic Logic,,"0022-4812, 1943-5886",10.1017/jsl.2017.42,https://www.cambridge.org/core/product/identifier/S0022481217000421/type/journal_article,"Abstract             This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.",2019-12,2022-01-30 4:56:46,2022-01-30 4:56:46,2020-11-22 5:00:04,1368-1381,,4,84,,J. symb. log.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 6,,,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9D7MHQDT,journalArticle,2019,"Trammell, Philip",Fixed-point solutions to the regress problem in normative uncertainty,Synthese,,1573-0964,10.1007/s11229-019-02098-9,https://doi.org/10.1007/s11229-019-02098-9,"When we are faced with a choice among acts, but are uncertain about the true state of the world, we may be uncertain about the acts’ “choiceworthiness”. Decision theories guide our choice by making normative claims about how we should respond to this uncertainty. If we are unsure which decision theory is correct, however, we may remain unsure of what we ought to do. Given this decision-theoretic uncertainty, meta-theories attempt to resolve the conflicts between our decision theories...but we may be unsure which meta-theory is correct as well. This reasoning can launch a regress of ever-higher-order uncertainty, which may leave one forever uncertain about what one ought to do. There is, fortunately, a class of circumstances under which this regress is not a problem. If one holds a cardinal understanding of subjective choiceworthiness, and accepts certain other criteria (which are too weak to specify any particular decision theory), one’s hierarchy of metanormative uncertainty ultimately converges to precise definitions of “subjective choiceworthiness” for any finite set of acts. If one allows the metanormative regress to extend to the transfinite ordinals, the convergence criteria can be weakened further. Finally, the structure of these results applies straightforwardly not just to decision-theoretic uncertainty, but also to other varieties of normative uncertainty, such as moral uncertainty.",2019-02-14,2022-01-30 4:55:29,2022-01-30 4:55:29,2020-12-13 23:49:11,,,,,,Synthese,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/QH6HTFAV/Trammell - 2019 - Fixed-point solutions to the regress problem in no.pdf,,MetaSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CD8ANBSK,journalArticle,2017,"Baum, Seth D.","The Social Science of Computerized Brains – Review of The Age of Em: Work, Love, and Life When Robots Rule the Earth by Robin Hanson (Oxford University Press, 2016)",Futures,,163287,10.1016/j.futures.2017.03.005,https://linkinghub.elsevier.com/retrieve/pii/S0016328716302518,,2017-06,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:52:35,61-63,,,90,,Futures,The Social Science of Computerized Brains – Review of The Age of Em,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X38XZZ28,journalArticle,2014,"Baum, Seth D",The great downside dilemma for risky emerging technologies,Physica Scripta,,"0031-8949, 1402-4896",10.1088/0031-8949/89/12/128004,http://stacks.iop.org/1402-4896/89/i=12/a=128004?key=crossref.f5938bc78a3023d740968f020cfa9970,,2014-12-01,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:49:17,128004,,12,89,,Phys. Scr.,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/4B6SN7S9/Baum - 2014 - The great downside dilemma for risky emerging tech.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NNMI86Z,journalArticle,2015,"Baum, Seth D.",The far future argument for confronting catastrophic threats to humanity: Practical significance and alternatives,Futures,,163287,10.1016/j.futures.2015.03.001,https://linkinghub.elsevier.com/retrieve/pii/S0016328715000312,,2015-09,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:45:25,86-96,,,72,,Futures,The far future argument for confronting catastrophic threats to humanity,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000025,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H8KFKUZ2,journalArticle,2013,"Baum, Seth D.; Wilson, Grant S.",The Ethics of Global Catastrophic Risk from Dual-Use Bioengineering,"Ethics in Biology, Engineering and Medicine",,2151-805X,10.1615/EthicsBiologyEngMed.2013007629,"http://www.dl.begellhouse.com/journals/6ed509641f7324e6,709fef245eef4861,06d520d747a5c0d1.html",,2013,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:49:31,59-72,,1,4,,Ethics Biology Eng Med,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WFRENGDG,journalArticle,2018,"Baum, Seth D.",Reconciliation between factions focused on near-term and long-term artificial intelligence,AI & Society,,,,,,2018,2022-01-30 4:55:20,2022-01-30 4:55:20,,565–572,,4,33,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000034,,/Users/jacquesthibodeau/Zotero/storage/Q3E37524/Baum - 2018 - Reconciliation between factions focused on near-te.pdf; /Users/jacquesthibodeau/Zotero/storage/VRCUNE3P/s00146-017-0734-3.html,,MetaSafety; GCRI,Artificial Intelligence; Artificial General Intelligence; Artificial Superintelligence; Long-Term Artificial Intelligence; Near-Term Artificial Intelligence; Societal Impacts of Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RP8S2RZK,journalArticle,2020,"Baum, Seth D.",Quantifying the probability of existential catastrophe: A reply to Beard et al.,Futures,,0016-3287,10.1016/j.futures.2020.102608,http://www.sciencedirect.com/science/article/pii/S0016328720300987,"A recent article by Beard, Rowe, and Fox (BRF) evaluates ten methodologies for quantifying the probability of existential catastrophe. This article builds on BRF’s valuable contribution. First, this article describes the conceptual and mathematical relationship between the probability of existential catastrophe and the severity of events that could result in existential catastrophe. It discusses complications in this relationship arising from catastrophes occurring at different speeds and from multiple concurrent catastrophes. Second, this article revisits the ten BRF methodologies, finding an inverse relationship between a methodology’s ease of use and the quality of results it produces—in other words, achieving a higher quality of analysis will in general require a larger investment in analysis. Third, the manuscript discusses the role of probability quantification in the management of existential risks, describing why the probability is only sometimes needed for decision-making and arguing that analyses should support real-world risk management decisions and not just be academic exercises. If the findings of this article are taken into account, together with BRF’s evaluations of specific methodologies, then risk analyses of existential catastrophe may tend to be more successful at understanding and reducing the risks.",2020-10-01,2022-01-30 4:55:20,2022-01-30 4:55:20,2020-12-19 3:08:57,102608,,,123,,Futures,Quantifying the probability of existential catastrophe,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/TFM6GRAD/S0016328720300987.html,,MetaSafety; AmbiguosSafety; GCRI,Existential risk; Global catastrophic risk; Probability; Risk analysis; Severity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58XWHNKX,journalArticle,2019,"Baum, Seth D.",Preparing for the unthinkable,Science,,"0036-8075, 1095-9203",10.1126/science.aay4219,http://www.sciencemag.org/lookup/doi/10.1126/science.aay4219,,2019-09-20,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:51:12,1254-1254,,6459,365,,Science,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIJHCRX3,journalArticle,2017,"Baum, Seth D.",On the promotion of safe and socially beneficial artificial intelligence,AI & Society,,"0951-5666, 1435-5655",10.1007/s00146-016-0677-0,http://link.springer.com/10.1007/s00146-016-0677-0,,2017-11,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:44:37,543-551,,4,32,,AI & Soc,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000080,,/Users/jacquesthibodeau/Zotero/storage/ZASDEGNA/papers.html,,MetaSafety; GCRI,artificial intelligence; artificial intelligence safety; beneficial artificial intelligence; social psychology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JA98S3JN,journalArticle,2017,"Barrett, Anthony Michael",Value of Global Catastrophic Risk (GCR) Information: Cost-Effectiveness-Based Approach for GCR Reduction,Decision Analysis,,"1545-8490, 1545-8504",10.1287/deca.2017.0350,http://pubsonline.informs.org/doi/10.1287/deca.2017.0350,,2017-09,2022-01-30 4:55:20,2022-01-30 4:55:20,2019-12-16 2:44:44,187-203,,3,14,,Decision Analysis,Value of Global Catastrophic Risk (GCR) Information,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/26R7JATJ/Barrett - 2017 - Value of Global Catastrophic Risk (GCR) Informatio.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DS8HKV3N,journalArticle,2018,"Baum, Seth",Superintelligence skepticism as a political tool,Information,,,,,,2018,2022-01-30 4:55:20,2022-01-30 4:55:20,,209,,9,9,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000019,,/Users/jacquesthibodeau/Zotero/storage/GTRGDFM2/Baum - 2018 - Superintelligence skepticism as a political tool.pdf; /Users/jacquesthibodeau/Zotero/storage/NRXVDB2J/Baum - 2018 - Superintelligence Skepticism as a Political Tool.pdf; /Users/jacquesthibodeau/Zotero/storage/IVKG4C2A/209.html,,MetaSafety; GCRI,artificial intelligence; skepticism; superintelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6VUTUU52,journalArticle,2020,"Baum, Seth D.",Social choice ethics in artificial intelligence,AI & Society,,"0951-5666, 1435-5655",10.1007/s00146-017-0760-1,http://link.springer.com/10.1007/s00146-017-0760-1,"A major approach to the ethics of artificial intelligence (AI) is to use social choice, in which the AI is designed to act according to the aggregate views of society. This is found in the AI ethics of “coherent extrapolated volition” and “bottom-up ethics”. This paper shows that the normative basis of AI social choice ethics is weak due to the fact that there is no one single aggregate ethical view of society. Instead, the design of social choice AI faces three sets of decisions: standing, concerning whose ethics views are included; measurement, concerning how their views are identified; and aggregation, concerning how individual views are combined to a single view that will guide AI behavior. These decisions must be made up front in the initial AI design—designers cannot “let the AI figure it out”. Each set of decisions poses difficult ethical dilemmas with major consequences for AI behavior, with some decision options yielding pathological or even catastrophic results. Furthermore, non-social choice ethics face similar issues, such as whether to count future generations or the AI itself. These issues can be more important than the question of whether or not to use social choice ethics. Attention should focus on these issues, not on social choice.",2020-03-01,2022-01-30 4:55:20,2022-01-30 4:55:20,2020-08-20 20:16:41,165-176,,1,35,,AI & Soc,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000060,,/Users/jacquesthibodeau/Zotero/storage/5EKPE8IC/Baum - 2017 - Social Choice Ethics in Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/NNB6BD38/Baum - 2020 - Social choice ethics in artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/JZWAP722/papers.html,,MetaSafety; AmbiguosSafety; GCRI,artificial intelligence; bottom-up ethics; coherent extrapolated volition; ethics; social choice,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XC4B3IJS,journalArticle,2021,"Owe, Andrea; Baum, Seth D.",Moral consideration of nonhumans in the ethics of artificial intelligence,AI and Ethics,,"2730-5953, 2730-5961",10.1007/s43681-021-00065-0,https://link.springer.com/10.1007/s43681-021-00065-0,,2021-11,2022-01-30 4:55:20,2022-01-30 4:55:20,2021-10-31 19:21:16,517-528,,4,1,,AI Ethics,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/BKTUR9DN/Owe and Baum - 2021 - Moral consideration of nonhumans in the ethics of .pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AEG34VHF,journalArticle,2013,"Wilson, Grant",Minimizing global catastrophic and existential risks from emerging technologies through international law,Va. Envtl. LJ,,,,,,2013,2022-01-30 4:55:19,2022-01-30 4:55:19,,307,,,31,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000049,,/Users/jacquesthibodeau/Zotero/storage/8UD9P888/LandingPage.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4UIDG8GK,journalArticle,2020,"Baum, Seth D.",Medium-Term Artificial Intelligence and Society,Information,,,10.3390/info11060290,https://www.mdpi.com/2078-2489/11/6/290,"There has been extensive attention to near-term and long-term AI technology and its accompanying societal issues, but the medium-term has gone largely overlooked. This paper develops the concept of medium-term AI, evaluates its importance, and analyzes some medium-term societal issues. Medium-term AI can be important in its own right and as a topic that can bridge the sometimes acrimonious divide between those who favor attention to near-term AI and those who prefer the long-term. The paper proposes the medium-term AI hypothesis: the medium-term is important from the perspectives of those who favor attention to near-term AI as well as those who favor attention to long-term AI. The paper analyzes medium-term AI in terms of governance institutions, collective action, corporate AI development, and military/national security communities. Across portions of these four areas, some support for the medium-term AI hypothesis is found, though in some cases the matter is unclear.",2020-06,2022-01-30 4:55:19,2022-01-30 4:55:19,2020-08-20 20:15:47,290,,6,11,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000009  Number: 6 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/DCX936S5/Baum - 2020 - Medium-Term Artificial Intelligence and Society.pdf; /Users/jacquesthibodeau/Zotero/storage/T5QSRRBW/290.html,,MetaSafety; GCRI,intermediate-term AI; long-term AI; medium-term AI; mid-term AI; near-term AI; societal implications of AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9D355IDI,journalArticle,2017,"White, Trevor N.; Baum, Seth D.",Liability For Present And Future Robotics Technology,Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence,,,,,,2017,2022-01-30 4:55:19,2022-01-30 4:55:19,,5,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/M4J3T437/books.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B8AWD78V,journalArticle,2021,"de Neufville, Robert; Baum, Seth D.",Collective action on artificial intelligence: A primer and review,Technology in Society,,0160791X,10.1016/j.techsoc.2021.101649,https://linkinghub.elsevier.com/retrieve/pii/S0160791X2100124X,,2021-08,2022-01-30 4:55:19,2022-01-30 4:55:19,2021-10-31 19:22:17,101649,,,66,,Technology in Society,Collective action on artificial intelligence,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000004,,,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JEU5BHGP,journalArticle,2021,"Owe, Andrea; Baum, Seth",Artificial Intelligence Needs Environmental Ethics,"Ethics, Policy, and Environment",,,,,,2021-11-14,2022-01-30 4:55:19,2022-01-30 4:55:19,,4,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/SXDZ5JXB/Baum - Artificial Intelligence Needs Environmental Ethics.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2H4QGF9B,journalArticle,2017,"Baum, Seth; Barrett, Anthony; Yampolskiy, Roman V.",Modeling and interpreting expert disagreement about artificial superintelligence,Informatica,,,,,,2017,2022-01-30 4:55:19,2022-01-30 4:55:19,,419–428,,7,41,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000018,,/Users/jacquesthibodeau/Zotero/storage/KGZTRH7I/Baum et al. - 2017 - Modeling and interpreting expert disagreement abou.pdf; /Users/jacquesthibodeau/Zotero/storage/27KSX2XF/papers.html; /Users/jacquesthibodeau/Zotero/storage/8ENBDWVI/papers.html,,MetaSafety; GCRI,artificial intelligence; risk analysis; artificial superintelligence; expert disagreement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UKKJMH24,journalArticle,2021,"Baum, Seth; Owe, Andrea",From AI for People to AI for the World and the Universe,AI & Society,,,,,,2021-11-30,2022-01-30 4:55:19,2022-01-30 4:55:19,,3,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/QXWKBC9Z/Baum - From AI for People to AI for the World and the Uni.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQF5DVFH,journalArticle,2018,"Baum, Seth",Countering Superintelligence Misinformation,Information,,,,,,2018,2022-01-30 4:55:19,2022-01-30 4:55:19,,244,,10,9,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/JG6I57DX/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/BDEHJSEN/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/W7VRFICJ/244.html,,MetaSafety; GCRI,artificial intelligence; superintelligence; misinformation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6VMXGGZ,journalArticle,2021,"Cihon, Peter; Schuett, Jonas; Baum, Seth D.",Corporate Governance of Artificial Intelligence in the Public Interest,Information,,2078-2489,10.3390/info12070275,https://www.mdpi.com/2078-2489/12/7/275,"Corporations play a major role in artificial intelligence (AI) research, development, and deployment, with profound consequences for society. This paper surveys opportunities to improve how corporations govern their AI activities so as to better advance the public interest. The paper focuses on the roles of and opportunities for a wide range of actors inside the corporation—managers, workers, and investors—and outside the corporation—corporate partners and competitors, industry consortia, nonprofit organizations, the public, the media, and governments. Whereas prior work on multistakeholder AI governance has proposed dedicated institutions to bring together diverse actors and stakeholders, this paper explores the opportunities they have even in the absence of dedicated multistakeholder institutions. The paper illustrates these opportunities with many cases, including the participation of Google in the U.S. Department of Defense Project Maven; the publication of potentially harmful AI research by OpenAI, with input from the Partnership on AI; and the sale of facial recognition technology to law enforcement by corporations including Amazon, IBM, and Microsoft. These and other cases demonstrate the wide range of mechanisms to advance AI corporate governance in the public interest, especially when diverse actors work together.",2021-07-05,2022-01-30 4:55:19,2022-01-30 4:55:19,2021-10-31 19:21:54,275,,7,12,,Information,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/63ATIFMI/Cihon et al. - 2021 - Corporate Governance of Artificial Intelligence in.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8HC9X6CU,journalArticle,2015,"Baum, Seth D.; Tonn, Bruce E.",Confronting future catastrophic threats to humanity,Futures,,163287,10.1016/j.futures.2015.08.004,https://linkinghub.elsevier.com/retrieve/pii/S0016328715001135,,2015-09,2022-01-30 4:55:19,2022-01-30 4:55:19,2019-12-16 2:52:43,1-3,,,72,,Futures,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000012,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PJH97DIN,journalArticle,2020,"Baum, Seth D.",Artificial Interdisciplinarity: Artificial Intelligence for Research on Complex Societal Problems,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00416-5,http://link.springer.com/10.1007/s13347-020-00416-5,"This paper considers the question: In what ways can artificial intelligence assist with interdisciplinary research for addressing complex societal problems and advancing the social good? Problems such as environmental protection, public health, and emerging technology governance do not fit neatly within traditional academic disciplines and therefore require an interdisciplinary approach. However, interdisciplinary research poses large cognitive challenges for human researchers that go beyond the substantial challenges of narrow disciplinary research. The challenges include epistemic divides between disciplines, the massive bodies of relevant literature, the peer review of work that integrates an eclectic mix of topics, and the transfer of interdisciplinary research insights from one problem to another. Artificial interdisciplinarity already helps with these challenges via search engines, recommendation engines, and automated content analysis. Future “strong artificial interdisciplinarity” based on human-level artificial general intelligence could excel at interdisciplinary research, but it may take a long time to develop and could pose major safety and ethical issues. Therefore, there is an important role for intermediate-term artificial interdisciplinarity systems that could make major contributions to addressing societal problems without the concerns associated with artificial general intelligence.",2020-07-16,2022-01-30 4:55:19,2022-01-30 4:55:19,2020-08-20 20:15:09,,,,,,Philos. Technol.,Artificial Interdisciplinarity,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/89NP8X9J/Baum - 2020 - Artificial Interdisciplinarity Artificial Intelli.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PVCGA76X,journalArticle,2021,"Galaz, Victor; Centeno, Miguel A.; Callahan, Peter W.; Causevic, Amar; Patterson, Thayer; Brass, Irina; Baum, Seth; Farber, Darryl; Fischer, Joern; Garcia, David; McPhearson, Timon; Jimenez, Daniel; King, Brian; Larcey, Paul; Levy, Karen","Artificial intelligence, systemic risks, and sustainability",Technology in Society,,0160791X,10.1016/j.techsoc.2021.101741,https://linkinghub.elsevier.com/retrieve/pii/S0160791X21002165,,2021-11,2022-01-30 4:55:19,2022-01-30 4:55:19,2021-10-31 19:21:00,101741,,,67,,Technology in Society,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 4,,"/Users/jacquesthibodeau/Zotero/storage/2HV437ZM/Galaz et al. - 2021 - Artificial intelligence, systemic risks, and susta.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TH67SMSJ,journalArticle,2021,"Cihon, Peter; Kleinaltenkamp, Moritz J.; Schuett, Jonas; Baum, Seth D.",AI CERTIFICATION: Advancing Ethical Practice by Reducing Information Asymmetries,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2021.3077595,https://ieeexplore.ieee.org/document/9427056/,,2021,2022-01-30 4:55:18,2022-01-30 4:55:18,2021-10-31 19:21:42,1-1,,,,,IEEE Trans. Technol. Soc.,AI CERTIFICATION,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/UE94D4B7/Cihon et al. - 2021 - AI CERTIFICATION Advancing Ethical Practice by Re.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IS3J9WCB,journalArticle,2017,"Barrett, Anthony M.; Baum, Seth D.",A model of pathways to artificial superintelligence catastrophe for risk and decision analysis,Journal of Experimental & Theoretical Artificial Intelligence,,"0952-813X, 1362-3079",10.1080/0952813X.2016.1186228,https://www.tandfonline.com/doi/full/10.1080/0952813X.2016.1186228,,2017-03-04,2022-01-30 4:55:18,2022-01-30 4:55:18,2019-12-16 2:44:58,397-414,,2,29,,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000044,,/Users/jacquesthibodeau/Zotero/storage/JFIRRI9N/Barrett and Baum - 2017 - A model of pathways to artificial superintelligenc.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AIE4CQPT,journalArticle,2017,"Jilk, David J.",Conceptual-Linguistic Superintelligence,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1875,"We argue that artificial intelligence capable of sustaining an uncontrolled intelligence explosion must have a conceptual-linguistic faculty with substantial functional similarity to the human faculty. We then argue for three subsidiary claims: first, that detecting the presence of such a faculty will be an important indicator of imminent superintelligence; second, that such a superintelligence will, in creating further increases in intelligence, both face and consider the same sorts of existential risks that humans face today; third, that such a superintelligence is likely to assess and question its own values, purposes, and drives.",2017-12-27,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-12-13 23:11:36,,,4,41,,,,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000006  Number: 4,,/Users/jacquesthibodeau/Zotero/storage/QRMR33VP/Jilk - 2017 - Conceptual-Linguistic Superintelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/KJZ6SVUS/1875.html; /Users/jacquesthibodeau/Zotero/storage/DQWGAKQA/1875.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BH567DIV,journalArticle,2014,"Halpern, Joseph Y.; Pass, Rafael; Seeman, Lior",Decision Theory with Resource-Bounded Agents,Topics in Cognitive Science,,17568757,10.1111/tops.12088,http://doi.wiley.com/10.1111/tops.12088,,2014-04,2022-01-30 4:59:45,2022-01-30 4:59:45,2020-11-22 1:47:43,245-257,,2,6,,Top Cogn Sci,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/C5BB6KK7/Halpern et al. - 2014 - Decision Theory with Resource-Bounded Agents.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6G5389AS,journalArticle,2006,"McLaren, B.M.","Computational Models of Ethical Reasoning: Challenges, Initial Steps, and Future Directions",IEEE Intelligent Systems,,1541-1672,10.1109/MIS.2006.67,http://ieeexplore.ieee.org/document/1667950/,,2006-07,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-11-22 2:23:04,29-37,,4,21,,IEEE Intell. Syst.,Computational Models of Ethical Reasoning,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000103,,/Users/jacquesthibodeau/Zotero/storage/C22P6XNW/McLaren - 2006 - Computational Models of Ethical Reasoning Challen.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39KNIC5R,journalArticle,2020,"Turchin, Alexey; Denkenberger, David",Classification of global catastrophic risks connected with artificial intelligence,AI & Society,,0951-5666,10.1007/s00146-018-0845-5,https://link.springer.com/epdf/10.1007/s00146-018-0845-5,"A classification of the global catastrophic risks of AI is presented, along with a comprehensive list of previously identified risks. This classification allows the identification of several new risks. We show that at each level of AI’s intelligence power, separate types of possible catastrophes dominate. Our classification demonstrates that the field of AI risks is diverse, and includes many scenarios beyond the commonly discussed cases of a paperclip maximizer or robot-caused unemployment. Global catastrophic failure could happen at various levels of AI development, namely, (1) before it starts self-improvement, (2) during its takeoff, when it uses various instruments to escape its initial confinement, or (3) after it successfully takes over the world and starts to implement its goal system, which could be plainly unaligned, or feature-flawed friendliness. AI could also halt at later stages of its development either due to technical glitches or ontological problems. Overall, we identified around several dozen scenarios of AI-driven global catastrophe. The extent of this list illustrates that there is no one simple solution to the problem of AI safety, and that AI safety theory is complex and must be customized for each AI development level.",2020,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-11-14 0:32:51,,,1,35,,,,,,,,,,en,,,,,www.readcube.com,,ZSCC: 0000071,,/Users/jacquesthibodeau/Zotero/storage/8EJZTBNT/Turchin and Denkenberger - 2020 - Classification of global catastrophic risks connec.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADDSIC2J,journalArticle,2019,"Cohen, Jeremy M.; Rosenfeld, Elan; Kolter, J. Zico",Certified Adversarial Robustness via Randomized Smoothing,"arXiv:1902.02918 [cs, stat]",,,,http://arxiv.org/abs/1902.02918,"We show how to turn any classiﬁer that classiﬁes well under Gaussian noise into a new classiﬁer that is certiﬁably robust to adversarial perturbations under the 2 norm. This “randomized smoothing” technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in 2 norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classiﬁer with e.g. a certiﬁed top-1 accuracy of 49% under adversarial perturbations with 2 norm less than 0.5 (=127/255). No certiﬁed defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certiﬁed 2 robustness are viable, smoothing delivers higher certiﬁed accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classiﬁcation. Code and models are available at http: //github.com/locuslab/smoothing.",2019-06-15,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-12-22 23:38:26,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000716  arXiv: 1902.02918,,/Users/jacquesthibodeau/Zotero/storage/D7VWNG9N/Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Sm.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IK622CNH,journalArticle,2012,"Hutter, Marcus",Can Intelligence Explode?,Journal of Consciousness Studies,,,,http://arxiv.org/abs/1202.6177,"The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.",2012-02-28,2022-01-30 4:59:43,2022-01-30 4:59:43,2020-11-22 4:16:06,,,,19,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000041  arXiv: 1202.6177,,/Users/jacquesthibodeau/Zotero/storage/PGN32NHD/Hutter - 2012 - Can Intelligence Explode.pdf; /Users/jacquesthibodeau/Zotero/storage/GXCCDRVV/1202.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Physics - Physics and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9334F5K2,journalArticle,2020,"Fischer, Ian; Alemi, Alexander A.",CEB Improves Model Robustness,Entropy,,,,http://arxiv.org/abs/2002.05380,"We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.",2020-02-13,2022-01-30 4:59:43,2022-01-30 4:59:43,2020-09-05 18:46:13,,,10,22,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 2002.05380,,/Users/jacquesthibodeau/Zotero/storage/JSUK3W5B/Fischer and Alemi - 2020 - CEB Improves Model Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/HZ5HWPQR/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UASN6CZQ,journalArticle,2017,"Batin, Mikhail; Turchin, Alexey; Sergey, Markov; Zhila, Alisa; Denkenberger, David",Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1797,"In this paper we focus on the most efficacious AI applications for life extension and anti-aging at three expected stages of AI development: narrow AI, AGI and superintelligence. First, we overview the existing research and commercial work performed by a select number of startups and academic projects. We find that at the current stage of “narrow” AI, the most promising areas for life extension are geroprotector-combination discovery, detection of aging biomarkers, and personalized anti-aging therapy. These advances could help currently living people reach longevity escape velocity and survive until more advanced AI appears. When AI comes close to human level, the main contribution to life extension will come from AI integration with humans through brain-computer interfaces, integrated AI assistants capable of autonomously diagnosing and treating health issues, and cyber systems embedded into human bodies. Lastly, we speculate about the more remote future, when AI reaches the level of superintelligence and such life-extension methods as uploading human minds and creating nanotechnological bodies may become possible, thus lowering the probability of human death close to zero. We conclude that medical AI based superintelligence is intrinsically safer than, say, military AI, as it may help humans to evolve into part of the future superintelligence via brain augmentation, uploading, and a network of self-improving humans. Medical AI’s value system is focused on human benefit.",2017-12-27,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-12-13 22:07:54,,,4,41,,,Artificial Intelligence in Life Extension,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000030  Number: 4,,/Users/jacquesthibodeau/Zotero/storage/9P7GCWS8/Batin et al. - 2017 - Artificial Intelligence in Life Extension from De.pdf; /Users/jacquesthibodeau/Zotero/storage/UB6XNVTH/1797.html; /Users/jacquesthibodeau/Zotero/storage/7VGQTWMP/1797.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P8ABUARU,journalArticle,2019,"Snyder-Beattie, Andrew E.; Ord, Toby; Bonsall, Michael B.",An upper bound for the background rate of human extinction,Scientific Reports,,2045-2322,10.1038/s41598-019-47540-7,https://www.nature.com/articles/s41598-019-47540-7,"We evaluate the total probability of human extinction from naturally occurring processes. Such processes include risks that are well characterized such as asteroid impacts and supervolcanic eruptions, as well as risks that remain unknown. Using only the information that Homo sapiens has existed at least 200,000 years, we conclude that the probability that humanity goes extinct from natural causes in any given year is almost guaranteed to be less than one in 14,000, and likely to be less than one in 87,000. Using the longer track record of survival for our entire genus Homo produces even tighter bounds, with an annual probability of natural extinction likely below one in 870,000. These bounds are unlikely to be affected by possible survivorship bias in the data, and are consistent with mammalian extinction rates, typical hominin species lifespans, the frequency of well-characterized risks, and the frequency of mass extinctions. No similar guarantee can be made for risks that our ancestors did not face, such as anthropogenic climate change or nuclear/biological warfare.",2019-07-30,2022-01-30 4:59:35,2022-01-30 4:59:35,2019-12-16 22:38:58,1-9,,1,9,,,,,,,,,,en,2019 The Author(s),,,,www.nature.com,,ZSCC: 0000017,,/Users/jacquesthibodeau/Zotero/storage/PM4KFW9G/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/IMA8B8KI/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/J3EUKR39/s41598-019-47540-7.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X8AFN559,journalArticle,2020,"McIlroy-Young, Reid; Sen, Siddhartha; Kleinberg, Jon; Anderson, Ashton",Aligning Superhuman AI with Human Behavior: Chess as a Model System,Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,,,10.1145/3394486.3403219,http://arxiv.org/abs/2006.01855,"As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.",2020-08-23,2022-01-30 4:59:34,2022-01-30 4:59:34,2020-11-14 0:50:30,1677-1687,,,,,,Aligning Superhuman AI with Human Behavior,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 2006.01855,,/Users/jacquesthibodeau/Zotero/storage/EHATD65D/McIlroy-Young et al. - 2020 - Aligning Superhuman AI with Human Behavior Chess .pdf; /Users/jacquesthibodeau/Zotero/storage/C93VXAWD/2006.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X63UF2U8,journalArticle,2004,"Kent, Adrian",A Critical Look at Risk Assessments for Global Catastrophes,Risk Analysis,,"0272-4332, 1539-6924",10.1111/j.0272-4332.2004.00419.x,https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0272-4332.2004.00419.x,,2004-02,2022-01-30 4:59:32,2022-01-30 4:59:32,2020-11-22 5:02:07,157-168,,1,24,,Risk Analysis,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000061,,/Users/jacquesthibodeau/Zotero/storage/CGPWV2PJ/Kent - 2004 - A Critical Look at Risk Assessments for Global Cat.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NWR885X,journalArticle,2019,"Irving, Geoffrey; Askell, Amanda",AI Safety Needs Social Scientists,Distill,,2476-0757,10.23915/distill.00014,https://distill.pub/2019/safety-needs-social-scientists,,2019-02-19,2022-01-30 4:58:18,2022-01-30 4:58:18,2019-12-16 20:00:35,10.23915/distill.00014,,2,4,,Distill,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/QVDDN5BC/distill-social-scientists.html,,MetaSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZUHK5GA,journalArticle,2021,"Goh, Gabriel; Cammarata, Nick; Voss, Chelsea; Carter, Shan; Petrov, Michael; Schubert, Ludwig; Radford, Alec; Olah, Chris",Multimodal Neurons in Artificial Neural Networks,Distill,,2476-0757,10.23915/distill.00030,https://distill.pub/2021/multimodal-neurons,,2021-03-04,2022-01-30 4:57:25,2022-01-30 4:57:25,2021-10-31 22:39:28,10.23915/distill.00030,,3,6,,Distill,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000034,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BXX6UCUX,journalArticle,2020,"Stiennon, Nisan; Ouyang, Long; Wu, Jeffrey; Ziegler, Daniel; Lowe, Ryan; Voss, Chelsea; Radford, Alec; Amodei, Dario; Christiano, Paul F.",Learning to summarize with human feedback,Advances in Neural Information Processing Systems,,,,,,2020,2022-01-30 4:57:25,2022-01-30 4:57:25,,,,,33,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005[s0],,/Users/jacquesthibodeau/Zotero/storage/QH3WZ2FT/Stiennon et al. - 2020 - Learning to summarize with human feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/GFUVXB6G/1f89885d556929e98d3ef9b86448f951-Abstract.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7KKMB7Q7,journalArticle,2015,"Sotala, Kaj; Yampolskiy, Roman V",Responses to catastrophic AGI risk: a survey,Physica Scripta,,"0031-8949, 1402-4896",10.1088/0031-8949/90/1/018001,https://iopscience.iop.org/article/10.1088/0031-8949/90/1/018001,,2015-01-01,2022-01-30 4:56:58,2022-01-30 4:56:58,2020-11-22 2:23:47,18001,,1,90,,Phys. Scr.,Responses to catastrophic AGI risk,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 115,,/Users/jacquesthibodeau/Zotero/storage/7HQTGDPG/Sotala and Yampolskiy - 2015 - Responses to catastrophic AGI risk a survey.pdf,,MetaSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSRNPTDU,journalArticle,2018,"Danzig, Richard",Managing Loss of Control as Many Militaries Pursue Technological Superiority,Arms Control Today,,,,,,2018-05-30,2022-01-30 5:00:00,2022-01-30 5:00:00,,40,,7,48,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000037,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADSHWBTF,journalArticle,1973,"Michie, Donald",Machines and the Theory of Intelligence,Nature,,"0028-0836, 1476-4687",10.1038/241507a0,http://www.nature.com/articles/241507a0,,1973-02,2022-01-30 5:00:00,2022-01-30 5:00:00,2020-11-22 2:23:08,507-512,,5391,241,,Nature,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000021,,,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NSAU4ME,journalArticle,2019,"Sarma, Gopal P.; Hay, Nick J.",Mammalian Value Systems,Informatica,,,,http://arxiv.org/abs/1607.08289,"Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical ""intelligence explosion,"" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The ""value alignment problem"" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of ""mammalian value systems"" points to a potential avenue for fundamental research in AI safety and AI ethics.",2019-01-21,2022-01-30 5:00:00,2022-01-30 5:00:00,2020-12-13 23:38:36,,,3,41,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 11  arXiv: 1607.08289,,/Users/jacquesthibodeau/Zotero/storage/WKTKGKUA/Sarma and Hay - 2019 - Mammalian Value Systems.pdf; /Users/jacquesthibodeau/Zotero/storage/FWKZNIR2/1607.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42ZE8I25,journalArticle,2011,"Sullins, John P.",Introduction: Open Questions in Roboethics,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-011-0043-6,http://link.springer.com/10.1007/s13347-011-0043-6,,2011-09,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-11-22 2:23:55,233-238,,3,24,,Philos. Technol.,Introduction,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000043,,/Users/jacquesthibodeau/Zotero/storage/6PNZQ3NW/Sullins - 2011 - Introduction Open Questions in Roboethics.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGZA24CE,journalArticle,2017,"Bogosian, Kyle",Implementation of Moral Uncertainty in Intelligent Machines,Minds and Machines,,1572-8641,10.1007/s11023-017-9448-z,https://doi.org/10.1007/s11023-017-9448-z,"The development of artificial intelligence will require systems of ethical decision making to be adapted for automatic computation. However, projects to implement moral reasoning in artificial moral agents so far have failed to satisfactorily address the widespread disagreement between competing approaches to moral philosophy. In this paper I argue that the proper response to this situation is to design machines to be fundamentally uncertain about morality. I describe a computational framework for doing so and show that it efficiently resolves common obstacles to the implementation of moral philosophy in intelligent machines.",2017-12-01,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-12-13 22:17:16,591-608,,4,27,,Minds & Machines,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/DSITHXNZ/Bogosian - 2017 - Implementation of Moral Uncertainty in Intelligent.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NV87FGU4,journalArticle,2019,"Riedl, Mark O.",Human-Centered Artificial Intelligence and Machine Learning,Human Behavior and Emerging Technologies,,,,http://arxiv.org/abs/1901.11184,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.",2019-01-30,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-11-14 0:32:19,,,1,1,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000092  arXiv: 1901.11184,,/Users/jacquesthibodeau/Zotero/storage/NMJIIGJ5/Riedl - 2019 - Human-Centered Artificial Intelligence and Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/7IGC3HKT/1901.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QP56TU5K,journalArticle,2018,"Vamplew, Peter; Dazeley, Richard; Foale, Cameron; Firmin, Sally; Mummery, Jane",Human-aligned artificial intelligence is a multiobjective problem,Ethics and Information Technology,,1572-8439,10.1007/s10676-017-9440-6,https://doi.org/10.1007/s10676-017-9440-6,"As the capabilities of artificial intelligence (AI) systems improve, it becomes important to constrain their actions to ensure their behaviour remains beneficial to humanity. A variety of ethical, legal and safety-based frameworks have been proposed as a basis for designing these constraints. Despite their variations, these frameworks share the common characteristic that decision-making must consider multiple potentially conflicting factors. We demonstrate that these alignment frameworks can be represented as utility functions, but that the widely used Maximum Expected Utility (MEU) paradigm provides insufficient support for such multiobjective decision-making. We show that a Multiobjective Maximum Expected Utility paradigm based on the combination of vector utilities and non-linear action–selection can overcome many of the issues which limit MEU’s effectiveness in implementing aligned AI. We examine existing approaches to multiobjective AI, and identify how these can contribute to the development of human-aligned intelligent agents.",2018-03-01,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-12-13 21:48:09,27-40,,1,20,,Ethics Inf Technol,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000067,,/Users/jacquesthibodeau/Zotero/storage/3M4DM2R7/Vamplew et al. - 2018 - Human-aligned artificial intelligence is a multiob.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87DAV94P,journalArticle,2011,"Baum, Seth D.; Goertzel, Ben; Goertzel, Ted G.",How long until human-level AI? Results from an expert assessment,Technological Forecasting and Social Change,,401625,10.1016/j.techfore.2010.09.006,https://linkinghub.elsevier.com/retrieve/pii/S0040162510002106,,2011-01,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-11-22 4:16:21,185-195,,1,78,,Technological Forecasting and Social Change,How long until human-level AI?,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000132,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3XXE6R8G,journalArticle,2018,"Liu, Hin-Yan; Lauta, Kristian Cedervall; Maas, Matthijs Michiel",Governing Boring Apocalypses: A new typology of existential vulnerabilities and exposures for existential risk research,Futures,,0016-3287,10.1016/j.futures.2018.04.009,http://www.sciencedirect.com/science/article/pii/S0016328717301623,"In recent years, the study of existential risks has explored a range of natural and man-made catastrophes, from supervolcano eruption to nuclear war, and from global pandemics to potential risks from misaligned AI. These risks share the prospect of causing outright human extinction were they to occur. In this approach, such identified existential risks are frequently characterised by relatively singular origin events and concrete pathways of harm which directly jeopardise the survival of humanity, or undercut its potential for long-term technological progress. While this approach aptly identifies the most cataclysmic fates which may befall humanity, we argue that catastrophic ‘existential outcomes’ may likely arise from a broader range of sources and societal vulnerabilities, and through the complex interactions of disparate social, cultural, and natural processes—many of which, taken in isolation, might not be seen to merit attention as a global catastrophic, let alone existential, risk. This article argues that an emphasis on mitigating the hazards (discrete causes) of existential risks is an unnecessarily narrow framing of the challenge facing humanity, one which risks prematurely curtailing the spectrum of policy responses considered. Instead, it argues existential risks constitute but a subset in a broader set of challenges which could directly or indirectly contribute to existential consequences for humanity. To illustrate, we introduce and examine a set of existential risks that often fall outside the scope of, or remain understudied within, the field. By focusing on vulnerability and exposure rather than existential hazards, we develop a new taxonomy which captures factors contributing to these existential risks. Latent structural vulnerabilities in our technological systems and in our societal arrangements may increase our susceptibility to existential hazards. Finally, different types of exposure of our society or its natural base determine if or how a given hazard can interface with pre-existing vulnerabilities, to trigger emergent existential risks. We argue that far from being peripheral footnotes to their more direct and immediately terminal counterparts, these “Boring Apocalypses” may well prove to be the more endemic and problematic, dragging down and undercutting short-term successes in mitigating more spectacular risks. If the cardinal concern is humanity’s continued survival and prosperity, then focussing academic and public advocacy efforts on reducing direct existential hazards may have the paradoxical potential of exacerbating humanity’s indirect susceptibility to such outcomes. Adopting law and policy perspectives allow us to foreground societal dimensions that complement and reinforce the discourse on existential risks.",2018-09-01,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-12-13 23:17:16,6-19,,,102,,Futures,Governing Boring Apocalypses,Futures of research in catastrophic and existential risk,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000030,,,,MetaSafety; AmbiguosSafety; Other-org,Boring Apocalypse; Civilisational collapse; Existential Risks; Exposure; Vulnerability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV2MTIZJ,journalArticle,2019,"Torres, Phil",Existential risks: a philosophical analysis,Inquiry,,0020-174X,10.1080/0020174X.2019.1658626,https://doi.org/10.1080/0020174X.2019.1658626,"This paper examines and analyzes five definitions of ‘existential risk.’ It tentatively adopts a pluralistic approach according to which the definition that scholars employ should depend upon the particular context of use. More specifically, the notion that existential risks are ‘risks of human extinction or civilizational collapse’ is best when communicating with the public, whereas equating existential risks with a ‘significant loss of expected value’ may be the most effective definition for establishing existential risk studies as a legitimate field of scientific and philosophical inquiry. In making these arguments, the present paper hopes to provide a modicum of clarity to foundational issues relating to the central concept of arguably the most important discussion of our times.",2019-08-23,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-11-14 1:16:56,1-26,,0,0,,,Existential risks,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000004  Publisher: Routledge _eprint: https://doi.org/10.1080/0020174X.2019.1658626,,/Users/jacquesthibodeau/Zotero/storage/F7KA53VC/0020174X.2019.html,,MetaSafety; AmbiguosSafety; Other-org,analysis; existential risk studies; Existential risks; global catastrophic risks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CHVTVF6Z,journalArticle,2018,"Green, Brian Patrick",Ethical Reflections on Artificial Intelligence,Scientia et Fides,,"2353-5636, 2300-7648",10.12775/SetF.2018.015,http://apcz.umk.pl/czasopisma/index.php/SetF/article/view/SetF.2018.015,,2018-10-09,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-12-13 23:07:52,9,,2,6,,SetF,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/ADZH5EW7/Green - 2018 - Ethical Reflections on Artificial Intelligence.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMZ3X84,journalArticle,2015,"Davis, Ernest",Ethical guidelines for a superintelligence,Artificial Intelligence,,43702,10.1016/j.artint.2014.12.003,https://linkinghub.elsevier.com/retrieve/pii/S0004370214001453,,2015-03,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-11-22 4:16:24,121-124,,,220,,Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000030,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S5HQK4S,journalArticle,2008,"Hanson, Robin",Economics of the singularity,IEEE Spectrum,,0018-9235,10.1109/MSPEC.2008.4531461,http://ieeexplore.ieee.org/document/4531461/,,2008-06,2022-01-30 4:59:47,2022-01-30 4:59:47,2020-11-22 2:22:43,45-50,,6,45,,IEEE Spectr.,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000085,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PV7TJE2E,journalArticle,1999,"Sobel, D.",Do the desires of rational agents converge?,Analysis,,"0003-2638, 1467-8284",10.1093/analys/59.3.137,https://academic.oup.com/analysis/article-lookup/doi/10.1093/analys/59.3.137,,1999-07-01,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-11-22 2:23:46,137-147,,3,59,,Analysis,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000040,,,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NYYLH2LN,journalArticle,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,arXiv:2109.13916 [cs],,,,http://arxiv.org/abs/2109.13916,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing deployment hazards (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-12-25,2022-03-09 22:57:07,2022-03-09 22:57:07,2022-03-09 22:57:07,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.13916,,/Users/jacquesthibodeau/Zotero/storage/PYX4ZIAY/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/RKAC9MSI/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U3SKYVI8,journalArticle,2021,"Laidlaw, Cassidy; Russell, Stuart",Uncertain Decisions Facilitate Better Preference Learning,"arXiv:2106.10394 [cs, stat]",,,,http://arxiv.org/abs/2106.10394,"Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",2021-10-28,2022-03-09 22:57:56,2022-03-09 22:57:56,2022-03-09 22:57:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.10394,,/Users/jacquesthibodeau/Zotero/storage/THNL7BX9/Laidlaw and Russell - 2021 - Uncertain Decisions Facilitate Better Preference L.pdf; /Users/jacquesthibodeau/Zotero/storage/EN2IKJX3/2106.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKEPU826,journalArticle,2021,"Roman, Charlotte; Dennis, Michael; Critch, Andrew; Russell, Stuart",Accumulating Risk Capital Through Investing in Cooperation,arXiv:2101.10305 [cs],,,,http://arxiv.org/abs/2101.10305,"Recent work on promoting cooperation in multi-agent learning has resulted in many methods which successfully promote cooperation at the cost of becoming more vulnerable to exploitation by malicious actors. We show that this is an unavoidable trade-off and propose an objective which balances these concerns, promoting both safety and long-term cooperation. Moreover, the trade-off between safety and cooperation is not severe, and you can receive exponentially large returns through cooperation from a small amount of risk. We study both an exact solution method and propose a method for training policies that targets this objective, Accumulating Risk Capital Through Investing in Cooperation (ARCTIC), and evaluate them in iterated Prisoner's Dilemma and Stag Hunt.",2021-04-20,2022-03-09 22:58:09,2022-03-11 1:37:20,2022-03-09 22:58:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.10305,,/Users/jacquesthibodeau/Zotero/storage/LKX27ABC/Roman et al. - 2021 - Accumulating Risk Capital Through Investing in Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/93KS3FYM/2101.html; /Users/jacquesthibodeau/Zotero/storage/2AWIMKR9/Roman et al. - 2021 - Accumulating Risk Capital Through Investing in Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/SR2STL4U/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EHPBTDLF,journalArticle,2022,"Hendrycks, Dan; Mazeika, Mantas; Zou, Andy; Patel, Sahil; Zhu, Christine; Navarro, Jesus; Song, Dawn; Li, Bo; Steinhardt, Jacob",What Would Jiminy Cricket Do? Towards Agents That Behave Morally,arXiv:2110.13136 [cs],,,,http://arxiv.org/abs/2110.13136,"When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",2022-02-07,2022-03-09 22:58:16,2022-03-11 1:37:25,2022-03-09 22:58:16,,,,,,,What Would Jiminy Cricket Do?,,,,,,,,,,,,arXiv.org,,arXiv: 2110.13136,,/Users/jacquesthibodeau/Zotero/storage/NCUMY87S/Hendrycks et al. - 2022 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/URSN934U/Hendrycks et al. - 2022 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/L68I3PS9/2110.html; /Users/jacquesthibodeau/Zotero/storage/J5I4QY85/2110.html; /Users/jacquesthibodeau/Zotero/storage/Y4FWJ689/Hendrycks et al. - 2021 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/K3PCNQBZ/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CCHG9XGR,journalArticle,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,arXiv:2103.03386 [cs],,,,http://arxiv.org/abs/2103.03386,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-03-09 22:58:27,2022-03-09 22:58:27,2022-03-09 22:58:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03386,,/Users/jacquesthibodeau/Zotero/storage/BMSKZG99/Filan et al. - 2021 - Clusterability in Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/9INK4IXF/2103.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MDPKLM9I,journalArticle,2021,"Zhuang, Simon; Hadfield-Menell, Dylan",Consequences of Misaligned AI,arXiv:2102.03896 [cs],,,,http://arxiv.org/abs/2102.03896,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-03-09 22:58:33,2022-03-09 22:58:33,2022-03-09 22:58:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.03896,,/Users/jacquesthibodeau/Zotero/storage/T6BV6ZB4/Zhuang and Hadfield-Menell - 2021 - Consequences of Misaligned AI.pdf; /Users/jacquesthibodeau/Zotero/storage/HP4ZPC8U/2102.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BCG7XGVQ,journalArticle,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,arXiv:2107.01969 [cs],,,,http://arxiv.org/abs/2107.01969,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-03-09 22:58:35,2022-03-09 22:58:35,2022-03-09 22:58:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.01969,,/Users/jacquesthibodeau/Zotero/storage/89IYMW6H/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/SJE63IZB/2107.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BFEB9Q94,journalArticle,2022,"Hod, Shlomi; Filan, Daniel; Casper, Stephen; Critch, Andrew; Russell, Stuart",Quantifying Local Specialization in Deep Neural Networks,arXiv:2110.08058 [cs],,,,http://arxiv.org/abs/2110.08058,"A neural network is locally specialized to the extent that parts of its computational graph (i.e. structure) can be abstractly represented as performing some comprehensible sub-task relevant to the overall task (i.e. functionality). Are modern deep neural networks locally specialized? How can this be quantified? In this paper, we consider the problem of taking a neural network whose neurons are partitioned into clusters, and quantifying how functionally specialized the clusters are. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal local specialization and that statistical methods can be used to automatedly screen for sets of neurons that can be understood abstractly.",2022-02-07,2022-03-09 22:58:40,2022-03-11 1:37:49,2022-03-09 22:58:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08058,,/Users/jacquesthibodeau/Zotero/storage/RXRBMQPT/Hod et al. - 2022 - Quantifying Local Specialization in Deep Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/Q6WYJA5N/2110.html; /Users/jacquesthibodeau/Zotero/storage/WBCI6CWN/Hod et al. - 2021 - Quantifying Local Specialization in Deep Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/PFKUDN8U/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GJ88798,journalArticle,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do by Simulating the Past,"arXiv:2104.03946 [cs, stat]",,,,http://arxiv.org/abs/2104.03946,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021-05-03,2022-03-09 22:58:45,2022-03-09 22:58:45,2022-03-09 22:58:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.03946,,/Users/jacquesthibodeau/Zotero/storage/Y8HNMMHK/Lindner et al. - 2021 - Learning What To Do by Simulating the Past.pdf; /Users/jacquesthibodeau/Zotero/storage/CUPUYI59/2104.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJQIBMRG,journalArticle,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,"arXiv:1902.04198 [cs, stat]",,,,http://arxiv.org/abs/1902.04198,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-04-18,2022-03-09 22:58:48,2022-03-09 22:58:48,2022-03-09 22:58:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.04198,,/Users/jacquesthibodeau/Zotero/storage/AW4CB7BH/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/YX7GB7ZZ/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IR98WD26,journalArticle,2021,"Brown, Daniel S.; Schneider, Jordan; Dragan, Anca D.; Niekum, Scott",Value Alignment Verification,arXiv:2012.01557 [cs],,,,http://arxiv.org/abs/2012.01557,"As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values. The goal is to construct a kind of ""driver's test"" that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents and propose and analyze heuristic and approximate value alignment verification tests in a wide range of gridworlds and a continuous autonomous driving domain. Finally, we prove that there exist sufficient conditions such that we can verify exact and approximate alignment across an infinite set of test environments via a constant-query-complexity alignment test.",2021-06-11,2022-03-09 23:00:34,2022-03-11 1:36:24,2022-03-09 23:00:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.01557,,/Users/jacquesthibodeau/Zotero/storage/NWT7CF7Y/Brown et al. - 2021 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/3CL3BXKY/2012.html; /Users/jacquesthibodeau/Zotero/storage/W7HB9SVE/Brown et al. - 2020 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/55WZNQ7K/Brown et al. - 2020 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/TA94RCWP/2012.html; /Users/jacquesthibodeau/Zotero/storage/R4DTXQIR/2012.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SEDFPFTC,journalArticle,2021,"Zhang, Tianjun; Rashidinejad, Paria; Jiao, Jiantao; Tian, Yuandong; Gonzalez, Joseph; Russell, Stuart",MADE: Exploration via Maximizing Deviation from Explored Regions,"arXiv:2106.10268 [cs, stat]",,,,http://arxiv.org/abs/2106.10268,"In online reinforcement learning (RL), efficient exploration remains particularly challenging in high-dimensional environments with sparse rewards. In low-dimensional environments, where tabular parameterization is possible, count-based upper confidence bound (UCB) exploration methods achieve minimax near-optimal rates. However, it remains unclear how to efficiently implement UCB in realistic RL tasks that involve non-linear function approximation. To address this, we propose a new exploration approach via \textit{maximizing} the deviation of the occupancy of the next policy from the explored regions. We add this term as an adaptive regularizer to the standard RL objective to balance exploration vs. exploitation. We pair the new objective with a provably convergent algorithm, giving rise to a new intrinsic reward that adjusts existing bonuses. The proposed intrinsic reward is easy to implement and combine with other existing RL algorithms to conduct exploration. As a proof of concept, we evaluate the new intrinsic reward on tabular examples across a variety of model-based and model-free algorithms, showing improvements over count-only exploration strategies. When tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach significantly improves sample efficiency over state-of-the-art methods. Our code is available at https://github.com/tianjunz/MADE.",2021-06-18,2022-03-09 23:02:49,2022-03-09 23:02:49,2022-03-09 23:02:49,,,,,,,MADE,,,,,,,,,,,,arXiv.org,,arXiv: 2106.10268,,/Users/jacquesthibodeau/Zotero/storage/9V52FPRY/Zhang et al. - 2021 - MADE Exploration via Maximizing Deviation from Ex.pdf; /Users/jacquesthibodeau/Zotero/storage/57TUF66N/2106.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DU935JQM,journalArticle,2021,"Dafoe, Allan; Zwetsloot, Remco; Cebul, Matthew",Reputations for Resolve and Higher-Order Beliefs in Crisis Bargaining,Journal of Conflict Resolution,,0022-0027,10.1177/0022002721995549,https://doi.org/10.1177/0022002721995549,"Reputations for resolve are said to be one of the few things worth fighting for, yet they remain inadequately understood. Discussions of reputation focus almost exclusively on first-order belief change—A stands firm, B updates its beliefs about A’s resolve. Such first-order reputational effects are important, but they are not the whole story. Higher-order beliefs—what A believes about B’s beliefs, and so on—matter a great deal as well. When A comes to believe that B is more resolved, this may decrease A’s resolve, and this in turn may increase B’s resolve, and so on. In other words, resolve is interdependent. We offer a framework for estimating higher-order effects, and find evidence of such reasoning in a survey experiment on quasi-elites. Our findings indicate both that states and leaders can develop potent reputations for resolve, and that higher-order beliefs are often responsible for a large proportion of these effects (40 percent to 70 percent in our experimental setting). We conclude by complementing the survey with qualitative evidence and laying the groundwork for future research.",2021-08-01,2022-03-09 23:04:04,2022-03-09 23:04:04,2022-03-09 23:04:04,1378-1404,,7-8,65,,Journal of Conflict Resolution,,,,,,,,en,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,/Users/jacquesthibodeau/Zotero/storage/GXRYYKPK/Dafoe et al. - 2021 - Reputations for Resolve and Higher-Order Beliefs i.pdf,,,bargaining; belief structure; conflict; game theory; survey experiment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q4IMBSVT,journalArticle,2021,"Ding, Jeffrey; Dafoe, Allan","Engines of Power: Electricity, AI, and General-Purpose Military Transformations","arXiv:2106.04338 [econ, q-fin]",,,,http://arxiv.org/abs/2106.04338,"Major theories of military innovation focus on relatively narrow technological developments, such as nuclear weapons or aircraft carriers. Arguably the most profound military implications of technological change, however, come from more fundamental advances arising from general purpose technologies, such as the steam engine, electricity, and the computer. With few exceptions, political scientists have not theorized about GPTs. Drawing from the economics literature on GPTs, we distill several propositions on how and when GPTs affect military affairs. We call these effects general-purpose military transformations. In particular, we argue that the impacts of GMTs on military effectiveness are broad, delayed, and shaped by indirect productivity spillovers. Additionally, GMTs differentially advantage those militaries that can draw from a robust industrial base in the GPT. To illustrate the explanatory value of our theory, we conduct a case study of the military consequences of electricity, the prototypical GPT. Finally, we apply our findings to artificial intelligence, which will plausibly cause a profound general-purpose military transformation.",2021-06-08,2022-03-09 23:04:48,2022-03-11 1:37:51,2022-03-09 23:04:47,,,,,,,Engines of Power,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04338,,"/Users/jacquesthibodeau/Zotero/storage/ZKT7UYR3/Ding and Dafoe - 2021 - Engines of Power Electricity, AI, and General-Pur.pdf; /Users/jacquesthibodeau/Zotero/storage/M7A9LFQT/2106.html; /Users/jacquesthibodeau/Zotero/storage/9YDRJUPU/Ding and Dafoe - 2021 - Engines of Power Electricity, AI, and General-Pur.pdf; /Users/jacquesthibodeau/Zotero/storage/N8UE9PUS/2106.html",,,Economics - General Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L58NK8PU,journalArticle,2021,"Ashurst, Carolyn; Hine, Emmie; Sedille, Paul; Carlier, Alexis",AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements,arXiv:2111.01705 [cs],,,,http://arxiv.org/abs/2111.01705,"Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",2021-11-02,2022-03-09 23:05:17,2022-03-11 1:36:34,2022-03-09 23:05:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.01705,,/Users/jacquesthibodeau/Zotero/storage/L4N8X3ZX/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/9LMZ9EW5/2111.html; /Users/jacquesthibodeau/Zotero/storage/4Q8AYI5N/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/N43N7BRA/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/NEZNP4G3/2111.html; /Users/jacquesthibodeau/Zotero/storage/5BVM9QMS/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GYZ35DBC,journalArticle,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),arXiv:2006.04948 [cs],,,,http://arxiv.org/abs/2006.04948,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",2020-05-29,2022-03-09 23:19:00,2022-03-09 23:19:00,2022-03-09 23:19:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04948,,/Users/jacquesthibodeau/Zotero/storage/SMG6MYD4/Critch and Krueger - 2020 - AI Research Considerations for Human Existential S.pdf; /Users/jacquesthibodeau/Zotero/storage/HHDPPASH/2006.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; 68T01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NZEILG9B,journalArticle,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,arXiv:1805.01109 [cs],,,,http://arxiv.org/abs/1805.01109,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-21,2022-03-09 23:19:06,2022-03-09 23:19:06,2022-03-09 23:19:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.01109,,/Users/jacquesthibodeau/Zotero/storage/VEG59QKP/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/EM889F4G/1805.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS7NKCD4,journalArticle,2021,"Zoe Cremer, Carla; Whittlestone, Jess",Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI,,,1989-1660,10.17863/CAM.65790,https://www.repository.cam.ac.uk/handle/1810/318673,"We propose a method for identifying early warning signs of transformative progress in artificial intelligence (AI), and discuss how these can support the anticipatory and democratic governance of AI. We call these early warning signs ‘canaries’, based on the use of canaries to provide early warnings of unsafe air pollution in coal mines. Our method combines expert elicitation and collaborative causal graphs to identify key milestones and identify the relationships between them. We present two illustrations of how this method could be used: to identify early warnings of harmful impacts of language models; and of progress towards high-level machine intelligence. Identifying early warning signs of transformative applications can support more efficient monitoring and timely regulation of progress in AI: as AI advances, its impacts on society may be too great to be governed retrospectively. It is essential that those impacted by AI have a say in how it is governed. Early warnings can give the public time and focus to influence emerging technologies using democratic, participatory technology assessments. We discuss the challenges in identifying early warning signals and propose directions for future work.",2021-01-01,2022-03-09 23:27:28,2022-03-09 23:27:28,2022-03-09 23:27:28,,,,,,,Artificial Canaries,,,,,,,en,Publisher's own licence,,,,www.repository.cam.ac.uk,,Accepted: 2021-03-12T00:30:30Z Publisher: Universidad Internacional de La Rioja,,/Users/jacquesthibodeau/Zotero/storage/6TI7TIS5/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGVFQ9W5,journalArticle,2021,"Sandberg, Anders; Manheim, David",WHAT IS THE UPPER LIMIT OF VALUE?,,,,,,"How much value can our decisions create? We argue that unless our current understanding of physics is wrong in fairly fundamental ways, there exists an upper limit of value relevant to our decisions. First, due to the speed of light and the deﬁnition and conception of economic growth, the limit to economic growth is a restrictive one. Additionally, a related far larger but still ﬁnite limit exists for value in a much broader sense due to the physics of information and the ability of physical beings to place value on outcomes. We discuss how this argument can handle lexicographic preferences, probabilities, and the implications for inﬁnite ethics and ethical uncertainty.",2021,2022-03-09 23:28:18,2022-03-09 23:28:18,,24,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/PMTQ9HDF/Sandberg and Manheim - 2021 - WHAT IS THE UPPER LIMIT OF VALUE.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B54WR9AZ,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,arXiv:2102.08686 [cs],,,,http://arxiv.org/abs/2102.08686,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-03-09 23:32:42,2022-03-09 23:32:42,2022-03-09 23:32:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.08686,,/Users/jacquesthibodeau/Zotero/storage/LFIE9KM6/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/A6K3L4JG/2102.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H5V92LXR,journalArticle,2021,"Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane",The Incentives that Shape Behaviour,arXiv:2001.07118 [cs],,,,http://arxiv.org/abs/2001.07118,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",2021-03-15,2022-03-09 23:32:56,2022-03-09 23:32:56,2022-03-09 23:32:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.07118,,/Users/jacquesthibodeau/Zotero/storage/UVM7AZLL/Carey et al. - 2021 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/B32JDJM5/2001.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3PA2PD9,journalArticle,2021,"Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane",Agent Incentives: A Causal Perspective,arXiv:2102.01685 [cs],,,,http://arxiv.org/abs/2102.01685,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.",2021-03-15,2022-03-09 23:33:13,2022-03-09 23:33:13,2022-03-09 23:33:13,,,,,,,Agent Incentives,,,,,,,,,,,,arXiv.org,,arXiv: 2102.01685,,/Users/jacquesthibodeau/Zotero/storage/LXH74N9Y/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/S3WS3ZW2/2102.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV2NX43N,journalArticle,2020,"Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus",Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,arXiv:2004.07213 [cs],,,,http://arxiv.org/abs/2004.07213,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",2020-04-20,2022-03-09 23:33:41,2022-03-09 23:33:41,2022-03-09 23:33:41,,,,,,,Toward Trustworthy AI Development,,,,,,,,,,,,arXiv.org,,arXiv: 2004.07213,,/Users/jacquesthibodeau/Zotero/storage/CLSUZ3NQ/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/MJCV3PS3/2004.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N94L867V,journalArticle,2020,"Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus",Asymptotically Unambitious Artificial General Intelligence,arXiv:1905.12186 [cs],,,,http://arxiv.org/abs/1905.12186,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",2020-07-21,2022-03-09 23:35:06,2022-03-09 23:35:06,2022-03-09 23:35:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12186,,/Users/jacquesthibodeau/Zotero/storage/T9VVQH77/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/HYHW77FG/1905.html,,,"Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9BS3DIFV,journalArticle,2019,"Armstrong, Stuart; Mindermann, Sören",Occam's razor is insufficient to infer the preferences of irrational agents,arXiv:1712.05812 [cs],,,,http://arxiv.org/abs/1712.05812,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",2019-01-11,2022-03-09 23:37:05,2022-03-09 23:37:05,2022-03-09 23:37:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1712.05812,,/Users/jacquesthibodeau/Zotero/storage/QVVBSXY6/Armstrong and Mindermann - 2019 - Occam's razor is insufficient to infer the prefere.pdf; /Users/jacquesthibodeau/Zotero/storage/X9NZ26SC/1712.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMSS2I2A,journalArticle,2018,"Schulze, Sebastian; Evans, Owain",Active Reinforcement Learning with Monte-Carlo Tree Search,"arXiv:1803.04926 [cs, stat]",,,,http://arxiv.org/abs/1803.04926,"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.",2018-03-26,2022-03-09 23:51:01,2022-03-09 23:51:01,2022-03-09 23:51:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04926 version: 3,,/Users/jacquesthibodeau/Zotero/storage/CITUQFDJ/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf; /Users/jacquesthibodeau/Zotero/storage/M8BI6MTF/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SRAKWE4I,journalArticle,2018,"Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José",Accounting for the Neglected Dimensions of AI Progress,arXiv:1806.00610 [cs],,,,http://arxiv.org/abs/1806.00610,"We analyze and reframe AI progress. In addition to the prevailing metrics of performance, we highlight the usually neglected costs paid in the development and deployment of a system, including: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, development time, etc. These costs are paid throughout the life cycle of an AI system, fall differentially on different individuals, and vary in magnitude depending on the replicability and generality of the AI solution. The multidimensional performance and cost space can be collapsed to a single utility metric for a user with transitive and complete preferences. Even absent a single utility function, AI advances can be generically assessed by whether they expand the Pareto (optimal) surface. We explore a subset of these neglected dimensions using the two case studies of Alpha* and ALE. This broadened conception of progress in AI should lead to novel ways of measuring success in AI, and can help set milestones for future progress.",2018-06-02,2022-03-09 23:51:04,2022-03-09 23:51:04,2022-03-09 23:51:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00610,,/Users/jacquesthibodeau/Zotero/storage/IMHG69XE/Martínez-Plumed et al. - 2018 - Accounting for the Neglected Dimensions of AI Prog.pdf; /Users/jacquesthibodeau/Zotero/storage/NJ4S5T52/1806.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZYKYZD5S,journalArticle,2017,"Aslanides, John; Leike, Jan; Hutter, Marcus",Universal Reinforcement Learning Algorithms: Survey and Experiments,arXiv:1705.10557 [cs],,,,http://arxiv.org/abs/1705.10557,"Many state-of-the-art reinforcement learning (RL) algorithms typically assume that the environment is an ergodic Markov Decision Process (MDP). In contrast, the field of universal reinforcement learning (URL) is concerned with algorithms that make as few assumptions as possible about the environment. The universal Bayesian agent AIXI and a family of related URL algorithms have been developed in this setting. While numerous theoretical optimality results have been proven for these agents, there has been no empirical investigation of their behavior to date. We present a short and accessible survey of these URL algorithms under a unified notation and framework, along with results of some experiments that qualitatively illustrate some properties of the resulting policies, and their relative performance on partially-observable gridworld environments. We also present an open-source reference implementation of the algorithms which we hope will facilitate further understanding of, and experimentation with, these ideas.",2017-05-30,2022-03-09 23:51:50,2022-03-09 23:51:50,2022-03-09 23:51:50,,,,,,,Universal Reinforcement Learning Algorithms,,,,,,,,,,,,arXiv.org,,arXiv: 1705.10557,,/Users/jacquesthibodeau/Zotero/storage/VGWAU6RD/Aslanides et al. - 2017 - Universal Reinforcement Learning Algorithms Surve.pdf; /Users/jacquesthibodeau/Zotero/storage/RYKPWQKB/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22F863J6,journalArticle,2017,"Bostrom, Nick","Strategic Implications of Openness in <span style=""font-variant:small-caps;"">AI</span> Development",Global Policy,,"1758-5880, 1758-5899",10.1111/1758-5899.12403,https://onlinelibrary.wiley.com/doi/10.1111/1758-5899.12403,"This paper attempts a preliminary analysis of the global desirability of different forms of openness in AI development (including openness about source code, science, data, safety techniques, capabilities, and goals). Short-term impacts of increased openness appear mostly socially beneﬁcial in expectation. The strategic implications of medium and long-term impacts are complex. The evaluation of long-term impacts, in particular, may depend on whether the objective is to beneﬁt the present generation or to promote a time-neutral aggregate of well-being of future generations. Some forms of openness are plausibly positive on both counts (openness about safety measures, openness about goals). Others (openness about source code, science, and possibly capability) could lead to a tightening of the competitive situation around the time of the introduction of advanced AI, increasing the probability that winning the AI race is incompatible with using any safety method that incurs a delay or limits performance. We identify several key factors that must be taken into account by any well-founded opinion on the matter.",2017-05,2022-03-09 23:52:15,2022-03-09 23:52:15,2022-03-09 23:52:15,135-148,,2,8,,Glob Policy,"Strategic Implications of Openness in <span style=""font-variant",,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/jacquesthibodeau/Zotero/storage/B6ZGD7GK/Bostrom - 2017 - Strategic Implications of Openness in span style=.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SBHN6IDW,journalArticle,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,arXiv:2102.05008 [cs],,,,http://arxiv.org/abs/2102.05008,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-03-09 23:52:51,2022-03-09 23:52:51,2022-03-09 23:52:51,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,,,,,,arXiv.org,,arXiv: 2102.05008,,/Users/jacquesthibodeau/Zotero/storage/ZNTEMY45/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/UF69D7DV/2102.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RWN45RJI,journalArticle,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,arXiv:2012.08630 [cs],,,,http://arxiv.org/abs/2012.08630,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-03-09 23:53:01,2022-03-09 23:53:01,2022-03-09 23:53:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.08630,,/Users/jacquesthibodeau/Zotero/storage/HGX8IQXG/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/S4BURQM3/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XK4AR4CU,journalArticle,2021,"Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan",Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers,arXiv:2105.02117 [cs],,,,http://arxiv.org/abs/2105.02117,"Machine learning (ML) and artificial intelligence (AI) researchers play an important role in the ethics and governance of AI, including taking action against what they perceive to be unethical uses of AI (Belfield, 2020; Van Noorden, 2020). Nevertheless, this influential group's attitudes are not well understood, which undermines our ability to discern consensuses or disagreements between AI/ML researchers. To examine these researchers' views, we conducted a survey of those who published in the top AI/ML conferences (N = 524). We compare these results with those from a 2016 survey of AI/ML researchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey of the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of AI in the public interest; moderate trust in most Western tech companies; and low trust in national militaries, Chinese tech companies, and Facebook. While the respondents were overwhelmingly opposed to AI/ML researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of AI, particularly logistics algorithms. A strong majority of respondents think that AI safety research should be prioritized and that ML institutions should conduct pre-publication review to assess potential harms. Being closer to the technology itself, AI/ML re-searchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. The findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for AI.",2021-05-05,2022-03-09 23:54:19,2022-03-09 23:54:19,2022-03-09 23:54:19,,,,,,,Ethics and Governance of Artificial Intelligence,,,,,,,,,,,,arXiv.org,,arXiv: 2105.02117,,/Users/jacquesthibodeau/Zotero/storage/J7PD2QEY/Zhang et al. - 2021 - Ethics and Governance of Artificial Intelligence .pdf; /Users/jacquesthibodeau/Zotero/storage/W4NM5IWV/2105.html,,,Computer Science - Computers and Society; K.7.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6I9Z4FC6,journalArticle,2018,"Dafoe, Allan; Affairs, Journal of International",GLOBAL POLITICS AND THE GOVERNANCE OF ARTIFICIAL INTELLIGENCE,Journal of International Affairs,,0022-197X,,https://www.jstor.org/stable/26588347,,2018,2022-03-09 23:55:09,2022-03-09 23:55:09,2022-03-09 23:55:09,121-126,,1,72,,,,,,,,,,,,,,,JSTOR,,Publisher: Journal of International Affairs Editorial Board,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXYAW24L,journalArticle,2021,"Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott",Risks from Learned Optimization in Advanced Machine Learning Systems,arXiv:1906.01820 [cs],,,,http://arxiv.org/abs/1906.01820,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",2021-12-01,2022-03-09 23:55:28,2022-03-11 1:38:51,2022-03-09 23:55:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.01820,,/Users/jacquesthibodeau/Zotero/storage/WQW7ZA5U/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/9EA6PSD4/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/U9S3RLJX/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/BN4RGP95/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/EKJ56UWQ/1906.html; /Users/jacquesthibodeau/Zotero/storage/6DS7CBXW/1906.html; /Users/jacquesthibodeau/Zotero/storage/3IA3S8WE/1906.html; /Users/jacquesthibodeau/Zotero/storage/GDJR6XCV/1906.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BMNPC7NB,journalArticle,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,"arXiv:2109.11513 [cs, math]",,,,http://arxiv.org/abs/2109.11513,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-03-10 13:38:50,2022-03-10 13:38:50,2022-03-10 13:38:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.11513,,/Users/jacquesthibodeau/Zotero/storage/FWFIXGY4/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/8BDWE93V/2109.html,,,Computer Science - Artificial Intelligence; Mathematics - Probability; Mathematics - Combinatorics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5A7SVW48,journalArticle,2021,"Galaz, Victor; Centeno, Miguel A.; Callahan, Peter W.; Causevic, Amar; Patterson, Thayer; Brass, Irina; Baum, Seth; Farber, Darryl; Fischer, Joern; Garcia, David; McPhearson, Timon; Jimenez, Daniel; King, Brian; Larcey, Paul; Levy, Karen","Artificial intelligence, systemic risks, and sustainability",Technology in Society,,0160-791X,10.1016/j.techsoc.2021.101741,https://www.sciencedirect.com/science/article/pii/S0160791X21002165,"Automated decision making and predictive analytics through artificial intelligence, in combination with rapid progress in technologies such as sensor technology and robotics are likely to change the way individuals, communities, governments and private actors perceive and respond to climate and ecological change. Methods based on various forms of artificial intelligence are already today being applied in a number of research fields related to climate change and environmental monitoring. Investments into applications of these technologies in agriculture, forestry and the extraction of marine resources also seem to be increasing rapidly. Despite a growing interest in, and deployment of AI-technologies in domains critical for sustainability, few have explored possible systemic risks in depth. This article offers a global overview of the progress of such technologies in sectors with high impact potential for sustainability like farming, forestry and the extraction of marine resources. We also identify possible systemic risks in these domains including a) algorithmic bias and allocative harms; b) unequal access and benefits; c) cascading failures and external disruptions, and d) trade-offs between efficiency and resilience. We explore these emerging risks, identify critical questions, and discuss the limitations of current governance mechanisms in addressing AI sustainability risks in these sectors.",2021-11-01,2022-03-10 16:29:12,2022-03-10 16:29:12,2022-03-10 16:29:12,101741,,,67,,Technology in Society,,,,,,,,en,,,,,ScienceDirect,,,,"/Users/jacquesthibodeau/Zotero/storage/PD366UDT/Galaz et al. - 2021 - Artificial intelligence, systemic risks, and susta.pdf",,,Artificial intelligence; Anthropocene; Automation; Climate change; Digitalization; Resilience; Social-ecological systems; Sustainability; Systemic risks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFFZBUY6,journalArticle,2021,"Dean, Sarah; Gilbert, Thomas Krendl; Lambert, Nathan; Zick, Tom",Axes for Sociotechnical Inquiry in AI Research,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2021.3074097,http://arxiv.org/abs/2105.06551,"The development of artificial intelligence (AI) technologies has far exceeded the investigation of their relationship with society. Sociotechnical inquiry is needed to mitigate the harms of new technologies whose potential impacts remain poorly understood. To date, subfields of AI research develop primarily individual views on their relationship with sociotechnics, while tools for external investigation, comparison, and cross-pollination are lacking. In this paper, we propose four directions for inquiry into new and evolving areas of technological development: value--what progress and direction does a field promote, optimization--how the defined system within a problem formulation relates to broader dynamics, consensus--how agreement is achieved and who is included in building it, and failure--what methods are pursued when the problem specification is found wanting. The paper provides a lexicon for sociotechnical inquiry and illustrates it through the example of consumer drone technology.",2021-06,2022-03-10 20:28:17,2022-03-10 20:28:17,2022-03-10 20:28:17,62-70,,2,2,,IEEE Trans. Technol. Soc.,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.06551,,/Users/jacquesthibodeau/Zotero/storage/X5WI5WWL/Dean et al. - 2021 - Axes for Sociotechnical Inquiry in AI Research.pdf; /Users/jacquesthibodeau/Zotero/storage/5Z89A5Y2/2105.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HGJPUHJK,journalArticle,2020,"Gabriel, Iason","Artificial Intelligence, Values and Alignment",Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-020-09539-2,http://arxiv.org/abs/2001.09768,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",2020-09,2022-03-10 20:28:34,2022-03-10 20:28:34,2022-03-10 20:28:34,411-437,,3,30,,Minds & Machines,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09768,,"/Users/jacquesthibodeau/Zotero/storage/ZQGRH8SN/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/L5J2SN95/2001.html",,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MJBVI3Q7,journalArticle,2021,"Cave, Stephen; Whittlestone, Jess; Nyrup, Rune; hEigeartaigh, Sean O.; Calvo, Rafael A.",Using AI ethically to tackle covid-19,BMJ,,1756-1833,10.1136/bmj.n364,https://www.bmj.com/content/372/bmj.n364,"<p>Taking a principled approach is crucial to the successful use of AI in pandemic management, say <b>Stephen Cave and colleagues</b></p>",2021-03-16,2022-03-10 20:29:42,2022-03-10 20:29:42,2022-03-10 20:29:42,n364,,,372,,BMJ,,,,,,,,en,"Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed under the terms of the Creative Commons Attribution IGO License (https://creativecommons.org/licenses/by-nc/3.0/igo/), which permits use, distribution, and reproduction for non-commercial purposes in any medium, provided the original work is properly cited.",,,,www.bmj.com,,Publisher: British Medical Journal Publishing Group Section: Analysis PMID: 33722807,,/Users/jacquesthibodeau/Zotero/storage/JASMH8AC/Cave et al. - 2021 - Using AI ethically to tackle covid-19.pdf;,http://www.ncbi.nlm.nih.gov/pubmed/33722807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J4LHQRIY,journalArticle,2019,"Solaiman, Irene; Brundage, Miles; Clark, Jack; Askell, Amanda; Herbert-Voss, Ariel; Wu, Jeff; Radford, Alec; Krueger, Gretchen; Kim, Jong Wook; Kreps, Sarah; McCain, Miles; Newhouse, Alex; Blazakis, Jason; McGuffie, Kris; Wang, Jasmine",Release Strategies and the Social Impacts of Language Models,arXiv:1908.09203 [cs],,,,http://arxiv.org/abs/1908.09203,"Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.",2019-11-12,2022-03-10 20:36:37,2022-03-10 20:36:37,2022-03-10 20:36:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.09203,,/Users/jacquesthibodeau/Zotero/storage/B5GSSJW7/Solaiman et al. - 2019 - Release Strategies and the Social Impacts of Langu.pdf; /Users/jacquesthibodeau/Zotero/storage/HW9ZEYIY/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language; I.2; I.2.7; K.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EY8JYQMC,journalArticle,2021,"Ramesh, Aditya; Pavlov, Mikhail; Goh, Gabriel; Gray, Scott; Voss, Chelsea; Radford, Alec; Chen, Mark; Sutskever, Ilya",Zero-Shot Text-to-Image Generation,arXiv:2102.12092 [cs],,,,http://arxiv.org/abs/2102.12092,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",2021-02-26,2022-03-10 20:36:52,2022-03-10 20:36:52,2022-03-10 20:36:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.12092,,/Users/jacquesthibodeau/Zotero/storage/BU725VYB/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf; /Users/jacquesthibodeau/Zotero/storage/AJLMCF72/2102.html,,,Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IUYBC4HD,journalArticle,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,arXiv:2107.03374 [cs],,,,http://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-14,2022-03-10 20:38:12,2022-03-10 20:38:12,2022-03-10 20:38:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.03374,,/Users/jacquesthibodeau/Zotero/storage/BUBKA67E/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/RRVHPXPL/2107.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TSEAGFB6,journalArticle,2016,"Brockman, Greg; Cheung, Vicki; Pettersson, Ludwig; Schneider, Jonas; Schulman, John; Tang, Jie; Zaremba, Wojciech",OpenAI Gym,arXiv:1606.01540 [cs],,,,http://arxiv.org/abs/1606.01540,"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",2016-06-05,2022-03-10 20:38:24,2022-03-10 20:38:24,2022-03-10 20:38:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.01540,,/Users/jacquesthibodeau/Zotero/storage/2C58UTVC/Brockman et al. - 2016 - OpenAI Gym.pdf; /Users/jacquesthibodeau/Zotero/storage/MXYQWT9G/1606.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G39HHA46,journalArticle,2020,"Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario",Language Models are Few-Shot Learners,arXiv:2005.14165 [cs],,,,http://arxiv.org/abs/2005.14165,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2020-07-22,2022-03-10 20:38:32,2022-03-11 1:36:43,2022-03-10 20:38:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.14165,,/Users/jacquesthibodeau/Zotero/storage/PUXYRW8X/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/EJCL7RHC/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/74LHPBWZ/2005.html; /Users/jacquesthibodeau/Zotero/storage/SARDP4KI/2005.html; /Users/jacquesthibodeau/Zotero/storage/9HH92G2G/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/PP7AKKE5/2005.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YRKHJSNG,journalArticle,2021,"Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul",Recursively Summarizing Books with Human Feedback,arXiv:2109.10862 [cs],,,,http://arxiv.org/abs/2109.10862,"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",2021-09-27,2022-03-10 20:38:53,2022-03-10 20:38:53,2022-03-10 20:38:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.10862,,/Users/jacquesthibodeau/Zotero/storage/SGPNRN2J/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/6G3CZCAV/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2P76UVF,journalArticle,2021,"Cammarata, Nick; Goh, Gabriel; Carter, Shan; Voss, Chelsea; Schubert, Ludwig; Olah, Chris",Curve Circuits,Distill,,2476-0757,10.23915/distill.00024.006,https://distill.pub/2020/circuits/curve-circuits,Reverse engineering the curve detection algorithm from InceptionV1 and reimplementing it from scratch.,2021-01-30,2022-03-10 20:42:58,2022-03-10 20:42:58,2022-03-10 20:42:58,e00024.006,,1,6,,Distill,,,,,,,,en,,,,,distill.pub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LNB8SL9F,journalArticle,2021,"Welbl, Johannes; Glaese, Amelia; Uesato, Jonathan; Dathathri, Sumanth; Mellor, John; Hendricks, Lisa Anne; Anderson, Kirsty; Kohli, Pushmeet; Coppin, Ben; Huang, Po-Sen",Challenges in Detoxifying Language Models,arXiv:2109.07445 [cs],,,,http://arxiv.org/abs/2109.07445,"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.",2021-09-15,2022-03-10 20:43:48,2022-03-10 20:43:48,2022-03-10 20:43:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.07445,,/Users/jacquesthibodeau/Zotero/storage/U3DAT2NL/Welbl et al. - 2021 - Challenges in Detoxifying Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/CJ49LCNY/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.6; Computer Science - Computation and Language; I.2.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G8SBLG2B,journalArticle,2021,"Open Ended Learning Team; Stooke, Adam; Mahajan, Anuj; Barros, Catarina; Deck, Charlie; Bauer, Jakob; Sygnowski, Jakub; Trebacz, Maja; Jaderberg, Max; Mathieu, Michael; McAleese, Nat; Bradley-Schmieg, Nathalie; Wong, Nathaniel; Porcel, Nicolas; Raileanu, Roberta; Hughes-Fitt, Steph; Dalibard, Valentin; Czarnecki, Wojciech Marian",Open-Ended Learning Leads to Generally Capable Agents,arXiv:2107.12808 [cs],,,,http://arxiv.org/abs/2107.12808,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",2021-07-31,2022-03-10 20:44:15,2022-03-11 1:39:33,2022-03-10 20:44:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.12808,,/Users/jacquesthibodeau/Zotero/storage/LH8MJ9MJ/Open Ended Learning Team et al. - 2021 - Open-Ended Learning Leads to Generally Capable Age.pdf; /Users/jacquesthibodeau/Zotero/storage/EED6JN7B/Open Ended Learning Team et al. - 2021 - Open-Ended Learning Leads to Generally Capable Age.pdf; /Users/jacquesthibodeau/Zotero/storage/7XJS5P2B/2107.html; /Users/jacquesthibodeau/Zotero/storage/TX886NWC/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68F2QNJ2,journalArticle,2021,"Gabriel, Iason",Towards a Theory of Justice for Artificial Intelligence,arXiv:2110.14419 [cs],,,,http://arxiv.org/abs/2110.14419,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society.",2021-10-27,2022-03-10 20:44:24,2022-03-10 20:44:24,2022-03-10 20:44:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.14419,,/Users/jacquesthibodeau/Zotero/storage/DMXBXRGB/Gabriel - 2021 - Towards a Theory of Justice for Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/DGJX6FK5/2110.html,,,Computer Science - Computers and Society; K.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PF74H37Z,journalArticle,2022,"Alex, Neel; Lifland, Eli; Tunstall, Lewis; Thakur, Abhishek; Maham, Pegah; Riedel, C. Jess; Hine, Emmie; Ashurst, Carolyn; Sedille, Paul; Carlier, Alexis; Noetel, Michael; Stuhlmüller, Andreas",RAFT: A Real-World Few-Shot Text Classification Benchmark,arXiv:2109.14076 [cs],,,,http://arxiv.org/abs/2109.14076,"Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org .",2022-01-18,2022-03-10 20:50:10,2022-03-11 1:37:14,2022-03-10 20:50:10,,,,,,,RAFT,,,,,,,,,,,,arXiv.org,,arXiv: 2109.14076,,/Users/jacquesthibodeau/Zotero/storage/NSWL9BX4/Alex et al. - 2022 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/7AWPQDMG/2109.html; /Users/jacquesthibodeau/Zotero/storage/F27E5UKQ/Alex et al. - 2021 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/6QYMUHHM/Alex et al. - 2021 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/WLVHVW4Z/2109.html; /Users/jacquesthibodeau/Zotero/storage/DKJU8Z4U/2109.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G82RS6Z9,journalArticle,2021,"Oesterheld, Caspar; Conitzer, Vincent",Safe Pareto Improvements for Delegated Game Playing,,,,,,"A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.",2021,2022-03-10 20:52:27,2022-03-10 20:52:27,,17,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/3R3K7SK6/Oesterheld and Conitzer - 2021 - Safe Pareto Improvements for Delegated Game Playin.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALFSA5Z4,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,arXiv:2008.02275 [cs],,,,http://arxiv.org/abs/2008.02275,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2021-07-24,2022-03-10 20:55:48,2022-03-10 20:55:48,2022-03-10 20:55:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.02275,,/Users/jacquesthibodeau/Zotero/storage/MGAGEE3K/Hendrycks et al. - 2021 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/DKDNH6A4/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GYISCUUW,journalArticle,2021,"Hernandez, Danny; Kaplan, Jared; Henighan, Tom; McCandlish, Sam",Scaling Laws for Transfer,arXiv:2102.01293 [cs],,,,http://arxiv.org/abs/2102.01293,"We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.",2021-02-01,2022-03-10 21:18:27,2022-03-10 21:18:27,2022-03-10 21:18:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.01293,,/Users/jacquesthibodeau/Zotero/storage/9VGIJ9L5/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf; /Users/jacquesthibodeau/Zotero/storage/788QIVAZ/2102.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EW6QZRN5,journalArticle,2021,"Mu, Jesse; Andreas, Jacob",Compositional Explanations of Neurons,"arXiv:2006.14032 [cs, stat]",,,,http://arxiv.org/abs/2006.14032,"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",2021-02-02,2022-03-10 21:26:59,2022-03-10 21:26:59,2022-03-10 21:26:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.14032,,/Users/jacquesthibodeau/Zotero/storage/IM56UFHZ/Mu and Andreas - 2021 - Compositional Explanations of Neurons.pdf; /Users/jacquesthibodeau/Zotero/storage/2ECK6GBR/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SJXHMS9,journalArticle,2019,"Das, Abhishek; Gkioxari, Georgia; Lee, Stefan; Parikh, Devi; Batra, Dhruv",Neural Modular Control for Embodied Question Answering,arXiv:1810.11181 [cs],,,,http://arxiv.org/abs/1810.11181,"We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.). We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies. On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.",2019-05-02,2022-03-10 21:27:19,2022-03-10 21:27:19,2022-03-10 21:27:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11181,,/Users/jacquesthibodeau/Zotero/storage/97QNXI89/Das et al. - 2019 - Neural Modular Control for Embodied Question Answe.pdf; /Users/jacquesthibodeau/Zotero/storage/EK4LJR7V/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSM79E58,journalArticle,2020,"Jeon, Hong Jun; Milli, Smitha; Dragan, Anca D.",Reward-rational (implicit) choice: A unifying formalism for reward learning,arXiv:2002.04833 [cs],,,,http://arxiv.org/abs/2002.04833,"It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",2020-12-11,2022-03-10 21:30:26,2022-03-11 1:38:27,2022-03-10 21:30:26,,,,,,,Reward-rational (implicit) choice,,,,,,,,,,,,arXiv.org,,arXiv: 2002.04833,,/Users/jacquesthibodeau/Zotero/storage/JBZJEXYL/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/KSBEA8B7/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/7ZBNWPTZ/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/9AUP7JBL/2002.html; /Users/jacquesthibodeau/Zotero/storage/YU23FGTP/2002.html; /Users/jacquesthibodeau/Zotero/storage/TA7VTUCB/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BJVFMNGC,journalArticle,2018,"Bau, David; Zhu, Jun-Yan; Strobelt, Hendrik; Zhou, Bolei; Tenenbaum, Joshua B.; Freeman, William T.; Torralba, Antonio",GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,arXiv:1811.10597 [cs],,,,http://arxiv.org/abs/1811.10597,"Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.",2018-12-08,2022-03-10 21:31:47,2022-03-11 1:38:40,2022-03-10 21:31:47,,,,,,,GAN Dissection,,,,,,,,,,,,arXiv.org,,arXiv: 1811.10597,,/Users/jacquesthibodeau/Zotero/storage/RDVZ2UIF/Bau et al. - 2018 - GAN Dissection Visualizing and Understanding Gene.pdf; /Users/jacquesthibodeau/Zotero/storage/2UA5JYZP/Bau et al. - 2018 - GAN Dissection Visualizing and Understanding Gene.pdf; /Users/jacquesthibodeau/Zotero/storage/UFQ64VTT/1811.html; /Users/jacquesthibodeau/Zotero/storage/IWI3G8UB/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YWE98K5X,journalArticle,2018,"Xu, Tian; Zhan, Jiayu; Garrod, Oliver G. B.; Torr, Philip H. S.; Zhu, Song-Chun; Ince, Robin A. A.; Schyns, Philippe G.",Deeper Interpretability of Deep Networks,arXiv:1811.07807 [cs],,,,http://arxiv.org/abs/1811.07807,"Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. There is an increasing demand for explainable AI as these systems are deployed in the real world. However, understanding the information represented and processed in CNNs remains in most cases challenging. Within this paper, we explore the use of new information theoretic techniques developed in the field of neuroscience to enable novel understanding of how a CNN represents information. We trained a 10-layer ResNet architecture to identify 2,000 face identities from 26M images generated using a rigorously controlled 3D face rendering model that produced variations of intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors (i.e. 3D pose, illumination, scale and 2D translation). With our methodology, we demonstrate that unlike human's network overgeneralizes face identities even with extreme changes of face shape, but it is more sensitive to changes of texture. To understand the processing of information underlying these counterintuitive properties, we visualize the features of shape and texture that the network processes to identify faces. Then, we shed a light into the inner workings of the black box and reveal how hidden layers represent these features and whether the representations are invariant to pose. We hope that our methodology will provide an additional valuable tool for interpretability of CNNs.",2018-11-20,2022-03-10 21:31:50,2022-03-11 1:38:38,2022-03-10 21:31:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07807,,/Users/jacquesthibodeau/Zotero/storage/JVPLJLEJ/Xu et al. - 2018 - Deeper Interpretability of Deep Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/H5PPH9ZH/Xu et al. - 2018 - Deeper Interpretability of Deep Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/SLX9FB2N/1811.html; /Users/jacquesthibodeau/Zotero/storage/U5RDJ8GQ/1811.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P674JIV8,journalArticle,2019,"Bahdanau, Dzmitry; Hill, Felix; Leike, Jan; Hughes, Edward; Hosseini, Arian; Kohli, Pushmeet; Grefenstette, Edward",Learning to Understand Goal Specifications by Modelling Reward,arXiv:1806.01946 [cs],,,,http://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.",2019-12-23,2022-03-10 21:32:52,2022-03-10 21:32:52,2022-03-10 21:32:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01946,,/Users/jacquesthibodeau/Zotero/storage/SQVYQXKM/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/ZZZC8CB3/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3QPWRQK4,journalArticle,2019,"Singh, Avi; Yang, Larry; Hartikainen, Kristian; Finn, Chelsea; Levine, Sergey",End-to-End Robotic Reinforcement Learning without Reward Engineering,"arXiv:1904.07854 [cs, stat]",,,,http://arxiv.org/abs/1904.07854,"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",2019-05-15,2022-03-10 21:51:33,2022-03-10 21:51:33,2022-03-10 21:51:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.07854,,/Users/jacquesthibodeau/Zotero/storage/5D3WPY5V/Singh et al. - 2019 - End-to-End Robotic Reinforcement Learning without .pdf; /Users/jacquesthibodeau/Zotero/storage/U2WNAYI7/1904.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6B2XCGSA,journalArticle,2021,"Cai, Feiyang; Ozdagli, Ali I.; Koutsoukos, Xenofon",Detection of Dataset Shifts in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression,arXiv:2104.06613 [cs],,,,http://arxiv.org/abs/2104.06613,"Cyber-physical systems (CPSs) use learning-enabled components (LECs) extensively to cope with various complex tasks under high-uncertainty environments. However, the dataset shifts between the training and testing phase may lead the LECs to become ineffective to make large-error predictions, and further, compromise the safety of the overall system. In our paper, we first provide the formal definitions for different types of dataset shifts in learning-enabled CPS. Then, we propose an approach to detect the dataset shifts effectively for regression problems. Our approach is based on the inductive conformal anomaly detection and utilizes a variational autoencoder for regression model which enables the approach to take into consideration both LEC input and output for detecting dataset shifts. Additionally, in order to improve the robustness of detection, layer-wise relevance propagation (LRP) is incorporated into our approach. We demonstrate our approach by using an advanced emergency braking system implemented in an open-source simulator for self-driving cars. The evaluation results show that our approach can detect different types of dataset shifts with a small number of false alarms while the execution time is smaller than the sampling period of the system.",2021-04-13,2022-03-10 21:59:51,2022-03-10 21:59:51,2022-03-10 21:59:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.06613,,/Users/jacquesthibodeau/Zotero/storage/2BI9MNDE/Cai et al. - 2021 - Detection of Dataset Shifts in Learning-Enabled Cy.pdf; /Users/jacquesthibodeau/Zotero/storage/EJKILAZB/2104.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IHUAK8C2,journalArticle,2019,"Chollet, François",On the Measure of Intelligence,arXiv:1911.01547 [cs],,,,http://arxiv.org/abs/1911.01547,"To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ""buy"" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.",2019-11-25,2022-03-10 22:02:10,2022-03-10 22:02:10,2022-03-10 22:02:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.01547,,/Users/jacquesthibodeau/Zotero/storage/S63K9L3I/Chollet - 2019 - On the Measure of Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/SDFPCCLB/1911.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRM6JQEV,journalArticle,1975,"Kerr, Steven","On the Folly of Rewarding A, While Hoping for B",Academy of Management Journal,,0001-4273,10.5465/255378,https://journals.aom.org/doi/full/10.5465/255378,"Illustrations are presented from society in general, and from organizations in particular, of reward systems that “pay off” for one behavior even though the rewarder hopes dearly for another. Portions of the reward systems of a manufacturing company and an insurance firm are examined and the consequences discussed.",1975-12,2022-03-10 22:05:16,2022-03-10 22:05:16,2022-03-10 22:05:16,769-783,,4,18,,AMJ,,,,,,,,,,,,,journals.aom.org (Atypon),,Publisher: Academy of Management,,,,,AWARDS; EMPLOYEES — Awards; INCENTIVES in industry — Research; INDUSTRIAL efficiency; INDUSTRIAL management — Research; MANAGEMENT styles; ORGANIZATION — Management; ORGANIZATIONAL effectiveness; ORGANIZATIONAL goals; PERFORMANCE awards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KULV732,journalArticle,2017,"Anthony, Thomas; Tian, Zheng; Barber, David",Thinking Fast and Slow with Deep Learning and Tree Search,arXiv:1705.08439 [cs],,,,http://arxiv.org/abs/1705.08439,"Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our ﬁnal tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.",2017-12-03,2022-03-10 22:11:57,2022-03-10 22:11:57,2022-03-10 22:11:57,,,,,,,,,,,,,,en,,,,,arXiv.org,,arXiv: 1705.08439,,/Users/jacquesthibodeau/Zotero/storage/ZA7ZFDBM/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5LPK4AFG,journalArticle,2017,"Doshi-Velez, Finale; Kim, Been",Towards A Rigorous Science of Interpretable Machine Learning,"arXiv:1702.08608 [cs, stat]",,,,http://arxiv.org/abs/1702.08608,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017-03-02,2022-03-10 22:15:12,2022-03-10 22:15:12,2022-03-10 22:15:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1702.08608,,/Users/jacquesthibodeau/Zotero/storage/4ZB2VZYE/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/AWGGGGH9/1702.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8LUTTTEJ,journalArticle,2012,"Neu, Gergely; Szepesvari, Csaba",Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods,"arXiv:1206.5264 [cs, stat]",,,,http://arxiv.org/abs/1206.5264,"In this paper we propose a novel gradient algorithm to learn a policy from an expert's observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm's aim is to find a reward function such that the resulting optimal policy matches well the expert's observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is over- come by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.",2012-06-20,2022-03-10 22:16:38,2022-03-10 22:16:38,2022-03-10 22:16:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1206.5264,,/Users/jacquesthibodeau/Zotero/storage/99292G7B/Neu and Szepesvari - 2012 - Apprenticeship Learning using Inverse Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/GBWMMHQ7/1206.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVG5R2C7,journalArticle,,"Ramachandran, Deepak",Bayesian Inverse Reinforcement Learning,,,,,,Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efﬁcient algorithms that ﬁnd solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.,,2022-03-10 22:17:12,2022-03-10 22:17:12,,6,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/4LIPHWI9/Ramachandran - Bayesian Inverse Reinforcement Learning.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I2AIWYLC,journalArticle,2020,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",Logical Induction,"arXiv:1609.03543 [cs, math]",,,,http://arxiv.org/abs/1609.03543,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]",2020-12-07,2022-03-10 22:23:27,2022-03-10 22:23:27,2022-03-10 22:23:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1609.03543,,/Users/jacquesthibodeau/Zotero/storage/W3QAZJ9V/Garrabrant et al. - 2020 - Logical Induction.pdf; /Users/jacquesthibodeau/Zotero/storage/FAMS64EX/1609.html,,,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Mathematics - Logic; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLCZ83MV,journalArticle,2018,"Yudkowsky, Eliezer; Soares, Nate",Functional Decision Theory: A New Theory of Instrumental Rationality,arXiv:1710.05060 [cs],,,,http://arxiv.org/abs/1710.05060,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",2018-05-22,2022-03-10 22:25:36,2022-03-10 22:25:36,2022-03-10 22:25:36,,,,,,,Functional Decision Theory,,,,,,,,,,,,arXiv.org,,arXiv: 1710.05060,,/Users/jacquesthibodeau/Zotero/storage/T8CUKSLW/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/SARGDYW9/1710.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TG4IEKSJ,journalArticle,2019,"Manheim, David; Garrabrant, Scott",Categorizing Variants of Goodhart's Law,"arXiv:1803.04585 [cs, q-fin, stat]",,,,http://arxiv.org/abs/1803.04585,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",2019-02-24,2022-03-10 22:26:31,2022-03-10 22:26:31,2022-03-10 22:26:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04585,,/Users/jacquesthibodeau/Zotero/storage/JXTSSQGG/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/NWZQRPYK/1803.html,,,91E45; Computer Science - Artificial Intelligence; Quantitative Finance - General Finance; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FRLRRRH3,journalArticle,2007,"Shoham, Yoav; Powers, Rob; Grenager, Trond","If multi-agent learning is the answer, what is the question?",Artificial Intelligence,,43702,10.1016/j.artint.2006.02.006,https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495,,2007-05,2022-03-10 22:28:04,2022-03-10 22:28:04,2022-03-10 22:28:04,365-377,,7,171,,Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,,,"/Users/jacquesthibodeau/Zotero/storage/JG363XVU/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FK7BM423,journalArticle,,"Fallenstein, Benja; Soares, Nate",Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents,,,,,,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.",,2022-03-10 22:29:07,2022-03-10 22:29:07,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/5H8P3D7Q/Fallenstein and Soares - Vingean Reﬂection Reliable Reasoning for Self-Imp.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R6IVH39E,journalArticle,2021,"Watson, Matthew; Hasan, Bashar Awwad Shiekh; Moubayed, Noura Al",Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations,arXiv:2105.06791 [cs],,,,http://arxiv.org/abs/2105.06791,"Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: $\approx$ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.",2021-10-30,2022-03-10 22:32:04,2022-03-10 22:32:04,2022-03-10 22:32:04,,,,,,,Agree to Disagree,,,,,,,,,,,,arXiv.org,,arXiv: 2105.06791,,/Users/jacquesthibodeau/Zotero/storage/INWNTP2U/Watson et al. - 2021 - Agree to Disagree When Deep Learning Models With .pdf; /Users/jacquesthibodeau/Zotero/storage/55CC6S4E/2105.html,,,Computer Science - Machine Learning; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S796YIG,journalArticle,1973,"Groves, Theodore",Incentives in Teams,Econometrica,,0012-9682,10.2307/1914085,https://www.jstor.org/stable/1914085,"This paper analyzes the problem of inducing the members of an organization to behave as if they formed a team. Considered is a conglomerate-type organization consisting of a set of semi-autonomous subunits that are coordinated by the organization's head. The head's incentive problem is to choose a set of employee compensation rules that will induce his subunit managers to communicate accurate information and take optimal decisions. The main result exhibits a particular set of compensation rules, an optimal incentive structure, that leads to team behavior. Particular attention is directed to the informational aspects of the problem. An extended example of a resource allocation model is discussed and the optimal incentive structure is interpreted in terms of prices charged by the head for resources allocated to the subunits.",1973,2022-03-10 22:34:57,2022-03-10 22:34:57,2022-03-10 22:34:57,617-631,,4,41,,,,,,,,,,,,,,,JSTOR,,"Publisher: [Wiley, Econometric Society]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ND3DRSC,journalArticle,1979,"Myerson, Roger B.",Incentive Compatibility and the Bargaining Problem,Econometrica,,0012-9682,10.2307/1912346,https://www.jstor.org/stable/1912346,"Collective choice problems are studied from the Bayesian viewpoint. It is shown that the set of expected utility allocations which are feasible with incentive-compatible mechanisms is compact and convex, and includes the equilibrium allocations for all other mechanisms. The generalized Nash solution proposed by Harsanyi and Selten is then applied to this set to define a bargaining solution for Bayesian collective choice problems.",1979,2022-03-10 22:34:59,2022-03-10 22:34:59,2022-03-10 22:34:59,61-73,,1,47,,,,,,,,,,,,,,,JSTOR,,"Publisher: [Wiley, Econometric Society]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TV3BRGY3,journalArticle,2013,"Goodman, Noah D.; Stuhlmüller, Andreas",Knowledge and implicature: modeling language understanding as social cognition,Topics in Cognitive Science,,1756-8765,10.1111/tops.12007,,"Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to ""invert"" this model of the speaker. We apply this framework to model scalar implicature (""some"" implies ""not all,"" and ""N"" implies ""not more than N""). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.",2013-01,2022-03-10 22:36:08,2022-03-10 22:36:08,,173-184,,1,5,,Top Cogn Sci,Knowledge and implicature,,,,,,,eng,,,,,PubMed,,PMID: 23335578,,/Users/jacquesthibodeau/Zotero/storage/N557FJFR/Goodman and Stuhlmüller - 2013 - Knowledge and implicature modeling language under.pdf;,http://www.ncbi.nlm.nih.gov/pubmed/23335578,,"Bayes Theorem; Comprehension; Data Interpretation, Statistical; Humans; Information Theory; Interpersonal Relations; Intuition; Judgment; Knowledge; Language; Models, Psychological; Psycholinguistics; Psychological Theory; Semantics; Social Behavior; Speech Perception",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVGCV8EY,journalArticle,2002,"Wang, Xiao Fan; Chen, Guanrong",Synchronization in Small-World Dynamical Networks,Int. J. Bifurc. Chaos,,,10.1142/S0218127402004292,,"It is shown that, for any given coupling strength and a suciently large number of cells, the small-world dynamical network will synchronize, even if the original nearest-neighbor coupled network cannot achieve synchronization under the same condition. We investigate synchronization in a network of continuous-time dynamical systems with smallworld connections. The small-world network is obtained by randomly adding a small fraction of connection in an originally nearest-neighbor coupled network. We show that, for any given coupling strength and a suciently large number of cells, the small-world dynamical network will synchronize, even if the original nearest-neighbor coupled network cannot achieve synchronization under the same condition.",2002,2022-03-10 22:38:44,2022-03-10 22:38:44,,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,https://www.semanticscholar.org/paper/Synchronization-in-Small-World-Dynamical-Networks-Wang-Chen/74d04fe177f02da2e94a895d1bb0b2a07918c20e?p2df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V77E9AAY,journalArticle,2002,"Bernstein, Daniel S.; Givan, Robert; Immerman, Neil; Zilberstein, Shlomo",The Complexity of Decentralized Control of Markov Decision Processes,Mathematics of Operations Research,,0364-765X,10.1287/moor.27.4.819.297,https://pubsonline.informs.org/doi/abs/10.1287/moor.27.4.819.297,"We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP ≠ NEXP, the problems require superexponential time to solve in the worst case.",2002-11,2022-03-10 22:38:53,2022-03-10 22:38:53,2022-03-10 22:38:53,819-840,,4,27,,Mathematics of OR,,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,,,,Computational complexity; decentralized control; Markov decision process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AJA9FL7H,journalArticle,2020,"Krueger, David; Maharaj, Tegan; Leike, Jan",Hidden Incentives for Auto-Induced Distributional Shift,"arXiv:2009.09153 [cs, stat]",,,,http://arxiv.org/abs/2009.09153,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",2020-09-18,2022-03-10 22:45:45,2022-03-10 22:45:45,2022-03-10 22:45:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09153,,/Users/jacquesthibodeau/Zotero/storage/BMHXUJKK/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf; /Users/jacquesthibodeau/Zotero/storage/RG2UB9JY/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VEDSTYU9,journalArticle,2019,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","arXiv:1906.09624 [cs, stat]",,,,http://arxiv.org/abs/1906.09624,"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",2019-06-23,2022-03-10 22:46:03,2022-03-10 22:46:03,2022-03-10 22:46:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.09624,,"/Users/jacquesthibodeau/Zotero/storage/WR3N7EQX/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf; /Users/jacquesthibodeau/Zotero/storage/BF5LA6UX/1906.html",,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9UMKK8YW,journalArticle,2020,"Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",Inverse Reward Design,arXiv:1711.02827 [cs],,,,http://arxiv.org/abs/1711.02827,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",2020-10-07,2022-03-10 22:46:20,2022-03-10 22:46:20,2022-03-10 22:46:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.02827,,/Users/jacquesthibodeau/Zotero/storage/E3Y6SDXV/Hadfield-Menell et al. - 2020 - Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/4NGCDF82/1711.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WXZEYNTU,journalArticle,2021,"Wainwright, Carroll L.; Eckersley, Peter",SafeLife 1.0: Exploring Side Effects in Complex Environments,arXiv:1912.01217 [cs],,,,http://arxiv.org/abs/1912.01217,"We present SafeLife, a publicly available reinforcement learning environment that tests the safety of reinforcement learning agents. It contains complex, dynamic, tunable, procedurally generated levels with many opportunities for unsafe behavior. Agents are graded both on their ability to maximize their explicit reward and on their ability to operate safely without unnecessary side effects. We train agents to maximize rewards using proximal policy optimization and score them on a suite of benchmark levels. The resulting agents are performant but not safe -- they tend to cause large side effects in their environments -- but they form a baseline against which future safety research can be measured.",2021-02-26,2022-03-10 22:48:05,2022-03-11 1:39:13,2022-03-10 22:48:05,,,,,,,SafeLife 1.0,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01217,,/Users/jacquesthibodeau/Zotero/storage/FJVXPPKD/Wainwright and Eckersley - 2021 - SafeLife 1.0 Exploring Side Effects in Complex En.pdf; /Users/jacquesthibodeau/Zotero/storage/22SEDBIN/Wainwright and Eckersley - 2021 - SafeLife 1.0 Exploring Side Effects in Complex En.pdf; /Users/jacquesthibodeau/Zotero/storage/VSK7Y643/1912.html; /Users/jacquesthibodeau/Zotero/storage/VNRKWCXE/1912.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKWNVHNB,journalArticle,2021,"Lindner, David; Matoba, Kyle; Meulemans, Alexander",Challenges for Using Impact Regularizers to Avoid Negative Side Effects,arXiv:2101.12509 [cs],,,,http://arxiv.org/abs/2101.12509,"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",2021-02-23,2022-03-10 22:53:21,2022-03-10 22:53:21,2022-03-10 22:53:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.12509,,/Users/jacquesthibodeau/Zotero/storage/YB68J72A/Lindner et al. - 2021 - Challenges for Using Impact Regularizers to Avoid .pdf; /Users/jacquesthibodeau/Zotero/storage/BVLWGZVU/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHQ4N3UT,journalArticle,2020,"Badia, Adrià Puigdomènech; Piot, Bilal; Kapturowski, Steven; Sprechmann, Pablo; Vitvitskyi, Alex; Guo, Daniel; Blundell, Charles",Agent57: Outperforming the Atari Human Benchmark,"arXiv:2003.13350 [cs, stat]",,,,http://arxiv.org/abs/2003.13350,"Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",2020-03-30,2022-03-10 23:01:27,2022-03-10 23:01:27,2022-03-10 23:01:27,,,,,,,Agent57,,,,,,,,,,,,arXiv.org,,arXiv: 2003.13350,,/Users/jacquesthibodeau/Zotero/storage/Z9TJN4M2/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf; /Users/jacquesthibodeau/Zotero/storage/AWF3YR3H/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L9QYGZFM,journalArticle,2018,"Serban, Iulian Vlad; Sankar, Chinnadhurai; Pieper, Michael; Pineau, Joelle; Bengio, Yoshua",The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach,"arXiv:1807.04723 [cs, stat]",,,,http://arxiv.org/abs/1807.04723,"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.",2018-07-12,2022-03-10 23:15:47,2022-03-10 23:15:47,2022-03-10 23:15:47,,,,,,,The Bottleneck Simulator,,,,,,,,,,,,arXiv.org,,arXiv: 1807.04723,,/Users/jacquesthibodeau/Zotero/storage/HVW8RPTW/Serban et al. - 2018 - The Bottleneck Simulator A Model-based Deep Reinf.pdf; /Users/jacquesthibodeau/Zotero/storage/BLXYLMWH/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; I.2.7; I.5.1; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VMV48MAY,journalArticle,2019,"Hafner, Danijar; Lillicrap, Timothy; Fischer, Ian; Villegas, Ruben; Ha, David; Lee, Honglak; Davidson, James",Learning Latent Dynamics for Planning from Pixels,"arXiv:1811.04551 [cs, stat]",,,,http://arxiv.org/abs/1811.04551,"Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.",2019-06-04,2022-03-10 23:16:11,2022-03-10 23:16:11,2022-03-10 23:16:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04551,,/Users/jacquesthibodeau/Zotero/storage/QDDDSDGI/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf; /Users/jacquesthibodeau/Zotero/storage/DJ293UAS/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C54BP9NQ,journalArticle,2018,"Oh, Junhyuk; Guo, Yijie; Singh, Satinder; Lee, Honglak",Self-Imitation Learning,"arXiv:1806.05635 [cs, stat]",,,,http://arxiv.org/abs/1806.05635,"This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",2018-06-14,2022-03-10 23:20:10,2022-03-10 23:20:10,2022-03-10 23:20:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05635,,/Users/jacquesthibodeau/Zotero/storage/X2EK7DN7/Oh et al. - 2018 - Self-Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/PD5YS5PS/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXGWPGWF,journalArticle,2020,"Hong, Zhang-Wei; Fu, Tsu-Jui; Shann, Tzu-Yun; Chang, Yi-Hsiang; Lee, Chun-Yi",Adversarial Active Exploration for Inverse Dynamics Model Learning,"arXiv:1806.10019 [cs, stat]",,,,http://arxiv.org/abs/1806.10019,"We present an adversarial active exploration for inverse dynamics model learning, a simple yet effective learning scheme that incentivizes exploration in an environment without any human intervention. Our framework consists of a deep reinforcement learning (DRL) agent and an inverse dynamics model contesting with each other. The former collects training samples for the latter, with an objective to maximize the error of the latter. The latter is trained with samples collected by the former, and generates rewards for the former when it fails to predict the actual action taken by the former. In such a competitive setting, the DRL agent learns to generate samples that the inverse dynamics model fails to predict correctly, while the inverse dynamics model learns to adapt to the challenging samples. We further propose a reward structure that ensures the DRL agent to collect only moderately hard samples but not overly hard ones that prevent the inverse model from predicting effectively. We evaluate the effectiveness of our method on several robotic arm and hand manipulation tasks against multiple baseline models. Experimental results show that our method is comparable to those directly trained with expert demonstrations, and superior to the other baselines even without any human priors.",2020-03-16,2022-03-10 23:20:15,2022-03-10 23:20:15,2022-03-10 23:20:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10019,,/Users/jacquesthibodeau/Zotero/storage/YKPHFUCB/Hong et al. - 2020 - Adversarial Active Exploration for Inverse Dynamic.pdf; /Users/jacquesthibodeau/Zotero/storage/PRE9BFQA/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IK562NSL,journalArticle,2022,"Anthony, Thomas; Eccles, Tom; Tacchetti, Andrea; Kramár, János; Gemp, Ian; Hudson, Thomas C.; Porcel, Nicolas; Lanctot, Marc; Pérolat, Julien; Everett, Richard; Werpachowski, Roman; Singh, Satinder; Graepel, Thore; Bachrach, Yoram",Learning to Play No-Press Diplomacy with Best Response Policy Iteration,"arXiv:2006.04635 [cs, stat]",,,,http://arxiv.org/abs/2006.04635,"Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",2022-01-04,2022-03-10 23:21:15,2022-03-10 23:21:15,2022-03-10 23:21:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04635,,/Users/jacquesthibodeau/Zotero/storage/I3DPELKX/Anthony et al. - 2022 - Learning to Play No-Press Diplomacy with Best Resp.pdf; /Users/jacquesthibodeau/Zotero/storage/NG5J3YBS/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27D3XH96,journalArticle,2020,"Eysenbach, Benjamin; Geng, Xinyang; Levine, Sergey; Salakhutdinov, Ruslan",Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"arXiv:2002.11089 [cs, stat]",,,,http://arxiv.org/abs/2002.11089,"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",2020-02-25,2022-03-10 23:21:21,2022-03-10 23:21:21,2022-03-10 23:21:21,,,,,,,Rewriting History with Inverse RL,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11089,,/Users/jacquesthibodeau/Zotero/storage/F6ZD5MDF/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf; /Users/jacquesthibodeau/Zotero/storage/TFJYY2W4/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXDGG86N,journalArticle,2020,"Li, Alexander C.; Pinto, Lerrel; Abbeel, Pieter",Generalized Hindsight for Reinforcement Learning,"arXiv:2002.11708 [cs, stat]",,,,http://arxiv.org/abs/2002.11708,"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",2020-02-26,2022-03-10 23:23:05,2022-03-10 23:23:05,2022-03-10 23:23:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11708,,/Users/jacquesthibodeau/Zotero/storage/NF8IWNNY/Li et al. - 2020 - Generalized Hindsight for Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/J6H7GUYL/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FY86WZUC,journalArticle,2019,"Arjona-Medina, Jose A.; Gillhofer, Michael; Widrich, Michael; Unterthiner, Thomas; Brandstetter, Johannes; Hochreiter, Sepp",RUDDER: Return Decomposition for Delayed Rewards,"arXiv:1806.07857 [cs, math, stat]",,,,http://arxiv.org/abs/1806.07857,"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({\lambda}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.",2019-09-10,2022-03-10 23:23:28,2022-03-10 23:23:28,2022-03-10 23:23:28,,,,,,,RUDDER,,,,,,,,,,,,arXiv.org,,arXiv: 1806.07857,,/Users/jacquesthibodeau/Zotero/storage/JTMQWGUL/Arjona-Medina et al. - 2019 - RUDDER Return Decomposition for Delayed Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/IGVXES3F/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Mathematics - Optimization and Control; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W47M8523,journalArticle,2020,"Andrychowicz, Marcin; Raichuk, Anton; Stańczyk, Piotr; Orsini, Manu; Girgin, Sertan; Marinier, Raphael; Hussenot, Léonard; Geist, Matthieu; Pietquin, Olivier; Michalski, Marcin; Gelly, Sylvain; Bachem, Olivier",What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,"arXiv:2006.05990 [cs, stat]",,,,http://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",2020-06-10,2022-03-10 23:23:29,2022-03-10 23:23:29,2022-03-10 23:23:28,,,,,,,What Matters In On-Policy Reinforcement Learning?,,,,,,,,,,,,arXiv.org,,arXiv: 2006.05990,,/Users/jacquesthibodeau/Zotero/storage/BCTI4FSA/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf; /Users/jacquesthibodeau/Zotero/storage/HMHRKYF8/2006.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52IVK8ZF,journalArticle,2020,"Kumar, Aviral; Gupta, Abhishek; Levine, Sergey",DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,"arXiv:2003.07305 [cs, stat]",,,,http://arxiv.org/abs/2003.07305,"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",2020-03-16,2022-03-10 23:23:45,2022-03-10 23:23:45,2022-03-10 23:23:45,,,,,,,DisCor,,,,,,,,,,,,arXiv.org,,arXiv: 2003.07305,,/Users/jacquesthibodeau/Zotero/storage/L8VU8MNM/Kumar et al. - 2020 - DisCor Corrective Feedback in Reinforcement Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/SGQT53TW/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QM2GFPBM,journalArticle,2021,"Janner, Michael; Mordatch, Igor; Levine, Sergey",Generative Temporal Difference Learning for Infinite-Horizon Prediction,arXiv:2010.14496 [cs],,,,http://arxiv.org/abs/2010.14496,"We introduce the $\gamma$-model, a predictive model of environment dynamics with an infinite probabilistic horizon. Replacing standard single-step models with $\gamma$-models leads to generalizations of the procedures central to model-based control, including the model rollout and model-based value estimation. The $\gamma$-model, trained with a generative reinterpretation of temporal difference learning, is a natural continuous analogue of the successor representation and a hybrid between model-free and model-based mechanisms. Like a value function, it contains information about the long-term future; like a standard predictive model, it is independent of task reward. We instantiate the $\gamma$-model as both a generative adversarial network and normalizing flow, discuss how its training reflects an inescapable tradeoff between training-time and testing-time compounding errors, and empirically investigate its utility for prediction and control.",2021-11-28,2022-03-10 23:23:53,2022-03-10 23:23:53,2022-03-10 23:23:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14496,,/Users/jacquesthibodeau/Zotero/storage/MRI4YNY5/Janner et al. - 2021 - Generative Temporal Difference Learning for Infini.pdf; /Users/jacquesthibodeau/Zotero/storage/3S49W7MN/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FJMI2TH6,journalArticle,2021,"Janner, Michael; Li, Qiyang; Levine, Sergey",Offline Reinforcement Learning as One Big Sequence Modeling Problem,arXiv:2106.02039 [cs],,,,http://arxiv.org/abs/2106.02039,"Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.",2021-11-28,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.02039,,/Users/jacquesthibodeau/Zotero/storage/G5C7IV9S/Janner et al. - 2021 - Offline Reinforcement Learning as One Big Sequence.pdf; /Users/jacquesthibodeau/Zotero/storage/4TLTNBLK/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8YUXMP6B,journalArticle,2018,"Wilson, Dennis G.; Cussat-Blanc, Sylvain; Luga, Hervé; Miller, Julian F.",Evolving simple programs for playing Atari games,arXiv:1806.05695 [cs],,,,http://arxiv.org/abs/1806.05695,"Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but effective strategies can be found.",2018-06-14,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05695,,/Users/jacquesthibodeau/Zotero/storage/X7H78VC2/Wilson et al. - 2018 - Evolving simple programs for playing Atari games.pdf; /Users/jacquesthibodeau/Zotero/storage/UZJHHK5G/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N6D9N3GB,journalArticle,2018,"Laterre, Alexandre; Fu, Yunguan; Jabri, Mohamed Khalil; Cohen, Alain-Sam; Kas, David; Hajjar, Karl; Dahl, Torbjorn S.; Kerkeni, Amine; Beguir, Karim",Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization,"arXiv:1807.01672 [cs, stat]",,,,http://arxiv.org/abs/1807.01672,"Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",2018-12-06,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,Ranked Reward,,,,,,,,,,,,arXiv.org,,arXiv: 1807.01672,,/Users/jacquesthibodeau/Zotero/storage/6VW9BITR/Laterre et al. - 2018 - Ranked Reward Enabling Self-Play Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/I5ZY5SBP/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NLAWIBAJ,journalArticle,2018,"Justesen, Niels; Torrado, Ruben Rodriguez; Bontrager, Philip; Khalifa, Ahmed; Togelius, Julian; Risi, Sebastian",Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation,"arXiv:1806.10729 [cs, stat]",,,,http://arxiv.org/abs/1806.10729,"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.",2018-11-29,2022-03-10 23:24:04,2022-03-10 23:24:04,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10729,,/Users/jacquesthibodeau/Zotero/storage/PH7H6EP2/Justesen et al. - 2018 - Illuminating Generalization in Deep Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/Q39A5EXV/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CU8THETM,journalArticle,2021,"Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor",Decision Transformer: Reinforcement Learning via Sequence Modeling,arXiv:2106.01345 [cs],,,,http://arxiv.org/abs/2106.01345,"We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",2021-06-24,2022-03-10 23:24:04,2022-03-10 23:24:04,2022-03-10 23:24:04,,,,,,,Decision Transformer,,,,,,,,,,,,arXiv.org,,arXiv: 2106.01345,,/Users/jacquesthibodeau/Zotero/storage/GNYB773T/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf; /Users/jacquesthibodeau/Zotero/storage/9QN2AB3A/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MTEYH83D,journalArticle,2020,"Tachet, Remi; Bachman, Philip; van Seijen, Harm",Learning Invariances for Policy Generalization,"arXiv:1809.02591 [cs, stat]",,,,http://arxiv.org/abs/1809.02591,"While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.",2020-12-12,2022-03-10 23:24:35,2022-03-10 23:24:35,2022-03-10 23:24:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.02591,,/Users/jacquesthibodeau/Zotero/storage/V775K3NR/Tachet et al. - 2020 - Learning Invariances for Policy Generalization.pdf; /Users/jacquesthibodeau/Zotero/storage/YUQ392Q4/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E4AA3RIR,journalArticle,2020,"Wu, Min; Wicker, Matthew; Ruan, Wenjie; Huang, Xiaowei; Kwiatkowska, Marta",A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees,Theoretical Computer Science,,3043975,10.1016/j.tcs.2019.05.046,http://arxiv.org/abs/1807.03571,"Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",2020-02,2022-03-10 23:29:06,2022-03-10 23:29:06,2022-03-10 23:29:06,298-329,,,807,,Theoretical Computer Science,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03571,,/Users/jacquesthibodeau/Zotero/storage/DSYWMKMR/Wu et al. - 2020 - A Game-Based Approximate Verification of Deep Neur.pdf; /Users/jacquesthibodeau/Zotero/storage/9KA3D6NF/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KRFE544,journalArticle,2020,"Hunt, Nathan; Fulton, Nathan; Magliacane, Sara; Hoang, Nghia; Das, Subhro; Solar-Lezama, Armando",Verifiably Safe Exploration for End-to-End Reinforcement Learning,arXiv:2007.01223 [cs],,,,http://arxiv.org/abs/2007.01223,"Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.",2020-07-02,2022-03-10 23:29:23,2022-03-10 23:29:23,2022-03-10 23:29:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2007.01223,,/Users/jacquesthibodeau/Zotero/storage/83D3JM8W/Hunt et al. - 2020 - Verifiably Safe Exploration for End-to-End Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/B97MVUSI/2007.html,,,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Computer Science - Machine Learning; F.3.1; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VDYLMZA,journalArticle,2021,"Sarma, Gopal; Koppel, James; Malecha, Gregory; Schultz, Patrick; Drexler, Eric; Kumar, Ramana; Roux, Cody; Zucker, Philip",Formal Methods for the Informal Engineer: Workshop Recommendations,"arXiv:2104.00739 [cs, q-bio]",,,10.31219/osf.io/t4qs8,http://arxiv.org/abs/2104.00739,"Formal Methods for the Informal Engineer (FMIE) was a workshop held at the Broad Institute of MIT and Harvard in 2021 to explore the potential role of verified software in the biomedical software ecosystem. The motivation for organizing FMIE was the recognition that the life sciences and medicine are undergoing a transition from being passive consumers of software and AI/ML technologies to fundamental drivers of new platforms, including those which will need to be mission and safety-critical. Drawing on conversations leading up to and during the workshop, we make five concrete recommendations to help software leaders organically incorporate tools, techniques, and perspectives from formal methods into their project planning and development trajectories.",2021-03-30,2022-03-10 23:29:25,2022-03-10 23:29:25,2022-03-10 23:29:25,,,,,,,Formal Methods for the Informal Engineer,,,,,,,,,,,,arXiv.org,,arXiv: 2104.00739,,/Users/jacquesthibodeau/Zotero/storage/L7QAPY7P/Sarma et al. - 2021 - Formal Methods for the Informal Engineer Workshop.pdf; /Users/jacquesthibodeau/Zotero/storage/BHT3UMSQ/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Programming Languages; Computer Science - Software Engineering; Quantitative Biology - Other Quantitative Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFLGZQLF,journalArticle,2018,"Wang, Shiqi; Chen, Yizheng; Abdou, Ahmed; Jana, Suman",MixTrain: Scalable Training of Verifiably Robust Neural Networks,"arXiv:1811.02625 [cs, stat]",,,,http://arxiv.org/abs/1811.02625,"Making neural networks robust against adversarial inputs has resulted in an arms race between new defenses and attacks. The most promising defenses, adversarially robust training and verifiably robust training, have limitations that restrict their practical applications. The adversarially robust training only makes the networks robust against a subclass of attackers and we reveal such weaknesses by developing a new attack based on interval gradients. By contrast, verifiably robust training provides protection against any L-p norm-bounded attacker but incurs orders of magnitude more computational and memory overhead than adversarially robust training. We propose two novel techniques, stochastic robust approximation and dynamic mixed training, to drastically improve the efficiency of verifiably robust training without sacrificing verified robustness. We leverage two critical insights: (1) instead of over the entire training set, sound over-approximations over randomly subsampled training data points are sufficient for efficiently guiding the robust training process; and (2) We observe that the test accuracy and verifiable robustness often conflict after certain training epochs. Therefore, we use a dynamic loss function to adaptively balance them for each epoch. We designed and implemented our techniques as part of MixTrain and evaluated it on six networks trained on three popular datasets including MNIST, CIFAR, and ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\%$ verified robust accuracy against $L_\infty$ norm-bounded attackers while taking $15$ and $3$ times less training time than state-of-the-art verifiably robust training and adversarially robust training schemes, respectively. Furthermore, MixTrain easily scales to larger networks like the one trained on ImageNet-200, significantly outperforming the existing verifiably robust training methods.",2018-12-01,2022-03-10 23:29:27,2022-03-10 23:29:27,2022-03-10 23:29:27,,,,,,,MixTrain,,,,,,,,,,,,arXiv.org,,arXiv: 1811.02625,,/Users/jacquesthibodeau/Zotero/storage/2XJBPJ2Q/Wang et al. - 2018 - MixTrain Scalable Training of Verifiably Robust N.pdf; /Users/jacquesthibodeau/Zotero/storage/6H87KUBP/1811.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSPG75ES,journalArticle,2021,"Everett, Michael; Lutjens, Bjorn; How, Jonathan P.",Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning,IEEE Transactions on Neural Networks and Learning Systems,,"2162-237X, 2162-2388",10.1109/TNNLS.2021.3056046,http://arxiv.org/abs/2004.06496,"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",2021,2022-03-10 23:29:29,2022-03-10 23:29:29,2022-03-10 23:29:29,1-15,,,,,IEEE Trans. Neural Netw. Learning Syst.,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.06496,,/Users/jacquesthibodeau/Zotero/storage/774PTZ8C/Everett et al. - 2021 - Certifiable Robustness to Adversarial State Uncert.pdf; /Users/jacquesthibodeau/Zotero/storage/B9C5PQ29/2004.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTH6VH9W,journalArticle,2018,"Bhupatiraju, Surya; Agrawal, Kumar Krishna; Singh, Rishabh",Towards Mixed Optimization for Reinforcement Learning with Program Synthesis,"arXiv:1807.00403 [cs, stat]",,,,http://arxiv.org/abs/1807.00403,"Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.",2018-07-03,2022-03-10 23:29:31,2022-03-10 23:29:31,2022-03-10 23:29:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00403,,/Users/jacquesthibodeau/Zotero/storage/ZMRTTGSN/Bhupatiraju et al. - 2018 - Towards Mixed Optimization for Reinforcement Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/MJMV8B8R/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E963R5SM,journalArticle,2019,"Gowal, Sven; Dvijotham, Krishnamurthy; Stanforth, Robert; Bunel, Rudy; Qin, Chongli; Uesato, Jonathan; Arandjelovic, Relja; Mann, Timothy; Kohli, Pushmeet",On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models,"arXiv:1810.12715 [cs, stat]",,,,http://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.",2019-08-29,2022-03-10 23:29:33,2022-03-10 23:29:33,2022-03-10 23:29:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.12715,,/Users/jacquesthibodeau/Zotero/storage/T8XPMGNA/Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf; /Users/jacquesthibodeau/Zotero/storage/72YVT3W8/1810.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S5879VA4,journalArticle,2020,"Liu, Changliu; Arnon, Tomer; Lazarus, Christopher; Strong, Christopher; Barrett, Clark; Kochenderfer, Mykel J.",Algorithms for Verifying Deep Neural Networks,"arXiv:1903.06758 [cs, stat]",,,,http://arxiv.org/abs/1903.06758,"Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.",2020-10-15,2022-03-10 23:29:34,2022-03-10 23:29:34,2022-03-10 23:29:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06758,,/Users/jacquesthibodeau/Zotero/storage/H2V6KQMT/Liu et al. - 2020 - Algorithms for Verifying Deep Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/3UX6DKD4/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LK87P32P,journalArticle,2019,"Anderson, Greg; Pailoor, Shankara; Dillig, Isil; Chaudhuri, Swarat",Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,,,10.1145/3314221.3314614,http://arxiv.org/abs/1904.09959,"In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and ({\delta}-)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2 , Reluplex, and Reluval.",2019-06-08,2022-03-10 23:29:36,2022-03-10 23:29:36,2022-03-10 23:29:36,731-744,,,,,,Optimization and Abstraction,,,,,,,,,,,,arXiv.org,,arXiv: 1904.09959,,/Users/jacquesthibodeau/Zotero/storage/M2KZN5CJ/Anderson et al. - 2019 - Optimization and Abstraction A Synergistic Approa.pdf; /Users/jacquesthibodeau/Zotero/storage/L2LEUNWR/1904.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALGPFQY,journalArticle,2018,"Dvijotham, Krishnamurthy; Gowal, Sven; Stanforth, Robert; Arandjelovic, Relja; O'Donoghue, Brendan; Uesato, Jonathan; Kohli, Pushmeet",Training verified learners with learned verifiers,"arXiv:1805.10265 [cs, stat]",,,,http://arxiv.org/abs/1805.10265,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.",2018-05-29,2022-03-10 23:29:37,2022-03-10 23:29:37,2022-03-10 23:29:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.10265,,/Users/jacquesthibodeau/Zotero/storage/KTWS6SJV/Dvijotham et al. - 2018 - Training verified learners with learned verifiers.pdf; /Users/jacquesthibodeau/Zotero/storage/94ZA5E22/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q33NH5ZE,journalArticle,2020,"Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet",Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,arXiv:2010.11645 [cs],,,,http://arxiv.org/abs/2010.11645,"Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.",2020-11-03,2022-03-10 23:29:39,2022-03-10 23:29:39,2022-03-10 23:29:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.11645,,/Users/jacquesthibodeau/Zotero/storage/CU84IIZX/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/FW45H5BS/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R8XHBNX8,journalArticle,2019,"Tjeng, Vincent; Xiao, Kai; Tedrake, Russ",Evaluating Robustness of Neural Networks with Mixed Integer Programming,arXiv:1711.07356 [cs],,,,http://arxiv.org/abs/1711.07356,"Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples - slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional networks with an order of magnitude more ReLUs than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_\infty$ norm $\epsilon=0.1$: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness (to perturbations with bounded norm) for the remainder. Across all robust training procedures and network architectures considered, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.",2019-02-17,2022-03-10 23:29:41,2022-03-10 23:29:41,2022-03-10 23:29:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.07356,,/Users/jacquesthibodeau/Zotero/storage/7PHSKGTD/Tjeng et al. - 2019 - Evaluating Robustness of Neural Networks with Mixe.pdf; /Users/jacquesthibodeau/Zotero/storage/DCVXYWTQ/1711.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSDX24P8,journalArticle,2020,"Anderson, Greg; Verma, Abhinav; Dillig, Isil; Chaudhuri, Swarat",Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"arXiv:2009.12612 [cs, stat]",,,,http://arxiv.org/abs/2009.12612,"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",2020-10-26,2022-03-10 23:29:46,2022-03-10 23:29:46,2022-03-10 23:29:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.12612,,/Users/jacquesthibodeau/Zotero/storage/IAV5W3C5/Anderson et al. - 2020 - Neurosymbolic Reinforcement Learning with Formally.pdf; /Users/jacquesthibodeau/Zotero/storage/2LRGZHWW/2009.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KB8PHF2Y,journalArticle,2020,"Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy",Certified Defenses against Adversarial Examples,arXiv:1801.09344 [cs],,,,http://arxiv.org/abs/1801.09344,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.",2020-10-31,2022-03-10 23:29:47,2022-03-10 23:29:47,2022-03-10 23:29:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1801.09344,,/Users/jacquesthibodeau/Zotero/storage/6RAHXUGT/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/RUEIXHE2/1801.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YXGV3JKA,journalArticle,2019,"Wu, Tailin; Tegmark, Max",Toward an AI Physicist for Unsupervised Learning,Physical Review E,,"2470-0045, 2470-0053",10.1103/PhysRevE.100.033311,http://arxiv.org/abs/1810.10525,"We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's razor, unification and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and ""snap"" learned theories into simple symbolic formulas. Theories are stored in a ""theory hub"", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the toy ""AI Physicist"" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.",2019-09-19,2022-03-10 23:29:51,2022-03-10 23:29:51,2022-03-10 23:29:51,33311,,3,100,,Phys. Rev. E,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.10525,,/Users/jacquesthibodeau/Zotero/storage/BK5ZQAGH/Wu and Tegmark - 2019 - Toward an AI Physicist for Unsupervised Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/K6K7LQJB/1810.html,,,Computer Science - Machine Learning; Condensed Matter - Disordered Systems and Neural Networks; Physics - Computational Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YLKUHEAL,journalArticle,2021,"Caron, Mathilde; Misra, Ishan; Mairal, Julien; Goyal, Priya; Bojanowski, Piotr; Joulin, Armand",Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,arXiv:2006.09882 [cs],,,,http://arxiv.org/abs/2006.09882,"Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",2021-01-08,2022-03-10 23:29:54,2022-03-10 23:29:54,2022-03-10 23:29:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.09882,,/Users/jacquesthibodeau/Zotero/storage/R6XYQHYG/Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf; /Users/jacquesthibodeau/Zotero/storage/QZGJEIKS/2006.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K29D7ZK6,journalArticle,2018,"Achille, Alessandro; Eccles, Tom; Matthey, Loic; Burgess, Christopher P.; Watters, Nick; Lerchner, Alexander; Higgins, Irina",Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"arXiv:1808.06508 [cs, stat]",,,,http://arxiv.org/abs/1808.06508,"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",2018-08-20,2022-03-10 23:29:56,2022-03-10 23:29:56,2022-03-10 23:29:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.06508,,/Users/jacquesthibodeau/Zotero/storage/UTSYSDTR/Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf; /Users/jacquesthibodeau/Zotero/storage/A2NWGPLN/1808.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LJ59K6J8,journalArticle,2020,"Chen, Xinlei; Fan, Haoqi; Girshick, Ross; He, Kaiming",Improved Baselines with Momentum Contrastive Learning,arXiv:2003.04297 [cs],,,,http://arxiv.org/abs/2003.04297,"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",2020-03-09,2022-03-10 23:29:58,2022-03-10 23:29:58,2022-03-10 23:29:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.04297,,/Users/jacquesthibodeau/Zotero/storage/XMD2BEG6/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/U28LPGU9/2003.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R2CRJMSZ,journalArticle,2020,"He, Kaiming; Fan, Haoqi; Wu, Yuxin; Xie, Saining; Girshick, Ross",Momentum Contrast for Unsupervised Visual Representation Learning,arXiv:1911.05722 [cs],,,,http://arxiv.org/abs/1911.05722,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",2020-03-23,2022-03-10 23:30:01,2022-03-10 23:30:01,2022-03-10 23:30:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.05722,,/Users/jacquesthibodeau/Zotero/storage/ZTMXW29F/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf; /Users/jacquesthibodeau/Zotero/storage/7ZVW4PJE/1911.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISG7EU3W,journalArticle,2020,"Chen, Ting; Kornblith, Simon; Norouzi, Mohammad; Hinton, Geoffrey",A Simple Framework for Contrastive Learning of Visual Representations,"arXiv:2002.05709 [cs, stat]",,,,http://arxiv.org/abs/2002.05709,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",2020-06-30,2022-03-10 23:30:04,2022-03-10 23:30:04,2022-03-10 23:30:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05709,,/Users/jacquesthibodeau/Zotero/storage/VTEC7QZ3/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf; /Users/jacquesthibodeau/Zotero/storage/HXE8H66X/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3SE2ADJU,journalArticle,2020,"Chen, Ting; Kornblith, Simon; Swersky, Kevin; Norouzi, Mohammad; Hinton, Geoffrey",Big Self-Supervised Models are Strong Semi-Supervised Learners,"arXiv:2006.10029 [cs, stat]",,,,http://arxiv.org/abs/2006.10029,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.",2020-10-25,2022-03-10 23:30:05,2022-03-10 23:30:05,2022-03-10 23:30:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.10029,,/Users/jacquesthibodeau/Zotero/storage/LPFI5GQ9/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf; /Users/jacquesthibodeau/Zotero/storage/NU5PK7GU/2006.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZ6UQ6KF,journalArticle,2020,"Tian, Yonglong; Sun, Chen; Poole, Ben; Krishnan, Dilip; Schmid, Cordelia; Isola, Phillip",What Makes for Good Views for Contrastive Learning?,arXiv:2005.10243 [cs],,,,http://arxiv.org/abs/2005.10243,"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",2020-12-18,2022-03-10 23:30:37,2022-03-10 23:30:37,2022-03-10 23:30:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.10243,,/Users/jacquesthibodeau/Zotero/storage/TEKWZMBS/Tian et al. - 2020 - What Makes for Good Views for Contrastive Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/ZBGSDZUW/2005.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XAJBTWKI,journalArticle,2019,"Hsu, Kyle; Levine, Sergey; Finn, Chelsea",Unsupervised Learning via Meta-Learning,"arXiv:1810.02334 [cs, stat]",,,,http://arxiv.org/abs/1810.02334,"A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.",2019-03-21,2022-03-10 23:30:40,2022-03-10 23:30:40,2022-03-10 23:30:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.02334,,/Users/jacquesthibodeau/Zotero/storage/MGR7CLFY/Hsu et al. - 2019 - Unsupervised Learning via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/V2AZ53GA/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KG28ZHXU,journalArticle,2019,"Donahue, Jeff; Simonyan, Karen",Large Scale Adversarial Representation Learning,"arXiv:1907.02544 [cs, stat]",,,,http://arxiv.org/abs/1907.02544,"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",2019-11-05,2022-03-10 23:30:42,2022-03-10 23:30:42,2022-03-10 23:30:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.02544,,/Users/jacquesthibodeau/Zotero/storage/3TRCNRT2/Donahue and Simonyan - 2019 - Large Scale Adversarial Representation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/NJ42RJ2H/1907.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5R85AEAQ,journalArticle,2019,"Such, Felipe Petroski; Rawal, Aditya; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff",Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data,"arXiv:1912.07768 [cs, stat]",,,,http://arxiv.org/abs/1912.07768,"This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",2019-12-16,2022-03-10 23:30:44,2022-03-10 23:30:44,2022-03-10 23:30:44,,,,,,,Generative Teaching Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1912.07768,,/Users/jacquesthibodeau/Zotero/storage/4L34H87B/Such et al. - 2019 - Generative Teaching Networks Accelerating Neural .pdf; /Users/jacquesthibodeau/Zotero/storage/4F4KFVMX/1912.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IH2ZRA7F,journalArticle,2019,"Poole, Ben; Ozair, Sherjil; Oord, Aaron van den; Alemi, Alexander A.; Tucker, George",On Variational Bounds of Mutual Information,"arXiv:1905.06922 [cs, stat]",,,,http://arxiv.org/abs/1905.06922,"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.",2019-05-16,2022-03-10 23:30:47,2022-03-10 23:30:47,2022-03-10 23:30:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.06922,,/Users/jacquesthibodeau/Zotero/storage/PR4PFNKJ/Poole et al. - 2019 - On Variational Bounds of Mutual Information.pdf; /Users/jacquesthibodeau/Zotero/storage/5ENBWI8C/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IGYJH9VF,journalArticle,2019,"Oord, Aaron van den; Li, Yazhe; Vinyals, Oriol",Representation Learning with Contrastive Predictive Coding,"arXiv:1807.03748 [cs, stat]",,,,http://arxiv.org/abs/1807.03748,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",2019-01-22,2022-03-10 23:30:51,2022-03-10 23:30:51,2022-03-10 23:30:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03748,,/Users/jacquesthibodeau/Zotero/storage/7DB8L8JV/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf; /Users/jacquesthibodeau/Zotero/storage/58J4WGLT/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBALVEK3,journalArticle,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Do Deep Generative Models Know What They Don't Know?,"arXiv:1810.09136 [cs, stat]",,,,http://arxiv.org/abs/1810.09136,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",2019-02-24,2022-03-10 23:30:54,2022-03-10 23:30:54,2022-03-10 23:30:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.09136,,/Users/jacquesthibodeau/Zotero/storage/WU6X5QAU/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf; /Users/jacquesthibodeau/Zotero/storage/SC7UELE7/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JWSF5CDY,journalArticle,2019,"Sarma, Gopal P.; Safron, Adam; Hay, Nick J.","Integrative Biological Simulation, Neuropsychology, and AI Safety","arXiv:1811.03493 [cs, q-bio]",,,,http://arxiv.org/abs/1811.03493,"We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.",2019-01-21,2022-03-10 23:30:57,2022-03-10 23:30:57,2022-03-10 23:30:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03493,,"/Users/jacquesthibodeau/Zotero/storage/Q646HVPK/Sarma et al. - 2019 - Integrative Biological Simulation, Neuropsychology.pdf; /Users/jacquesthibodeau/Zotero/storage/NA46YJRB/1811.html",,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCNAU9SA,journalArticle,2021,"Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey",Alignment of Language Agents,arXiv:2103.14659 [cs],,,,http://arxiv.org/abs/2103.14659,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",2021-03-26,2022-03-10 23:31:14,2022-03-10 23:31:14,2022-03-10 23:31:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.14659,,/Users/jacquesthibodeau/Zotero/storage/I6V3PXR9/Kenton et al. - 2021 - Alignment of Language Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/XNKXXRLT/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7DHHIFUP,journalArticle,2018,"Hadfield-Menell, Dylan; Hadfield, Gillian",Incomplete Contracting and AI Alignment,arXiv:1804.04268 [cs],,,,http://arxiv.org/abs/1804.04268,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",2018-04-11,2022-03-10 23:31:19,2022-03-10 23:31:19,2022-03-10 23:31:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.04268,,/Users/jacquesthibodeau/Zotero/storage/4EI2HVCR/Hadfield-Menell and Hadfield - 2018 - Incomplete Contracting and AI Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/P92DGWTY/1804.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIS5R3VI,journalArticle,2020,"Arora, Saurabh; Doshi, Prashant","A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress","arXiv:1806.06877 [cs, stat]",,,,http://arxiv.org/abs/1806.06877,"Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",2020-11-18,2022-03-10 23:31:23,2022-03-10 23:31:23,2022-03-10 23:31:23,,,,,,,A Survey of Inverse Reinforcement Learning,,,,,,,,,,,,arXiv.org,,arXiv: 1806.06877,,/Users/jacquesthibodeau/Zotero/storage/WEK8IJYX/Arora and Doshi - 2020 - A Survey of Inverse Reinforcement Learning Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/XWANUQBY/1806.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A6HKXX9U,journalArticle,2018,"Fu, Justin; Luo, Katie; Levine, Sergey",Learning Robust Rewards with Adversarial Inverse Reinforcement Learning,arXiv:1710.11248 [cs],,,,http://arxiv.org/abs/1710.11248,"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",2018-08-13,2022-03-10 23:31:27,2022-03-10 23:31:27,2022-03-10 23:31:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1710.11248,,/Users/jacquesthibodeau/Zotero/storage/N6ZR9CXI/Fu et al. - 2018 - Learning Robust Rewards with Adversarial Inverse R.pdf; /Users/jacquesthibodeau/Zotero/storage/L7F788SS/1710.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
338YA4C6,journalArticle,2019,"Finlayson, Samuel G.; Chung, Hyung Won; Kohane, Isaac S.; Beam, Andrew L.",Adversarial Attacks Against Medical Deep Learning Systems,"arXiv:1804.05296 [cs, stat]",,,,http://arxiv.org/abs/1804.05296,"The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.",2019-02-04,2022-03-10 23:31:30,2022-03-10 23:31:30,2022-03-10 23:31:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05296,,/Users/jacquesthibodeau/Zotero/storage/GLJQX445/Finlayson et al. - 2019 - Adversarial Attacks Against Medical Deep Learning .pdf; /Users/jacquesthibodeau/Zotero/storage/7FMGFWFQ/1804.html,,,Computer Science - Computers and Society; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WLKIYYC3,journalArticle,2019,"Wang, Haohan; He, Zexue; Lipton, Zachary C.; Xing, Eric P.",Learning Robust Representations by Projecting Superficial Statistics Out,arXiv:1903.06256 [cs],,,,http://arxiv.org/abs/1903.06256,"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.",2019-03-01,2022-03-10 23:31:33,2022-03-10 23:31:33,2022-03-10 23:31:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06256,,/Users/jacquesthibodeau/Zotero/storage/VVT9JGP2/Wang et al. - 2019 - Learning Robust Representations by Projecting Supe.pdf; /Users/jacquesthibodeau/Zotero/storage/2BF7TC59/1903.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR9VMLKF,journalArticle,2019,"Shu, Jun; Xie, Qi; Yi, Lixuan; Zhao, Qian; Zhou, Sanping; Xu, Zongben; Meng, Deyu",Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,"arXiv:1902.07379 [cs, stat]",,,,http://arxiv.org/abs/1902.07379,"Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",2019-09-26,2022-03-10 23:31:37,2022-03-10 23:31:37,2022-03-10 23:31:36,,,,,,,Meta-Weight-Net,,,,,,,,,,,,arXiv.org,,arXiv: 1902.07379,,/Users/jacquesthibodeau/Zotero/storage/3G9Q94E8/Shu et al. - 2019 - Meta-Weight-Net Learning an Explicit Mapping For .pdf; /Users/jacquesthibodeau/Zotero/storage/AISTQGAD/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5PRKZMIW,journalArticle,2020,"Wang, Jingkang; Liu, Yang; Li, Bo",Reinforcement Learning with Perturbed Rewards,"arXiv:1810.01032 [cs, stat]",,,,http://arxiv.org/abs/1810.01032,"Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.",2020-02-01,2022-03-10 23:31:41,2022-03-10 23:31:41,2022-03-10 23:31:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.01032,,/Users/jacquesthibodeau/Zotero/storage/8KAIDS5X/Wang et al. - 2020 - Reinforcement Learning with Perturbed Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/WSD7A3JE/1810.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UGPIVX9K,journalArticle,2019,"Lee, Gilwoo; Hou, Brian; Mandalika, Aditya; Lee, Jeongseok; Choudhury, Sanjiban; Srinivasa, Siddhartha S.",Bayesian Policy Optimization for Model Uncertainty,arXiv:1810.01014 [cs],,,,http://arxiv.org/abs/1810.01014,"Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.",2019-05-08,2022-03-10 23:31:45,2022-03-10 23:31:45,2022-03-10 23:31:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.01014,,/Users/jacquesthibodeau/Zotero/storage/RQHIJMH5/Lee et al. - 2019 - Bayesian Policy Optimization for Model Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/D95ZFZJB/1810.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMK3D49K,journalArticle,2019,"Geirhos, Robert; Rubisch, Patricia; Michaelis, Claudio; Bethge, Matthias; Wichmann, Felix A.; Brendel, Wieland",ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"arXiv:1811.12231 [cs, q-bio, stat]",,,,http://arxiv.org/abs/1811.12231,"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",2019-01-14,2022-03-10 23:31:48,2022-03-10 23:31:48,2022-03-10 23:31:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.12231,,/Users/jacquesthibodeau/Zotero/storage/XG9RN9SM/Geirhos et al. - 2019 - ImageNet-trained CNNs are biased towards texture; .pdf; /Users/jacquesthibodeau/Zotero/storage/2P45AY2Z/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MS2W42JX,journalArticle,2019,"Ford, Nic; Gilmer, Justin; Carlini, Nicolas; Cubuk, Dogus",Adversarial Examples Are a Natural Consequence of Test Error in Noise,"arXiv:1901.10513 [cs, stat]",,,,http://arxiv.org/abs/1901.10513,"Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.",2019-01-29,2022-03-10 23:31:53,2022-03-10 23:31:53,2022-03-10 23:31:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.10513,,/Users/jacquesthibodeau/Zotero/storage/Y4497GGQ/Ford et al. - 2019 - Adversarial Examples Are a Natural Consequence of .pdf; /Users/jacquesthibodeau/Zotero/storage/U9HB466V/1901.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8VI85HKE,journalArticle,2019,"Li, Mingchen; Soltanolkotabi, Mahdi; Oymak, Samet",Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,"arXiv:1903.11680 [cs, stat]",,,,http://arxiv.org/abs/1903.11680,"Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this, somewhat paradoxically, neural network models trained via first-order methods continue to predict well on yet unseen test data. This paper takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels despite overparameterization. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) to start to overfit to the noisy labels network must stray rather far from from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.",2019-07-03,2022-03-10 23:32:08,2022-03-10 23:32:08,2022-03-10 23:32:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.11680,,/Users/jacquesthibodeau/Zotero/storage/L82S5IKG/Li et al. - 2019 - Gradient Descent with Early Stopping is Provably R.pdf; /Users/jacquesthibodeau/Zotero/storage/NU5BI2H5/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2QAHLQLS,journalArticle,2020,"Jin, Di; Jin, Zhijing; Zhou, Joey Tianyi; Szolovits, Peter",Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,arXiv:1907.11932 [cs],,,,http://arxiv.org/abs/1907.11932,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",2020-04-08,2022-03-10 23:32:12,2022-03-10 23:32:12,2022-03-10 23:32:12,,,,,,,Is BERT Really Robust?,,,,,,,,,,,,arXiv.org,,arXiv: 1907.11932,,/Users/jacquesthibodeau/Zotero/storage/A7EEW2Q3/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf; /Users/jacquesthibodeau/Zotero/storage/KZHEFNXF/1907.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VZ4VTA6B,journalArticle,2021,"Knott, Paul; Carroll, Micah; Devlin, Sam; Ciosek, Kamil; Hofmann, Katja; Dragan, A. D.; Shah, Rohin",Evaluating the Robustness of Collaborative Agents,arXiv:2101.05507 [cs],,,,http://arxiv.org/abs/2101.05507,"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",2021-01-14,2022-03-10 23:32:16,2022-03-10 23:32:16,2022-03-10 23:32:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.05507,,/Users/jacquesthibodeau/Zotero/storage/YXLZ9BE4/Knott et al. - 2021 - Evaluating the Robustness of Collaborative Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/6QGHNQAW/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XTDQMV5,journalArticle,2020,"Fischer, Ian; Alemi, Alexander A.",CEB Improves Model Robustness,Entropy,,1099-4300,10.3390/e22101081,http://arxiv.org/abs/2002.05380,"We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.",2020-09-25,2022-03-10 23:32:21,2022-03-10 23:32:21,2022-03-10 23:32:21,1081,,10,22,,Entropy,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05380,,/Users/jacquesthibodeau/Zotero/storage/5G7CCSFQ/Fischer and Alemi - 2020 - CEB Improves Model Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/H8B8ED27/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KPQ9HIXG,journalArticle,2020,"Fischer, Ian",The Conditional Entropy Bottleneck,"arXiv:2002.05379 [cs, stat]",,,,http://arxiv.org/abs/2002.05379,"Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.",2020-02-13,2022-03-10 23:32:24,2022-03-10 23:32:24,2022-03-10 23:32:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05379,,/Users/jacquesthibodeau/Zotero/storage/JHTX5VQV/Fischer - 2020 - The Conditional Entropy Bottleneck.pdf; /Users/jacquesthibodeau/Zotero/storage/PD5DB5JZ/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NY834NMT,journalArticle,2020,"Lohn, Andrew J.",Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,"arXiv:2009.00802 [cs, stat]",,,,http://arxiv.org/abs/2009.00802,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.",2020-09-01,2022-03-10 23:32:26,2022-03-10 23:32:26,2022-03-10 23:32:26,,,,,,,Estimating the Brittleness of AI,,,,,,,,,,,,arXiv.org,,arXiv: 2009.00802,,/Users/jacquesthibodeau/Zotero/storage/TE6LIEY2/Lohn - 2020 - Estimating the Brittleness of AI Safety Integrity.pdf; /Users/jacquesthibodeau/Zotero/storage/T4F57XBG/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computers and Society; Computer Science - Machine Learning; Computer Science - Software Engineering; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HMWHWPUK,journalArticle,2020,"D'Amour, Alexander; Heller, Katherine; Moldovan, Dan; Adlam, Ben; Alipanahi, Babak; Beutel, Alex; Chen, Christina; Deaton, Jonathan; Eisenstein, Jacob; Hoffman, Matthew D.; Hormozdiari, Farhad; Houlsby, Neil; Hou, Shaobo; Jerfel, Ghassen; Karthikesalingam, Alan; Lucic, Mario; Ma, Yian; McLean, Cory; Mincu, Diana; Mitani, Akinori; Montanari, Andrea; Nado, Zachary; Natarajan, Vivek; Nielson, Christopher; Osborne, Thomas F.; Raman, Rajiv; Ramasamy, Kim; Sayres, Rory; Schrouff, Jessica; Seneviratne, Martin; Sequeira, Shannon; Suresh, Harini; Veitch, Victor; Vladymyrov, Max; Wang, Xuezhi; Webster, Kellie; Yadlowsky, Steve; Yun, Taedong; Zhai, Xiaohua; Sculley, D.",Underspecification Presents Challenges for Credibility in Modern Machine Learning,"arXiv:2011.03395 [cs, stat]",,,,http://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020-11-24,2022-03-10 23:32:31,2022-03-10 23:32:31,2022-03-10 23:32:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.03395,,/Users/jacquesthibodeau/Zotero/storage/QIEG7P5B/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf; /Users/jacquesthibodeau/Zotero/storage/DHIPDEGK/2011.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U9PCCVX2,journalArticle,2020,"Nakkiran, Preetum; Bansal, Yamini",Distributional Generalization: A New Kind of Generalization,"arXiv:2009.08092 [cs, math, stat]",,,,http://arxiv.org/abs/2009.08092,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",2020-10-14,2022-03-10 23:32:36,2022-03-10 23:32:36,2022-03-10 23:32:36,,,,,,,Distributional Generalization,,,,,,,,,,,,arXiv.org,,arXiv: 2009.08092,,/Users/jacquesthibodeau/Zotero/storage/978GXKWJ/Nakkiran and Bansal - 2020 - Distributional Generalization A New Kind of Gener.pdf; /Users/jacquesthibodeau/Zotero/storage/5YCRXHJB/2009.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAAPD5JD,journalArticle,2020,"Xie, Qizhe; Luong, Minh-Thang; Hovy, Eduard; Le, Quoc V.",Self-training with Noisy Student improves ImageNet classification,"arXiv:1911.04252 [cs, stat]",,,,http://arxiv.org/abs/1911.04252,"We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.",2020-06-19,2022-03-10 23:32:45,2022-03-10 23:32:45,2022-03-10 23:32:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.04252,,/Users/jacquesthibodeau/Zotero/storage/YGGXT272/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf; /Users/jacquesthibodeau/Zotero/storage/L5L4K4XS/1911.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XR37HB9N,journalArticle,2020,"Cerezo, Sergio Hernandez; Ballester, Guillem Duran",Fractal AI: A fragile theory of intelligence,arXiv:1803.05049 [cs],,,,http://arxiv.org/abs/1803.05049,"Fractal AI is a theory for general artificial intelligence. It allows deriving new mathematical tools that constitute the foundations for a new kind of stochastic calculus, by modelling information using cellular automaton-like structures instead of smooth functions. In the repository included we are presenting a new Agent, derived from the first principles of the theory, which is capable of solving Atari games several orders of magnitude more efficiently than other similar techniques, like Monte Carlo Tree Search. The code provided shows how it is now possible to beat some of the current State of The Art benchmarks on Atari games, without previous learning and using less than 1000 samples to calculate each one of the actions when standard MCTS uses 3 Million samples. Among other things, Fractal AI makes it possible to generate a huge database of top performing examples with a very little amount of computation required, transforming Reinforcement Learning into a supervised problem. The algorithm presented is capable of solving the exploration vs exploitation dilemma on both the discrete and continuous cases, while maintaining control over any aspect of the behaviour of the Agent. From a general approach, new techniques presented here have direct applications to other areas such as Non-equilibrium thermodynamics, chemistry, quantum physics, economics, information theory, and non-linear control theory.",2020-07-30,2022-03-10 23:36:16,2022-03-10 23:36:16,2022-03-10 23:36:16,,,,,,,Fractal AI,,,,,,,,,,,,arXiv.org,,arXiv: 1803.05049,,/Users/jacquesthibodeau/Zotero/storage/UNHUJV2B/Cerezo and Ballester - 2020 - Fractal AI A fragile theory of intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/899LAPUD/1803.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GFGVSXE5,journalArticle,2018,"Yaworsky, Paul",A Model for General Intelligence,arXiv:1811.02546 [cs],,,,http://arxiv.org/abs/1811.02546,"The overarching problem in artificial intelligence (AI) is that we do not understand the intelligence process well enough to enable the development of adequate computational models. Much work has been done in AI over the years at lower levels, but a big part of what has been missing involves the high level, abstract, general nature of intelligence. We address this gap by developing a model for general intelligence. To accomplish this, we focus on three basic aspects of intelligence. First, we must realize the general order and nature of intelligence at a high level. Second, we must come to know what these realizations mean with respect to the overall intelligence process. Third, we must describe these realizations as clearly as possible. We propose a hierarchical model to help capture and exploit the order within intelligence. The underlying order involves patterns of signals that become organized, stored and activated in space and time. These patterns can be described using a simple, general hierarchy, with physical signals at the lowest level, information in the middle, and abstract signal representations at the top. This high level perspective provides a big picture that literally helps us see the intelligence process, thereby enabling fundamental realizations, a better understanding and clear descriptions of the intelligence process. The resulting model can be used to support all kinds of information processing across multiple levels of abstraction. As computer technology improves, and as cooperation increases between humans and computers, people will become more efficient and more productive in performing their information processing tasks.",2018-11-14,2022-03-10 23:36:18,2022-03-10 23:36:18,2022-03-10 23:36:18,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.02546,,/Users/jacquesthibodeau/Zotero/storage/DVUHFKUS/Yaworsky - 2018 - A Model for General Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/PE5NA37N/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QXGWDD3E,journalArticle,2018,"Deng, Fei; Ren, Jinsheng; Chen, Feng",Abstraction Learning,arXiv:1809.03956 [cs],,,,http://arxiv.org/abs/1809.03956,"There has been a gap between artificial intelligence and human intelligence. In this paper, we identify three key elements forming human intelligence, and suggest that abstraction learning combines these elements and is thus a way to bridge the gap. Prior researches in artificial intelligence either specify abstraction by human experts, or take abstraction as a qualitative explanation for the model. This paper aims to learn abstraction directly. We tackle three main challenges: representation, objective function, and learning algorithm. Specifically, we propose a partition structure that contains pre-allocated abstraction neurons; we formulate abstraction learning as a constrained optimization problem, which integrates abstraction properties; we develop a network evolution algorithm to solve this problem. This complete framework is named ONE (Optimization via Network Evolution). In our experiments on MNIST, ONE shows elementary human-like intelligence, including low energy consumption, knowledge sharing, and lifelong learning.",2018-09-11,2022-03-10 23:36:20,2022-03-10 23:36:20,2022-03-10 23:36:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03956,,/Users/jacquesthibodeau/Zotero/storage/G3G8QHL4/Deng et al. - 2018 - Abstraction Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/FQPL97DC/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z4YAFQJQ,journalArticle,2018,"Orseau, Laurent; McGill, Simon McGregor; Legg, Shane",Agents and Devices: A Relative Definition of Agency,"arXiv:1805.12387 [cs, stat]",,,,http://arxiv.org/abs/1805.12387,"According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.",2018-05-31,2022-03-10 23:36:22,2022-03-10 23:36:22,2022-03-10 23:36:22,,,,,,,Agents and Devices,,,,,,,,,,,,arXiv.org,,arXiv: 1805.12387,,/Users/jacquesthibodeau/Zotero/storage/SHCBW8Y6/Orseau et al. - 2018 - Agents and Devices A Relative Definition of Agenc.pdf; /Users/jacquesthibodeau/Zotero/storage/474DSKEW/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZP6MSHPS,journalArticle,2018,"Gal, Yarin; Smith, Lewis",Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,"arXiv:1806.00667 [cs, stat]",,,,http://arxiv.org/abs/1806.00667,"We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.",2018-06-28,2022-03-10 23:36:24,2022-03-10 23:36:24,2022-03-10 23:36:24,,,,,,,Sufficient Conditions for Idealised Models to Have No Adversarial Examples,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00667,,/Users/jacquesthibodeau/Zotero/storage/5EHLFZKI/Gal and Smith - 2018 - Sufficient Conditions for Idealised Models to Have.pdf; /Users/jacquesthibodeau/Zotero/storage/XXFSTRIK/1806.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CPZW5CXE,journalArticle,2018,"Ebrahimi, Javid; Lowd, Daniel; Dou, Dejing",On Adversarial Examples for Character-Level Neural Machine Translation,arXiv:1806.09030 [cs],,,,http://arxiv.org/abs/1806.09030,"Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.",2018-06-23,2022-03-10 23:36:27,2022-03-10 23:36:27,2022-03-10 23:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09030,,/Users/jacquesthibodeau/Zotero/storage/ZJZ2XPGZ/Ebrahimi et al. - 2018 - On Adversarial Examples for Character-Level Neural.pdf; /Users/jacquesthibodeau/Zotero/storage/MHDTZNYR/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTAJPKGK,journalArticle,2019,"Bortolussi, Luca; Sanguinetti, Guido",Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence,"arXiv:1811.03571 [cs, stat]",,,,http://arxiv.org/abs/1811.03571,"The success of modern Artificial Intelligence (AI) technologies depends critically on the ability to learn non-linear functional dependencies from large, high dimensional data sets. Despite recent high-profile successes, empirical evidence indicates that the high predictive performance is often paired with low robustness, making AI systems potentially vulnerable to adversarial attacks. In this report, we provide a simple intuitive argument suggesting that high performance and vulnerability are intrinsically coupled, and largely dependent on the geometry of typical, high-dimensional data sets. Our work highlights a major potential pitfall of modern AI systems, and suggests practical research directions to ameliorate the problem.",2019-01-24,2022-03-10 23:36:28,2022-03-10 23:36:28,2022-03-10 23:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03571,,/Users/jacquesthibodeau/Zotero/storage/FR2BWT9G/Bortolussi and Sanguinetti - 2019 - Intrinsic Geometric Vulnerability of High-Dimensio.pdf; /Users/jacquesthibodeau/Zotero/storage/U9Q6ZTWZ/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7BXPZQ5,journalArticle,2018,"Kannan, Harini; Kurakin, Alexey; Goodfellow, Ian",Adversarial Logit Pairing,"arXiv:1803.06373 [cs, stat]",,,,http://arxiv.org/abs/1803.06373,"In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.",2018-03-16,2022-03-10 23:36:29,2022-03-10 23:36:29,2022-03-10 23:36:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.06373,,/Users/jacquesthibodeau/Zotero/storage/G7EYQKZQ/Kannan et al. - 2018 - Adversarial Logit Pairing.pdf; /Users/jacquesthibodeau/Zotero/storage/KCF3UUH9/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ITT2S8WI,journalArticle,2018,"Goodfellow, Ian",Defense Against the Dark Arts: An overview of adversarial example security research and future research directions,"arXiv:1806.04169 [cs, stat]",,,,http://arxiv.org/abs/1806.04169,This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.,2018-06-11,2022-03-10 23:36:31,2022-03-10 23:36:31,2022-03-10 23:36:31,,,,,,,Defense Against the Dark Arts,,,,,,,,,,,,arXiv.org,,arXiv: 1806.04169,,/Users/jacquesthibodeau/Zotero/storage/HCT43IQT/Goodfellow - 2018 - Defense Against the Dark Arts An overview of adve.pdf; /Users/jacquesthibodeau/Zotero/storage/29E7B9CZ/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YP7EQA3G,journalArticle,2021,"Yuan, Li; Xiao, Will; Kreiman, Gabriel; Tay, Francis E. H.; Feng, Jiashi; Livingstone, Margaret S.",Adversarial images for the primate brain,"arXiv:2011.05623 [cs, eess, q-bio]",,,,http://arxiv.org/abs/2011.05623,"Convolutional neural networks (CNNs) are vulnerable to adversarial attack, the phenomenon that adding minuscule noise to an image can fool CNNs into misclassifying it. Because this noise is nearly imperceptible to human viewers, biological vision is assumed to be robust to adversarial attack. Despite this apparent difference in robustness, CNNs are currently the best models of biological vision, revealing a gap in explaining how the brain responds to adversarial images. Indeed, sensitivity to adversarial attack has not been measured for biological vision under normal conditions, nor have attack methods been specifically designed to affect biological vision. We studied the effects of adversarial attack on primate vision, measuring both monkey neuronal responses and human behavior. Adversarial images were created by modifying images from one category(such as human faces) to look like a target category(such as monkey faces), while limiting pixel value change. We tested three attack directions via several attack methods, including directly using CNN adversarial images and using a CNN-based predictive model to guide monkey visual neuron responses. We considered a wide range of image change magnitudes that covered attack success rates up to>90%. We found that adversarial images designed for CNNs were ineffective in attacking primate vision. Even when considering the best attack method, primate vision was more robust to adversarial attack than an ensemble of CNNs, requiring over 100-fold larger image change to attack successfully. The success of individual attack methods and images was correlated between monkey neurons and human behavior, but was less correlated between either and CNN categorization. Consistently, CNN-based models of neurons, when trained on natural images, did not generalize to explain neuronal responses to adversarial images.",2021-12-21,2022-03-10 23:36:34,2022-03-10 23:36:34,2022-03-10 23:36:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.05623,,/Users/jacquesthibodeau/Zotero/storage/YWZBQ374/Yuan et al. - 2021 - Adversarial images for the primate brain.pdf; /Users/jacquesthibodeau/Zotero/storage/G3GP9NLA/2011.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing; Electrical Engineering and Systems Science - Image and Video Processing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HEYUPS54,journalArticle,2018,"Elsayed, Gamaleldin F.; Goodfellow, Ian; Sohl-Dickstein, Jascha",Adversarial Reprogramming of Neural Networks,"arXiv:1806.11146 [cs, stat]",,,,http://arxiv.org/abs/1806.11146,"Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.",2018-11-29,2022-03-10 23:36:37,2022-03-10 23:36:37,2022-03-10 23:36:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.11146,,/Users/jacquesthibodeau/Zotero/storage/JSXYXNMB/Elsayed et al. - 2018 - Adversarial Reprogramming of Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/ELFN2RIC/1806.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3CBTR2R,journalArticle,2018,"Engstrom, Logan; Ilyas, Andrew; Athalye, Anish",Evaluating and Understanding the Robustness of Adversarial Logit Pairing,"arXiv:1807.10272 [cs, stat]",,,,http://arxiv.org/abs/1807.10272,"We evaluate the robustness of Adversarial Logit Pairing, a recently proposed defense against adversarial examples. We find that a network trained with Adversarial Logit Pairing achieves 0.6% accuracy in the threat model in which the defense is considered. We provide a brief overview of the defense and the threat models/claims considered, as well as a discussion of the methodology and results of our attack, which may offer insights into the reasons underlying the vulnerability of ALP to adversarial attack.",2018-11-23,2022-03-10 23:36:38,2022-03-10 23:36:38,2022-03-10 23:36:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.10272,,/Users/jacquesthibodeau/Zotero/storage/HFYZ4XE6/Engstrom et al. - 2018 - Evaluating and Understanding the Robustness of Adv.pdf; /Users/jacquesthibodeau/Zotero/storage/HAVM33IR/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AFKVZASW,journalArticle,2018,"Lamb, Alex; Binas, Jonathan; Goyal, Anirudh; Serdyuk, Dmitriy; Subramanian, Sandeep; Mitliagkas, Ioannis; Bengio, Yoshua",Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations,"arXiv:1804.02485 [cs, stat]",,,,http://arxiv.org/abs/1804.02485,"Deep networks have achieved impressive results across a variety of important tasks. However a known weakness is a failure to perform well when evaluated on data which differ from the training distribution, even if these differences are very small, as is the case with adversarial examples. We propose Fortified Networks, a simple transformation of existing networks, which fortifies the hidden layers in a deep network by identifying when the hidden states are off of the data manifold, and maps these hidden states back to parts of the data manifold where the network performs well. Our principal contribution is to show that fortifying these hidden states improves the robustness of deep networks and our experiments (i) demonstrate improved robustness to standard adversarial attacks in both black-box and white-box threat models; (ii) suggest that our improvements are not primarily due to the gradient masking problem and (iii) show the advantage of doing this fortification in the hidden layers instead of the input space.",2018-04-06,2022-03-10 23:36:40,2022-03-10 23:36:40,2022-03-10 23:36:40,,,,,,,Fortified Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1804.02485,,/Users/jacquesthibodeau/Zotero/storage/93KU2MHU/Lamb et al. - 2018 - Fortified Networks Improving the Robustness of De.pdf; /Users/jacquesthibodeau/Zotero/storage/ULASFUXQ/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5KM7MWAP,journalArticle,2018,"Moosavi-Dezfooli, Seyed-Mohsen; Fawzi, Alhussein; Uesato, Jonathan; Frossard, Pascal","Robustness via curvature regularization, and vice versa","arXiv:1811.09716 [cs, stat]",,,,http://arxiv.org/abs/1811.09716,"State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.",2018-11-23,2022-03-10 23:36:44,2022-03-10 23:36:44,2022-03-10 23:36:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09716,,"/Users/jacquesthibodeau/Zotero/storage/PS4I6B3R/Moosavi-Dezfooli et al. - 2018 - Robustness via curvature regularization, and vice .pdf; /Users/jacquesthibodeau/Zotero/storage/93M52PK5/1811.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q6KXS2K9,journalArticle,2019,"Simon-Gabriel, Carl-Johann; Ollivier, Yann; Bottou, Léon; Schölkopf, Bernhard; Lopez-Paz, David",First-order Adversarial Vulnerability of Neural Networks and Input Dimension,"arXiv:1802.01421 [cs, stat]",,,,http://arxiv.org/abs/1802.01421,"Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the $\ell_1$-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.",2019-06-16,2022-03-10 23:36:46,2022-03-10 23:36:46,2022-03-10 23:36:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.01421,,/Users/jacquesthibodeau/Zotero/storage/RK6SY8BM/Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/TCV6Y6DE/1802.html,,,68T45; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QDAX6VWK,journalArticle,2019,"Zhang, Hongyang; Yu, Yaodong; Jiao, Jiantao; Xing, Eric P.; Ghaoui, Laurent El; Jordan, Michael I.",Theoretically Principled Trade-off between Robustness and Accuracy,"arXiv:1901.08573 [cs, stat]",,,,http://arxiv.org/abs/1901.08573,"We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.",2019-06-24,2022-03-10 23:36:48,2022-03-10 23:36:48,2022-03-10 23:36:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.08573,,/Users/jacquesthibodeau/Zotero/storage/6JKPK8NZ/Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf; /Users/jacquesthibodeau/Zotero/storage/YVV2VMZG/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2D7FVE8A,journalArticle,2019,"Jordan, Matt; Manoj, Naren; Goel, Surbhi; Dimakis, Alexandros G.",Quantifying Perceptual Distortion of Adversarial Examples,"arXiv:1902.08265 [cs, stat]",,,,http://arxiv.org/abs/1902.08265,"Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks. To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.",2019-02-21,2022-03-10 23:36:50,2022-03-10 23:36:50,2022-03-10 23:36:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.08265,,/Users/jacquesthibodeau/Zotero/storage/WAGJS4SD/Jordan et al. - 2019 - Quantifying Perceptual Distortion of Adversarial E.pdf; /Users/jacquesthibodeau/Zotero/storage/Q2SFVJI7/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMWMGE28,journalArticle,2019,"Finlay, Chris; Pooladian, Aram-Alexandre; Oberman, Adam M.",The LogBarrier adversarial attack: making effective use of decision boundary information,"arXiv:1903.10396 [cs, stat]",,,,http://arxiv.org/abs/1903.10396,"Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the established logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks.",2019-03-25,2022-03-10 23:36:52,2022-03-10 23:36:52,2022-03-10 23:36:51,,,,,,,The LogBarrier adversarial attack,,,,,,,,,,,,arXiv.org,,arXiv: 1903.10396,,/Users/jacquesthibodeau/Zotero/storage/XC7DFM7F/Finlay et al. - 2019 - The LogBarrier adversarial attack making effectiv.pdf; /Users/jacquesthibodeau/Zotero/storage/R7QTYS5A/1903.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIHKAN3K,journalArticle,2019,"Kettunen, Markus; Härkönen, Erik; Lehtinen, Jaakko",E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles,arXiv:1906.03973 [cs],,,,http://arxiv.org/abs/1906.03973,"It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of ""perceptual convexity"" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.",2019-06-11,2022-03-10 23:36:57,2022-03-10 23:36:57,2022-03-10 23:36:54,,,,,,,E-LPIPS,,,,,,,,,,,,arXiv.org,,arXiv: 1906.03973,,/Users/jacquesthibodeau/Zotero/storage/KP7GYK23/Kettunen et al. - 2019 - E-LPIPS Robust Perceptual Image Similarity via Ra.pdf; /Users/jacquesthibodeau/Zotero/storage/L3GBFZTY/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KDNV35KT,journalArticle,2018,"Gilmer, Justin; Adams, Ryan P.; Goodfellow, Ian; Andersen, David; Dahl, George E.",Motivating the Rules of the Game for Adversarial Example Research,"arXiv:1807.06732 [cs, stat]",,,,http://arxiv.org/abs/1807.06732,"Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",2018-07-19,2022-03-10 23:36:59,2022-03-10 23:36:59,2022-03-10 23:36:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06732,,/Users/jacquesthibodeau/Zotero/storage/2WV4PEAD/Gilmer et al. - 2018 - Motivating the Rules of the Game for Adversarial E.pdf; /Users/jacquesthibodeau/Zotero/storage/WN72FWE5/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NKLEKPLA,journalArticle,2021,"Gleave, Adam; Dennis, Michael; Wild, Cody; Kant, Neel; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,"arXiv:1905.10615 [cs, stat]",,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2021-01-17,2022-03-10 23:37:03,2022-03-10 23:37:03,2022-03-10 23:36:56,,,,,,,Adversarial Policies,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10615,,/Users/jacquesthibodeau/Zotero/storage/EPI9IHI6/Gleave et al. - 2021 - Adversarial Policies Attacking Deep Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/WF7YJTHQ/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FKUXY23K,journalArticle,2018,"Schott, Lukas; Rauber, Jonas; Bethge, Matthias; Brendel, Wieland",Towards the first adversarially robust neural network model on MNIST,arXiv:1805.09190 [cs],,,,http://arxiv.org/abs/1805.09190,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.",2018-09-20,2022-03-10 23:37:05,2022-03-10 23:37:05,2022-03-10 23:37:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.09190,,/Users/jacquesthibodeau/Zotero/storage/TMDQ4BX8/Schott et al. - 2018 - Towards the first adversarially robust neural netw.pdf; /Users/jacquesthibodeau/Zotero/storage/PA5BMM7I/1805.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZ2PRLZN,journalArticle,2018,"Charles, Zachary; Rosenberg, Harrison; Papailiopoulos, Dimitris",A Geometric Perspective on the Transferability of Adversarial Directions,"arXiv:1811.03531 [cs, stat]",,,,http://arxiv.org/abs/1811.03531,"State-of-the-art machine learning models frequently misclassify inputs that have been perturbed in an adversarial manner. Adversarial perturbations generated for a given input and a specific classifier often seem to be effective on other inputs and even different classifiers. In other words, adversarial perturbations seem to transfer between different inputs, models, and even different neural network architectures. In this work, we show that in the context of linear classifiers and two-layer ReLU networks, there provably exist directions that give rise to adversarial perturbations for many classifiers and data points simultaneously. We show that these ""transferable adversarial directions"" are guaranteed to exist for linear separators of a given set, and will exist with high probability for linear classifiers trained on independent sets drawn from the same distribution. We extend our results to large classes of two-layer ReLU networks. We further show that adversarial directions for ReLU networks transfer to linear classifiers while the reverse need not hold, suggesting that adversarial perturbations for more complex models are more likely to transfer to other classifiers. We validate our findings empirically, even for deeper ReLU networks.",2018-11-08,2022-03-10 23:37:05,2022-03-10 23:37:05,2022-03-10 23:37:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03531,,/Users/jacquesthibodeau/Zotero/storage/QU7KMIED/Charles et al. - 2018 - A Geometric Perspective on the Transferability of .pdf; /Users/jacquesthibodeau/Zotero/storage/WFWVUZV3/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKPF69JY,journalArticle,2020,"Kang, Daniel; Sun, Yi; Hendrycks, Dan; Brown, Tom; Steinhardt, Jacob",Testing Robustness Against Unforeseen Adversaries,"arXiv:1908.08016 [cs, stat]",,,,http://arxiv.org/abs/1908.08016,"Most existing adversarial defenses only measure robustness to L_p adversarial attacks. Not only are adversaries unlikely to exclusively create small L_p perturbations, adversaries are unlikely to remain fixed. Adversaries adapt and evolve their attacks; hence adversarial defenses must be robust to a broad range of unforeseen attacks. We address this discrepancy between research and reality by proposing a new evaluation framework called ImageNet-UA. Our framework enables the research community to test ImageNet model robustness against attacks not encountered during training. To create ImageNet-UA's diverse attack suite, we introduce a total of four novel adversarial attacks. We also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf robustness assessments give a narrow account of model robustness. By evaluating current defenses with ImageNet-UA, we find they provide little robustness to unforeseen attacks. We hope the greater variety and realism of ImageNet-UA enables development of more robust defenses which can generalize beyond attacks seen during training.",2020-06-09,2022-03-10 23:37:13,2022-03-10 23:37:13,2022-03-10 23:37:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.08016,,/Users/jacquesthibodeau/Zotero/storage/APXQY9W3/Kang et al. - 2020 - Testing Robustness Against Unforeseen Adversaries.pdf; /Users/jacquesthibodeau/Zotero/storage/LTQEEULC/1908.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SYMA2IBW,journalArticle,2021,"Hendrycks, Dan; Zhao, Kevin; Basart, Steven; Steinhardt, Jacob; Song, Dawn",Natural Adversarial Examples,"arXiv:1907.07174 [cs, stat]",,,,http://arxiv.org/abs/1907.07174,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",2021-03-04,2022-03-10 23:37:17,2022-03-10 23:37:17,2022-03-10 23:37:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.07174,,/Users/jacquesthibodeau/Zotero/storage/QZ3CNMKJ/Hendrycks et al. - 2021 - Natural Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/3SR2KER9/1907.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DNI2UC9Q,journalArticle,2019,"Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Ilyas, Andrew; Engstrom, Logan; Madry, Aleksander",Image Synthesis with a Single (Robust) Classifier,"arXiv:1906.09453 [cs, stat]",,,,http://arxiv.org/abs/1906.09453,"We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.",2019-08-08,2022-03-10 23:37:25,2022-03-10 23:37:25,2022-03-10 23:37:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.09453,,/Users/jacquesthibodeau/Zotero/storage/9US3I935/Santurkar et al. - 2019 - Image Synthesis with a Single (Robust) Classifier.pdf; /Users/jacquesthibodeau/Zotero/storage/9G7JF2MI/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVP3C2Y4,journalArticle,2019,"Engstrom, Logan; Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Madry, Aleksander",Adversarial Robustness as a Prior for Learned Representations,"arXiv:1906.00945 [cs, stat]",,,,http://arxiv.org/abs/1906.00945,"An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",2019-09-27,2022-03-10 23:37:32,2022-03-10 23:37:32,2022-03-10 23:37:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.00945,,/Users/jacquesthibodeau/Zotero/storage/ASWRIKYU/Engstrom et al. - 2019 - Adversarial Robustness as a Prior for Learned Repr.pdf; /Users/jacquesthibodeau/Zotero/storage/6EYP38HZ/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I8LDKNL5,journalArticle,2019,"Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Engstrom, Logan; Tran, Brandon; Madry, Aleksander","Adversarial Examples Are Not Bugs, They Are Features","arXiv:1905.02175 [cs, stat]",,,,http://arxiv.org/abs/1905.02175,"Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",2019-08-12,2022-03-10 23:37:41,2022-03-10 23:37:41,2022-03-10 23:37:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.02175,,"/Users/jacquesthibodeau/Zotero/storage/7FU5N6AS/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf; /Users/jacquesthibodeau/Zotero/storage/AUGF4UH4/1905.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9XBGVE2J,journalArticle,2021,"Jones, Andy L.",Scaling Scaling Laws with Board Games,arXiv:2104.03113 [cs],,,,http://arxiv.org/abs/2104.03113,"The largest experiments in machine learning now require resources far beyond the budget of all but a few institutions. Fortunately, it has recently been shown that the results of these huge experiments can often be extrapolated from the results of a sequence of far smaller, cheaper experiments. In this work, we show that not only can the extrapolation be done based on the size of the model, but on the size of the problem as well. By conducting a sequence of experiments using AlphaZero and Hex, we show that the performance achievable with a fixed amount of compute degrades predictably as the game gets larger and harder. Along with our main result, we further show that the test-time and train-time compute available to an agent can be traded off while maintaining performance.",2021-04-15,2022-03-10 23:41:22,2022-03-10 23:41:22,2022-03-10 23:41:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.03113,,/Users/jacquesthibodeau/Zotero/storage/ED3ZQ86B/Jones - 2021 - Scaling Scaling Laws with Board Games.pdf; /Users/jacquesthibodeau/Zotero/storage/G33J7KPL/2104.html,,,Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XTL7FQYV,journalArticle,2021,"Aghajanyan, Armen; Gupta, Anchit; Shrivastava, Akshat; Chen, Xilun; Zettlemoyer, Luke; Gupta, Sonal",Muppet: Massive Multi-task Representations with Pre-Finetuning,arXiv:2101.11038 [cs],,,,http://arxiv.org/abs/2101.11038,"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.~RoBERTa) and generation models (e.g.~BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.",2021-01-26,2022-03-10 23:41:29,2022-03-10 23:41:29,2022-03-10 23:41:28,,,,,,,Muppet,,,,,,,,,,,,arXiv.org,,arXiv: 2101.11038,,/Users/jacquesthibodeau/Zotero/storage/RSPFUDDT/Aghajanyan et al. - 2021 - Muppet Massive Multi-task Representations with Pr.pdf; /Users/jacquesthibodeau/Zotero/storage/6JXYC29H/2101.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MHTZMIFW,journalArticle,2021,"Wu, Zonghan; Pan, Shirui; Chen, Fengwen; Long, Guodong; Zhang, Chengqi; Yu, Philip S.",A Comprehensive Survey on Graph Neural Networks,IEEE Transactions on Neural Networks and Learning Systems,,"2162-237X, 2162-2388",10.1109/TNNLS.2020.2978386,http://arxiv.org/abs/1901.00596,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",2021-01,2022-03-10 23:41:31,2022-03-10 23:41:31,2022-03-10 23:41:31,4-24,,1,32,,IEEE Trans. Neural Netw. Learning Syst.,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.00596,,/Users/jacquesthibodeau/Zotero/storage/QPJLPMR2/Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/R2448YMR/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2PGPHFH,journalArticle,2020,"Zhou, Hattie; Lan, Janice; Liu, Rosanne; Yosinski, Jason","Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","arXiv:1905.01067 [cs, stat]",,,,http://arxiv.org/abs/1905.01067,"The recent ""Lottery Ticket Hypothesis"" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).",2020-03-03,2022-03-10 23:42:04,2022-03-10 23:42:04,2022-03-10 23:42:04,,,,,,,Deconstructing Lottery Tickets,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01067,,"/Users/jacquesthibodeau/Zotero/storage/KNZQ86PM/Zhou et al. - 2020 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf; /Users/jacquesthibodeau/Zotero/storage/6XAB8N4C/1905.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQPSYRCT,journalArticle,2019,"Wang, Po-Wei; Donti, Priya L.; Wilder, Bryan; Kolter, Zico",SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,"arXiv:1905.12149 [cs, stat]",,,,http://arxiv.org/abs/1905.12149,"Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a ""visual Sudok"" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.",2019-05-28,2022-03-10 23:42:10,2022-03-10 23:42:10,2022-03-10 23:42:10,,,,,,,SATNet,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12149,,/Users/jacquesthibodeau/Zotero/storage/3K55GX52/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoni.pdf; /Users/jacquesthibodeau/Zotero/storage/UWD3W7BW/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3W62CSPY,journalArticle,2019,"Nakkiran, Preetum",More Data Can Hurt for Linear Regression: Sample-wise Double Descent,"arXiv:1912.07242 [cs, math, stat]",,,,http://arxiv.org/abs/1912.07242,"In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples. In other words, more data actually hurts the estimator. This behavior is implicit in a recent line of theoretical works analyzing ""double-descent"" phenomenon in linear models. In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates. In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.",2019-12-16,2022-03-10 23:42:14,2022-03-10 23:42:14,2022-03-10 23:42:14,,,,,,,More Data Can Hurt for Linear Regression,,,,,,,,,,,,arXiv.org,,arXiv: 1912.07242,,/Users/jacquesthibodeau/Zotero/storage/6LRJ62CD/Nakkiran - 2019 - More Data Can Hurt for Linear Regression Sample-w.pdf; /Users/jacquesthibodeau/Zotero/storage/3UGH6U85/1912.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8NXGZC5P,journalArticle,2020,"Yang, Zitong; Yu, Yaodong; You, Chong; Steinhardt, Jacob; Ma, Yi",Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,"arXiv:2002.11328 [cs, stat]",,,,http://arxiv.org/abs/2002.11328,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",2020-12-07,2022-03-10 23:42:22,2022-03-10 23:42:22,2022-03-10 23:42:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11328,,/Users/jacquesthibodeau/Zotero/storage/L6J8CFUW/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf; /Users/jacquesthibodeau/Zotero/storage/VQB52LKM/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SN2C3E8H,journalArticle,2021,"Lu, Kevin; Grover, Aditya; Abbeel, Pieter; Mordatch, Igor",Pretrained Transformers as Universal Computation Engines,arXiv:2103.05247 [cs],,,,http://arxiv.org/abs/2103.05247,"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",2021-06-30,2022-03-10 23:42:42,2022-03-10 23:42:42,2022-03-10 23:42:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.05247,,/Users/jacquesthibodeau/Zotero/storage/RRW63G5Z/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf; /Users/jacquesthibodeau/Zotero/storage/K4M9R59B/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDPBRRCK,journalArticle,2021,"Perez, Ethan; Kiela, Douwe; Cho, Kyunghyun",True Few-Shot Learning with Language Models,"arXiv:2105.11447 [cs, stat]",,,,http://arxiv.org/abs/2105.11447,"Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (""prompts""). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",2021-05-24,2022-03-10 23:42:48,2022-03-10 23:42:48,2022-03-10 23:42:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.11447,,/Users/jacquesthibodeau/Zotero/storage/2SNVCTPG/Perez et al. - 2021 - True Few-Shot Learning with Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/GILNT25Q/2105.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5QMD5MDU,journalArticle,2021,"Austin, Jacob; Odena, Augustus; Nye, Maxwell; Bosma, Maarten; Michalewski, Henryk; Dohan, David; Jiang, Ellen; Cai, Carrie; Terry, Michael; Le, Quoc; Sutton, Charles",Program Synthesis with Large Language Models,arXiv:2108.07732 [cs],,,,http://arxiv.org/abs/2108.07732,"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",2021-08-15,2022-03-10 23:42:57,2022-03-10 23:42:57,2022-03-10 23:42:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.07732,,/Users/jacquesthibodeau/Zotero/storage/HT3SYX4A/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/EXGXSEVP/2108.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3MG4RYUH,journalArticle,2020,"Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V.",Towards a Human-like Open-Domain Chatbot,"arXiv:2001.09977 [cs, stat]",,,,http://arxiv.org/abs/2001.09977,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2020-02-27,2022-03-10 23:43:03,2022-03-10 23:43:03,2022-03-10 23:43:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09977,,/Users/jacquesthibodeau/Zotero/storage/9N7RF3HY/Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf; /Users/jacquesthibodeau/Zotero/storage/IUABBQL7/2001.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PRNNR53J,journalArticle,2019,"Rosenfeld, Jonathan S.; Rosenfeld, Amir; Belinkov, Yonatan; Shavit, Nir",A Constructive Prediction of the Generalization Error Across Scales,"arXiv:1909.12673 [cs, stat]",,,,http://arxiv.org/abs/1909.12673,"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.",2019-12-20,2022-03-10 23:43:10,2022-03-10 23:43:10,2022-03-10 23:43:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.12673,,/Users/jacquesthibodeau/Zotero/storage/FU3S8BXC/Rosenfeld et al. - 2019 - A Constructive Prediction of the Generalization Er.pdf; /Users/jacquesthibodeau/Zotero/storage/G2D29J3X/1909.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SIN4RSU7,journalArticle,2020,"Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario",Scaling Laws for Neural Language Models,"arXiv:2001.08361 [cs, stat]",,,,http://arxiv.org/abs/2001.08361,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",2020-01-22,2022-03-10 23:43:13,2022-03-11 1:38:09,2022-03-10 23:43:13,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.08361,,/Users/jacquesthibodeau/Zotero/storage/ATE2YTHH/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/DPVDJBAM/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/P64K2X38/2001.html; /Users/jacquesthibodeau/Zotero/storage/WPPV6WBH/2001.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77X22GNW,journalArticle,2018,"Chauvet, Jean-Marie",The 30-Year Cycle In The AI Debate,arXiv:1810.04053 [cs],,,,http://arxiv.org/abs/1810.04053,"In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.",2018-10-08,2022-03-10 23:43:17,2022-03-10 23:43:17,2022-03-10 23:43:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04053,,/Users/jacquesthibodeau/Zotero/storage/HTASGEW6/Chauvet - 2018 - The 30-Year Cycle In The AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/9W3UJPQK/1810.html,,,Computer Science - Artificial Intelligence; I.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EM6S3KBF,journalArticle,2022,"Klinger, Joel; Mateos-Garcia, Juan; Stathoulopoulos, Konstantinos",A narrowing of AI research?,arXiv:2009.10385 [cs],,,,http://arxiv.org/abs/2009.10385,"The arrival of deep learning techniques able to infer patterns from large datasets has dramatically improved the performance of Artificial Intelligence (AI) systems. Deep learning's rapid development and adoption, in great part led by large technology companies, has however created concerns about a premature narrowing in the technological trajectory of AI research despite its weaknesses, which include lack of robustness, high environmental costs, and potentially unfair outcomes. We seek to improve the evidence base with a semantic analysis of AI research in arXiv, a popular pre-prints database. We study the evolution of the thematic diversity of AI research, compare the thematic diversity of AI research in academia and the private sector and measure the influence of private companies in AI research through the citations they receive and their collaborations with other institutions. Our results suggest that diversity in AI research has stagnated in recent years, and that AI research involving the private sector tends to be less diverse and more influential than research in academia. We also find that private sector AI researchers tend to specialise in data-hungry and computationally intensive deep learning methods at the expense of research involving other AI methods, research that considers the societal and ethical implications of AI, and applications in sectors like health. Our results provide a rationale for policy action to prevent a premature narrowing of AI research that could constrain its societal benefits, but we note the informational, incentive and scale hurdles standing in the way of such interventions.",2022-01-11,2022-03-10 23:43:21,2022-03-10 23:43:21,2022-03-10 23:43:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.10385,,/Users/jacquesthibodeau/Zotero/storage/S64T9AAN/Klinger et al. - 2022 - A narrowing of AI research.pdf; /Users/jacquesthibodeau/Zotero/storage/EDQUJXVG/2009.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHSHAGTW,journalArticle,2019,"Carbune, Victor; Coppey, Thierry; Daryin, Alexander; Deselaers, Thomas; Sarda, Nikhil; Yagnik, Jay",SmartChoices: Hybridizing Programming and Machine Learning,"arXiv:1810.00619 [cs, stat]",,,,http://arxiv.org/abs/1810.00619,"We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages which we see as one way to lower the entrance cost to applying ML to problems in new domains. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by providing a 3-call API which we expose through an object called SmartChoice. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algorithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems, our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse -- allowing for a safe deployment in critical applications in real life.",2019-06-13,2022-03-10 23:43:26,2022-03-10 23:43:26,2022-03-10 23:43:25,,,,,,,SmartChoices,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00619,,/Users/jacquesthibodeau/Zotero/storage/GGXH2H3R/Carbune et al. - 2019 - SmartChoices Hybridizing Programming and Machine .pdf; /Users/jacquesthibodeau/Zotero/storage/B67S53QR/1810.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NMIUFE6,journalArticle,2019,"Haldar, Malay; Abdool, Mustafa; Ramanathan, Prashant; Xu, Tao; Yang, Shulin; Duan, Huizhong; Zhang, Qing; Barrow-Williams, Nick; Turnbull, Bradley C.; Collins, Brendan M.; Legrand, Thomas",Applying Deep Learning To Airbnb Search,Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,,,10.1145/3292500.3330658,http://arxiv.org/abs/1810.09591,"The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!",2019-07-25,2022-03-10 23:43:31,2022-03-10 23:43:31,2022-03-10 23:43:31,1927-1935,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.09591,,/Users/jacquesthibodeau/Zotero/storage/CLQZCJRI/Haldar et al. - 2019 - Applying Deep Learning To Airbnb Search.pdf; /Users/jacquesthibodeau/Zotero/storage/392UNZHS/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Information Retrieval; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2829PFQA,journalArticle,2019,"Dulac-Arnold, Gabriel; Mankowitz, Daniel; Hester, Todd",Challenges of Real-World Reinforcement Learning,"arXiv:1904.12901 [cs, stat]",,,,http://arxiv.org/abs/1904.12901,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",2019-04-29,2022-03-10 23:43:48,2022-03-10 23:43:48,2022-03-10 23:43:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.12901,,/Users/jacquesthibodeau/Zotero/storage/2HBDXPXL/Dulac-Arnold et al. - 2019 - Challenges of Real-World Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/CVKGCPKQ/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIJW3MYL,journalArticle,2019,"Rolnick, David; Donti, Priya L.; Kaack, Lynn H.; Kochanski, Kelly; Lacoste, Alexandre; Sankaran, Kris; Ross, Andrew Slavin; Milojevic-Dupont, Nikola; Jaques, Natasha; Waldman-Brown, Anna; Luccioni, Alexandra; Maharaj, Tegan; Sherwin, Evan D.; Mukkavilli, S. Karthik; Kording, Konrad P.; Gomes, Carla; Ng, Andrew Y.; Hassabis, Demis; Platt, John C.; Creutzig, Felix; Chayes, Jennifer; Bengio, Yoshua",Tackling Climate Change with Machine Learning,"arXiv:1906.05433 [cs, stat]",,,,http://arxiv.org/abs/1906.05433,"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.",2019-11-05,2022-03-10 23:43:50,2022-03-10 23:43:50,2022-03-10 23:43:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.05433,,/Users/jacquesthibodeau/Zotero/storage/HVKSMKWY/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/464T74WB/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PA59LU7D,journalArticle,2019,"Kott, Alexander; Théron, Paul; Drašar, Martin; Dushku, Edlira; LeBlanc, Benoît; Losiewicz, Paul; Guarino, Alessandro; Mancini, Luigi; Panico, Agostino; Pihelgas, Mauno; Rzadca, Krzysztof",Autonomous Intelligent Cyber-defense Agent (AICA) Reference Architecture. Release 2.0,arXiv:1803.10664 [cs],,,,http://arxiv.org/abs/1803.10664,"This report - a major revision of its previous release - describes a reference architecture for intelligent software agents performing active, largely autonomous cyber-defense actions on military networks of computing and communicating devices. The report is produced by the North Atlantic Treaty Organization (NATO) Research Task Group (RTG) IST-152 ""Intelligent Autonomous Agents for Cyber Defense and Resilience"". In a conflict with a technically sophisticated adversary, NATO military tactical networks will operate in a heavily contested battlefield. Enemy software cyber agents - malware - will infiltrate friendly networks and attack friendly command, control, communications, computers, intelligence, surveillance, and reconnaissance and computerized weapon systems. To fight them, NATO needs artificial cyber hunters - intelligent, autonomous, mobile agents specialized in active cyber defense. With this in mind, in 2016, NATO initiated RTG IST-152. Its objective has been to help accelerate the development and transition to practice of such software agents by producing a reference architecture and technical roadmap. This report presents the concept and architecture of an Autonomous Intelligent Cyber-defense Agent (AICA). We describe the rationale of the AICA concept, explain the methodology and purpose that drive the definition of the AICA Reference Architecture, and review some of the main features and challenges of AICAs.",2019-09-18,2022-03-10 23:43:53,2022-03-10 23:43:53,2022-03-10 23:43:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.10664,,/Users/jacquesthibodeau/Zotero/storage/CSTKWQ8A/Kott et al. - 2019 - Autonomous Intelligent Cyber-defense Agent (AICA) .pdf; /Users/jacquesthibodeau/Zotero/storage/VDUGIA2E/1803.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5SQCEQGA,journalArticle,2019,"Ovadya, Aviv; Whittlestone, Jess",Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning,arXiv:1907.11274 [cs],,,,http://arxiv.org/abs/1907.11274,"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.",2019-07-28,2022-03-10 23:44:01,2022-03-10 23:44:01,2022-03-10 23:44:01,,,,,,,Reducing malicious use of synthetic media research,,,,,,,,,,,,arXiv.org,,arXiv: 1907.11274,,/Users/jacquesthibodeau/Zotero/storage/PJEWY7UN/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/BPNB88MY/1907.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5YUDGWXF,journalArticle,2018,"Hwang, Tim",Computational Power and the Social Impact of Artificial Intelligence,arXiv:1803.08971 [cs],,,,http://arxiv.org/abs/1803.08971,"Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.",2018-03-23,2022-03-10 23:44:05,2022-03-10 23:44:05,2022-03-10 23:44:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.08971,,/Users/jacquesthibodeau/Zotero/storage/A9RPSM6T/Hwang - 2018 - Computational Power and the Social Impact of Artif.pdf; /Users/jacquesthibodeau/Zotero/storage/8W66HSA9/1803.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82GP3J4H,journalArticle,2020,"Zellers, Rowan; Holtzman, Ari; Rashkin, Hannah; Bisk, Yonatan; Farhadi, Ali; Roesner, Franziska; Choi, Yejin",Defending Against Neural Fake News,arXiv:1905.12616 [cs],,,,http://arxiv.org/abs/1905.12616,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",2020-12-11,2022-03-10 23:44:16,2022-03-10 23:44:16,2022-03-10 23:44:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12616,,/Users/jacquesthibodeau/Zotero/storage/B6J63LEU/Zellers et al. - 2020 - Defending Against Neural Fake News.pdf; /Users/jacquesthibodeau/Zotero/storage/U7HZRI7Y/1905.html,,,Computer Science - Computation and Language; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YTTIVTBF,journalArticle,2020,"Tucker, Aaron D.; Anderljung, Markus; Dafoe, Allan",Social and Governance Implications of Improved Data Efficiency,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375863,http://arxiv.org/abs/2001.05068,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the ""AI production function"", will be key to understanding the development of the AI industry and its societal impacts.",2020-02-07,2022-03-10 23:44:19,2022-03-10 23:44:19,2022-03-10 23:44:19,378-384,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.05068,,/Users/jacquesthibodeau/Zotero/storage/D82TK4WV/Tucker et al. - 2020 - Social and Governance Implications of Improved Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/XAXBBTL2/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDG2FD5K,journalArticle,2021,"Abdalla, Mohamed; Abdalla, Moustafa","The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462563,http://arxiv.org/abs/2009.13676,"As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",2021-07-21,2022-03-10 23:44:21,2022-03-10 23:44:21,2022-03-10 23:44:20,287-297,,,,,,The Grey Hoodie Project,,,,,,,,,,,,arXiv.org,,arXiv: 2009.13676,,"/Users/jacquesthibodeau/Zotero/storage/FD6UC3C3/Abdalla and Abdalla - 2021 - The Grey Hoodie Project Big Tobacco, Big Tech, an.pdf; /Users/jacquesthibodeau/Zotero/storage/YBF7XPL5/2009.html",,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NCR6MLKN,journalArticle,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,arXiv:2110.06674 [cs],,,,http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-03-10 23:44:22,2022-03-10 23:44:22,2022-03-10 23:44:21,,,,,,,Truthful AI,,,,,,,,,,,,arXiv.org,,arXiv: 2110.06674,,/Users/jacquesthibodeau/Zotero/storage/BIY2B2AL/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/RSAQUYYN/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; I.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QPPDDWEL,journalArticle,2020,"Shevlane, Toby; Dafoe, Allan",The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,arXiv:2001.00463 [cs],,,,http://arxiv.org/abs/2001.00463,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",2020-01-09,2022-03-10 23:44:22,2022-03-10 23:44:22,2022-03-10 23:44:22,,,,,,,The Offense-Defense Balance of Scientific Knowledge,,,,,,,,,,,,arXiv.org,,arXiv: 2001.00463,,/Users/jacquesthibodeau/Zotero/storage/N97PVNYU/Shevlane and Dafoe - 2020 - The Offense-Defense Balance of Scientific Knowledg.pdf; /Users/jacquesthibodeau/Zotero/storage/ZIUK4FSW/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCKNIL9X,journalArticle,2021,"Bahri, Yasaman; Dyer, Ethan; Kaplan, Jared; Lee, Jaehoon; Sharma, Utkarsh",Explaining Neural Scaling Laws,"arXiv:2102.06701 [cond-mat, stat]",,,,http://arxiv.org/abs/2102.06701,"The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.",2021-02-12,2022-03-10 23:44:33,2022-03-10 23:44:33,2022-03-10 23:44:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.06701,,/Users/jacquesthibodeau/Zotero/storage/8S68RB3V/Bahri et al. - 2021 - Explaining Neural Scaling Laws.pdf; /Users/jacquesthibodeau/Zotero/storage/JPAKSCXB/2102.html,,,Computer Science - Machine Learning; Condensed Matter - Disordered Systems and Neural Networks; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCQZU39W,journalArticle,2020,"Sharma, Utkarsh; Kaplan, Jared",A Neural Scaling Law from the Dimension of the Data Manifold,"arXiv:2004.10802 [cs, stat]",,,,http://arxiv.org/abs/2004.10802,"When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",2020-04-22,2022-03-10 23:44:35,2022-03-10 23:44:35,2022-03-10 23:44:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.10802,,/Users/jacquesthibodeau/Zotero/storage/MZSQ5X5L/Sharma and Kaplan - 2020 - A Neural Scaling Law from the Dimension of the Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/GMV8XV59/2004.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ACYS2UU4,journalArticle,2018,"Trask, Andrew; Hill, Felix; Reed, Scott; Rae, Jack; Dyer, Chris; Blunsom, Phil",Neural Arithmetic Logic Units,arXiv:1808.00508 [cs],,,,http://arxiv.org/abs/1808.00508,"Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.",2018-08-01,2022-03-10 23:46:58,2022-03-10 23:46:58,2022-03-10 23:46:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.00508,,/Users/jacquesthibodeau/Zotero/storage/7F3XLF6L/Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf; /Users/jacquesthibodeau/Zotero/storage/5W4UZ5TJ/1808.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCS66UJY,journalArticle,2019,"Ardizzone, Lynton; Kruse, Jakob; Wirkert, Sebastian; Rahner, Daniel; Pellegrini, Eric W.; Klessen, Ralf S.; Maier-Hein, Lena; Rother, Carsten; Köthe, Ullrich",Analyzing Inverse Problems with Invertible Neural Networks,"arXiv:1808.04730 [cs, stat]",,,,http://arxiv.org/abs/1808.04730,"In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Although INNs are not new, they have, so far, received little attention in literature. While classical neural networks attempt to solve the ambiguous inverse problem directly, INNs are able to learn it jointly with the well-defined forward process, using additional latent output variables to capture the information otherwise lost. Given a specific measurement and sampled latent variables, the inverse pass of the INN provides a full distribution over parameter space. We verify experimentally, on artificial data and real-world problems from astrophysics and medicine, that INNs are a powerful analysis tool to find multi-modalities in parameter space, to uncover parameter correlations, and to identify unrecoverable parameters.",2019-02-06,2022-03-10 23:47:05,2022-03-10 23:47:05,2022-03-10 23:47:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.04730,,/Users/jacquesthibodeau/Zotero/storage/2Q8DNWCN/Ardizzone et al. - 2019 - Analyzing Inverse Problems with Invertible Neural .pdf; /Users/jacquesthibodeau/Zotero/storage/N3XJWWNW/1808.html,,,68T01; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L4QAIU2V,journalArticle,2018,"Tang, Gongbo; Müller, Mathias; Rios, Annette; Sennrich, Rico",Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,arXiv:1808.08946 [cs],,,,http://arxiv.org/abs/1808.08946,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",2018-11-11,2022-03-10 23:47:08,2022-03-10 23:47:08,2022-03-10 23:47:08,,,,,,,Why Self-Attention?,,,,,,,,,,,,arXiv.org,,arXiv: 1808.08946,,/Users/jacquesthibodeau/Zotero/storage/8MZ5XB8H/Tang et al. - 2018 - Why Self-Attention A Targeted Evaluation of Neura.pdf; /Users/jacquesthibodeau/Zotero/storage/HR28YSX9/1808.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MDWBVRU3,journalArticle,2020,"Anil, Rohan; Pereyra, Gabriel; Passos, Alexandre; Ormandi, Robert; Dahl, George E.; Hinton, Geoffrey E.",Large scale distributed neural network training through online distillation,"arXiv:1804.03235 [cs, stat]",,,,http://arxiv.org/abs/1804.03235,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",2020-08-20,2022-03-10 23:47:10,2022-03-10 23:47:10,2022-03-10 23:47:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.03235,,/Users/jacquesthibodeau/Zotero/storage/JY45HHWP/Anil et al. - 2020 - Large scale distributed neural network training th.pdf; /Users/jacquesthibodeau/Zotero/storage/UEP8DWDL/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RVNWRTNL,journalArticle,2018,"Hamrick, Jessica B.; Allen, Kelsey R.; Bapst, Victor; Zhu, Tina; McKee, Kevin R.; Tenenbaum, Joshua B.; Battaglia, Peter W.",Relational inductive bias for physical construction in humans and machines,"arXiv:1806.01203 [cs, stat]",,,,http://arxiv.org/abs/1806.01203,"While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a ""relational inductive bias"": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.",2018-06-04,2022-03-10 23:47:17,2022-03-10 23:47:17,2022-03-10 23:47:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01203,,/Users/jacquesthibodeau/Zotero/storage/WJA4VXIQ/Hamrick et al. - 2018 - Relational inductive bias for physical constructio.pdf; /Users/jacquesthibodeau/Zotero/storage/VHIYX6TE/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7TQ9R75L,journalArticle,2018,"Steenbrugge, Xander; Leroux, Sam; Verbelen, Tim; Dhoedt, Bart",Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations,"arXiv:1811.04784 [cs, stat]",,,,http://arxiv.org/abs/1811.04784,"In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.",2018-11-12,2022-03-10 23:47:20,2022-03-10 23:47:20,2022-03-10 23:47:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04784,,/Users/jacquesthibodeau/Zotero/storage/H7QT7665/Steenbrugge et al. - 2018 - Improving Generalization for Abstract Reasoning Ta.pdf; /Users/jacquesthibodeau/Zotero/storage/X9YDGXDG/1811.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HUCET9H5,journalArticle,2019,"Oliver, Avital; Odena, Augustus; Raffel, Colin; Cubuk, Ekin D.; Goodfellow, Ian J.",Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"arXiv:1804.09170 [cs, stat]",,,,http://arxiv.org/abs/1804.09170,"Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.",2019-06-17,2022-03-10 23:47:21,2022-03-10 23:47:21,2022-03-10 23:47:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09170,,/Users/jacquesthibodeau/Zotero/storage/AYAJ6TJU/Oliver et al. - 2019 - Realistic Evaluation of Deep Semi-Supervised Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/H9NQXYF6/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNEVAD9D,journalArticle,2019,"Kim, Hyunjik; Mnih, Andriy; Schwarz, Jonathan; Garnelo, Marta; Eslami, Ali; Rosenbaum, Dan; Vinyals, Oriol; Teh, Yee Whye",Attentive Neural Processes,"arXiv:1901.05761 [cs, stat]",,,,http://arxiv.org/abs/1901.05761,"Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.",2019-07-09,2022-03-10 23:47:23,2022-03-10 23:47:23,2022-03-10 23:47:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.05761,,/Users/jacquesthibodeau/Zotero/storage/AU9HZ9EI/Kim et al. - 2019 - Attentive Neural Processes.pdf; /Users/jacquesthibodeau/Zotero/storage/WDY7XIK7/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WW2UAZ2P,journalArticle,2019,"MacKay, Matthew; Vicol, Paul; Lorraine, Jon; Duvenaud, David; Grosse, Roger",Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions,"arXiv:1903.03088 [cs, stat]",,,,http://arxiv.org/abs/1903.03088,"Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer. We justify this approximation by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. We fit this model using a gradient-based hyperparameter optimization algorithm which alternates between approximating the best-response around the current hyperparameters and optimizing the hyperparameters using the approximate best-response function. Unlike other gradient-based approaches, we do not require differentiating the training loss with respect to the hyperparameters, allowing us to tune discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Because the hyperparameters are adapted online, our approach discovers hyperparameter schedules that can outperform fixed hyperparameter values. Empirically, our approach outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, Self-Tuning Networks (STNs).",2019-03-07,2022-03-10 23:47:25,2022-03-10 23:47:25,2022-03-10 23:47:24,,,,,,,Self-Tuning Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03088,,/Users/jacquesthibodeau/Zotero/storage/95BA94VN/MacKay et al. - 2019 - Self-Tuning Networks Bilevel Optimization of Hype.pdf; /Users/jacquesthibodeau/Zotero/storage/M5QYWIGR/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6EVUL2MF,journalArticle,2018,"Chen, Mia Xu; Firat, Orhan; Bapna, Ankur; Johnson, Melvin; Macherey, Wolfgang; Foster, George; Jones, Llion; Parmar, Niki; Schuster, Mike; Chen, Zhifeng; Wu, Yonghui; Hughes, Macduff",The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,arXiv:1804.09849 [cs],,,,http://arxiv.org/abs/1804.09849,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",2018-04-26,2022-03-10 23:47:27,2022-03-10 23:47:27,2022-03-10 23:47:27,,,,,,,The Best of Both Worlds,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09849,,/Users/jacquesthibodeau/Zotero/storage/UKZWXNL4/Chen et al. - 2018 - The Best of Both Worlds Combining Recent Advances.pdf; /Users/jacquesthibodeau/Zotero/storage/9V5GXEL4/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZRKTHAP,journalArticle,2019,"Schaul, Tom; Borsa, Diana; Modayil, Joseph; Pascanu, Razvan",Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,"arXiv:1904.11455 [cs, stat]",,,,http://arxiv.org/abs/1904.11455,"Rather than proposing a new method, this paper investigates an issue present in existing learning algorithms. We study the learning dynamics of reinforcement learning (RL), specifically a characteristic coupling between learning and data generation that arises because RL agents control their future data distribution. In the presence of function approximation, this coupling can lead to a problematic type of 'ray interference', characterized by learning dynamics that sequentially traverse a number of performance plateaus, effectively constraining the agent to learn one thing at a time even when learning in parallel is better. We establish the conditions under which ray interference occurs, show its relation to saddle points and obtain the exact learning dynamics in a restricted setting. We characterize a number of its properties and discuss possible remedies.",2019-04-25,2022-03-10 23:47:29,2022-03-10 23:47:29,2022-03-10 23:47:29,,,,,,,Ray Interference,,,,,,,,,,,,arXiv.org,,arXiv: 1904.11455,,/Users/jacquesthibodeau/Zotero/storage/ZLSQNWIP/Schaul et al. - 2019 - Ray Interference a Source of Plateaus in Deep Rei.pdf; /Users/jacquesthibodeau/Zotero/storage/UGMWMRU3/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIYDJD87,journalArticle,2020,"Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,arXiv:1906.08237 [cs],,,,http://arxiv.org/abs/1906.08237,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",2020-01-02,2022-03-10 23:47:33,2022-03-10 23:47:33,2022-03-10 23:47:33,,,,,,,XLNet,,,,,,,,,,,,arXiv.org,,arXiv: 1906.08237,,/Users/jacquesthibodeau/Zotero/storage/7QEBR9FP/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf; /Users/jacquesthibodeau/Zotero/storage/QYND8SGV/1906.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z65LPG6N,journalArticle,2018,"Yu, Yuan; Abadi, Martín; Barham, Paul; Brevdo, Eugene; Burrows, Mike; Davis, Andy; Dean, Jeff; Ghemawat, Sanjay; Harley, Tim; Hawkins, Peter; Isard, Michael; Kudlur, Manjunath; Monga, Rajat; Murray, Derek; Zheng, Xiaoqiang",Dynamic Control Flow in Large-Scale Machine Learning,Proceedings of the Thirteenth EuroSys Conference,,,10.1145/3190508.3190551,http://arxiv.org/abs/1805.01772,"Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.",2018-04-23,2022-03-10 23:47:37,2022-03-10 23:47:37,2022-03-10 23:47:37,1-15,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.01772,,/Users/jacquesthibodeau/Zotero/storage/V66Q5PTK/Yu et al. - 2018 - Dynamic Control Flow in Large-Scale Machine Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/SY4UKQAR/1805.html,,,"Computer Science - Distributed, Parallel, and Cluster Computing; Computer Science - Machine Learning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TZYB42SZ,journalArticle,2019,"Gaier, Adam; Ha, David",Weight Agnostic Neural Networks,"arXiv:1906.04358 [cs, stat]",,,,http://arxiv.org/abs/1906.04358,"Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",2019-09-05,2022-03-10 23:47:49,2022-03-10 23:47:49,2022-03-10 23:47:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.04358,,/Users/jacquesthibodeau/Zotero/storage/2PCFWPKS/Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/JB8YT5GI/1906.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NZIZU342,journalArticle,2019,"Lample, Guillaume; Charton, François",Deep Learning for Symbolic Mathematics,arXiv:1912.01412 [cs],,,,http://arxiv.org/abs/1912.01412,"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",2019-12-02,2022-03-10 23:47:52,2022-03-10 23:47:52,2022-03-10 23:47:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01412,,/Users/jacquesthibodeau/Zotero/storage/QEPEBQYK/Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf; /Users/jacquesthibodeau/Zotero/storage/FTWCHZBI/1912.html,,,Computer Science - Machine Learning; Computer Science - Symbolic Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SXT53EYL,journalArticle,2019,"Kornblith, Simon; Shlens, Jonathon; Le, Quoc V.",Do Better ImageNet Models Transfer Better?,"arXiv:1805.08974 [cs, stat]",,,,http://arxiv.org/abs/1805.08974,"Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.",2019-06-17,2022-03-10 23:47:55,2022-03-10 23:47:55,2022-03-10 23:47:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08974,,/Users/jacquesthibodeau/Zotero/storage/869AYEJ8/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf; /Users/jacquesthibodeau/Zotero/storage/F28AXZEB/1805.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WKUQCA4,journalArticle,2020,"Frankle, Jonathan; Dziugaite, Gintare Karolina; Roy, Daniel M.; Carbin, Michael",Linear Mode Connectivity and the Lottery Ticket Hypothesis,"arXiv:1912.05671 [cs, stat]",,,,http://arxiv.org/abs/1912.05671,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).",2020-07-18,2022-03-10 23:47:57,2022-03-10 23:47:57,2022-03-10 23:47:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.05671,,/Users/jacquesthibodeau/Zotero/storage/6G762FLV/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf; /Users/jacquesthibodeau/Zotero/storage/477CKHZF/1912.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QTXTB7E,journalArticle,2020,"Real, Esteban; Liang, Chen; So, David R.; Le, Quoc V.",AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"arXiv:2003.03384 [cs, stat]",,,,http://arxiv.org/abs/2003.03384,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.",2020-06-30,2022-03-10 23:48:01,2022-03-10 23:48:01,2022-03-10 23:48:01,,,,,,,AutoML-Zero,,,,,,,,,,,,arXiv.org,,arXiv: 2003.03384,,/Users/jacquesthibodeau/Zotero/storage/6BZVBDCJ/Real et al. - 2020 - AutoML-Zero Evolving Machine Learning Algorithms .pdf; /Users/jacquesthibodeau/Zotero/storage/5SRYQUMU/2003.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; I.2.2; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KK2AMHZK,journalArticle,2018,"Srinivas, Aravind; Jabri, Allan; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea",Universal Planning Networks,"arXiv:1804.00645 [cs, stat]",,,,http://arxiv.org/abs/1804.00645,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",2018-04-04,2022-03-10 23:48:04,2022-03-10 23:48:04,2022-03-10 23:48:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.00645,,/Users/jacquesthibodeau/Zotero/storage/MAMYQKQW/Srinivas et al. - 2018 - Universal Planning Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/QJ39MKK6/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EIB68Z5I,journalArticle,2020,"Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng",GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,"arXiv:2006.16668 [cs, stat]",,,,http://arxiv.org/abs/2006.16668,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2020-06-30,2022-03-10 23:48:17,2022-03-10 23:48:17,2022-03-10 23:48:17,,,,,,,GShard,,,,,,,,,,,,arXiv.org,,arXiv: 2006.16668,,/Users/jacquesthibodeau/Zotero/storage/K6M84XP7/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Comp.pdf; /Users/jacquesthibodeau/Zotero/storage/N77EETHT/2006.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMJFPXQ9,journalArticle,2019,"Chandra, Kartik; Meijer, Erik; Andow, Samantha; Arroyo-Fang, Emilio; Dea, Irene; George, Johann; Grueter, Melissa; Hosmer, Basil; Stumpos, Steffi; Tempest, Alanna; Yang, Shannon",Gradient Descent: The Ultimate Optimizer,"arXiv:1909.13371 [cs, stat]",,,,http://arxiv.org/abs/1909.13371,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimization, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad infinitum. As these towers of gradient-based optimizers grow, they become significantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.",2019-09-29,2022-03-10 23:48:20,2022-03-10 23:48:20,2022-03-10 23:48:20,,,,,,,Gradient Descent,,,,,,,,,,,,arXiv.org,,arXiv: 1909.13371,,/Users/jacquesthibodeau/Zotero/storage/NQC9CVXB/Chandra et al. - 2019 - Gradient Descent The Ultimate Optimizer.pdf; /Users/jacquesthibodeau/Zotero/storage/GGEKXAY4/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F56HULHH,journalArticle,2021,"Hutter, Marcus",Learning Curve Theory,"arXiv:2102.04074 [cs, stat]",,,,http://arxiv.org/abs/2102.04074,"Recently a number of empirical ""universal"" scaling law papers have been published, most notably by OpenAI. `Scaling laws' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\beta}$ learning curves for arbitrary power $\beta>0$, and determine whether power laws are universal or depend on the data distribution.",2021-02-08,2022-03-10 23:48:27,2022-03-10 23:48:27,2022-03-10 23:48:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.04074,,/Users/jacquesthibodeau/Zotero/storage/CJHPJ7UB/Hutter - 2021 - Learning Curve Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/Z23SDKW9/2102.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6DTSSI7,journalArticle,2019,"Scholten, Jan; Wout, Daan; Celemin, Carlos; Kober, Jens",Deep Reinforcement Learning with Feedback-based Exploration,2019 IEEE 58th Conference on Decision and Control (CDC),,,10.1109/CDC40024.2019.9029503,http://arxiv.org/abs/1903.06151,"Deep Reinforcement Learning has enabled the control of increasingly complex and high-dimensional problems. However, the need of vast amounts of data before reasonable performance is attained prevents its widespread application. We employ binary corrective feedback as a general and intuitive manner to incorporate human intuition and domain knowledge in model-free machine learning. The uncertainty in the policy and the corrective feedback is combined directly in the action space as probabilistic conditional exploration. As a result, the greatest part of the otherwise ignorant learning process can be avoided. We demonstrate the proposed method, Predictive Probabilistic Merging of Policies (PPMP), in combination with DDPG. In experiments on continuous control problems of the OpenAI Gym, we achieve drastic improvements in sample efficiency, final performance, and robustness to erroneous feedback, both for human and synthetic feedback. Additionally, we show solutions beyond the demonstrated knowledge.",2019-12,2022-03-10 23:51:33,2022-03-10 23:51:33,2022-03-10 23:51:33,803-808,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06151,,/Users/jacquesthibodeau/Zotero/storage/HTQJ9X8G/Scholten et al. - 2019 - Deep Reinforcement Learning with Feedback-based Ex.pdf; /Users/jacquesthibodeau/Zotero/storage/KC59WTZV/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDRVYL6S,journalArticle,2019,"Jiang, Jiechuan; Lu, Zongqing",Generative Exploration and Exploitation,"arXiv:1904.09605 [cs, stat]",,,,http://arxiv.org/abs/1904.09605,"Sparse reward is one of the biggest challenges in reinforcement learning (RL). In this paper, we propose a novel method called Generative Exploration and Exploitation (GENE) to overcome sparse reward. GENE automatically generates start states to encourage the agent to explore the environment and to exploit received reward signals. GENE can adaptively tradeoff between exploration and exploitation according to the varying distributions of states experienced by the agent as the learning progresses. GENE relies on no prior knowledge about the environment and can be combined with any RL algorithm, no matter on-policy or off-policy, single-agent or multi-agent. Empirically, we demonstrate that GENE significantly outperforms existing methods in three tasks with only binary rewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies verify the emergence of progressive exploration and automatic reversing.",2019-11-20,2022-03-10 23:51:35,2022-03-10 23:51:35,2022-03-10 23:51:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.09605,,/Users/jacquesthibodeau/Zotero/storage/KYH6PZF2/Jiang and Lu - 2019 - Generative Exploration and Exploitation.pdf; /Users/jacquesthibodeau/Zotero/storage/VG7S2VDU/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AQMJ7REU,journalArticle,2019,"Janz, David; Hron, Jiri; Mazur, Przemysław; Hofmann, Katja; Hernández-Lobato, José Miguel; Tschiatschek, Sebastian",Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,"arXiv:1810.06530 [cs, stat]",,,,http://arxiv.org/abs/1810.06530,"Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.",2019-12-03,2022-03-10 23:51:37,2022-03-10 23:51:37,2022-03-10 23:51:37,,,,,,,Successor Uncertainties,,,,,,,,,,,,arXiv.org,,arXiv: 1810.06530,,/Users/jacquesthibodeau/Zotero/storage/UGE2B9CS/Janz et al. - 2019 - Successor Uncertainties Exploration and Uncertain.pdf; /Users/jacquesthibodeau/Zotero/storage/I7F47DX3/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJJEGWTA,journalArticle,2020,"Tavakoli, Arash; Levdik, Vitaly; Islam, Riashat; Smith, Christopher M.; Kormushev, Petar",Exploring Restart Distributions,"arXiv:1811.11298 [cs, stat]",,,,http://arxiv.org/abs/1811.11298,"We consider the generic approach of using an experience memory to help exploration by adapting a restart distribution. That is, given the capacity to reset the state with those corresponding to the agent's past observations, we help exploration by promoting faster state-space coverage via restarting the agent from a more diverse set of initial states, as well as allowing it to restart in states associated with significant past experiences. This approach is compatible with both on-policy and off-policy methods. However, a caveat is that altering the distribution of initial states could change the optimal policies when searching within a restricted class of policies. To reduce this unsought learning bias, we evaluate our approach in deep reinforcement learning which benefits from the high representational capacity of deep neural networks. We instantiate three variants of our approach, each inspired by an idea in the context of experience replay. Using these variants, we show that performance gains can be achieved, especially in hard exploration problems.",2020-08-17,2022-03-10 23:51:38,2022-03-10 23:51:38,2022-03-10 23:51:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.11298,,/Users/jacquesthibodeau/Zotero/storage/XEB9QTDB/Tavakoli et al. - 2020 - Exploring Restart Distributions.pdf; /Users/jacquesthibodeau/Zotero/storage/56E8FMI9/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KK2NPY8B,journalArticle,2019,"Azar, Mohammad Gheshlaghi; Piot, Bilal; Pires, Bernardo Avila; Grill, Jean-Bastien; Altché, Florent; Munos, Rémi",World Discovery Models,"arXiv:1902.07685 [cs, stat]",,,,http://arxiv.org/abs/1902.07685,"As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.",2019-03-01,2022-03-10 23:51:40,2022-03-10 23:51:40,2022-03-10 23:51:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.07685,,/Users/jacquesthibodeau/Zotero/storage/7ZFXMJ5D/Azar et al. - 2019 - World Discovery Models.pdf; /Users/jacquesthibodeau/Zotero/storage/UYURCN33/1902.html,,,Computer Science - Artificial Intelligence; Statistics - Applications; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZRDNG74J,journalArticle,2019,"Chen, Tao; Gupta, Saurabh; Gupta, Abhinav",Learning Exploration Policies for Navigation,arXiv:1903.01959 [cs],,,,http://arxiv.org/abs/1903.01959,"Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that the use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Code and Videos are available at: https://sites.google.com/view/exploration-for-nav.",2019-03-05,2022-03-10 23:51:43,2022-03-10 23:51:43,2022-03-10 23:51:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01959,,/Users/jacquesthibodeau/Zotero/storage/AJIMVJB9/Chen et al. - 2019 - Learning Exploration Policies for Navigation.pdf; /Users/jacquesthibodeau/Zotero/storage/M9XNH4QG/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZUWLZG56,journalArticle,2020,"Sekar, Ramanan; Rybkin, Oleh; Daniilidis, Kostas; Abbeel, Pieter; Hafner, Danijar; Pathak, Deepak",Planning to Explore via Self-Supervised World Models,"arXiv:2005.05960 [cs, stat]",,,,http://arxiv.org/abs/2005.05960,"Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/",2020-06-30,2022-03-10 23:51:45,2022-03-10 23:51:45,2022-03-10 23:51:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.05960,,/Users/jacquesthibodeau/Zotero/storage/4RSYKVMM/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/DD2UQI5H/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E556NSLA,journalArticle,2021,"Matusch, Brendon; Ba, Jimmy; Hafner, Danijar",Evaluating Agents without Rewards,arXiv:2012.11538 [cs],,,,http://arxiv.org/abs/2012.11538,"Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.",2021-02-09,2022-03-10 23:51:46,2022-03-10 23:51:46,2022-03-10 23:51:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.11538,,/Users/jacquesthibodeau/Zotero/storage/TBB5F2HB/Matusch et al. - 2021 - Evaluating Agents without Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/N243R9R3/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3YDU6KVX,journalArticle,2019,"Lee, Gyeong Taek; Kim, Chang Ouk",Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution,arXiv:1901.05856 [cs],,,,http://arxiv.org/abs/1901.05856,"This paper proposes a new reinforcement learning (RL) algorithm that enhances exploration by amplifying the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We argue that these two algorithms complement each other and that combining these two algorithms can amplify the imitation effect for exploration. In addition, by adding an intrinsic penalty reward to the state that the RL agent frequently visits and using replay memory for learning the feature state when using an exploration bonus, the proposed approach leads to deep exploration and deviates from the current converged policy. We verified the exploration performance of the algorithm through experiments in a two-dimensional grid environment. In addition, we applied the algorithm to a simulated environment of unmanned combat aerial vehicle (UCAV) mission execution, and the empirical results show that AIE is very effective for finding the UCAV's shortest flight path to avoid an enemy's missiles.",2019-01-17,2022-03-10 23:51:52,2022-03-10 23:51:52,2022-03-10 23:51:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.05856,,/Users/jacquesthibodeau/Zotero/storage/6MB2HMTR/Lee and Kim - 2019 - Amplifying the Imitation Effect for Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/E5H6PFTC/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5X6VEY8M,journalArticle,2019,"Paine, Tom Le; Gulcehre, Caglar; Shahriari, Bobak; Denil, Misha; Hoffman, Matt; Soyer, Hubert; Tanburn, Richard; Kapturowski, Steven; Rabinowitz, Neil; Williams, Duncan; Barth-Maron, Gabriel; Wang, Ziyu; de Freitas, Nando; Team, Worlds",Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,arXiv:1909.01387 [cs],,,,http://arxiv.org/abs/1909.01387,"This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.",2019-09-03,2022-03-10 23:52:08,2022-03-10 23:52:08,2022-03-10 23:52:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.01387,,/Users/jacquesthibodeau/Zotero/storage/HBX4J5VH/Paine et al. - 2019 - Making Efficient Use of Demonstrations to Solve Ha.pdf; /Users/jacquesthibodeau/Zotero/storage/RM9BJJL9/1909.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7M4CQKS4,journalArticle,2019,"Jakubovitz, Daniel; Giryes, Raja; Rodrigues, Miguel R. D.",Generalization Error in Deep Learning,"arXiv:1808.01174 [cs, stat]",,,,http://arxiv.org/abs/1808.01174,"Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",2019-04-06,2022-03-10 23:52:10,2022-03-10 23:52:10,2022-03-10 23:52:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.01174,,/Users/jacquesthibodeau/Zotero/storage/X3GG28IB/Jakubovitz et al. - 2019 - Generalization Error in Deep Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/MI6R4EY9/1808.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSS55QMN,journalArticle,2019,"Metz, Luke; Maheswaranathan, Niru; Cheung, Brian; Sohl-Dickstein, Jascha",Meta-Learning Update Rules for Unsupervised Representation Learning,"arXiv:1804.00222 [cs, stat]",,,,http://arxiv.org/abs/1804.00222,"A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",2019-02-26,2022-03-10 23:52:12,2022-03-10 23:52:12,2022-03-10 23:52:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.00222,,/Users/jacquesthibodeau/Zotero/storage/AK87KNNA/Metz et al. - 2019 - Meta-Learning Update Rules for Unsupervised Repres.pdf; /Users/jacquesthibodeau/Zotero/storage/4ADBD27W/1804.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L9PLTSVS,journalArticle,2018,"LaLonde, Rodney; Bagci, Ulas",Capsules for Object Segmentation,"arXiv:1804.04241 [cs, stat]",,,,http://arxiv.org/abs/1804.04241,"Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy.",2018-04-11,2022-03-10 23:52:17,2022-03-10 23:52:17,2022-03-10 23:52:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.04241,,/Users/jacquesthibodeau/Zotero/storage/95RBHWXE/LaLonde and Bagci - 2018 - Capsules for Object Segmentation.pdf; /Users/jacquesthibodeau/Zotero/storage/34VPVJPY/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBHIZI8K,journalArticle,2020,"Frankle, Jonathan; Dziugaite, Gintare Karolina; Roy, Daniel M.; Carbin, Michael",Stabilizing the Lottery Ticket Hypothesis,"arXiv:1903.01611 [cs, stat]",,,,http://arxiv.org/abs/1903.01611,"Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the ""lottery ticket hypothesis"" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1% to 7% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork ""stability,"" finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis",2020-07-20,2022-03-10 23:52:21,2022-03-10 23:52:21,2022-03-10 23:52:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01611,,/Users/jacquesthibodeau/Zotero/storage/4JD2525V/Frankle et al. - 2020 - Stabilizing the Lottery Ticket Hypothesis.pdf; /Users/jacquesthibodeau/Zotero/storage/4PMYLKYB/1903.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2Z66B3U,journalArticle,2018,"Chang, Oscar; Lipson, Hod",Neural Network Quine,arXiv:1803.05859 [cs],,,,http://arxiv.org/abs/1803.05859,"Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.",2018-05-24,2022-03-10 23:52:28,2022-03-10 23:52:28,2022-03-10 23:52:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.05859,,/Users/jacquesthibodeau/Zotero/storage/67DZNFIM/Chang and Lipson - 2018 - Neural Network Quine.pdf; /Users/jacquesthibodeau/Zotero/storage/U3TBS7IJ/1803.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVBHJYKY,journalArticle,2019,"Jing, Longlong; Tian, Yingli",Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,arXiv:1902.06162 [cs],,,,http://arxiv.org/abs/1902.06162,"Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.",2019-02-16,2022-03-10 23:52:31,2022-03-10 23:52:31,2022-03-10 23:52:31,,,,,,,Self-supervised Visual Feature Learning with Deep Neural Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06162,,/Users/jacquesthibodeau/Zotero/storage/GHWTQLCP/Jing and Tian - 2019 - Self-supervised Visual Feature Learning with Deep .pdf; /Users/jacquesthibodeau/Zotero/storage/KF96U6ZJ/1902.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XEWFWNQQ,journalArticle,2019,"Belkin, Mikhail; Hsu, Daniel; Ma, Siyuan; Mandal, Soumik",Reconciling modern machine learning practice and the bias-variance trade-off,"arXiv:1812.11118 [cs, stat]",,,,http://arxiv.org/abs/1812.11118,"Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ""double descent"" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.",2019-09-10,2022-03-10 23:52:34,2022-03-10 23:52:34,2022-03-10 23:52:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1812.11118,,/Users/jacquesthibodeau/Zotero/storage/DHGHKYL6/Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf; /Users/jacquesthibodeau/Zotero/storage/S6XT726D/1812.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M489WRD8,journalArticle,2020,"Marcus, Gary",The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,arXiv:2002.06177 [cs],,,,http://arxiv.org/abs/2002.06177,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.",2020-02-19,2022-03-10 23:52:38,2022-03-10 23:52:38,2022-03-10 23:52:38,,,,,,,The Next Decade in AI,,,,,,,,,,,,arXiv.org,,arXiv: 2002.06177,,/Users/jacquesthibodeau/Zotero/storage/GPY8PUHP/Marcus - 2020 - The Next Decade in AI Four Steps Towards Robust A.pdf; /Users/jacquesthibodeau/Zotero/storage/YLZBI5SL/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3GFTLZ3V,journalArticle,2019,"Yadav, Chhavi; Bottou, Léon",Cold Case: The Lost MNIST Digits,"arXiv:1905.10498 [cs, stat]",,,,http://arxiv.org/abs/1905.10498,"Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",2019-11-04,2022-03-10 23:52:41,2022-03-10 23:52:41,2022-03-10 23:52:41,,,,,,,Cold Case,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10498,,/Users/jacquesthibodeau/Zotero/storage/SZUKZJAU/Yadav and Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf; /Users/jacquesthibodeau/Zotero/storage/KPHPDRB2/1905.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NG5KNHI5,journalArticle,2020,"Shuster, Kurt; Urbanek, Jack; Dinan, Emily; Szlam, Arthur; Weston, Jason",Deploying Lifelong Open-Domain Dialogue Learning,arXiv:2008.08076 [cs],,,,http://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",2020-08-19,2022-03-10 23:52:45,2022-03-10 23:52:45,2022-03-10 23:52:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.08076,,/Users/jacquesthibodeau/Zotero/storage/AJBDTBVQ/Shuster et al. - 2020 - Deploying Lifelong Open-Domain Dialogue Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/5X7FQZQ7/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2FB6ZUEK,journalArticle,2021,"Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,arXiv:2010.11929 [cs],,,,http://arxiv.org/abs/2010.11929,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",2021-06-03,2022-03-10 23:53:02,2022-03-10 23:53:02,2022-03-10 23:53:02,,,,,,,An Image is Worth 16x16 Words,,,,,,,,,,,,arXiv.org,,arXiv: 2010.11929,,/Users/jacquesthibodeau/Zotero/storage/ZZZU74PJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf; /Users/jacquesthibodeau/Zotero/storage/5LTEPGQJ/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YGHJ37U4,journalArticle,2018,"Zhou, Yanqi; Ebrahimi, Siavash; Arık, Sercan Ö; Yu, Haonan; Liu, Hairong; Diamos, Greg",Resource-Efficient Neural Architect,arXiv:1806.07912 [cs],,,,http://arxiv.org/abs/1806.07912,"Neural Architecture Search (NAS) is a laborious process. Prior work on automated NAS targets mainly on improving accuracy, but lacks consideration of computational resource use. We propose the Resource-Efficient Neural Architect (RENA), an efficient resource-constrained NAS using reinforcement learning with network embedding. RENA uses a policy network to process the network embeddings to generate new configurations. We demonstrate RENA on image recognition and keyword spotting (KWS) problems. RENA can find novel architectures that achieve high performance even with tight resource constraints. For CIFAR10, it achieves 2.95% test error when compute intensity is greater than 100 FLOPs/byte, and 3.87% test error when model size is less than 3M parameters. For Google Speech Commands Dataset, RENA achieves the state-of-the-art accuracy without resource constraints, and it outperforms the optimized architectures with tight resource constraints.",2018-06-12,2022-03-10 23:53:06,2022-03-10 23:53:06,2022-03-10 23:53:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.07912,,/Users/jacquesthibodeau/Zotero/storage/EF6F9NH4/Zhou et al. - 2018 - Resource-Efficient Neural Architect.pdf; /Users/jacquesthibodeau/Zotero/storage/W76VHVT6/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SIZ8EK8A,journalArticle,2019,"Liu, Hanxiao; Simonyan, Karen; Yang, Yiming",DARTS: Differentiable Architecture Search,"arXiv:1806.09055 [cs, stat]",,,,http://arxiv.org/abs/1806.09055,"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",2019-04-23,2022-03-10 23:53:08,2022-03-10 23:53:08,2022-03-10 23:53:07,,,,,,,DARTS,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09055,,/Users/jacquesthibodeau/Zotero/storage/QDG9WX2J/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf; /Users/jacquesthibodeau/Zotero/storage/SMUJLUJJ/1806.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZJ9FITUL,journalArticle,2021,"Lester, Brian; Al-Rfou, Rami; Constant, Noah",The Power of Scale for Parameter-Efficient Prompt Tuning,arXiv:2104.08691 [cs],,,,http://arxiv.org/abs/2104.08691,"In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",2021-09-02,2022-03-10 23:53:14,2022-03-10 23:53:14,2022-03-10 23:53:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.08691,,/Users/jacquesthibodeau/Zotero/storage/AL6RX66Q/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf; /Users/jacquesthibodeau/Zotero/storage/DUG8DW4S/2104.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XAFA4A9U,journalArticle,2019,"Dehghani, Mostafa; Gouws, Stephan; Vinyals, Oriol; Uszkoreit, Jakob; Kaiser, Łukasz",Universal Transformers,"arXiv:1807.03819 [cs, stat]",,,,http://arxiv.org/abs/1807.03819,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",2019-03-05,2022-03-10 23:53:17,2022-03-10 23:53:17,2022-03-10 23:53:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03819,,/Users/jacquesthibodeau/Zotero/storage/E8IN3VJC/Dehghani et al. - 2019 - Universal Transformers.pdf; /Users/jacquesthibodeau/Zotero/storage/UV9M5L4W/1807.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4EE938X2,journalArticle,2021,"Ortega, Pedro A.; Kunesch, Markus; Delétang, Grégoire; Genewein, Tim; Grau-Moya, Jordi; Veness, Joel; Buchli, Jonas; Degrave, Jonas; Piot, Bilal; Perolat, Julien; Everitt, Tom; Tallec, Corentin; Parisotto, Emilio; Erez, Tom; Chen, Yutian; Reed, Scott; Hutter, Marcus; de Freitas, Nando; Legg, Shane",Shaking the foundations: delusions in sequence models for interaction and control,arXiv:2110.10819 [cs],,,,http://arxiv.org/abs/2110.10819,"The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models ""lack the understanding of the cause and effect of their actions"" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.",2021-10-20,2022-03-10 23:53:20,2022-03-10 23:53:20,2022-03-10 23:53:20,,,,,,,Shaking the foundations,,,,,,,,,,,,arXiv.org,,arXiv: 2110.10819,,/Users/jacquesthibodeau/Zotero/storage/KXGQE6LU/Ortega et al. - 2021 - Shaking the foundations delusions in sequence mod.pdf; /Users/jacquesthibodeau/Zotero/storage/83LBVJIS/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGLBC3YV,journalArticle,2018,"Odena, Augustus; Goodfellow, Ian",TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,"arXiv:1807.10875 [cs, stat]",,,,http://arxiv.org/abs/1807.10875,"Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.",2018-07-27,2022-03-10 23:53:22,2022-03-10 23:53:22,2022-03-10 23:53:22,,,,,,,TensorFuzz,,,,,,,,,,,,arXiv.org,,arXiv: 1807.10875,,/Users/jacquesthibodeau/Zotero/storage/DHEIV74L/Odena and Goodfellow - 2018 - TensorFuzz Debugging Neural Networks with Coverag.pdf; /Users/jacquesthibodeau/Zotero/storage/KX77ZN7D/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVREE8AG,journalArticle,2020,"Yang, Jiachen; Nakhaei, Alireza; Isele, David; Fujimura, Kikuo; Zha, Hongyuan",CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,"arXiv:1809.05188 [cs, stat]",,,,http://arxiv.org/abs/1809.05188,"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.",2020-01-24,2022-03-10 23:55:31,2022-03-10 23:55:31,2022-03-10 23:55:31,,,,,,,CM3,,,,,,,,,,,,arXiv.org,,arXiv: 1809.05188,,/Users/jacquesthibodeau/Zotero/storage/PY3SBANR/Yang et al. - 2020 - CM3 Cooperative Multi-goal Multi-stage Multi-agen.pdf; /Users/jacquesthibodeau/Zotero/storage/YD4S7YNM/1809.html,,,Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TG5WPPZB,journalArticle,2021,"Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha",Emergent Social Learning via Multi-agent Reinforcement Learning,"arXiv:2010.00581 [cs, stat]",,,,http://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.",2021-06-22,2022-03-10 23:55:32,2022-03-10 23:55:32,2022-03-10 23:55:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.00581,,/Users/jacquesthibodeau/Zotero/storage/HPDA867X/Ndousse et al. - 2021 - Emergent Social Learning via Multi-agent Reinforce.pdf; /Users/jacquesthibodeau/Zotero/storage/P7GQ444W/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2LGSSL9R,journalArticle,2019,"Lerer, Adam; Peysakhovich, Alexander",Learning Existing Social Conventions via Observationally Augmented Self-Play,arXiv:1806.10071 [cs],,,,http://arxiv.org/abs/1806.10071,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",2019-03-13,2022-03-10 23:55:32,2022-03-10 23:55:32,2022-03-10 23:55:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10071,,/Users/jacquesthibodeau/Zotero/storage/QZ8JDCN4/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observati.pdf; /Users/jacquesthibodeau/Zotero/storage/RDKG466A/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MBCX7HTK,journalArticle,2018,"Ortega, Pedro A.; Legg, Shane",Modeling Friends and Foes,arXiv:1807.00196 [cs],,,,http://arxiv.org/abs/1807.00196,"How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.",2018-06-30,2022-03-10 23:55:36,2022-03-10 23:55:36,2022-03-10 23:55:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00196,,/Users/jacquesthibodeau/Zotero/storage/KWDE53TZ/Ortega and Legg - 2018 - Modeling Friends and Foes.pdf; /Users/jacquesthibodeau/Zotero/storage/GBJNZ64T/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KHX99S7S,journalArticle,2018,"Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano",Multi-Agent Generative Adversarial Imitation Learning,"arXiv:1807.09936 [cs, stat]",,,,http://arxiv.org/abs/1807.09936,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",2018-07-25,2022-03-10 23:55:38,2022-03-10 23:55:38,2022-03-10 23:55:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.09936,,/Users/jacquesthibodeau/Zotero/storage/LU7MTBYM/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/MP8XWNQ5/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78YIGW9Z,journalArticle,2019,"Gallego, Victor; Naveiro, Roi; Insua, David Rios",Reinforcement Learning under Threats,Proceedings of the AAAI Conference on Artificial Intelligence,,"2374-3468, 2159-5399",10.1609/aaai.v33i01.33019939,http://arxiv.org/abs/1809.01560,"In several reinforcement learning (RL) scenarios, mainly in security settings, there may be adversaries trying to interfere with the reward generating process. In this paper, we introduce Threatened Markov Decision Processes (TMDPs), which provide a framework to support a decision maker against a potential adversary in RL. Furthermore, we propose a level-$k$ thinking scheme resulting in a new learning framework to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries while the agent learns.",2019-07-17,2022-03-10 23:55:40,2022-03-10 23:55:40,2022-03-10 23:55:40,9939-9940,,,33,,AAAI,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.01560,,/Users/jacquesthibodeau/Zotero/storage/U23QEDYQ/Gallego et al. - 2019 - Reinforcement Learning under Threats.pdf; /Users/jacquesthibodeau/Zotero/storage/63QD4DVP/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RMMDNFZ,journalArticle,2020,"Mazumdar, Eric; Ratliff, Lillian J.; Sastry, S. Shankar",On Gradient-Based Learning in Continuous Games,SIAM Journal on Mathematics of Data Science,,2577-0187,10.1137/18M1231298,http://arxiv.org/abs/1804.05464,"We formulate a general framework for competitive gradient-based learning that encompasses a wide breadth of multi-agent learning algorithms, and analyze the limiting behavior of competitive gradient-based learning algorithms using dynamical systems theory. For both general-sum and potential games, we characterize a non-negligible subset of the local Nash equilibria that will be avoided if each agent employs a gradient-based learning algorithm. We also shed light on the issue of convergence to non-Nash strategies in general- and zero-sum games, which may have no relevance to the underlying game, and arise solely due to the choice of algorithm. The existence and frequency of such strategies may explain some of the difficulties encountered when using gradient descent in zero-sum games as, e.g., in the training of generative adversarial networks. To reinforce the theoretical contributions, we provide empirical results that highlight the frequency of linear quadratic dynamic games (a benchmark for multi-agent reinforcement learning) that admit global Nash equilibria that are almost surely avoided by policy gradient.",2020-01,2022-03-10 23:55:42,2022-03-10 23:55:42,2022-03-10 23:55:42,103-131,,1,2,,SIAM Journal on Mathematics of Data Science,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05464,,/Users/jacquesthibodeau/Zotero/storage/F4V3WVHB/Mazumdar et al. - 2020 - On Gradient-Based Learning in Continuous Games.pdf; /Users/jacquesthibodeau/Zotero/storage/23LBJDM5/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3RRNBU9D,journalArticle,2019,"Rhinehart, Nicholas; McAllister, Rowan; Kitani, Kris; Levine, Sergey",PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,"arXiv:1905.01296 [cs, stat]",,,,http://arxiv.org/abs/1905.01296,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",2019-09-30,2022-03-10 23:55:43,2022-03-10 23:55:43,2022-03-10 23:55:43,,,,,,,PRECOG,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01296,,/Users/jacquesthibodeau/Zotero/storage/MJTWYKWZ/Rhinehart et al. - 2019 - PRECOG PREdiction Conditioned On Goals in Visual .pdf; /Users/jacquesthibodeau/Zotero/storage/N3EUA5B8/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3K67UH64,journalArticle,2020,"Rivera, Corban G.; Lyons, Olivia; Summitt, Arielle; Fatima, Ayman; Pak, Ji; Shao, William; Chalmers, Robert; Englander, Aryeh; Staley, Edward W.; Wang, I.-Jeng; Llorens, Ashley J.",TanksWorld: A Multi-Agent Environment for AI Safety Research,arXiv:2002.11174 [cs],,,,http://arxiv.org/abs/2002.11174,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work.",2020-02-25,2022-03-10 23:55:45,2022-03-10 23:55:45,2022-03-10 23:55:45,,,,,,,TanksWorld,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11174,,/Users/jacquesthibodeau/Zotero/storage/FCFX3F9Q/Rivera et al. - 2020 - TanksWorld A Multi-Agent Environment for AI Safet.pdf; /Users/jacquesthibodeau/Zotero/storage/ZNXK7FFW/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UZBSAV9C,journalArticle,2022,"Strouse, D. J.; McKee, Kevin R.; Botvinick, Matt; Hughes, Edward; Everett, Richard",Collaborating with Humans without Human Data,arXiv:2110.08176 [cs],,,,http://arxiv.org/abs/2110.08176,"Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train ""human-aware"" agents (""behavioral cloning play"", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.",2022-01-07,2022-03-10 23:55:50,2022-03-11 1:38:41,2022-03-10 23:55:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08176,,/Users/jacquesthibodeau/Zotero/storage/ZZX3SPXN/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf; /Users/jacquesthibodeau/Zotero/storage/6BLZ2JJL/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf; /Users/jacquesthibodeau/Zotero/storage/HMP8NIXA/2110.html; /Users/jacquesthibodeau/Zotero/storage/AT6RPJK8/2110.html,,,Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8FQ6JESE,journalArticle,2018,"Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian K.",Legible Normativity for AI Alignment: The Value of Silly Rules,arXiv:1811.01267 [cs],,,,http://arxiv.org/abs/1811.01267,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems, in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules--rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",2018-11-03,2022-03-10 23:55:51,2022-03-10 23:55:51,2022-03-10 23:55:51,,,,,,,Legible Normativity for AI Alignment,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01267,,/Users/jacquesthibodeau/Zotero/storage/STP5MYTC/Hadfield-Menell et al. - 2018 - Legible Normativity for AI Alignment The Value of.pdf; /Users/jacquesthibodeau/Zotero/storage/TJNA8KPW/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9SZQL6YJ,journalArticle,2019,"Gruetzemacher, Ross; Paradice, David; Lee, Kang Bok",Forecasting Transformative AI: An Expert Survey,arXiv:1901.08579 [cs],,,,http://arxiv.org/abs/1901.08579,"Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.",2019-07-16,2022-03-10 23:55:54,2022-03-10 23:55:54,2022-03-10 23:55:53,,,,,,,Forecasting Transformative AI,,,,,,,,,,,,arXiv.org,,arXiv: 1901.08579,,/Users/jacquesthibodeau/Zotero/storage/PQDL8ZYC/Gruetzemacher et al. - 2019 - Forecasting Transformative AI An Expert Survey.pdf; /Users/jacquesthibodeau/Zotero/storage/BLZZY2LH/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8DQ3CDFV,journalArticle,2020,"Gruetzemacher, Ross; Dorner, Florian; Bernaola-Alvarez, Niko; Giattino, Charlie; Manheim, David",Forecasting AI Progress: A Research Agenda,arXiv:2008.01848 [cs],,,,http://arxiv.org/abs/2008.01848,"Forecasting AI progress is essential to reducing uncertainty in order to appropriately plan for research efforts on AI safety and AI governance. While this is generally considered to be an important topic, little work has been conducted on it and there is no published document that gives and objective overview of the field. Moreover, the field is very diverse and there is no published consensus regarding its direction. This paper describes the development of a research agenda for forecasting AI progress which utilized the Delphi technique to elicit and aggregate experts' opinions on what questions and methods to prioritize. The results of the Delphi are presented; the remainder of the paper follow the structure of these results, briefly reviewing relevant literature and suggesting future work for each topic. Experts indicated that a wide variety of methods should be considered for forecasting AI progress. Moreover, experts identified salient questions that were both general and completely unique to the problem of forecasting AI progress. Some of the highest priority topics include the validation of (partially unresolved) forecasts, how to make forecasting action-guiding and the quality of different performance metrics. While statistical methods seem more promising, there is also recognition that supplementing judgmental techniques can be quite beneficial.",2020-08-04,2022-03-10 23:55:56,2022-03-10 23:55:56,2022-03-10 23:55:56,,,,,,,Forecasting AI Progress,,,,,,,,,,,,arXiv.org,,arXiv: 2008.01848,,/Users/jacquesthibodeau/Zotero/storage/LRHMLW4G/Gruetzemacher et al. - 2020 - Forecasting AI Progress A Research Agenda.pdf; /Users/jacquesthibodeau/Zotero/storage/4VCPDBQW/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3G6WZSBV,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Zou, Andy; Mazeika, Mantas; Song, Dawn; Steinhardt, Jacob",Measuring Massive Multitask Language Understanding,arXiv:2009.03300 [cs],,,,http://arxiv.org/abs/2009.03300,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",2021-01-12,2022-03-10 23:55:59,2022-03-10 23:55:59,2022-03-10 23:55:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.03300,,/Users/jacquesthibodeau/Zotero/storage/9PPPMYDK/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf; /Users/jacquesthibodeau/Zotero/storage/6ZG62BJY/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82EI8G54,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Kadavath, Saurav; Arora, Akul; Basart, Steven; Tang, Eric; Song, Dawn; Steinhardt, Jacob",Measuring Mathematical Problem Solving With the MATH Dataset,arXiv:2103.03874 [cs],,,,http://arxiv.org/abs/2103.03874,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",2021-11-08,2022-03-10 23:56:00,2022-03-10 23:56:00,2022-03-10 23:56:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03874,,/Users/jacquesthibodeau/Zotero/storage/Z4C7U8IP/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf; /Users/jacquesthibodeau/Zotero/storage/6TBZ9V2X/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C4BLM25M,journalArticle,2020,"Henighan, Tom; Kaplan, Jared; Katz, Mor; Chen, Mark; Hesse, Christopher; Jackson, Jacob; Jun, Heewoo; Brown, Tom B.; Dhariwal, Prafulla; Gray, Scott; Hallacy, Chris; Mann, Benjamin; Radford, Alec; Ramesh, Aditya; Ryder, Nick; Ziegler, Daniel M.; Schulman, John; Amodei, Dario; McCandlish, Sam",Scaling Laws for Autoregressive Generative Modeling,arXiv:2010.14701 [cs],,,,http://arxiv.org/abs/2010.14701,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",2020-11-05,2022-03-10 23:56:02,2022-03-11 1:37:00,2022-03-10 23:56:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14701,,/Users/jacquesthibodeau/Zotero/storage/HVL9XGE5/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf; /Users/jacquesthibodeau/Zotero/storage/MCEEWHXM/2010.html; /Users/jacquesthibodeau/Zotero/storage/7DQHXUL8/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf; /Users/jacquesthibodeau/Zotero/storage/AQCFSXDE/2010.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7UVT7BWI,journalArticle,2021,"Hendrycks, Dan; Basart, Steven; Kadavath, Saurav; Mazeika, Mantas; Arora, Akul; Guo, Ethan; Burns, Collin; Puranik, Samir; He, Horace; Song, Dawn; Steinhardt, Jacob",Measuring Coding Challenge Competence With APPS,arXiv:2105.09938 [cs],,,,http://arxiv.org/abs/2105.09938,"While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.",2021-11-08,2022-03-10 23:56:04,2022-03-10 23:56:04,2022-03-10 23:56:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.09938,,/Users/jacquesthibodeau/Zotero/storage/GENWHUH7/Hendrycks et al. - 2021 - Measuring Coding Challenge Competence With APPS.pdf; /Users/jacquesthibodeau/Zotero/storage/JDUAMF25/2105.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WDNU877J,journalArticle,2019,"Kim, Byungju; Kim, Hyunwoo; Kim, Kyungsu; Kim, Sungjin; Kim, Junmo",Learning Not to Learn: Training Deep Neural Networks with Biased Data,arXiv:1812.10352 [cs],,,,http://arxiv.org/abs/1812.10352,"We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.",2019-04-15,2022-03-10 23:56:07,2022-03-10 23:56:07,2022-03-10 23:56:06,,,,,,,Learning Not to Learn,,,,,,,,,,,,arXiv.org,,arXiv: 1812.10352,,/Users/jacquesthibodeau/Zotero/storage/QH8ESMDV/Kim et al. - 2019 - Learning Not to Learn Training Deep Neural Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/32KDX9Q5/1812.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9H6U5FLE,journalArticle,2019,"Jiang, Heinrich; Nachum, Ofir",Identifying and Correcting Label Bias in Machine Learning,"arXiv:1901.04966 [cs, stat]",,,,http://arxiv.org/abs/1901.04966,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.",2019-01-15,2022-03-10 23:56:08,2022-03-10 23:56:08,2022-03-10 23:56:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.04966,,/Users/jacquesthibodeau/Zotero/storage/NJ7VFGBP/Jiang and Nachum - 2019 - Identifying and Correcting Label Bias in Machine L.pdf; /Users/jacquesthibodeau/Zotero/storage/LUQIBFRB/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KCGAUFB8,journalArticle,2020,"Fazelpour, Sina; Lipton, Zachary C.",Algorithmic Fairness from a Non-ideal Perspective,"arXiv:2001.09773 [cs, stat]",,,,http://arxiv.org/abs/2001.09773,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",2020-01-08,2022-03-10 23:56:11,2022-03-10 23:56:11,2022-03-10 23:56:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09773,,/Users/jacquesthibodeau/Zotero/storage/LUTZYLVM/Fazelpour and Lipton - 2020 - Algorithmic Fairness from a Non-ideal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/599X2E2M/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YVBURMER,journalArticle,2018,"Annasamy, Raghuram Mandyam; Sycara, Katia",Towards Better Interpretability in Deep Q-Networks,"arXiv:1809.05630 [cs, stat]",,,,http://arxiv.org/abs/1809.05630,"Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",2018-11-14,2022-03-10 23:59:32,2022-03-10 23:59:32,2022-03-10 23:59:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.05630,,/Users/jacquesthibodeau/Zotero/storage/QSXAIQI2/Annasamy and Sycara - 2018 - Towards Better Interpretability in Deep Q-Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/JEINU682/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8VFZBET9,journalArticle,2018,"Yeh, Chih-Kuan; Kim, Joon Sik; Yen, Ian E. H.; Ravikumar, Pradeep",Representer Point Selection for Explaining Deep Neural Networks,"arXiv:1811.09720 [cs, stat]",,,,http://arxiv.org/abs/1811.09720,"We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.",2018-11-23,2022-03-10 23:59:35,2022-03-10 23:59:35,2022-03-10 23:59:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09720,,/Users/jacquesthibodeau/Zotero/storage/UT98EUMF/Yeh et al. - 2018 - Representer Point Selection for Explaining Deep Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/MLLPF6LY/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IIVPKJE,journalArticle,2018,"Ross, Andrew Slavin",Training Machine Learning Models by Regularizing their Explanations,"arXiv:1810.00869 [cs, stat]",,,,http://arxiv.org/abs/1810.00869,"Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).",2018-09-29,2022-03-10 23:59:37,2022-03-10 23:59:37,2022-03-10 23:59:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00869,,/Users/jacquesthibodeau/Zotero/storage/4467TYAZ/Ross - 2018 - Training Machine Learning Models by Regularizing t.pdf; /Users/jacquesthibodeau/Zotero/storage/P3MJPJZ2/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KW9N2RD8,journalArticle,2018,"Yang, John; Lee, Gyujeong; Hyun, Minsung; Chang, Simyung; Kwak, Nojun",Towards Governing Agent's Efficacy: Action-Conditional $\beta$-VAE for Deep Transparent Reinforcement Learning,"arXiv:1811.04350 [cs, stat]",,,,http://arxiv.org/abs/1811.04350,"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",2018-11-10,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:33,,,,,,,Towards Governing Agent's Efficacy,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04350,,/Users/jacquesthibodeau/Zotero/storage/KW8UMGWH/Yang et al. - 2018 - Towards Governing Agent's Efficacy Action-Conditi.pdf; /Users/jacquesthibodeau/Zotero/storage/Y2FGVKGI/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPM7DBTI,journalArticle,2019,"Mittelstadt, Brent; Russell, Chris; Wachter, Sandra",Explaining Explanations in AI,"Proceedings of the Conference on Fairness, Accountability, and Transparency",,,10.1145/3287560.3287574,http://arxiv.org/abs/1811.01439,"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.",2019-01-29,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:34,279-288,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01439,,/Users/jacquesthibodeau/Zotero/storage/XA2RNLCU/Mittelstadt et al. - 2019 - Explaining Explanations in AI.pdf; /Users/jacquesthibodeau/Zotero/storage/GH95LPHE/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5TEGNSIS,journalArticle,2020,"Plumb, Gregory; Al-Shedivat, Maruan; Cabrera, Angel Alexander; Perer, Adam; Xing, Eric; Talwalkar, Ameet",Regularizing Black-box Models for Improved Interpretability,"arXiv:1902.06787 [cs, stat]",,,,http://arxiv.org/abs/1902.06787,"Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, ExpO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for ExpO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.",2020-11-08,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06787,,/Users/jacquesthibodeau/Zotero/storage/ASJ2KIDM/Plumb et al. - 2020 - Regularizing Black-box Models for Improved Interpr.pdf; /Users/jacquesthibodeau/Zotero/storage/PRXLSQKN/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U362KKGR,journalArticle,2017,"Ross, Andrew Slavin; Hughes, Michael C.; Doshi-Velez, Finale",Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"arXiv:1703.03717 [cs, stat]",,,,http://arxiv.org/abs/1703.03717,"Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",2017-05-25,2022-03-10 23:59:39,2022-03-10 23:59:39,2022-03-10 23:59:39,,,,,,,Right for the Right Reasons,,,,,,,,,,,,arXiv.org,,arXiv: 1703.03717,,/Users/jacquesthibodeau/Zotero/storage/LUFSDJPR/Ross et al. - 2017 - Right for the Right Reasons Training Differentiab.pdf; /Users/jacquesthibodeau/Zotero/storage/RUZF3ZM6/1703.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3VT773I5,journalArticle,2018,"Zhang, Quanshi; Yang, Yu; Liu, Yuchen; Wu, Ying Nian; Zhu, Song-Chun",Unsupervised Learning of Neural Networks to Explain Neural Networks,arXiv:1805.07468 [cs],,,,http://arxiv.org/abs/1805.07468,"This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.",2018-05-18,2022-03-10 23:59:42,2022-03-10 23:59:42,2022-03-10 23:59:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07468,,/Users/jacquesthibodeau/Zotero/storage/5IVJIRHP/Zhang et al. - 2018 - Unsupervised Learning of Neural Networks to Explai.pdf; /Users/jacquesthibodeau/Zotero/storage/VH3CYUM5/1805.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8KNGYUD6,journalArticle,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracing Gradient Descent,"arXiv:2002.08484 [cs, stat]",,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.",2020-11-14,2022-03-10 23:59:48,2022-03-10 23:59:48,2022-03-10 23:59:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.08484,,/Users/jacquesthibodeau/Zotero/storage/KY6XNKTY/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracing Grad.pdf; /Users/jacquesthibodeau/Zotero/storage/CTE9JBE9/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WMYZYDC6,journalArticle,2021,"Poursabzi-Sangdeh, Forough; Goldstein, Daniel G.; Hofman, Jake M.; Vaughan, Jennifer Wortman; Wallach, Hanna",Manipulating and Measuring Model Interpretability,arXiv:1802.07810 [cs],,,,http://arxiv.org/abs/1802.07810,"With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",2021-08-15,2022-03-10 23:59:52,2022-03-10 23:59:52,2022-03-10 23:59:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.07810,,/Users/jacquesthibodeau/Zotero/storage/3JZSNQ53/Poursabzi-Sangdeh et al. - 2021 - Manipulating and Measuring Model Interpretability.pdf; /Users/jacquesthibodeau/Zotero/storage/IIQ9P9V4/1802.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42E8CDCP,journalArticle,2021,"Hilgard, Sophie; Rosenfeld, Nir; Banaji, Mahzarin R.; Cao, Jack; Parkes, David C.","Learning Representations by Humans, for Humans","arXiv:1905.12686 [cs, stat]",,,,http://arxiv.org/abs/1905.12686,"When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This ""Mind Composed with Machine"" framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",2021-09-15,2022-03-11 0:00:00,2022-03-11 0:00:00,2022-03-10 23:59:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12686,,"/Users/jacquesthibodeau/Zotero/storage/RKWEUSMD/Hilgard et al. - 2021 - Learning Representations by Humans, for Humans.pdf; /Users/jacquesthibodeau/Zotero/storage/24HVBKZQ/1905.html",,,Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GAN9YD2L,journalArticle,2020,"Ghorbani, Amirata; Zou, James",Neuron Shapley: Discovering the Responsible Neurons,"arXiv:2002.09815 [cs, stat]",,,,http://arxiv.org/abs/2002.09815,"We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.",2020-11-13,2022-03-11 0:00:01,2022-03-11 0:00:01,2022-03-11 0:00:01,,,,,,,Neuron Shapley,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09815,,/Users/jacquesthibodeau/Zotero/storage/UTLNPKTB/Ghorbani and Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf; /Users/jacquesthibodeau/Zotero/storage/R7KD3Y9H/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3R6AHTBG,journalArticle,2021,"Wong, Eric; Santurkar, Shibani; Mądry, Aleksander",Leveraging Sparse Linear Layers for Debuggable Deep Networks,"arXiv:2105.04857 [cs, stat]",,,,http://arxiv.org/abs/2105.04857,"We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at https://github.com/madrylab/debuggabledeepnetworks.",2021-05-11,2022-03-11 0:00:11,2022-03-11 0:00:11,2022-03-11 0:00:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.04857,,/Users/jacquesthibodeau/Zotero/storage/ZBVZ5K79/Wong et al. - 2021 - Leveraging Sparse Linear Layers for Debuggable Dee.pdf; /Users/jacquesthibodeau/Zotero/storage/AUQFCHGA/2105.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PETL843C,journalArticle,2020,"Modhe, Nirbhay; Chattopadhyay, Prithvijit; Sharma, Mohit; Das, Abhishek; Parikh, Devi; Batra, Dhruv; Vedantam, Ramakrishna",IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL,Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,,,10.24963/ijcai.2020/280,http://arxiv.org/abs/1907.10580,"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor et.al., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.",2020-07,2022-03-11 0:00:16,2022-03-11 0:00:16,2022-03-11 0:00:16,2022-2028,,,,,,IR-VIC,,,,,,,,,,,,arXiv.org,,arXiv: 1907.10580,,/Users/jacquesthibodeau/Zotero/storage/2SM7IM2X/Modhe et al. - 2020 - IR-VIC Unsupervised Discovery of Sub-goals for Tr.pdf; /Users/jacquesthibodeau/Zotero/storage/ZV8LRYIY/1907.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJI948CT,journalArticle,2020,"Sharma, Archit; Gu, Shixiang; Levine, Sergey; Kumar, Vikash; Hausman, Karol",Dynamics-Aware Unsupervised Discovery of Skills,"arXiv:1907.01657 [cs, stat]",,,,http://arxiv.org/abs/1907.01657,"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",2020-02-14,2022-03-11 0:00:29,2022-03-11 0:00:29,2022-03-11 0:00:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.01657,,/Users/jacquesthibodeau/Zotero/storage/8J8HLLY8/Sharma et al. - 2020 - Dynamics-Aware Unsupervised Discovery of Skills.pdf; /Users/jacquesthibodeau/Zotero/storage/LF7FRGVS/1907.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MS999SGE,journalArticle,2019,"Merel, Josh; Ahuja, Arun; Pham, Vu; Tunyasuvunakool, Saran; Liu, Siqi; Tirumala, Dhruva; Heess, Nicolas; Wayne, Greg",Hierarchical visuomotor control of humanoids,arXiv:1811.09656 [cs],,,,http://arxiv.org/abs/1811.09656,"We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .",2019-01-15,2022-03-11 0:00:31,2022-03-11 0:00:31,2022-03-11 0:00:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09656,,/Users/jacquesthibodeau/Zotero/storage/8J8S7YEY/Merel et al. - 2019 - Hierarchical visuomotor control of humanoids.pdf; /Users/jacquesthibodeau/Zotero/storage/84H72A5P/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C54IQMQI,journalArticle,2019,"Osa, Takayuki; Tangkaratt, Voot; Sugiyama, Masashi",Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,"arXiv:1901.01365 [cs, stat]",,,,http://arxiv.org/abs/1901.01365,"Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.",2019-03-07,2022-03-11 0:00:33,2022-03-11 0:00:33,2022-03-11 0:00:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.01365,,/Users/jacquesthibodeau/Zotero/storage/CN58PRL7/Osa et al. - 2019 - Hierarchical Reinforcement Learning via Advantage-.pdf; /Users/jacquesthibodeau/Zotero/storage/UEGT374T/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9I3AUFA2,journalArticle,2019,"Wu, Bohan; Gupta, Jayesh K.; Kochenderfer, Mykel J.",Model Primitive Hierarchical Lifelong Reinforcement Learning,arXiv:1903.01567 [cs],,,,http://arxiv.org/abs/1903.01567,"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",2019-03-04,2022-03-11 0:00:36,2022-03-11 0:00:36,2022-03-11 0:00:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01567,,/Users/jacquesthibodeau/Zotero/storage/WNXT3R2H/Wu et al. - 2019 - Model Primitive Hierarchical Lifelong Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/2YP26PQX/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
695GMJ6V,journalArticle,2020,"Igl, Maximilian; Gambardella, Andrew; He, Jinke; Nardelli, Nantas; Siddharth, N.; Böhmer, Wendelin; Whiteson, Shimon",Multitask Soft Option Learning,"arXiv:1904.01033 [cs, stat]",,,,http://arxiv.org/abs/1904.01033,"We present Multitask Soft Option Learning(MSOL), a hierarchical multitask framework based on Planning as Inference. MSOL extends the concept of options, using separate variational posteriors for each task, regularized by a shared prior. This ''soft'' version of options avoids several instabilities during training in a multitask setting, and provides a natural way to learn both intra-option policies and their terminations. Furthermore, it allows fine-tuning of options for new tasks without forgetting their learned policies, leading to faster training without reducing the expressiveness of the hierarchical policy. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines.",2020-06-21,2022-03-11 0:00:39,2022-03-11 0:00:39,2022-03-11 0:00:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.01033,,/Users/jacquesthibodeau/Zotero/storage/ZWQ6NWCV/Igl et al. - 2020 - Multitask Soft Option Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/JJL84VCP/1904.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGBEN2DE,journalArticle,2019,"Levy, Andrew; Platt, Robert; Saenko, Kate",Hierarchical Reinforcement Learning with Hindsight,"arXiv:1805.08180 [cs, stat]",,,,http://arxiv.org/abs/1805.08180,"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.",2019-03-08,2022-03-11 0:00:41,2022-03-11 0:00:41,2022-03-11 0:00:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08180,,/Users/jacquesthibodeau/Zotero/storage/KSWE988D/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKA52ESY,journalArticle,2019,"Goyal, Anirudh; Sodhani, Shagun; Binas, Jonathan; Peng, Xue Bin; Levine, Sergey; Bengio, Yoshua",Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives,"arXiv:1906.10667 [cs, stat]",,,,http://arxiv.org/abs/1906.10667,"Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.",2019-06-25,2022-03-11 0:00:44,2022-03-11 0:00:44,2022-03-11 0:00:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10667,,/Users/jacquesthibodeau/Zotero/storage/CACZBKN2/Goyal et al. - 2019 - Reinforcement Learning with Competitive Ensembles .pdf; /Users/jacquesthibodeau/Zotero/storage/YNDP5B2S/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XL394W8S,journalArticle,2019,"Baumann, Tobias; Graepel, Thore; Shawe-Taylor, John",Adaptive Mechanism Design: Learning to Promote Cooperation,arXiv:1806.04067 [cs],,,,http://arxiv.org/abs/1806.04067,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.",2019-11-20,2022-03-11 0:00:48,2022-03-11 0:00:48,2022-03-11 0:00:47,,,,,,,Adaptive Mechanism Design,,,,,,,,,,,,arXiv.org,,arXiv: 1806.04067,,/Users/jacquesthibodeau/Zotero/storage/5KL5REW3/Baumann et al. - 2019 - Adaptive Mechanism Design Learning to Promote Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/Q47ZCUHU/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM7X35JM,journalArticle,2018,"Cao, Kris; Lazaridou, Angeliki; Lanctot, Marc; Leibo, Joel Z.; Tuyls, Karl; Clark, Stephen",Emergent Communication through Negotiation,arXiv:1804.03980 [cs],,,,http://arxiv.org/abs/1804.03980,"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols -- one grounded in the semantics of the game, and one which is \textit{a priori} ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.",2018-04-11,2022-03-11 0:00:50,2022-03-11 0:00:50,2022-03-11 0:00:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.03980,,/Users/jacquesthibodeau/Zotero/storage/E2MY25ZK/Cao et al. - 2018 - Emergent Communication through Negotiation.pdf; /Users/jacquesthibodeau/Zotero/storage/BNE4HTTC/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZI5T2IHL,journalArticle,2019,"Zhan, Eric; Zheng, Stephan; Yue, Yisong; Sha, Long; Lucey, Patrick",Generating Multi-Agent Trajectories using Programmatic Weak Supervision,"arXiv:1803.07612 [cs, stat]",,,,http://arxiv.org/abs/1803.07612,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.",2019-02-22,2022-03-11 0:00:53,2022-03-11 0:00:53,2022-03-11 0:00:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.07612,,/Users/jacquesthibodeau/Zotero/storage/W4JRS7FL/Zhan et al. - 2019 - Generating Multi-Agent Trajectories using Programm.pdf; /Users/jacquesthibodeau/Zotero/storage/IH3AC9ES/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2HBSFRC,journalArticle,2018,"Bobu, Andreea; Bajcsy, Andrea; Fisac, Jaime F.; Dragan, Anca D.",Learning under Misspecified Objective Spaces,"arXiv:1810.05157 [cs, stat]",,,,http://arxiv.org/abs/1810.05157,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator.",2018-10-26,2022-03-11 0:04:10,2022-03-11 0:04:10,2022-03-11 0:04:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.05157,,/Users/jacquesthibodeau/Zotero/storage/QBEPYDZB/Bobu et al. - 2018 - Learning under Misspecified Objective Spaces.pdf; /Users/jacquesthibodeau/Zotero/storage/RVGYEV9B/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J6AK7JCQ,journalArticle,2019,"Chevalier-Boisvert, Maxime; Bahdanau, Dzmitry; Lahlou, Salem; Willems, Lucas; Saharia, Chitwan; Nguyen, Thien Huu; Bengio, Yoshua",BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,arXiv:1810.08272 [cs],,,,http://arxiv.org/abs/1810.08272,"Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.",2019-12-19,2022-03-11 0:04:13,2022-03-11 0:04:13,2022-03-11 0:04:13,,,,,,,BabyAI,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08272,,/Users/jacquesthibodeau/Zotero/storage/DY53NJLL/Chevalier-Boisvert et al. - 2019 - BabyAI A Platform to Study the Sample Efficiency .pdf; /Users/jacquesthibodeau/Zotero/storage/DGP6ISHI/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSJHPY6U,journalArticle,2019,"Co-Reyes, John D.; Gupta, Abhishek; Sanjeev, Suvansh; Altieri, Nick; Andreas, Jacob; DeNero, John; Abbeel, Pieter; Levine, Sergey",Guiding Policies with Language via Meta-Learning,arXiv:1811.07882 [cs],,,,http://arxiv.org/abs/1811.07882,"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",2019-01-29,2022-03-11 0:04:15,2022-03-11 0:04:15,2022-03-11 0:04:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07882,,/Users/jacquesthibodeau/Zotero/storage/ZGLA7ZB7/Co-Reyes et al. - 2019 - Guiding Policies with Language via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/CG3EWGJN/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZYJ5LVA9,journalArticle,2020,"Peng, Xue Bin; Kanazawa, Angjoo; Toyer, Sam; Abbeel, Pieter; Levine, Sergey","Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow","arXiv:1810.00821 [cs, stat]",,,,http://arxiv.org/abs/1810.00821,"Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",2020-08-24,2022-03-11 0:04:17,2022-03-11 0:04:17,2022-03-11 0:04:17,,,,,,,Variational Discriminator Bottleneck,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00821,,/Users/jacquesthibodeau/Zotero/storage/A2QICRFQ/Peng et al. - 2020 - Variational Discriminator Bottleneck Improving Im.pdf; /Users/jacquesthibodeau/Zotero/storage/Z5KPFJX4/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XJX26EQ,journalArticle,2019,"Yu, Tianhe; Shevchuk, Gleb; Sadigh, Dorsa; Finn, Chelsea",Unsupervised Visuomotor Control through Distributional Planning Networks,"arXiv:1902.05542 [cs, stat]",,,,http://arxiv.org/abs/1902.05542,"While reinforcement learning (RL) has the potential to enable robots to autonomously acquire a wide range of skills, in practice, RL usually requires manual, per-task engineering of reward functions, especially in real world settings where aspects of the environment needed to compute progress are not directly accessible. To enable robots to autonomously learn skills, we instead consider the problem of reinforcement learning without access to rewards. We aim to learn an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our approach explicitly optimizes for a metric space under which action sequences that reach a particular state are optimal when the goal is the final state reached. This enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms. Our experiments on three simulated environments and two real-world manipulation problems show that our method can learn effective goal metrics from unlabeled interaction, and use the learned goal metrics for autonomous reinforcement learning.",2019-02-14,2022-03-11 0:04:19,2022-03-11 0:04:19,2022-03-11 0:04:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.05542,,/Users/jacquesthibodeau/Zotero/storage/CPVRKPC4/Yu et al. - 2019 - Unsupervised Visuomotor Control through Distributi.pdf; /Users/jacquesthibodeau/Zotero/storage/TGGF8UWW/1902.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S3LPQXT,journalArticle,2019,"Milli, Smitha; Dragan, Anca D.",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,arXiv:1903.03877 [cs],,,,http://arxiv.org/abs/1903.03877,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (""robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",2019-06-28,2022-03-11 0:04:21,2022-03-11 0:04:21,2022-03-11 0:04:21,,,,,,,Literal or Pedagogic Human?,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03877,,/Users/jacquesthibodeau/Zotero/storage/IJYXDX3K/Milli and Dragan - 2019 - Literal or Pedagogic Human Analyzing Human Model .pdf; /Users/jacquesthibodeau/Zotero/storage/KUMDIKJ8/1903.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EVSXTIBW,journalArticle,2019,"Arumugam, Dilip; Lee, Jun Ki; Saskin, Sophie; Littman, Michael L.",Deep Reinforcement Learning from Policy-Dependent Human Feedback,"arXiv:1902.04257 [cs, stat]",,,,http://arxiv.org/abs/1902.04257,"To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",2019-02-12,2022-03-11 0:04:24,2022-03-11 0:04:24,2022-03-11 0:04:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.04257,,/Users/jacquesthibodeau/Zotero/storage/G2VKKUDZ/Arumugam et al. - 2019 - Deep Reinforcement Learning from Policy-Dependent .pdf; /Users/jacquesthibodeau/Zotero/storage/F5UWFQAB/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCNPV2LK,journalArticle,2019,"Brown, Daniel S.; Goo, Wonjoon; Nagarajan, Prabhat; Niekum, Scott",Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,"arXiv:1904.06387 [cs, stat]",,,,http://arxiv.org/abs/1904.06387,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",2019-07-08,2022-03-11 0:04:35,2022-03-11 0:04:35,2022-03-11 0:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.06387,,/Users/jacquesthibodeau/Zotero/storage/M2TSN68L/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf; /Users/jacquesthibodeau/Zotero/storage/4X8966I4/1904.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5B4VDQ2W,journalArticle,2020,"Ke, Liyiming; Choudhury, Sanjiban; Barnes, Matt; Sun, Wen; Lee, Gilwoo; Srinivasa, Siddhartha",Imitation Learning as $f$-Divergence Minimization,"arXiv:1905.12888 [cs, math, stat]",,,,http://arxiv.org/abs/1905.12888,"We address the problem of imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argue that in many tasks it is sufficient to imitate any one of them. We show that the state-of-the-art methods such as GAIL and behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight is to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We propose a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we are able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results show that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.",2020-05-31,2022-03-11 0:04:35,2022-03-11 0:04:35,2022-03-11 0:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12888,,/Users/jacquesthibodeau/Zotero/storage/K3LHVVMH/Ke et al. - 2020 - Imitation Learning as $f$-Divergence Minimization.pdf; /Users/jacquesthibodeau/Zotero/storage/JWQGEIQS/1905.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKDNWQAL,journalArticle,2019,"Xu, Danfei; Denil, Misha",Positive-Unlabeled Reward Learning,"arXiv:1911.00459 [cs, stat]",,,,http://arxiv.org/abs/1911.00459,"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",2019-11-01,2022-03-11 0:04:36,2022-03-11 0:04:36,2022-03-11 0:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.00459,,/Users/jacquesthibodeau/Zotero/storage/FCF2YMHI/Xu and Denil - 2019 - Positive-Unlabeled Reward Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/97WWEK2F/1911.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SJTECJE2,journalArticle,2019,"Brown, Daniel S.; Goo, Wonjoon; Niekum, Scott",Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations,"arXiv:1907.03976 [cs, stat]",,,,http://arxiv.org/abs/1907.03976,"The performance of imitation learning is typically upper-bounded by the performance of the demonstrator. While recent empirical results demonstrate that ranked demonstrations allow for better-than-demonstrator performance, preferences over demonstrations may be difficult to obtain, and little is known theoretically about when such methods can be expected to successfully extrapolate beyond the performance of the demonstrator. To address these issues, we first contribute a sufficient condition for better-than-demonstrator imitation learning and provide theoretical results showing why preferences over demonstrations can better reduce reward function ambiguity when performing inverse reinforcement learning. Building on this theory, we introduce Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning method that injects noise into a policy learned through behavioral cloning to automatically generate ranked demonstrations. These ranked demonstrations are used to efficiently learn a reward function that can then be optimized using reinforcement learning. We empirically validate our approach on simulated robot and Atari imitation learning benchmarks and show that D-REX outperforms standard imitation learning approaches and can significantly surpass the performance of the demonstrator. D-REX is the first imitation learning approach to achieve significant extrapolation beyond the demonstrator's performance without additional side-information or supervision, such as rewards or human preferences. By generating rankings automatically, we show that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",2019-10-14,2022-03-11 0:04:36,2022-03-11 0:04:36,2022-03-11 0:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.03976,,/Users/jacquesthibodeau/Zotero/storage/JJG2UYZC/Brown et al. - 2019 - Better-than-Demonstrator Imitation Learning via Au.pdf; /Users/jacquesthibodeau/Zotero/storage/WIKH9Y5L/1907.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9XK3US9W,journalArticle,2018,"Warnell, Garrett; Waytowich, Nicholas; Lawhern, Vernon; Stone, Peter",Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces,arXiv:1709.10163 [cs],,,,http://arxiv.org/abs/1709.10163,"While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.",2018-01-19,2022-03-11 0:04:55,2022-03-11 0:04:55,2022-03-11 0:04:55,,,,,,,Deep TAMER,,,,,,,,,,,,arXiv.org,,arXiv: 1709.10163,,/Users/jacquesthibodeau/Zotero/storage/KFS9J423/Warnell et al. - 2018 - Deep TAMER Interactive Agent Shaping in High-Dime.pdf; /Users/jacquesthibodeau/Zotero/storage/EZZ2QAIV/1709.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UB98BTXM,journalArticle,2017,"MacGlashan, James; Ho, Mark K.; Loftin, Robert; Peng, Bei; Roberts, David; Taylor, Matthew E.; Littman, Michael L.",Interactive Learning from Policy-Dependent Human Feedback,arXiv:1701.06049 [cs],,,,http://arxiv.org/abs/1701.06049,"For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false---whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.",2017-01-21,2022-03-11 0:04:57,2022-03-11 0:04:57,2022-03-11 0:04:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1701.06049,,/Users/jacquesthibodeau/Zotero/storage/UGK5A8CQ/MacGlashan et al. - 2017 - Interactive Learning from Policy-Dependent Human F.pdf; /Users/jacquesthibodeau/Zotero/storage/JNFKWSYF/1701.html,,,Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z7GHCDU3,journalArticle,2018,"Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario",Reward learning from human preferences and demonstrations in Atari,"arXiv:1811.06521 [cs, stat]",,,,http://arxiv.org/abs/1811.06521,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",2018-11-15,2022-03-11 0:04:59,2022-03-11 0:04:59,2022-03-11 0:04:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.06521,,/Users/jacquesthibodeau/Zotero/storage/9QDHZKM9/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/9RUG6HJD/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FZZXBWL2,journalArticle,2021,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan",Learning Human Objectives by Evaluating Hypothetical Behavior,"arXiv:1912.05652 [cs, stat]",,,,http://arxiv.org/abs/1912.05652,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",2021-03-24,2022-03-11 0:05:01,2022-03-11 1:38:25,2022-03-11 0:05:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.05652,,/Users/jacquesthibodeau/Zotero/storage/W5NX3B7V/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/RVWLACVB/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/3I2JDXY9/1912.html; /Users/jacquesthibodeau/Zotero/storage/4B542U2D/1912.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U79SKTKT,journalArticle,2015,"Javdani, Shervin; Srinivasa, Siddhartha S.; Bagnell, J. Andrew",Shared Autonomy via Hindsight Optimization,arXiv:1503.07619 [cs],,,,http://arxiv.org/abs/1503.07619,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",2015-04-17,2022-03-11 0:05:04,2022-03-11 0:05:04,2022-03-11 0:05:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1503.07619,,/Users/jacquesthibodeau/Zotero/storage/SU3VLID5/Javdani et al. - 2015 - Shared Autonomy via Hindsight Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/EXWK6RUB/1503.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QDYV7XI,journalArticle,2020,"Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane",Pitfalls of learning a reward function online,arXiv:2004.13654 [cs],,,,http://arxiv.org/abs/2004.13654,"In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.",2020-04-28,2022-03-11 0:05:06,2022-03-11 0:05:06,2022-03-11 0:05:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.13654,,/Users/jacquesthibodeau/Zotero/storage/VUG3B7N5/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/UTVE8ETZ/2004.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7HJTT8R4,journalArticle,2021,"Lynch, Corey; Sermanet, Pierre",Language Conditioned Imitation Learning over Unstructured Data,arXiv:2005.07648 [cs],,,,http://arxiv.org/abs/2005.07648,"Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with a task id or goal image -- something that is often impractical in open-world environments. On the other hand, previous approaches in instruction following allow agent behavior to be guided by language, but typically assume structure in the observations, actuators, or language that limit their applicability to complex settings like robotics. In this work, we present a method for incorporating free-form natural language conditioning into imitation learning. Our approach learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network. Unlike prior work in imitation learning, our method is able to incorporate unlabeled and unstructured demonstration data (i.e. no task or language labels). We show this dramatically improves language conditioned performance, while reducing the cost of language annotation to less than 1% of total data. At test time, a single language conditioned visuomotor policy trained with our method can perform a wide variety of robotic manipulation skills in a 3D environment, specified only with natural language descriptions of each task (e.g. ""open the drawer...now pick up the block...now press the green button...""). To scale up the number of instructions an agent can follow, we propose combining text conditioned policies with large pretrained neural language models. We find this allows a policy to be robust to many out-of-distribution synonym instructions, without requiring new demonstrations. See videos of a human typing live text commands to our agent at language-play.github.io",2021-07-07,2022-03-11 0:05:24,2022-03-11 0:05:24,2022-03-11 0:05:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.07648,,/Users/jacquesthibodeau/Zotero/storage/Y8DZNKMW/Lynch and Sermanet - 2021 - Language Conditioned Imitation Learning over Unstr.pdf; /Users/jacquesthibodeau/Zotero/storage/2Y8VWVWI/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7LXTN6NY,journalArticle,2020,"Hill, Felix; Mokra, Sona; Wong, Nathaniel; Harley, Tim",Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,arXiv:2005.09382 [cs],,,,http://arxiv.org/abs/2005.09382,"Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",2020-05-19,2022-03-11 0:05:26,2022-03-11 0:05:26,2022-03-11 0:05:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.09382,,/Users/jacquesthibodeau/Zotero/storage/F4XVCPYC/Hill et al. - 2020 - Human Instruction-Following with Deep Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/EZ576CQ3/2005.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BSR77V4,journalArticle,2021,"Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan",Quantifying Differences in Reward Functions,"arXiv:2006.13900 [cs, stat]",,,,http://arxiv.org/abs/2006.13900,"For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards.",2021-03-17,2022-03-11 0:05:28,2022-03-11 0:05:28,2022-03-11 0:05:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13900,,/Users/jacquesthibodeau/Zotero/storage/5D274AIX/Gleave et al. - 2021 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/7QT547WD/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FFS8VH5C,journalArticle,2021,"Bobu, Andreea; Wiggert, Marius; Tomlin, Claire; Dragan, Anca D.",Feature Expansive Reward Learning: Rethinking Human Input,Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,10.1145/3434073.3444667,http://arxiv.org/abs/2006.13208,"When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.",2021-03-08,2022-03-11 0:05:33,2022-03-11 0:05:33,2022-03-11 0:05:33,216-224,,,,,,Feature Expansive Reward Learning,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13208,,/Users/jacquesthibodeau/Zotero/storage/IUBKLWZG/Bobu et al. - 2021 - Feature Expansive Reward Learning Rethinking Huma.pdf; /Users/jacquesthibodeau/Zotero/storage/DN6AGS7C/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ZM6YULI,journalArticle,2019,"Rhinehart, Nicholas; McAllister, Rowan; Levine, Sergey","Deep Imitative Models for Flexible Inference, Planning, and Control","arXiv:1810.06544 [cs, stat]",,,,http://arxiv.org/abs/1810.06544,"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road.",2019-09-30,2022-03-11 0:05:35,2022-03-11 0:05:35,2022-03-11 0:05:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.06544,,"/Users/jacquesthibodeau/Zotero/storage/QEVSDEDH/Rhinehart et al. - 2019 - Deep Imitative Models for Flexible Inference, Plan.pdf; /Users/jacquesthibodeau/Zotero/storage/GIHT83S4/1810.html",,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQ2J89WY,journalArticle,2020,"Swamy, Gokul; Schulz, Jens; Choudhury, Rohan; Hadfield-Menell, Dylan; Dragan, Anca",On the Utility of Model Learning in HRI,"arXiv:1901.01291 [cs, stat]",,,,http://arxiv.org/abs/1901.01291,"Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a ""theory of mind"" to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods for an autonomous driving task under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot's theory of mind for the human are wrong, as they inevitably will be in practice.",2020-05-21,2022-03-11 0:05:37,2022-03-11 0:05:37,2022-03-11 0:05:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.01291,,/Users/jacquesthibodeau/Zotero/storage/G44U74VG/Swamy et al. - 2020 - On the Utility of Model Learning in HRI.pdf; /Users/jacquesthibodeau/Zotero/storage/YGSF3IJ7/1901.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PD9H6GNQ,journalArticle,2018,"Wang, Xin; Chen, Wenhu; Wang, Yuan-Fang; Wang, William Yang",No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,arXiv:1804.09160 [cs],,,,http://arxiv.org/abs/1804.09160,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",2018-07-08,2022-03-11 0:05:39,2022-03-11 0:05:39,2022-03-11 0:05:39,,,,,,,No Metrics Are Perfect,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09160,,/Users/jacquesthibodeau/Zotero/storage/RRI4D35P/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/LHRVXR7E/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28Z838EQ,journalArticle,2019,"Woodward, Mark; Finn, Chelsea; Hausman, Karol",Learning to Interactively Learn and Assist,arXiv:1906.10187 [cs],,,,http://arxiv.org/abs/1906.10187,"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.",2019-11-19,2022-03-11 0:05:56,2022-03-11 0:05:56,2022-03-11 0:05:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10187,,/Users/jacquesthibodeau/Zotero/storage/RUACYQZ7/Woodward et al. - 2019 - Learning to Interactively Learn and Assist.pdf; /Users/jacquesthibodeau/Zotero/storage/PCUYVP4Z/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A222AIBT,journalArticle,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"arXiv:1805.08010 [cs, stat]",,,,http://arxiv.org/abs/1805.08010,"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",2019-01-05,2022-03-11 0:05:58,2022-03-11 0:05:58,2022-03-11 0:05:57,,,,,,,Where Do You Think You're Going?,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08010,,/Users/jacquesthibodeau/Zotero/storage/P554XAWK/Reddy et al. - 2019 - Where Do You Think You're Going Inferring Belief.pdf; /Users/jacquesthibodeau/Zotero/storage/CBATLDJS/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D6TU5FZV,journalArticle,2019,"Perez, Ethan; Karamcheti, Siddharth; Fergus, Rob; Weston, Jason; Kiela, Douwe; Cho, Kyunghyun",Finding Generalizable Evidence by Learning to Convince Q&A Models,arXiv:1909.05863 [cs],,,,http://arxiv.org/abs/1909.05863,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",2019-09-12,2022-03-11 0:06:03,2022-03-11 0:06:03,2022-03-11 0:06:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.05863,,/Users/jacquesthibodeau/Zotero/storage/TDRGQLPA/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf; /Users/jacquesthibodeau/Zotero/storage/Z7REYBD6/1909.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2QJBG3W,journalArticle,2018,"Irving, Geoffrey; Christiano, Paul; Amodei, Dario",AI safety via debate,"arXiv:1805.00899 [cs, stat]",,,,http://arxiv.org/abs/1805.00899,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",2018-10-22,2022-03-11 0:06:23,2022-03-11 0:06:23,2022-03-11 0:06:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.00899,,/Users/jacquesthibodeau/Zotero/storage/JQH8VEZ6/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/RVH3AFRF/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKYNN4QF,journalArticle,2021,"Kovařík, Vojtěch; Carey, Ryan",(When) Is Truth-telling Favored in AI Debate?,arXiv:1911.04266 [cs],,,,http://arxiv.org/abs/1911.04266,"For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.",2021-03-16,2022-03-11 0:06:26,2022-03-11 0:06:26,2022-03-11 0:06:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.04266,,/Users/jacquesthibodeau/Zotero/storage/BQ8UNV7R/Kovařík and Carey - 2021 - (When) Is Truth-telling Favored in AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/CVLA9QMS/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEYVH9F5,journalArticle,2020,"Perez, Ethan; Lewis, Patrick; Yih, Wen-tau; Cho, Kyunghyun; Kiela, Douwe",Unsupervised Question Decomposition for Question Answering,arXiv:2002.09758 [cs],,,,http://arxiv.org/abs/2002.09758,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.",2020-10-06,2022-03-11 0:06:29,2022-03-11 0:06:29,2022-03-11 0:06:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09758,,/Users/jacquesthibodeau/Zotero/storage/MZCSI8KF/Perez et al. - 2020 - Unsupervised Question Decomposition for Question A.pdf; /Users/jacquesthibodeau/Zotero/storage/32F7CJGY/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZCCMWPQX,journalArticle,2021,"Perez, Ethan; Kiela, Douwe; Cho, Kyunghyun",Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,"arXiv:2103.03872 [cs, stat]",,,,http://arxiv.org/abs/2103.03872,"We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.",2021-03-05,2022-03-11 0:06:32,2022-03-11 0:06:32,2022-03-11 0:06:31,,,,,,,Rissanen Data Analysis,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03872,,/Users/jacquesthibodeau/Zotero/storage/KBEK28YX/Perez et al. - 2021 - Rissanen Data Analysis Examining Dataset Characte.pdf; /Users/jacquesthibodeau/Zotero/storage/ER72NW7P/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84S7DU6T,journalArticle,2018,"Wagstaff, Kiri L.; Lee, Jake",Interpretable Discovery in Large Image Data Sets,"arXiv:1806.08340 [cs, stat]",,,,http://arxiv.org/abs/1806.08340,"Automated detection of new, interesting, unusual, or anomalous images within large data sets has great value for applications from surveillance (e.g., airport security) to science (observations that don't fit a given theory can lead to new discoveries). Many image data analysis systems are turning to convolutional neural networks (CNNs) to represent image content due to their success in achieving high classification accuracy rates. However, CNN representations are notoriously difficult for humans to interpret. We describe a new strategy that combines novelty detection with CNN image features to achieve rapid discovery with interpretable explanations of novel image content. We applied this technique to familiar images from ImageNet as well as to a scientific image collection from planetary science.",2018-06-21,2022-03-11 0:06:44,2022-03-11 0:06:44,2022-03-11 0:06:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.08340,,/Users/jacquesthibodeau/Zotero/storage/3QAIRJS9/Wagstaff and Lee - 2018 - Interpretable Discovery in Large Image Data Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/HWSNIZHR/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WCMPZI9A,journalArticle,2018,"Brown, Alexander; Petrik, Marek",Interpretable Reinforcement Learning with Ensemble Methods,"arXiv:1809.06995 [cs, stat]",,,,http://arxiv.org/abs/1809.06995,"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",2018-09-18,2022-03-11 0:06:47,2022-03-11 0:06:47,2022-03-11 0:06:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.06995,,/Users/jacquesthibodeau/Zotero/storage/NJNRSBJW/Brown and Petrik - 2018 - Interpretable Reinforcement Learning with Ensemble.pdf; /Users/jacquesthibodeau/Zotero/storage/7R5ZDJNH/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MC6XR8A9,journalArticle,2018,"Papernot, Nicolas; McDaniel, Patrick","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning","arXiv:1803.04765 [cs, stat]",,,,http://arxiv.org/abs/1803.04765,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.",2018-03-13,2022-03-11 0:06:49,2022-03-11 0:06:49,2022-03-11 0:06:49,,,,,,,Deep k-Nearest Neighbors,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04765,,"/Users/jacquesthibodeau/Zotero/storage/RJYVXNTF/Papernot and McDaniel - 2018 - Deep k-Nearest Neighbors Towards Confident, Inter.pdf; /Users/jacquesthibodeau/Zotero/storage/V3W3Q4DX/1803.html",,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AR2YI7ZB,journalArticle,2018,"Preece, Alun; Harborne, Dan; Braines, Dave; Tomsett, Richard; Chakraborty, Supriyo",Stakeholders in Explainable AI,arXiv:1810.00184 [cs],,,,http://arxiv.org/abs/1810.00184,"There is general consensus that it is important for artificial intelligence (AI) and machine learning systems to be explainable and/or interpretable. However, there is no general consensus over what is meant by 'explainable' and 'interpretable'. In this paper, we argue that this lack of consensus is due to there being several distinct stakeholder communities. We note that, while the concerns of the individual communities are broadly compatible, they are not identical, which gives rise to different intents and requirements for explainability/interpretability. We use the software engineering distinction between validation and verification, and the epistemological distinctions between knowns/unknowns, to tease apart the concerns of the stakeholder communities and highlight the areas where their foci overlap or diverge. It is not the purpose of the authors of this paper to 'take sides' - we count ourselves as members, to varying degrees, of multiple communities - but rather to help disambiguate what stakeholders mean when they ask 'Why?' of an AI.",2018-09-29,2022-03-11 0:06:52,2022-03-11 0:06:52,2022-03-11 0:06:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00184,,/Users/jacquesthibodeau/Zotero/storage/DPMXNX87/Preece et al. - 2018 - Stakeholders in Explainable AI.pdf; /Users/jacquesthibodeau/Zotero/storage/I48XICL6/1810.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMAS3LL4,journalArticle,2019,"Jain, Sarthak; Wallace, Byron C.",Attention is not Explanation,arXiv:1902.10186 [cs],,,,http://arxiv.org/abs/1902.10186,"Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.",2019-05-08,2022-03-11 0:07:02,2022-03-11 0:07:02,2022-03-11 0:07:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.10186,,/Users/jacquesthibodeau/Zotero/storage/CPVGIC25/Jain and Wallace - 2019 - Attention is not Explanation.pdf; /Users/jacquesthibodeau/Zotero/storage/7XYFBN47/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N8HZQWPQ,journalArticle,2020,"Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been",Sanity Checks for Saliency Maps,"arXiv:1810.03292 [cs, stat]",,,,http://arxiv.org/abs/1810.03292,"Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.",2020-11-06,2022-03-11 0:07:02,2022-03-11 0:07:02,2022-03-11 0:07:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.03292,,/Users/jacquesthibodeau/Zotero/storage/WGKXZYE9/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf; /Users/jacquesthibodeau/Zotero/storage/468MMA7A/1810.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JU7ZW94W,journalArticle,2019,"Fuchs, Fabian B.; Groth, Oliver; Kosiorek, Adam R.; Bewley, Alex; Wulfmeier, Markus; Vedaldi, Andrea; Posner, Ingmar",Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes,"arXiv:1806.05502 [cs, stat]",,,,http://arxiv.org/abs/1806.05502,"Visually predicting the stability of block towers is a popular task in the domain of intuitive physics. While previous work focusses on prediction accuracy, a one-dimensional performance measure, we provide a broader analysis of the learned physical understanding of the final model and how the learning process can be guided. To this end, we introduce neural stethoscopes as a general purpose framework for quantifying the degree of importance of specific factors of influence in deep neural networks as well as for actively promoting and suppressing information as appropriate. In doing so, we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We apply neural stethoscopes to analyse the state-of-the-art neural network for stability prediction. We show that the baseline model is susceptible to being misled by incorrect visual cues. This leads to a performance breakdown to the level of random guessing when training on scenarios where visual cues are inversely correlated with stability. Using stethoscopes to promote meaningful feature extraction increases performance from 51% to 90% prediction accuracy. Conversely, training on an easy dataset where visual cues are positively correlated with stability, the baseline model learns a bias leading to poor performance on a harder dataset. Using an adversarial stethoscope, the network is successfully de-biased, leading to a performance increase from 66% to 88%.",2019-09-06,2022-03-11 0:07:03,2022-03-11 0:07:03,2022-03-11 0:07:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05502,,/Users/jacquesthibodeau/Zotero/storage/I4MIQ4MC/Fuchs et al. - 2019 - Scrutinizing and De-Biasing Intuitive Physics with.pdf; /Users/jacquesthibodeau/Zotero/storage/46IPAJ5C/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A9R4Y5CD,journalArticle,2019,"Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana",Explaining Explanations: An Overview of Interpretability of Machine Learning,"arXiv:1806.00069 [cs, stat]",,,,http://arxiv.org/abs/1806.00069,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",2019-02-03,2022-03-11 0:07:07,2022-03-11 0:07:07,2022-03-11 0:07:07,,,,,,,Explaining Explanations,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00069,,/Users/jacquesthibodeau/Zotero/storage/R5WBGP2Q/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf; /Users/jacquesthibodeau/Zotero/storage/LM4PFV2M/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V6PF8CJ2,journalArticle,2021,"Déletang, Grégoire; Grau-Moya, Jordi; Martic, Miljan; Genewein, Tim; McGrath, Tom; Mikulik, Vladimir; Kunesch, Markus; Legg, Shane; Ortega, Pedro A.",Causal Analysis of Agent Behavior for AI Safety,arXiv:2103.03938 [cs],,,,http://arxiv.org/abs/2103.03938,"As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.",2021-03-05,2022-03-11 0:07:17,2022-03-11 0:07:17,2022-03-11 0:07:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03938,,/Users/jacquesthibodeau/Zotero/storage/DH69WVLP/Déletang et al. - 2021 - Causal Analysis of Agent Behavior for AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/LMQXFRKT/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VUK3E2UM,journalArticle,2019,"Mindermann, Sören; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan",Active Inverse Reward Design,"arXiv:1809.03060 [cs, stat]",,,,http://arxiv.org/abs/1809.03060,"Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",2019-11-06,2022-03-11 0:10:09,2022-03-11 0:10:09,2022-03-11 0:10:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03060,,/Users/jacquesthibodeau/Zotero/storage/KU4EL7EE/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/QMDKRICM/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AIYWN2BB,journalArticle,2018,"Xie, Annie; Singh, Avi; Levine, Sergey; Finn, Chelsea",Few-Shot Goal Inference for Visuomotor Learning and Planning,"arXiv:1810.00482 [cs, stat]",,,,http://arxiv.org/abs/1810.00482,"Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.",2018-09-30,2022-03-11 0:10:10,2022-03-11 0:10:10,2022-03-11 0:10:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00482,,/Users/jacquesthibodeau/Zotero/storage/GMXYB7BF/Xie et al. - 2018 - Few-Shot Goal Inference for Visuomotor Learning an.pdf; /Users/jacquesthibodeau/Zotero/storage/3P9SGNTU/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YGKAJXS5,journalArticle,2018,"Kostrikov, Ilya; Agrawal, Kumar Krishna; Dwibedi, Debidatta; Levine, Sergey; Tompson, Jonathan",Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,"arXiv:1809.02925 [cs, stat]",,,,http://arxiv.org/abs/1809.02925,"We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",2018-10-15,2022-03-11 0:10:12,2022-03-11 0:10:12,2022-03-11 0:10:12,,,,,,,Discriminator-Actor-Critic,,,,,,,,,,,,arXiv.org,,arXiv: 1809.02925,,/Users/jacquesthibodeau/Zotero/storage/QSVHPY2N/Kostrikov et al. - 2018 - Discriminator-Actor-Critic Addressing Sample Inef.pdf; /Users/jacquesthibodeau/Zotero/storage/XKD9T8VY/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JRKQE3RX,journalArticle,2018,"Yu, Tianhe; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea",One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks,"arXiv:1810.11043 [cs, stat]",,,,http://arxiv.org/abs/1810.11043,"We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by ""watching"" a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs.",2018-10-25,2022-03-11 0:10:14,2022-03-11 0:10:14,2022-03-11 0:10:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11043,,/Users/jacquesthibodeau/Zotero/storage/VC8LZ59T/Yu et al. - 2018 - One-Shot Hierarchical Imitation Learning of Compou.pdf; /Users/jacquesthibodeau/Zotero/storage/23QGMQJC/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YUN7N5ST,journalArticle,2019,"Behbahani, Feryal; Shiarlis, Kyriacos; Chen, Xi; Kurin, Vitaly; Kasewa, Sudhanshu; Stirbu, Ciprian; Gomes, João; Paul, Supratik; Oliehoek, Frans A.; Messias, João; Whiteson, Shimon",Learning from Demonstration in the Wild,"arXiv:1811.03516 [cs, stat]",,,,http://arxiv.org/abs/1811.03516,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.",2019-03-25,2022-03-11 0:10:16,2022-03-11 0:10:16,2022-03-11 0:10:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03516,,/Users/jacquesthibodeau/Zotero/storage/DUMMU8DE/Behbahani et al. - 2019 - Learning from Demonstration in the Wild.pdf; /Users/jacquesthibodeau/Zotero/storage/MAJVRBGG/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAVWIY5J,journalArticle,2018,"Pereira, Ramon Fraga; Meneguzzi, Felipe",Heuristic Approaches for Goal Recognition in Incomplete Domain Models,arXiv:1804.05917 [cs],,,,http://arxiv.org/abs/1804.05917,"Recent approaches to goal recognition have progressively relaxed the assumptions about the amount and correctness of domain knowledge and available observations, yielding accurate and efficient algorithms. These approaches, however, assume completeness and correctness of the domain theory against which their algorithms match observations: this is too strong for most real-world domains. In this paper, we develop goal recognition techniques that are capable of recognizing goals using \textit{incomplete} (and possibly incorrect) domain theories. We show the efficiency and accuracy of our approaches empirically against a large dataset of goal and plan recognition problems with incomplete domains.",2018-04-16,2022-03-11 0:10:21,2022-03-11 0:10:21,2022-03-11 0:10:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05917,,/Users/jacquesthibodeau/Zotero/storage/KYUDUWV9/Pereira and Meneguzzi - 2018 - Heuristic Approaches for Goal Recognition in Incom.pdf; /Users/jacquesthibodeau/Zotero/storage/23GREC2E/1804.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5YHSY828,journalArticle,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,"arXiv:1905.11108 [cs, stat]",,,,http://arxiv.org/abs/1905.11108,"Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.",2019-09-25,2022-03-11 0:10:31,2022-03-11 0:10:31,2022-03-11 0:10:31,,,,,,,SQIL,,,,,,,,,,,,arXiv.org,,arXiv: 1905.11108,,/Users/jacquesthibodeau/Zotero/storage/FA9668CI/Reddy et al. - 2019 - SQIL Imitation Learning via Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/KWRQSXZM/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V8IRKMEU,journalArticle,2018,"Carey, Ryan",Incorrigibility in the CIRL Framework,arXiv:1709.06275 [cs],,,,http://arxiv.org/abs/1709.06275,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",2018-06-03,2022-03-11 0:10:35,2022-03-11 0:10:35,2022-03-11 0:10:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1709.06275,,/Users/jacquesthibodeau/Zotero/storage/ETZHNHZN/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/M2YW8ZFS/1709.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZ33MT8Q,journalArticle,2019,"Gandhi, Sunil; Oates, Tim; Mohsenin, Tinoosh; Waytowich, Nicholas",Learning from Observations Using a Single Video Demonstration and Human Feedback,"arXiv:1909.13392 [cs, stat]",,,,http://arxiv.org/abs/1909.13392,"In this paper, we present a method for learning from video demonstrations by using human feedback to construct a mapping between the standard representation of the agent and the visual representation of the demonstration. In this way, we leverage the advantages of both these representations, i.e., we learn the policy using standard state representations, but are able to specify the expected behavior using video demonstration. We train an autonomous agent using a single video demonstration and use human feedback (using numerical similarity rating) to map the standard representation to the visual representation with a neural network. We show the effectiveness of our method by teaching a hopper agent in the MuJoCo to perform a backflip using a single video demonstration generated in MuJoCo as well as from a real-world YouTube video of a person performing a backflip. Additionally, we show that our method can transfer to new tasks, such as hopping, with very little human feedback.",2019-09-29,2022-03-11 0:10:36,2022-03-11 0:10:36,2022-03-11 0:10:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.13392,,/Users/jacquesthibodeau/Zotero/storage/IW5G8AVX/Gandhi et al. - 2019 - Learning from Observations Using a Single Video De.pdf; /Users/jacquesthibodeau/Zotero/storage/H3YCXJK7/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
867E786W,journalArticle,2018,"Chitnis, Rohan; Kaelbling, Leslie Pack; Lozano-Pérez, Tomás",Learning What Information to Give in Partially Observed Domains,arXiv:1805.08263 [cs],,,,http://arxiv.org/abs/1805.08263,"In many robotic applications, an autonomous agent must act within and explore a partially observed environment that is unobserved by its human teammate. We consider such a setting in which the agent can, while acting, transmit declarative information to the human that helps them understand aspects of this unseen environment. In this work, we address the algorithmic question of how the agent should plan out what actions to take and what information to transmit. Naturally, one would expect the human to have preferences, which we model information-theoretically by scoring transmitted information based on the change it induces in weighted entropy of the human's belief state. We formulate this setting as a belief MDP and give a tractable algorithm for solving it approximately. Then, we give an algorithm that allows the agent to learn the human's preferences online, through exploration. We validate our approach experimentally in simulated discrete and continuous partially observed search-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a supplementary video.",2018-09-27,2022-03-11 0:10:40,2022-03-11 0:10:40,2022-03-11 0:10:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08263,,/Users/jacquesthibodeau/Zotero/storage/MMMVAIWX/Chitnis et al. - 2018 - Learning What Information to Give in Partially Obs.pdf; /Users/jacquesthibodeau/Zotero/storage/5JJ3TVCT/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PQHYG3UP,journalArticle,2018,"Lee, Kyungjae; Choi, Sungjoon; Oh, Songhwai",Maximum Causal Tsallis Entropy Imitation Learning,"arXiv:1805.08336 [cs, stat]",,,,http://arxiv.org/abs/1805.08336,"In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.",2018-05-28,2022-03-11 0:10:43,2022-03-11 0:10:43,2022-03-11 0:10:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08336,,/Users/jacquesthibodeau/Zotero/storage/XQ9B8MB3/Lee et al. - 2018 - Maximum Causal Tsallis Entropy Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/RGMP4F4J/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y6UVCAKM,journalArticle,2020,"Cabi, Serkan; Colmenarejo, Sergio Gómez; Novikov, Alexander; Konyushkova, Ksenia; Reed, Scott; Jeong, Rae; Zolna, Konrad; Aytar, Yusuf; Budden, David; Vecerik, Mel; Sushkov, Oleg; Barker, David; Scholz, Jonathan; Denil, Misha; de Freitas, Nando; Wang, Ziyu",Scaling data-driven robotics with reward sketching and batch reinforcement learning,arXiv:1909.12200 [cs],,,,http://arxiv.org/abs/1909.12200,"We present a framework for data-driven robotics that makes use of a large dataset of recorded robot experience and scales to several tasks using learned reward functions. We show how to apply this framework to accomplish three different object manipulation tasks on a real robot platform. Given demonstrations of a task together with task-agnostic recorded experience, we use a special form of human annotation as supervision to learn a reward function, which enables us to deal with real-world tasks where the reward signal cannot be acquired directly. Learned rewards are used in combination with a large dataset of experience from different tasks to learn a robot policy offline using batch RL. We show that using our approach it is possible to train agents to perform a variety of challenging manipulation tasks including stacking rigid objects and handling cloth.",2020-06-04,2022-03-11 0:10:46,2022-03-11 0:10:46,2022-03-11 0:10:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.12200,,/Users/jacquesthibodeau/Zotero/storage/99B36PA2/Cabi et al. - 2020 - Scaling data-driven robotics with reward sketching.pdf; /Users/jacquesthibodeau/Zotero/storage/BWQ55G4I/1909.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SK4DX5JM,journalArticle,2018,"Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang",Learning Safe Policies with Expert Guidance,"arXiv:1805.08313 [cs, stat]",,,,http://arxiv.org/abs/1805.08313,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",2018-11-21,2022-03-11 0:10:48,2022-03-11 0:10:48,2022-03-11 0:10:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08313,,/Users/jacquesthibodeau/Zotero/storage/AVY28JIX/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/BII4T32N/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SILYUB7,journalArticle,2020,"Milani, Stephanie; Topin, Nicholay; Houghton, Brandon; Guss, William H.; Mohanty, Sharada P.; Nakata, Keisuke; Vinyals, Oriol; Kuno, Noboru Sean",Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning,"arXiv:2003.05012 [cs, stat]",,,,http://arxiv.org/abs/2003.05012,"To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.",2020-06-18,2022-03-11 0:10:50,2022-03-11 0:10:50,2022-03-11 0:10:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.05012,,/Users/jacquesthibodeau/Zotero/storage/94GUJR7G/Milani et al. - 2020 - Retrospective Analysis of the 2019 MineRL Competit.pdf; /Users/jacquesthibodeau/Zotero/storage/DM6P7QPW/2003.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DU98KHIJ,journalArticle,2018,"Rabinowitz, Neil C.; Perbet, Frank; Song, H. Francis; Zhang, Chiyuan; Eslami, S. M. Ali; Botvinick, Matthew",Machine Theory of Mind,arXiv:1802.07740 [cs],,,,http://arxiv.org/abs/1802.07740,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",2018-03-12,2022-03-11 0:10:53,2022-03-11 0:10:53,2022-03-11 0:10:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.07740,,/Users/jacquesthibodeau/Zotero/storage/ME6YSGC2/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf; /Users/jacquesthibodeau/Zotero/storage/J6N2E4NE/1802.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NWS8W55,journalArticle,2020,"Wilder, Bryan; Horvitz, Eric; Kamar, Ece",Learning to Complement Humans,arXiv:2005.00582 [cs],,,,http://arxiv.org/abs/2005.00582,"A rising vision for AI in the open world centers on the development of systems that can complement humans for perceptual, diagnostic, and reasoning tasks. To date, systems aimed at complementing the skills of people have employed models trained to be as accurate as possible in isolation. We demonstrate how an end-to-end learning strategy can be harnessed to optimize the combined performance of human-machine teams by considering the distinct abilities of people and machines. The goal is to focus machine learning on problem instances that are difficult for humans, while recognizing instances that are difficult for the machine and seeking human input on them. We demonstrate in two real-world domains (scientific discovery and medical diagnosis) that human-machine teams built via these methods outperform the individual performance of machines and people. We then analyze conditions under which this complementarity is strongest, and which training methods amplify it. Taken together, our work provides the first systematic investigation of how machine learning systems can be trained to complement human reasoning.",2020-05-01,2022-03-11 0:10:56,2022-03-11 0:10:56,2022-03-11 0:10:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.00582,,/Users/jacquesthibodeau/Zotero/storage/7DAB2XTC/Wilder et al. - 2020 - Learning to Complement Humans.pdf; /Users/jacquesthibodeau/Zotero/storage/892PQTMS/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BYSZMG73,journalArticle,2020,"Brown, Daniel S.; Coleman, Russell; Srinivasan, Ravi; Niekum, Scott",Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"arXiv:2002.09089 [cs, stat]",,,,http://arxiv.org/abs/2002.09089,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.",2020-12-17,2022-03-11 0:11:04,2022-03-11 0:11:04,2022-03-11 0:11:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09089,,/Users/jacquesthibodeau/Zotero/storage/VMC9WQPZ/Brown et al. - 2020 - Safe Imitation Learning via Fast Bayesian Reward I.pdf; /Users/jacquesthibodeau/Zotero/storage/KAYJMDCL/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LYLUIQEK,journalArticle,2021,"Barde, Paul; Roy, Julien; Jeon, Wonseok; Pineau, Joelle; Pal, Christopher; Nowrouzezahrai, Derek",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,"arXiv:2006.13258 [cs, stat]",,,,http://arxiv.org/abs/2006.13258,"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.",2021-04-16,2022-03-11 0:11:06,2022-03-11 0:11:06,2022-03-11 0:11:06,,,,,,,Adversarial Soft Advantage Fitting,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13258,,/Users/jacquesthibodeau/Zotero/storage/LQRUF8T9/Barde et al. - 2021 - Adversarial Soft Advantage Fitting Imitation Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/DD4ZHZN5/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I47TWHNP,journalArticle,2021,"Spencer, Jonathan; Choudhury, Sanjiban; Venkatraman, Arun; Ziebart, Brian; Bagnell, J. Andrew",Feedback in Imitation Learning: The Three Regimes of Covariate Shift,"arXiv:2102.02872 [cs, stat]",,,,http://arxiv.org/abs/2102.02872,"Imitation learning practitioners have often noted that conditioning policies on previous actions leads to a dramatic divergence between ""held out"" error and performance of the learner in situ. Interactive approaches can provably address this divergence but require repeated querying of a demonstrator. Recent work identifies this divergence as stemming from a ""causal confound"" in predicting the current action, and seek to ablate causal aspects of current state using tools from causal inference. In this work, we argue instead that this divergence is simply another manifestation of covariate shift, exacerbated particularly by settings of feedback between decisions and input features. The learner often comes to rely on features that are strongly predictive of decisions, but are subject to strong covariate shift. Our work demonstrates a broad class of problems where this shift can be mitigated, both theoretically and practically, by taking advantage of a simulator but without any further querying of expert demonstration. We analyze existing benchmarks used to test imitation learning approaches and find that these benchmarks are realizable and simple and thus insufficient for capturing the harder regimes of error compounding seen in real-world decision making problems. We find, in a surprising contrast with previous literature, but consistent with our theory, that naive behavioral cloning provides excellent results. We detail the need for new standardized benchmarks that capture the phenomena seen in robotics problems.",2021-02-11,2022-03-11 0:11:29,2022-03-11 0:11:29,2022-03-11 0:11:29,,,,,,,Feedback in Imitation Learning,,,,,,,,,,,,arXiv.org,,arXiv: 2102.02872,,/Users/jacquesthibodeau/Zotero/storage/JDID352M/Spencer et al. - 2021 - Feedback in Imitation Learning The Three Regimes .pdf; /Users/jacquesthibodeau/Zotero/storage/K4J4CEUU/2102.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXNKQ9YC,journalArticle,2021,"Jonnavittula, Ananth; Losey, Dylan P.",I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set,arXiv:2011.06118 [cs],,,,http://arxiv.org/abs/2011.06118,"Assistive robots have the potential to help people perform everyday tasks. However, these robots first need to learn what it is their user wants them to do. Teaching assistive robots is hard for inexperienced users, elderly users, and users living with physical disabilities, since often these individuals are unable to show the robot their desired behavior. We know that inclusive learners should give human teachers credit for what they cannot demonstrate. But today's robots do the opposite: they assume every user is capable of providing any demonstration. As a result, these robots learn to mimic the demonstrated behavior, even when that behavior is not what the human really meant! Here we propose a different approach to reward learning: robots that reason about the user's demonstrations in the context of similar or simpler alternatives. Unlike prior works -- which err towards overestimating the human's capabilities -- here we err towards underestimating what the human can input (i.e., their choice set). Our theoretical analysis proves that underestimating the human's choice set is risk-averse, with better worst-case performance than overestimating. We formalize three properties to generate similar and simpler alternatives. Across simulations and a user study, our resulting algorithm better extrapolates the human's objective. See the user study here: https://youtu.be/RgbH2YULVRo",2021-04-05,2022-03-11 0:11:31,2022-03-11 0:11:31,2022-03-11 0:11:31,,,,,,,I Know What You Meant,,,,,,,,,,,,arXiv.org,,arXiv: 2011.06118,,/Users/jacquesthibodeau/Zotero/storage/INGCJRKV/Jonnavittula and Losey - 2021 - I Know What You Meant Learning Human Objectives b.pdf; /Users/jacquesthibodeau/Zotero/storage/LMBKJT75/2011.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z5EX63YW,journalArticle,2020,"Freire, Pedro; Gleave, Adam; Toyer, Sam; Russell, Stuart",DERAIL: Diagnostic Environments for Reward And Imitation Learning,arXiv:2012.01365 [cs],,,,http://arxiv.org/abs/2012.01365,"The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals .",2020-12-02,2022-03-11 0:11:33,2022-03-11 0:11:33,2022-03-11 0:11:33,,,,,,,DERAIL,,,,,,,,,,,,arXiv.org,,arXiv: 2012.01365,,/Users/jacquesthibodeau/Zotero/storage/JDLGXBJF/Freire et al. - 2020 - DERAIL Diagnostic Environments for Reward And Imi.pdf; /Users/jacquesthibodeau/Zotero/storage/N8RTCGS9/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EUJRNI9R,journalArticle,2021,"Zellers, Rowan; Holtzman, Ari; Clark, Elizabeth; Qin, Lianhui; Farhadi, Ali; Choi, Yejin",TuringAdvice: A Generative and Dynamic Evaluation of Language Use,arXiv:2004.03607 [cs],,,,http://arxiv.org/abs/2004.03607,"We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.",2021-04-12,2022-03-11 0:11:35,2022-03-11 0:11:35,2022-03-11 0:11:35,,,,,,,TuringAdvice,,,,,,,,,,,,arXiv.org,,arXiv: 2004.03607,,/Users/jacquesthibodeau/Zotero/storage/Q6PWCR5P/Zellers et al. - 2021 - TuringAdvice A Generative and Dynamic Evaluation .pdf; /Users/jacquesthibodeau/Zotero/storage/J86WM8PA/2004.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2AE4C5I5,journalArticle,2021,"Orsini, Manu; Raichuk, Anton; Hussenot, Léonard; Vincent, Damien; Dadashi, Robert; Girgin, Sertan; Geist, Matthieu; Bachem, Olivier; Pietquin, Olivier; Andrychowicz, Marcin",What Matters for Adversarial Imitation Learning?,arXiv:2106.00672 [cs],,,,http://arxiv.org/abs/2106.00672,"Adversarial imitation learning has become a popular framework for imitation in continuous control. Over the years, several variations of its components were proposed to enhance the performance of the learned policies as well as the sample complexity of the algorithm. In practice, these choices are rarely tested all together in rigorous empirical studies. It is therefore difficult to discuss and understand what choices, among the high-level algorithmic options as well as low-level implementation details, matter. To tackle this issue, we implement more than 50 of these choices in a generic adversarial imitation learning framework and investigate their impacts in a large-scale study (>500k trained agents) with both synthetic and human-generated demonstrations. While many of our findings confirm common practices, some of them are surprising or even contradict prior work. In particular, our results suggest that artificial demonstrations are not a good proxy for human data and that the very common practice of evaluating imitation algorithms only with synthetic demonstrations may lead to algorithms which perform poorly in the more realistic scenarios with human demonstrations.",2021-06-01,2022-03-11 0:11:37,2022-03-11 0:11:37,2022-03-11 0:11:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.00672,,/Users/jacquesthibodeau/Zotero/storage/MK3UABRT/Orsini et al. - 2021 - What Matters for Adversarial Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/8Z5H574Z/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BYYMMKY3,journalArticle,2021,"Mandlekar, Ajay; Xu, Danfei; Wong, Josiah; Nasiriany, Soroush; Wang, Chen; Kulkarni, Rohun; Fei-Fei, Li; Savarese, Silvio; Zhu, Yuke; Martín-Martín, Roberto",What Matters in Learning from Offline Human Demonstrations for Robot Manipulation,arXiv:2108.03298 [cs],,,,http://arxiv.org/abs/2108.03298,"Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at https://arise-initiative.github.io/robomimic-web/",2021-09-24,2022-03-11 0:11:40,2022-03-11 0:11:40,2022-03-11 0:11:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.03298,,/Users/jacquesthibodeau/Zotero/storage/N64P5YH4/Mandlekar et al. - 2021 - What Matters in Learning from Offline Human Demons.pdf; /Users/jacquesthibodeau/Zotero/storage/EHKXMTQY/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J9EWASK5,journalArticle,2022,"Wei, Jason; Bosma, Maarten; Zhao, Vincent Y.; Guu, Kelvin; Yu, Adams Wei; Lester, Brian; Du, Nan; Dai, Andrew M.; Le, Quoc V.",Finetuned Language Models Are Zero-Shot Learners,arXiv:2109.01652 [cs],,,,http://arxiv.org/abs/2109.01652,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",2022-02-08,2022-03-11 0:11:42,2022-03-11 0:11:42,2022-03-11 0:11:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.01652,,/Users/jacquesthibodeau/Zotero/storage/HNM4R5FW/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/V86IZXQ3/2109.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SLKSCBIT,journalArticle,2021,"Zhong, Ruiqi; Lee, Kristy; Zhang, Zheng; Klein, Dan",Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections,arXiv:2104.04670 [cs],,,,http://arxiv.org/abs/2104.04670,"Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can ""prompt"" the LM with the review and the label description ""Does the user like this movie?"", and ask whether the next word is ""yes"" or ""no"". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.",2021-09-08,2022-03-11 0:11:44,2022-03-11 0:11:44,2022-03-11 0:11:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.04670,,/Users/jacquesthibodeau/Zotero/storage/8VHEZWHD/Zhong et al. - 2021 - Adapting Language Models for Zero-shot Learning by.pdf; /Users/jacquesthibodeau/Zotero/storage/FKDT6BMJ/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNKTN89R,journalArticle,2021,"Cruz, Christian Arzate; Igarashi, Takeo",Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors,arXiv:2105.12938 [cs],,,,http://arxiv.org/abs/2105.12938,"Reinforcement learning techniques successfully generate convincing agent behaviors, but it is still difficult to tailor the behavior to align with a user's specific preferences. What is missing is a communication method for the system to explain the behavior and for the user to repair it. In this paper, we present a novel interaction method that uses interactive explanations using templates of natural language as a communication method. The main advantage of this interaction method is that it enables a two-way communication channel between users and the agent; the bot can explain its thinking procedure to the users, and the users can communicate their behavior preferences to the bot using the same interactive explanations. In this manner, the thinking procedure of the bot is transparent, and users can provide corrections to the bot that include a suggested action to take, a goal to achieve, and the reasons behind these decisions. We tested our proposed method in a clone of the video game named \textit{Super Mario Bros.}, and the results demonstrate that our interactive explanation approach is effective at diagnosing and repairing bot behaviors.",2021-05-27,2022-03-11 0:17:19,2022-03-11 0:17:19,2022-03-11 0:17:19,,,,,,,Interactive Explanations,,,,,,,,,,,,arXiv.org,,arXiv: 2105.12938,,/Users/jacquesthibodeau/Zotero/storage/WRLG2GFY/Cruz and Igarashi - 2021 - Interactive Explanations Diagnosis and Repair of .pdf; /Users/jacquesthibodeau/Zotero/storage/3LFVBSW2/2105.html,,,Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S56TUWLU,journalArticle,2021,"Pauly, Leo; Agboh, Wisdom C.; Hogg, David C.; Fuentes, Raul",O2A: One-shot Observational learning with Action vectors,Frontiers in Robotics and AI,,2296-9144,10.3389/frobt.2021.686368,http://arxiv.org/abs/1810.07483,"We present O2A, a novel method for learning to perform robotic manipulation tasks from a single (one-shot) third-person demonstration video. To our knowledge, it is the first time this has been done for a single demonstration. The key novelty lies in pre-training a feature extractor for creating a perceptual representation for actions that we call 'action vectors'. The action vectors are extracted using a 3D-CNN model pre-trained as an action classifier on a generic action dataset. The distance between the action vectors from the observed third-person demonstration and trial robot executions is used as a reward for reinforcement learning of the demonstrated task. We report on experiments in simulation and on a real robot, with changes in viewpoint of observation, properties of the objects involved, scene background and morphology of the manipulator between the demonstration and the learning domains. O2A outperforms baseline approaches under different domain shifts and has comparable performance with an oracle (that uses an ideal reward function).",2021-08-02,2022-03-11 0:17:21,2022-03-11 0:17:21,2022-03-11 0:17:21,686368,,,8,,Front. Robot. AI,O2A,,,,,,,,,,,,arXiv.org,,arXiv: 1810.07483,,/Users/jacquesthibodeau/Zotero/storage/IMZEK5Z7/Pauly et al. - 2021 - O2A One-shot Observational learning with Action v.pdf; /Users/jacquesthibodeau/Zotero/storage/YCQVGP4M/1810.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E5CBT6GF,journalArticle,2018,"Tucker, Aaron; Gleave, Adam; Russell, Stuart",Inverse reinforcement learning for video games,"arXiv:1810.10593 [cs, stat]",,,,http://arxiv.org/abs/1810.10593,"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",2018-10-24,2022-03-11 0:17:23,2022-03-11 0:17:23,2022-03-11 0:17:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.10593,,/Users/jacquesthibodeau/Zotero/storage/MM4MGX6Y/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf; /Users/jacquesthibodeau/Zotero/storage/5UG6M4YY/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ULFYRANH,journalArticle,2018,"Goecks, Vinicius G.; Gremillion, Gregory M.; Lawhern, Vernon J.; Valasek, John; Waytowich, Nicholas R.",Efficiently Combining Human Demonstrations and Interventions for Safe Training of Autonomous Systems in Real-Time,arXiv:1810.11545 [cs],,,,http://arxiv.org/abs/1810.11545,"This paper investigates how to utilize different forms of human interaction to safely train autonomous systems in real-time by learning from both human demonstrations and interventions. We implement two components of the Cycle-of-Learning for Autonomous Systems, which is our framework for combining multiple modalities of human interaction. The current effort employs human demonstrations to teach a desired behavior via imitation learning, then leverages intervention data to correct for undesired behaviors produced by the imitation learner to teach novel tasks to an autonomous agent safely, after only minutes of training. We demonstrate this method in an autonomous perching task using a quadrotor with continuous roll, pitch, yaw, and throttle commands and imagery captured from a downward-facing camera in a high-fidelity simulated environment. Our method improves task completion performance for the same amount of human interaction when compared to learning from demonstrations alone, while also requiring on average 32% less data to achieve that performance. This provides evidence that combining multiple modes of human interaction can increase both the training speed and overall performance of policies for autonomous systems.",2018-11-28,2022-03-11 0:17:25,2022-03-11 0:17:25,2022-03-11 0:17:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11545,,/Users/jacquesthibodeau/Zotero/storage/REII9KGZ/Goecks et al. - 2018 - Efficiently Combining Human Demonstrations and Int.pdf; /Users/jacquesthibodeau/Zotero/storage/FLM6BWV2/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIDMEW8Z,journalArticle,2019,"Angelov, Daniel; Hristov, Yordan; Ramamoorthy, Subramanian",Using Causal Analysis to Learn Specifications from Task Demonstrations,arXiv:1903.01267 [cs],,,,http://arxiv.org/abs/1903.01267,"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.",2019-03-04,2022-03-11 0:17:27,2022-03-11 0:17:27,2022-03-11 0:17:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01267,,/Users/jacquesthibodeau/Zotero/storage/F2H9HQWW/Angelov et al. - 2019 - Using Causal Analysis to Learn Specifications from.pdf; /Users/jacquesthibodeau/Zotero/storage/H48RB47Z/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LP46NSDW,journalArticle,2019,"Goyal, Prasoon; Niekum, Scott; Mooney, Raymond J.",Using Natural Language for Reward Shaping in Reinforcement Learning,"arXiv:1903.02020 [cs, stat]",,,,http://arxiv.org/abs/1903.02020,"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",2019-05-31,2022-03-11 0:17:29,2022-03-11 0:17:29,2022-03-11 0:17:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.02020,,/Users/jacquesthibodeau/Zotero/storage/RUUCJ3J6/Goyal et al. - 2019 - Using Natural Language for Reward Shaping in Reinf.pdf; /Users/jacquesthibodeau/Zotero/storage/JSWI5CYE/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML33SSRP,journalArticle,2019,"Frye, Christopher; Feige, Ilya",Parenting: Safe Reinforcement Learning from Human Input,"arXiv:1902.06766 [cs, stat]",,,,http://arxiv.org/abs/1902.06766,"Autonomous agents trained via reinforcement learning present numerous safety concerns: reward hacking, negative side effects, and unsafe exploration, among others. In the context of near-future autonomous agents, operating in environments where humans understand the existing dangers, human involvement in the learning process has proved a promising approach to AI Safety. Here we demonstrate that a precise framework for learning from human input, loosely inspired by the way humans parent children, solves a broad class of safety problems in this context. We show that our Parenting algorithm solves these problems in the relevant AI Safety gridworlds of Leike et al. (2017), that an agent can learn to outperform its parent as it ""matures"", and that policies learnt through Parenting are generalisable to new environments.",2019-02-18,2022-03-11 0:17:31,2022-03-11 0:17:31,2022-03-11 0:17:31,,,,,,,Parenting,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06766,,/Users/jacquesthibodeau/Zotero/storage/Q5CMCH8C/Frye and Feige - 2019 - Parenting Safe Reinforcement Learning from Human .pdf; /Users/jacquesthibodeau/Zotero/storage/AUTFYDC3/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y8EERDM4,journalArticle,2019,"Frazier, Spencer; Riedl, Mark",Improving Deep Reinforcement Learning in Minecraft with Action Advice,"arXiv:1908.01007 [cs, stat]",,,,http://arxiv.org/abs/1908.01007,"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",2019-08-02,2022-03-11 0:17:33,2022-03-11 0:17:33,2022-03-11 0:17:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.01007,,/Users/jacquesthibodeau/Zotero/storage/TWDSZU9T/Frazier and Riedl - 2019 - Improving Deep Reinforcement Learning in Minecraft.pdf; /Users/jacquesthibodeau/Zotero/storage/AWZVQQE9/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDY6PQ7G,journalArticle,2020,"Scheller, Christian; Schraner, Yanick; Vogel, Manfred",Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft,"arXiv:2003.06066 [cs, stat]",,,,http://arxiv.org/abs/2003.06066,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",2020-03-12,2022-03-11 0:17:36,2022-03-11 0:17:36,2022-03-11 0:17:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.06066,,/Users/jacquesthibodeau/Zotero/storage/7768A4MV/Scheller et al. - 2020 - Sample Efficient Reinforcement Learning through Le.pdf; /Users/jacquesthibodeau/Zotero/storage/3HM5DDV9/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7ID57CX,journalArticle,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,"arXiv:1805.08882 [cs, stat]",,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2022-03-11 0:17:38,2022-03-11 0:17:38,2022-03-11 0:17:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08882,,/Users/jacquesthibodeau/Zotero/storage/8PJGNWY2/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/MNTKHFBJ/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CXWCEBI4,journalArticle,2019,"Torabi, Faraz; Warnell, Garrett; Stone, Peter",Imitation Learning from Video by Leveraging Proprioception,"arXiv:1905.09335 [cs, stat]",,,,http://arxiv.org/abs/1905.09335,"Classically, imitation learning algorithms have been developed for idealized situations, e.g., the demonstrations are often required to be collected in the exact same environment and usually include the demonstrator's actions. Recently, however, the research community has begun to address some of these shortcomings by offering algorithmic solutions that enable imitation learning from observation (IfO), e.g., learning to perform a task from visual demonstrations that may be in a different environment and do not include actions. Motivated by the fact that agents often also have access to their own internal states (i.e., proprioception), we propose and study an IfO algorithm that leverages this information in the policy learning process. The proposed architecture learns policies over proprioceptive state representations and compares the resulting trajectories visually to the demonstration data. We experimentally test the proposed technique on several MuJoCo domains and show that it outperforms other imitation from observation algorithms by a large margin.",2019-06-18,2022-03-11 0:17:40,2022-03-11 0:17:40,2022-03-11 0:17:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.09335,,/Users/jacquesthibodeau/Zotero/storage/UJV54IYL/Torabi et al. - 2019 - Imitation Learning from Video by Leveraging Propri.pdf; /Users/jacquesthibodeau/Zotero/storage/GISGDQJB/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HMC4K8IP,journalArticle,2019,"Edwards, Ashley D.; Sahni, Himanshu; Schroecker, Yannick; Isbell, Charles L.",Imitating Latent Policies from Observation,"arXiv:1805.07914 [cs, stat]",,,,http://arxiv.org/abs/1805.07914,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",2019-05-13,2022-03-11 0:17:43,2022-03-11 0:17:43,2022-03-11 0:17:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07914,,/Users/jacquesthibodeau/Zotero/storage/AB44C2PN/Edwards et al. - 2019 - Imitating Latent Policies from Observation.pdf; /Users/jacquesthibodeau/Zotero/storage/JUE4EDQQ/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LJWTJ9BU,journalArticle,2018,"Malik, Dhruv; Palaniappan, Malayandi; Fisac, Jaime F.; Hadfield-Menell, Dylan; Russell, Stuart; Dragan, Anca D.","An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",arXiv:1806.03820 [cs],,,,http://arxiv.org/abs/1806.03820,"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",2018-06-11,2022-03-11 0:17:45,2022-03-11 0:17:45,2022-03-11 0:17:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.03820,,"/Users/jacquesthibodeau/Zotero/storage/9X3G7DLV/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Coope.pdf; /Users/jacquesthibodeau/Zotero/storage/A9AIXFSA/1806.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3TFNTENG,journalArticle,2020,"Zhi-Xuan, Tan; Mann, Jordyn L.; Silver, Tom; Tenenbaum, Joshua B.; Mansinghka, Vikash K.",Online Bayesian Goal Inference for Boundedly-Rational Planning Agents,arXiv:2006.07532 [cs],,,,http://arxiv.org/abs/2006.07532,"People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.",2020-10-24,2022-03-11 0:17:47,2022-03-11 0:17:47,2022-03-11 0:17:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.07532,,/Users/jacquesthibodeau/Zotero/storage/GWP9QNXU/Zhi-Xuan et al. - 2020 - Online Bayesian Goal Inference for Boundedly-Ratio.pdf; /Users/jacquesthibodeau/Zotero/storage/33KMYZ46/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GFQXFZP,journalArticle,2020,"Arenz, Oleg; Neumann, Gerhard",Non-Adversarial Imitation Learning and its Connections to Adversarial Methods,"arXiv:2008.03525 [cs, math, stat]",,,,http://arxiv.org/abs/2008.03525,"Many modern methods for imitation learning and inverse reinforcement learning, such as GAIL or AIRL, are based on an adversarial formulation. These methods apply GANs to match the expert's distribution over states and actions with the implicit state-action distribution induced by the agent's policy. However, by framing imitation learning as a saddle point problem, adversarial methods can suffer from unstable optimization, and convergence can only be shown for small policy updates. We address these problems by proposing a framework for non-adversarial imitation learning. The resulting algorithms are similar to their adversarial counterparts and, thus, provide insights for adversarial imitation learning methods. Most notably, we show that AIRL is an instance of our non-adversarial formulation, which enables us to greatly simplify its derivations and obtain stronger convergence guarantees. We also show that our non-adversarial formulation can be used to derive novel algorithms by presenting a method for offline imitation learning that is inspired by the recent ValueDice algorithm, but does not rely on small policy updates for convergence. In our simulated robot experiments, our offline method for non-adversarial imitation learning seems to perform best when using many updates for policy and discriminator at each iteration and outperforms behavioral cloning and ValueDice.",2020-08-08,2022-03-11 0:17:49,2022-03-11 0:17:49,2022-03-11 0:17:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.03525,,/Users/jacquesthibodeau/Zotero/storage/8JWASMVW/Arenz and Neumann - 2020 - Non-Adversarial Imitation Learning and its Connect.pdf; /Users/jacquesthibodeau/Zotero/storage/4YUY77YE/2008.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8DZDHVC,journalArticle,2020,"Cui, Yuchen; Zhang, Qiping; Allievi, Alessandro; Stone, Peter; Niekum, Scott; Knox, W. Bradley",The EMPATHIC Framework for Task Learning from Implicit Human Feedback,arXiv:2009.13649 [cs],,,,http://arxiv.org/abs/2009.13649,"Reactions such as gestures, facial expressions, and vocalizations are an abundant, naturally occurring channel of information that humans provide during interactions. A robot or other agent could leverage an understanding of such implicit human feedback to improve its task performance at no cost to the human. This approach contrasts with common agent teaching methods based on demonstrations, critiques, or other guidance that need to be attentively and intentionally provided. In this paper, we first define the general problem of learning from implicit human feedback and then propose to address this problem through a novel data-driven framework, EMPATHIC. This two-stage method consists of (1) mapping implicit human feedback to relevant task statistics such as reward, optimality, and advantage; and (2) using such a mapping to learn a task. We instantiate the first stage and three second-stage evaluations of the learned mapping. To do so, we collect a dataset of human facial reactions while participants observe an agent execute a sub-optimal policy for a prescribed training task. We train a deep neural network on this data and demonstrate its ability to (1) infer relative reward ranking of events in the training task from prerecorded human facial reactions; (2) improve the policy of an agent in the training task using live human facial reactions; and (3) transfer to a novel domain in which it evaluates robot manipulation trajectories.",2020-12-07,2022-03-11 0:17:52,2022-03-11 0:17:52,2022-03-11 0:17:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.13649,,/Users/jacquesthibodeau/Zotero/storage/IZKD27SP/Cui et al. - 2020 - The EMPATHIC Framework for Task Learning from Impl.pdf; /Users/jacquesthibodeau/Zotero/storage/UD3SXIXJ/2009.html,,,Computer Science - Human-Computer Interaction; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCXY7LCP,journalArticle,2021,"Tangkaratt, Voot; Charoenphakdee, Nontawat; Sugiyama, Masashi",Robust Imitation Learning from Noisy Demonstrations,"arXiv:2010.10181 [cs, stat]",,,,http://arxiv.org/abs/2010.10181,"Robust learning from noisy demonstrations is a practical but highly challenging problem in imitation learning. In this paper, we first theoretically show that robust imitation learning can be achieved by optimizing a classification risk with a symmetric loss. Based on this theoretical finding, we then propose a new imitation learning method that optimizes the classification risk by effectively combining pseudo-labeling with co-training. Unlike existing methods, our method does not require additional labels or strict assumptions about noise distributions. Experimental results on continuous-control benchmarks show that our method is more robust compared to state-of-the-art methods.",2021-02-19,2022-03-11 0:17:55,2022-03-11 0:17:55,2022-03-11 0:17:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.10181,,/Users/jacquesthibodeau/Zotero/storage/RRJCQUS7/Tangkaratt et al. - 2021 - Robust Imitation Learning from Noisy Demonstration.pdf; /Users/jacquesthibodeau/Zotero/storage/L7QM75RV/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CCXMUU37,journalArticle,2019,"Lin, Xiaomin; Adams, Stephen C.; Beling, Peter A.",Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.1.11541,http://arxiv.org/abs/1806.09795,"This paper addresses the problem of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurate estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.",2019-10-15,2022-03-11 0:18:01,2022-03-11 0:18:01,2022-03-11 0:18:01,473-502,,,66,,jair,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09795,,/Users/jacquesthibodeau/Zotero/storage/WA8AARXV/Lin et al. - 2019 - Multi-agent Inverse Reinforcement Learning for Cer.pdf; /Users/jacquesthibodeau/Zotero/storage/2FWEGVEI/1806.html,,,Computer Science - Computer Science and Game Theory; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XLBQCLB,journalArticle,2018,"Pan, Xinlei; Ohn-Bar, Eshed; Rhinehart, Nicholas; Xu, Yan; Shen, Yilin; Kitani, Kris M.",Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning,arXiv:1806.08479 [cs],,,,http://arxiv.org/abs/1806.08479,"Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.",2018-06-21,2022-03-11 0:18:03,2022-03-11 0:18:03,2022-03-11 0:18:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.08479,,/Users/jacquesthibodeau/Zotero/storage/CJLT5WVE/Pan et al. - 2018 - Human-Interactive Subgoal Supervision for Efficien.pdf; /Users/jacquesthibodeau/Zotero/storage/N8CIB62F/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC4PVURJ,journalArticle,2020,"Michaud, Eric J.; Gleave, Adam; Russell, Stuart",Understanding Learned Reward Functions,arXiv:2012.05862 [cs],,,,http://arxiv.org/abs/2012.05862,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.",2020-12-10,2022-03-11 0:18:05,2022-03-11 0:18:05,2022-03-11 0:18:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05862,,/Users/jacquesthibodeau/Zotero/storage/SE59KWID/Michaud et al. - 2020 - Understanding Learned Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/C9YM3G2E/2012.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9S3TH6M6,journalArticle,2021,"Abramson, Josh; Ahuja, Arun; Barr, Iain; Brussee, Arthur; Carnevale, Federico; Cassin, Mary; Chhaparia, Rachita; Clark, Stephen; Damoc, Bogdan; Dudzik, Andrew; Georgiev, Petko; Guy, Aurelia; Harley, Tim; Hill, Felix; Hung, Alden; Kenton, Zachary; Landon, Jessica; Lillicrap, Timothy; Mathewson, Kory; Mokrá, Soňa; Muldal, Alistair; Santoro, Adam; Savinov, Nikolay; Varma, Vikrant; Wayne, Greg; Williams, Duncan; Wong, Nathaniel; Yan, Chen; Zhu, Rui",Imitating Interactive Intelligence,arXiv:2012.05672 [cs],,,,http://arxiv.org/abs/2012.05672,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",2021-01-20,2022-03-11 0:18:07,2022-03-11 0:18:07,2022-03-11 0:18:07,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05672,,/Users/jacquesthibodeau/Zotero/storage/NI36MRW2/Abramson et al. - 2021 - Imitating Interactive Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/UWK5ZID9/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC743544,journalArticle,2021,"Sumers, Theodore R.; Ho, Mark K.; Hawkins, Robert D.; Narasimhan, Karthik; Griffiths, Thomas L.",Learning Rewards from Linguistic Feedback,arXiv:2009.14715 [cs],,,,http://arxiv.org/abs/2009.14715,"We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the ""pragmatic"" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.",2021-07-03,2022-03-11 0:18:10,2022-03-11 0:18:10,2022-03-11 0:18:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.14715,,/Users/jacquesthibodeau/Zotero/storage/WHV6HNJG/Sumers et al. - 2021 - Learning Rewards from Linguistic Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/DXNC24JF/2009.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SNFMQHR7,journalArticle,2019,"Waytowich, Nicholas; Barton, Sean L.; Lawhern, Vernon; Warnell, Garrett",A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,arXiv:1911.00497 [cs],,,,http://arxiv.org/abs/1911.00497,"While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.",2019-10-31,2022-03-11 0:18:12,2022-03-11 0:18:12,2022-03-11 0:18:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.00497,,/Users/jacquesthibodeau/Zotero/storage/KU6QTFDC/Waytowich et al. - 2019 - A Narration-based Reward Shaping Approach using Gr.pdf; /Users/jacquesthibodeau/Zotero/storage/PKVWYY9W/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EQC7KN5G,journalArticle,2021,"Eysenbach, Benjamin; Levine, Sergey; Salakhutdinov, Ruslan",Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,arXiv:2103.12656 [cs],,,,http://arxiv.org/abs/2103.12656,"Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.",2021-12-30,2022-03-11 0:18:26,2022-03-11 0:18:26,2022-03-11 0:18:26,,,,,,,Replacing Rewards with Examples,,,,,,,,,,,,arXiv.org,,arXiv: 2103.12656,,/Users/jacquesthibodeau/Zotero/storage/UINBC7L3/Eysenbach et al. - 2021 - Replacing Rewards with Examples Example-Based Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/AW5LK8HW/2103.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XYIWZSBY,journalArticle,2019,"Torabi, Faraz; Warnell, Garrett; Stone, Peter",Generative Adversarial Imitation from Observation,"arXiv:1807.06158 [cs, stat]",,,,http://arxiv.org/abs/1807.06158,"Imitation from observation (IfO) is the problem of learning directly from state-only demonstrations without having access to the demonstrator's actions. The lack of action information both distinguishes IfO from most of the literature in imitation learning, and also sets it apart as a method that may enable agents to learn from a large set of previously inapplicable resources such as internet videos. In this paper, we propose both a general framework for IfO approaches and also a new IfO approach based on generative adversarial networks called generative adversarial imitation from observation (GAIfO). We conduct experiments in two different settings: (1) when demonstrations consist of low-dimensional, manually-defined state features, and (2) when demonstrations consist of high-dimensional, raw visual data. We demonstrate that our approach performs comparably to classical imitation learning approaches (which have access to the demonstrator's actions) and significantly outperforms existing imitation from observation methods in high-dimensional simulation environments.",2019-06-18,2022-03-11 0:18:29,2022-03-11 0:18:29,2022-03-11 0:18:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06158,,/Users/jacquesthibodeau/Zotero/storage/QXJZYI5A/Torabi et al. - 2019 - Generative Adversarial Imitation from Observation.pdf; /Users/jacquesthibodeau/Zotero/storage/ZFYDM6EY/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2LDCTX9Y,journalArticle,2021,"Garg, Divyansh; Chakraborty, Shuvam; Cundy, Chris; Song, Jiaming; Ermon, Stefano",IQ-Learn: Inverse soft-Q Learning for Imitation,arXiv:2106.12142 [cs],,,,http://arxiv.org/abs/2106.12142,"In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.",2021-12-01,2022-03-11 0:18:39,2022-03-11 0:18:39,2022-03-11 0:18:39,,,,,,,IQ-Learn,,,,,,,,,,,,arXiv.org,,arXiv: 2106.12142,,/Users/jacquesthibodeau/Zotero/storage/C89NSUTN/Garg et al. - 2021 - IQ-Learn Inverse soft-Q Learning for Imitation.pdf; /Users/jacquesthibodeau/Zotero/storage/52JPGFUA/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H6LBTTXY,journalArticle,2019,"Tangkaratt, Voot; Han, Bo; Khan, Mohammad Emtiyaz; Sugiyama, Masashi",VILD: Variational Imitation Learning with Diverse-quality Demonstrations,"arXiv:1909.06769 [cs, stat]",,,,http://arxiv.org/abs/1909.06769,"The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",2019-09-15,2022-03-11 0:18:40,2022-03-11 0:18:40,2022-03-11 0:18:40,,,,,,,VILD,,,,,,,,,,,,arXiv.org,,arXiv: 1909.06769,,/Users/jacquesthibodeau/Zotero/storage/TY56XKN5/Tangkaratt et al. - 2019 - VILD Variational Imitation Learning with Diverse-.pdf; /Users/jacquesthibodeau/Zotero/storage/KBPTSQ7T/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2T527W5,journalArticle,2021,"Chan, Lawrence; Critch, Andrew; Dragan, Anca",Human irrationality: both bad and good for reward inference,arXiv:2111.06956 [cs],,,,http://arxiv.org/abs/2111.06956,"Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference. We find that wrongly modeling a systematically irrational human as noisy-rational performs a lot worse than correctly capturing these biases -- so much so that it can be better to skip inference altogether and stick to the prior! More importantly, we show that an irrational human, when correctly modelled, can communicate more information about the reward than a perfectly rational human can. That is, if a robot has the correct model of a human's irrationality, it can make an even stronger inference than it ever could if the human were rational. Irrationality fundamentally helps rather than hinder reward inference, but it needs to be correctly accounted for.",2021-11-12,2022-03-11 0:18:43,2022-03-11 0:18:43,2022-03-11 0:18:43,,,,,,,Human irrationality,,,,,,,,,,,,arXiv.org,,arXiv: 2111.06956,,/Users/jacquesthibodeau/Zotero/storage/IZGJ4DLM/Chan et al. - 2021 - Human irrationality both bad and good for reward .pdf; /Users/jacquesthibodeau/Zotero/storage/QWMBWI3B/2111.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CR38PYPP,journalArticle,2019,"Gencoglu, Oguzhan; van Gils, Mark; Guldogan, Esin; Morikawa, Chamin; Süzen, Mehmet; Gruber, Mathias; Leinonen, Jussi; Huttunen, Heikki",HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,arXiv:1904.07633 [cs],,,,http://arxiv.org/abs/1904.07633,"Recent advancements in machine learning research, i.e., deep learning, introduced methods that excel conventional algorithms as well as humans in several complex tasks, ranging from detection of objects in images and speech recognition to playing difficult strategic games. However, the current methodology of machine learning research and consequently, implementations of the real-world applications of such algorithms, seems to have a recurring HARKing (Hypothesizing After the Results are Known) issue. In this work, we elaborate on the algorithmic, economic and social reasons and consequences of this phenomenon. We present examples from current common practices of conducting machine learning research (e.g. avoidance of reporting negative results) and failure of generalization ability of the proposed algorithms and datasets in actual real-life usage. Furthermore, a potential future trajectory of machine learning research and development from the perspective of accountable, unbiased, ethical and privacy-aware algorithmic decision making is discussed. We would like to emphasize that with this discussion we neither claim to provide an exhaustive argumentation nor blame any specific institution or individual on the raised issues. This is simply a discussion put forth by us, insiders of the machine learning field, reflecting on us.",2019-04-16,2022-03-11 0:21:21,2022-03-11 1:38:52,2022-03-11 0:21:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.07633,,/Users/jacquesthibodeau/Zotero/storage/3ING4DCQ/Gencoglu et al. - 2019 - HARK Side of Deep Learning -- From Grad Student De.pdf; /Users/jacquesthibodeau/Zotero/storage/5EAG7L9J/Gencoglu et al. - 2019 - HARK Side of Deep Learning -- From Grad Student De.pdf; /Users/jacquesthibodeau/Zotero/storage/MNGUN86J/1904.html; /Users/jacquesthibodeau/Zotero/storage/7T2L4ZTU/1904.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
APDKJF8Y,journalArticle,2020,"Clune, Jeff","AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",arXiv:1905.10985 [cs],,,,http://arxiv.org/abs/1905.10985,"Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ""manual AI approach"". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.",2020-01-31,2022-03-11 0:21:25,2022-03-11 0:21:25,2022-03-11 0:21:25,,,,,,,AI-GAs,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10985,,"/Users/jacquesthibodeau/Zotero/storage/FCYKE3CS/Clune - 2020 - AI-GAs AI-generating algorithms, an alternate par.pdf; /Users/jacquesthibodeau/Zotero/storage/WVHB7TJ2/1905.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WPLL8LEW,journalArticle,2019,"Rusu, Andrei A.; Rao, Dushyant; Sygnowski, Jakub; Vinyals, Oriol; Pascanu, Razvan; Osindero, Simon; Hadsell, Raia",Meta-Learning with Latent Embedding Optimization,"arXiv:1807.05960 [cs, stat]",,,,http://arxiv.org/abs/1807.05960,"Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",2019-03-26,2022-03-11 0:21:27,2022-03-11 0:21:27,2022-03-11 0:21:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.05960,,/Users/jacquesthibodeau/Zotero/storage/9LN7MYIK/Rusu et al. - 2019 - Meta-Learning with Latent Embedding Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/GQMZXF35/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KFXRLHC9,journalArticle,2018,"Chen, Boyu; Lu, Wenlian; Fokoue, Ernest",Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training,"arXiv:1805.08462 [cs, stat]",,,,http://arxiv.org/abs/1805.08462,"Meta-learning is a promising method to achieve efficient training method towards deep neural net and has been attracting increases interests in recent years. But most of the current methods are still not capable to train complex neuron net model with long-time training process. In this paper, a novel second-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF) approach, is proposed based on the Hessian-Free approach. Two recurrent neural networks are established to generate the damping and the precondition matrix of this Hessian-Free framework. A series of techniques to meta-train the MLHF towards stable and reinforce the meta-training of this optimizer, including the gradient calculation of $H$. Numerical experiments on deep convolution neural nets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and ILSVRC2012, indicate that the MLHF shows good and continuous training performance during the whole long-time training process, i.e., both the rapid-decreasing early stage and the steadily-deceasing later stage, and so is a promising meta-learning framework towards elevating the training efficiency in real-world deep neural nets.",2018-09-07,2022-03-11 0:21:29,2022-03-11 0:21:29,2022-03-11 0:21:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08462,,/Users/jacquesthibodeau/Zotero/storage/RBLZTRU2/Chen et al. - 2018 - Meta-Learning with Hessian-Free Approach in Deep N.pdf; /Users/jacquesthibodeau/Zotero/storage/IF8GBRQK/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSNVCFBH,journalArticle,2020,"Yu, Tianhe; Kumar, Saurabh; Gupta, Abhishek; Levine, Sergey; Hausman, Karol; Finn, Chelsea",Gradient Surgery for Multi-Task Learning,"arXiv:2001.06782 [cs, stat]",,,,http://arxiv.org/abs/2001.06782,"While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.",2020-12-21,2022-03-11 0:21:30,2022-03-11 0:21:30,2022-03-11 0:21:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.06782,,/Users/jacquesthibodeau/Zotero/storage/LBQPTYYJ/Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/VYYYS3FF/2001.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJY58CHM,journalArticle,2019,"Ortega, Pedro A.; Wang, Jane X.; Rowland, Mark; Genewein, Tim; Kurth-Nelson, Zeb; Pascanu, Razvan; Heess, Nicolas; Veness, Joel; Pritzel, Alex; Sprechmann, Pablo; Jayakumar, Siddhant M.; McGrath, Tom; Miller, Kevin; Azar, Mohammad; Osband, Ian; Rabinowitz, Neil; György, András; Chiappa, Silvia; Osindero, Simon; Teh, Yee Whye; van Hasselt, Hado; de Freitas, Nando; Botvinick, Matthew; Legg, Shane",Meta-learning of Sequential Strategies,"arXiv:1905.03030 [cs, stat]",,,,http://arxiv.org/abs/1905.03030,"In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.",2019-07-18,2022-03-11 0:21:32,2022-03-11 0:21:32,2022-03-11 0:21:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.03030,,/Users/jacquesthibodeau/Zotero/storage/U8DBK45M/Ortega et al. - 2019 - Meta-learning of Sequential Strategies.pdf; /Users/jacquesthibodeau/Zotero/storage/MR89SNXK/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DK8Q24FZ,journalArticle,2019,"Zintgraf, Luisa M.; Shiarlis, Kyriacos; Kurin, Vitaly; Hofmann, Katja; Whiteson, Shimon",Fast Context Adaptation via Meta-Learning,"arXiv:1810.03642 [cs, stat]",,,,http://arxiv.org/abs/1810.03642,"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.",2019-06-10,2022-03-11 0:21:35,2022-03-11 0:21:35,2022-03-11 0:21:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.03642,,/Users/jacquesthibodeau/Zotero/storage/7LT489CU/Zintgraf et al. - 2019 - Fast Context Adaptation via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/Q6JC3GJ9/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHEV394W,journalArticle,2019,"Rabinowitz, Neil C.",Meta-learners' learning dynamics are unlike learners',"arXiv:1905.01320 [cs, stat]",,,,http://arxiv.org/abs/1905.01320,"Meta-learning is a tool that allows us to build sample-efficient learning systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just faster learners than their sample-inefficient deep learning (DL) and reinforcement learning (RL) brethren, but that they actually pursue fundamentally different learning trajectories. We study their learning dynamics on three sets of structured tasks for which the corresponding learning dynamics of DL and RL systems have been previously described: linear regression (Saxe et al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and contextual bandits (Schaul et al., 2019). In each case, while sample-inefficient DL and RL Learners uncover the task structure in a staggered manner, meta-trained LSTM Meta-Learners uncover almost all task structure concurrently, congruent with the patterns expected from Bayes-optimal inference algorithms. This has implications for research areas wherever the learning behaviour itself is of interest, such as safety, curriculum design, and human-in-the-loop machine learning.",2019-05-03,2022-03-11 0:21:36,2022-03-11 0:21:36,2022-03-11 0:21:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01320,,/Users/jacquesthibodeau/Zotero/storage/4MDPU3UD/Rabinowitz - 2019 - Meta-learners' learning dynamics are unlike learne.pdf; /Users/jacquesthibodeau/Zotero/storage/VFPVJYMS/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QNDLM3TU,journalArticle,2018,"Jamal, Muhammad Abdullah; Qi, Guo-Jun; Shah, Mubarak",Task-Agnostic Meta-Learning for Few-shot Learning,"arXiv:1805.07722 [cs, stat]",,,,http://arxiv.org/abs/1805.07722,"Meta-learning approaches have been proposed to tackle the few-shot learning problem.Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined.Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",2018-05-20,2022-03-11 0:21:38,2022-03-11 0:21:38,2022-03-11 0:21:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07722,,/Users/jacquesthibodeau/Zotero/storage/MCPE5Y7C/Jamal et al. - 2018 - Task-Agnostic Meta-Learning for Few-shot Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/JLUVXKGM/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GQRHWJI,journalArticle,2020,"Yin, Mingzhang; Tucker, George; Zhou, Mingyuan; Levine, Sergey; Finn, Chelsea",Meta-Learning without Memorization,"arXiv:1912.03820 [cs, stat]",,,,http://arxiv.org/abs/1912.03820,"The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.",2020-04-27,2022-03-11 0:21:41,2022-03-11 0:21:41,2022-03-11 0:21:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.03820,,/Users/jacquesthibodeau/Zotero/storage/ANSZTYKA/Yin et al. - 2020 - Meta-Learning without Memorization.pdf; /Users/jacquesthibodeau/Zotero/storage/AU3TIN42/1912.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A78INE69,journalArticle,2020,"Beaulieu, Shawn; Frati, Lapo; Miconi, Thomas; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff; Cheney, Nick",Learning to Continually Learn,"arXiv:2002.09571 [cs, stat]",,,,http://arxiv.org/abs/2002.09571,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).",2020-03-03,2022-03-11 0:21:44,2022-03-11 0:21:44,2022-03-11 0:21:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09571,,/Users/jacquesthibodeau/Zotero/storage/FYJKSKG8/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/UGAND2BI/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T8ZI6LQM,journalArticle,2020,"Triantafillou, Eleni; Zhu, Tyler; Dumoulin, Vincent; Lamblin, Pascal; Evci, Utku; Xu, Kelvin; Goroshin, Ross; Gelada, Carles; Swersky, Kevin; Manzagol, Pierre-Antoine; Larochelle, Hugo",Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,"arXiv:1903.03096 [cs, stat]",,,,http://arxiv.org/abs/1903.03096,"Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.",2020-04-08,2022-03-11 0:21:46,2022-03-11 0:21:46,2022-03-11 0:21:46,,,,,,,Meta-Dataset,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03096,,/Users/jacquesthibodeau/Zotero/storage/KUFEKE6U/Triantafillou et al. - 2020 - Meta-Dataset A Dataset of Datasets for Learning t.pdf; /Users/jacquesthibodeau/Zotero/storage/9T7R8TYP/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7M9Y82F4,journalArticle,2021,"Yu, Tianhe; Quillen, Deirdre; He, Zhanpeng; Julian, Ryan; Narayan, Avnish; Shively, Hayden; Bellathur, Adithya; Hausman, Karol; Finn, Chelsea; Levine, Sergey",Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,"arXiv:1910.10897 [cs, stat]",,,,http://arxiv.org/abs/1910.10897,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.",2021-06-14,2022-03-11 0:21:48,2022-03-11 0:21:48,2022-03-11 0:21:48,,,,,,,Meta-World,,,,,,,,,,,,arXiv.org,,arXiv: 1910.10897,,/Users/jacquesthibodeau/Zotero/storage/AKSAE5M2/Yu et al. - 2021 - Meta-World A Benchmark and Evaluation for Multi-T.pdf; /Users/jacquesthibodeau/Zotero/storage/IJNB5JED/1910.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75NGI4UC,journalArticle,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,arXiv:2105.14111 [cs],,,,http://arxiv.org/abs/2105.14111,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-06-08,2022-03-11 0:22:24,2022-03-11 0:22:24,2022-03-11 0:22:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.14111,,/Users/jacquesthibodeau/Zotero/storage/5GZB85I8/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/4IYP7S7S/2105.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AEYFC5S4,journalArticle,2018,"Baudart, Guillaume; Hirzel, Martin; Mandel, Louis",Deep Probabilistic Programming Languages: A Qualitative Study,arXiv:1804.06458 [cs],,,,http://arxiv.org/abs/1804.06458,"Deep probabilistic programming languages try to combine the advantages of deep learning with those of probabilistic programming languages. If successful, this would be a big step forward in machine learning and programming languages. Unfortunately, as of now, this new crop of languages is hard to use and understand. This paper addresses this problem directly by explaining deep probabilistic programming languages and indirectly by characterizing their current strengths and weaknesses.",2018-04-17,2022-03-11 0:22:31,2022-03-11 0:22:31,2022-03-11 0:22:31,,,,,,,Deep Probabilistic Programming Languages,,,,,,,,,,,,arXiv.org,,arXiv: 1804.06458,,/Users/jacquesthibodeau/Zotero/storage/R6G367U3/Baudart et al. - 2018 - Deep Probabilistic Programming Languages A Qualit.pdf; /Users/jacquesthibodeau/Zotero/storage/5235IPGE/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJSJWAGE,journalArticle,2021,"Dehghani, Mostafa; Tay, Yi; Gritsenko, Alexey A.; Zhao, Zhe; Houlsby, Neil; Diaz, Fernando; Metzler, Donald; Vinyals, Oriol",The Benchmark Lottery,arXiv:2107.07002 [cs],,,,http://arxiv.org/abs/2107.07002,"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",2021-07-14,2022-03-11 0:22:33,2022-03-11 0:22:33,2022-03-11 0:22:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.07002,,/Users/jacquesthibodeau/Zotero/storage/8S6JFLEC/Dehghani et al. - 2021 - The Benchmark Lottery.pdf; /Users/jacquesthibodeau/Zotero/storage/GL8YTKSX/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Information Retrieval; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3R2BW7HB,journalArticle,2020,"McAllester, David; Stratos, Karl",Formal Limitations on the Measurement of Mutual Information,"arXiv:1811.04251 [cs, math, stat]",,,,http://arxiv.org/abs/1811.04251,"Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N ).",2020-05-20,2022-03-11 0:22:36,2022-03-11 0:22:36,2022-03-11 0:22:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04251,,/Users/jacquesthibodeau/Zotero/storage/Q8UQEU4N/McAllester and Stratos - 2020 - Formal Limitations on the Measurement of Mutual In.pdf; /Users/jacquesthibodeau/Zotero/storage/IMSEKSAZ/1811.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V3SZ48EP,journalArticle,2021,"Ecoffet, Adrien; Lehman, Joel",Reinforcement Learning Under Moral Uncertainty,arXiv:2006.04734 [cs],,,,http://arxiv.org/abs/2006.04734,"An ambitious goal for machine learning is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed, e.g. fully autonomous vehicles will encounter charged moral decisions that complicate their deployment. While ethical agents could be trained by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement about the nature of morality. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. This paper translates such insights to the field of reinforcement learning, proposes two training methods that realize different points among competing desiderata, and trains agents in simple environments to act under moral uncertainty. The results illustrate (1) how such uncertainty can help curb extreme behavior from commitment to single theories and (2) several technical complications arising from attempting to ground moral philosophy in RL (e.g. how can a principled trade-off between two competing but incomparable reward functions be reached). The aim is to catalyze progress towards morally-competent agents and highlight the potential of RL to contribute towards the computational grounding of moral philosophy.",2021-07-19,2022-03-11 0:22:38,2022-03-11 0:22:38,2022-03-11 0:22:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04734,,/Users/jacquesthibodeau/Zotero/storage/YDMWYTIH/Ecoffet and Lehman - 2021 - Reinforcement Learning Under Moral Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/4AIRQYHA/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9AUTPFW,journalArticle,2018,"Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang",Building Ethics into Artificial Intelligence,arXiv:1812.02953 [cs],,,,http://arxiv.org/abs/1812.02953,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",2018-12-07,2022-03-11 0:22:40,2022-03-11 0:22:40,2022-03-11 0:22:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1812.02953,,/Users/jacquesthibodeau/Zotero/storage/2NZ4Z6IL/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/S35IUAF9/1812.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8S3PDAQL,journalArticle,2019,"Bussmann, Bart; Heinerman, Jacqueline; Lehman, Joel",Towards Empathic Deep Q-Learning,arXiv:1906.10918 [cs],,,,http://arxiv.org/abs/1906.10918,"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",2019-06-26,2022-03-11 0:22:43,2022-03-11 0:22:43,2022-03-11 0:22:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10918,,/Users/jacquesthibodeau/Zotero/storage/3YDENZMK/Bussmann et al. - 2019 - Towards Empathic Deep Q-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/NC37S6L2/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J7SFG9RW,journalArticle,2018,"Wang, Baoxiang; Sun, Tongfang; Zheng, Xianjun Sam",Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning,"arXiv:1807.00366 [cs, stat]",,,,http://arxiv.org/abs/1807.00366,"In recent years, reinforcement learning (RL) methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go, and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding different players' diverse behaviors. In this paper, we present a novel method called Multi-Motivation Behavior Modeling (MMBM) that takes the multifaceted human motivations into consideration and models the underlying value structure of the players using inverse RL. Our approach does not require the access to the dynamic of the system, making it feasible to model complex interactive environments such as massively multiplayer online games. MMBM is tested on the World of Warcraft Avatar History dataset, which recorded over 70,000 users' gameplay spanning three years period. Our model reveals the significant difference of value structures among different player groups. Using the results of motivation modeling, we also predict and explain their diverse gameplay behaviors and provide a quantitative assessment of how the redesign of the game environment impacts players' behaviors.",2018-07-05,2022-03-11 0:22:47,2022-03-11 0:22:47,2022-03-11 0:22:47,,,,,,,Beyond Winning and Losing,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00366,,/Users/jacquesthibodeau/Zotero/storage/BD6PN9RF/Wang et al. - 2018 - Beyond Winning and Losing Modeling Human Motivati.pdf; /Users/jacquesthibodeau/Zotero/storage/7KWTSYUH/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LGIEPNAD,journalArticle,2018,"Hristov, Yordan; Lascarides, Alex; Ramamoorthy, Subramanian",Interpretable Latent Spaces for Learning from Demonstration,arXiv:1807.06583 [cs],,,,http://arxiv.org/abs/1807.06583,"Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.",2018-10-02,2022-03-11 0:22:50,2022-03-11 0:22:50,2022-03-11 0:22:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06583,,/Users/jacquesthibodeau/Zotero/storage/PM7CKTEL/Hristov et al. - 2018 - Interpretable Latent Spaces for Learning from Demo.pdf; /Users/jacquesthibodeau/Zotero/storage/UWU527DM/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GJBMZ656,journalArticle,2020,"Hoang, Lê Nguyên",A Roadmap for Robust End-to-End Alignment,arXiv:1809.01036 [cs],,,,http://arxiv.org/abs/1809.01036,"This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.",2020-02-25,2022-03-11 0:22:52,2022-03-11 0:22:52,2022-03-11 0:22:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.01036,,/Users/jacquesthibodeau/Zotero/storage/Q3QFTPUW/Hoang - 2020 - A Roadmap for Robust End-to-End Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/X4EMRJRW/1809.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MWZQMXT5,journalArticle,2018,"Garmulewicz, Michał; Michalewski, Henryk; Miłoś, Piotr",Expert-augmented actor-critic for ViZDoom and Montezumas Revenge,"arXiv:1809.03447 [cs, stat]",,,,http://arxiv.org/abs/1809.03447,"We propose an expert-augmented actor-critic algorithm, which we evaluate on two environments with sparse rewards: Montezumas Revenge and a demanding maze from the ViZDoom suite. In the case of Montezumas Revenge, an agent trained with our method achieves very good results consistently scoring above 27,000 points (in many experiments beating the first world). With an appropriate choice of hyperparameters, our algorithm surpasses the performance of the expert data. In a number of experiments, we have observed an unreported bug in Montezumas Revenge which allowed the agent to score more than 800,000 points.",2018-09-10,2022-03-11 0:22:55,2022-03-11 0:22:55,2022-03-11 0:22:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03447,,/Users/jacquesthibodeau/Zotero/storage/ERETBZCC/Garmulewicz et al. - 2018 - Expert-augmented actor-critic for ViZDoom and Mont.pdf; /Users/jacquesthibodeau/Zotero/storage/6V6ZR5YQ/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XPDIWET2,journalArticle,2018,"Pathak, Deepak; Mahmoudieh, Parsa; Luo, Guanghao; Agrawal, Pulkit; Chen, Dian; Shentu, Yide; Shelhamer, Evan; Malik, Jitendra; Efros, Alexei A.; Darrell, Trevor",Zero-Shot Visual Imitation,"arXiv:1804.08606 [cs, stat]",,,,http://arxiv.org/abs/1804.08606,"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/",2018-04-23,2022-03-11 0:22:57,2022-03-11 0:22:57,2022-03-11 0:22:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.08606,,/Users/jacquesthibodeau/Zotero/storage/QLYCC974/Pathak et al. - 2018 - Zero-Shot Visual Imitation.pdf; /Users/jacquesthibodeau/Zotero/storage/8VSM6QA7/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVD32MFH,journalArticle,2019,"Sarafian, Elad; Tamar, Aviv; Kraus, Sarit",Constrained Policy Improvement for Safe and Efficient Reinforcement Learning,"arXiv:1805.07805 [cs, stat]",,,,http://arxiv.org/abs/1805.07805,"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",2019-07-10,2022-03-11 0:23:02,2022-03-11 0:23:02,2022-03-11 0:23:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07805,,/Users/jacquesthibodeau/Zotero/storage/KXWEZZFT/Sarafian et al. - 2019 - Constrained Policy Improvement for Safe and Effici.pdf; /Users/jacquesthibodeau/Zotero/storage/R4ZBJMQZ/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UFBQJEQH,journalArticle,2021,"Arora, Saurabh; Doshi, Prashant; Banerjee, Bikramjit",A Framework and Method for Online Inverse Reinforcement Learning,Autonomous Agents and Multi-Agent Systems,,"1387-2532, 1573-7454",10.1007/s10458-020-09485-4,http://arxiv.org/abs/1805.07871,"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",2021-04,2022-03-11 0:23:05,2022-03-11 0:23:05,2022-03-11 0:23:04,4,,1,35,,Auton Agent Multi-Agent Syst,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07871,,/Users/jacquesthibodeau/Zotero/storage/VXIK6NUU/Arora et al. - 2021 - A Framework and Method for Online Inverse Reinforc.pdf; /Users/jacquesthibodeau/Zotero/storage/MRYL3D2Q/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG25PBKC,journalArticle,2019,"Justesen, Niels; Duque, Miguel Gonzalez; Jaramillo, Daniel Cabarcas; Mouret, Jean-Baptiste; Risi, Sebastian",Learning a Behavioral Repertoire from Demonstrations,arXiv:1907.03046 [cs],,,,http://arxiv.org/abs/1907.03046,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",2019-07-05,2022-03-11 0:23:06,2022-03-11 0:23:06,2022-03-11 0:23:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.03046,,/Users/jacquesthibodeau/Zotero/storage/9URB9V2X/Justesen et al. - 2019 - Learning a Behavioral Repertoire from Demonstratio.pdf; /Users/jacquesthibodeau/Zotero/storage/EF3YPC9C/1907.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YWBZAMIA,journalArticle,2020,"Schneider, Johannes",Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,arXiv:2009.09266 [cs],,,,http://arxiv.org/abs/2009.09266,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",2020-09-19,2022-03-11 0:23:09,2022-03-11 0:23:09,2022-03-11 0:23:08,,,,,,,Humans learn too,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09266,,/Users/jacquesthibodeau/Zotero/storage/4HBXZ7DM/Schneider - 2020 - Humans learn too Better Human-AI Interaction usin.pdf; /Users/jacquesthibodeau/Zotero/storage/GWMQR73W/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B3WEYDWG,journalArticle,2019,"Lütjens, Björn; Everett, Michael; How, Jonathan P.",Safe Reinforcement Learning with Model Uncertainty Estimates,arXiv:1810.08700 [cs],,,,http://arxiv.org/abs/1810.08700,"Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.",2019-03-01,2022-03-11 0:27:25,2022-03-11 0:27:25,2022-03-11 0:27:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08700,,/Users/jacquesthibodeau/Zotero/storage/2B4WHVQY/Lütjens et al. - 2019 - Safe Reinforcement Learning with Model Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/AX5M4IK5/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BHI37WRL,journalArticle,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency via Attainable Utility Preservation,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",2020-02-07,2022-03-11 0:27:28,2022-03-11 0:27:28,2022-03-11 0:27:27,385-391,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/ZABTJAMX/Turner et al. - 2020 - Conservative Agency via Attainable Utility Preserv.pdf; /Users/jacquesthibodeau/Zotero/storage/ZGXPZFAR/1902.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R2TT2S7H,journalArticle,2021,"Thananjeyan, Brijen; Balakrishna, Ashwin; Nair, Suraj; Luo, Michael; Srinivasan, Krishnan; Hwang, Minho; Gonzalez, Joseph E.; Ibarz, Julian; Finn, Chelsea; Goldberg, Ken",Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones,arXiv:2010.15920 [cs],,,,http://arxiv.org/abs/2010.15920,"Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2 - 20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",2021-05-17,2022-03-11 0:27:29,2022-03-11 0:27:29,2022-03-11 0:27:29,,,,,,,Recovery RL,,,,,,,,,,,,arXiv.org,,arXiv: 2010.15920,,/Users/jacquesthibodeau/Zotero/storage/VY8X9J3J/Thananjeyan et al. - 2021 - Recovery RL Safe Reinforcement Learning with Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/4HWNEV5H/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N66IUBBR,journalArticle,2020,"Srinivasan, Krishnan; Eysenbach, Benjamin; Ha, Sehoon; Tan, Jie; Finn, Chelsea",Learning to be Safe: Deep RL with a Safety Critic,arXiv:2010.14603 [cs],,,,http://arxiv.org/abs/2010.14603,"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.",2020-10-27,2022-03-11 0:27:32,2022-03-11 0:27:32,2022-03-11 0:27:32,,,,,,,Learning to be Safe,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14603,,/Users/jacquesthibodeau/Zotero/storage/QAKENCAU/Srinivasan et al. - 2020 - Learning to be Safe Deep RL with a Safety Critic.pdf; /Users/jacquesthibodeau/Zotero/storage/VNCX7R62/2010.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VQPHB3C,journalArticle,2021,"Giacobbe, Mirco; Hasanbeig, Mohammadhosein; Kroening, Daniel; Wijk, Hjalmar",Shielding Atari Games with Bounded Prescience,arXiv:2101.08153 [cs],,,,http://arxiv.org/abs/2101.08153,"Deep reinforcement learning (DRL) is applied in safety-critical domains such as robotics and autonomous driving. It achieves superhuman abilities in many tasks, however whether DRL agents can be shown to act safely is an open problem. Atari games are a simple yet challenging exemplar for evaluating the safety of DRL agents and feature a diverse portfolio of game mechanics. The safety of neural agents has been studied before using methods that either require a model of the system dynamics or an abstraction; unfortunately, these are unsuitable to Atari games because their low-level dynamics are complex and hidden inside their emulator. We present the first exact method for analysing and ensuring the safety of DRL agents for Atari games. Our method only requires access to the emulator. First, we give a set of 43 properties that characterise ""safe behaviour"" for 30 games. Second, we develop a method for exploring all traces induced by an agent and a game and consider a variety of sources of game non-determinism. We observe that the best available DRL agents reliably satisfy only very few properties; several critical properties are violated by all agents. Finally, we propose a countermeasure that combines a bounded explicit-state exploration with shielding. We demonstrate that our method improves the safety of all agents over multiple properties.",2021-01-22,2022-03-11 0:27:34,2022-03-11 0:27:34,2022-03-11 0:27:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.08153,,/Users/jacquesthibodeau/Zotero/storage/INALW3AF/Giacobbe et al. - 2021 - Shielding Atari Games with Bounded Prescience.pdf; /Users/jacquesthibodeau/Zotero/storage/YN9BZ4UC/2101.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GHBNC544,journalArticle,2019,"Krakovna, Victoria; Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane",Penalizing side effects using stepwise relative reachability,"arXiv:1806.01186 [cs, stat]",,,,http://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",2019-03-08,2022-03-11 0:27:37,2022-03-11 0:27:37,2022-03-11 0:27:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01186,,/Users/jacquesthibodeau/Zotero/storage/4IKN8UWW/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/HBZ82Z86/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VCC8ID63,journalArticle,2021,"Leibo, Joel Z.; Duéñez-Guzmán, Edgar; Vezhnevets, Alexander Sasha; Agapiou, John P.; Sunehag, Peter; Koster, Raphael; Matyas, Jayd; Beattie, Charles; Mordatch, Igor; Graepel, Thore",Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot,arXiv:2107.06857 [cs],,,,http://arxiv.org/abs/2107.06857,"Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.",2021-07-14,2022-03-11 0:27:39,2022-03-11 0:27:39,2022-03-11 0:27:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.06857,,/Users/jacquesthibodeau/Zotero/storage/WLHS7YMG/Leibo et al. - 2021 - Scalable Evaluation of Multi-Agent Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/6QF6MR3D/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCCGZ43G,journalArticle,2019,"Wang, Yixin; Blei, David M.",The Blessings of Multiple Causes,"arXiv:1805.06826 [cs, stat]",,,,http://arxiv.org/abs/1805.06826,"Causal inference from observational data often assumes ""ignorability,"" that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.",2019-04-14,2022-03-11 0:28:10,2022-03-11 0:28:10,2022-03-11 0:28:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.06826,,/Users/jacquesthibodeau/Zotero/storage/WVASPT36/Wang and Blei - 2019 - The Blessings of Multiple Causes.pdf; /Users/jacquesthibodeau/Zotero/storage/LPE4QRY3/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Statistics - Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ULMJWB94,journalArticle,2018,"Trazzi, Michaël; Yampolskiy, Roman V.",Building Safer AGI by introducing Artificial Stupidity,arXiv:1808.03644 [cs],,,,http://arxiv.org/abs/1808.03644,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",2018-08-10,2022-03-11 0:28:14,2022-03-11 0:28:14,2022-03-11 0:28:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.03644,,/Users/jacquesthibodeau/Zotero/storage/VX3GHL26/Trazzi and Yampolskiy - 2018 - Building Safer AGI by introducing Artificial Stupi.pdf; /Users/jacquesthibodeau/Zotero/storage/AB6X3PZG/1808.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L2JGDWI9,journalArticle,2021,"Gabriel, Iason; Ghazavi, Vafa",The Challenge of Value Alignment: from Fairer Algorithms to AI Safety,arXiv:2101.06060 [cs],,,,http://arxiv.org/abs/2101.06060,"This paper addresses the question of how to align AI systems with human values and situates it within a wider body of thought regarding technology and value. Far from existing in a vacuum, there has long been an interest in the ability of technology to 'lock-in' different value systems. There has also been considerable thought about how to align technologies with specific social values, including through participatory design-processes. In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before. Drawing important continuities between the work of the fairness, accountability, transparency and ethics community, and work being done by technical AI safety researchers, we suggest that more attention needs to be paid to the question of 'social value alignment' - that is, how to align AI systems with the plurality of values endorsed by groups of people, especially on the global level.",2021-01-18,2022-03-11 0:28:21,2022-03-11 0:28:21,2022-03-11 0:28:21,,,,,,,The Challenge of Value Alignment,,,,,,,,,,,,arXiv.org,,arXiv: 2101.06060,,/Users/jacquesthibodeau/Zotero/storage/WKC623JG/Gabriel and Ghazavi - 2021 - The Challenge of Value Alignment from Fairer Algo.pdf; /Users/jacquesthibodeau/Zotero/storage/L5QUXVDP/2101.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EZ3D88F9,journalArticle,2021,"Gruetzemacher, Ross; Whittlestone, Jess",The Transformative Potential of Artificial Intelligence,arXiv:1912.00747 [cs],,,,http://arxiv.org/abs/1912.00747,"The terms 'human-level artificial intelligence' and 'artificial general intelligence' are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term 'transformative AI' is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be 'transformative' is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.",2021-10-23,2022-03-11 0:28:26,2022-03-11 0:28:26,2022-03-11 0:28:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.00747,,/Users/jacquesthibodeau/Zotero/storage/2LPPT7JQ/Gruetzemacher and Whittlestone - 2021 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/Q4TGQNJJ/1912.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8DCH2NZW,journalArticle,2020,"Shah, Ankit; Li, Shen; Shah, Julie",Planning With Uncertain Specifications (PUnS),IEEE Robotics and Automation Letters,,"2377-3766, 2377-3774",10.1109/LRA.2020.2977217,http://arxiv.org/abs/1906.03218,"Reward engineering is crucial to high performance in reinforcement learning systems. Prior research into reward design has largely focused on Markovian functions representing the reward. While there has been research into expressing non-Markov rewards as linear temporal logic (LTL) formulas, this has focused on task specifications directly defined by the user. However, in many real-world applications, task specifications are ambiguous, and can only be expressed as a belief over LTL formulas. In this paper, we introduce planning with uncertain specifications (PUnS), a novel formulation that addresses the challenge posed by non-Markovian specifications expressed as beliefs over LTL formulas. We present four criteria that capture the semantics of satisfying a belief over specifications for different applications, and analyze the qualitative implications of these criteria within a synthetic domain. We demonstrate the existence of an equivalent Markov decision process (MDP) for any instance of PUnS. Finally, we demonstrate our approach on the real-world task of setting a dinner table automatically with a robot that inferred task specifications from human demonstrations.",2020-04,2022-03-11 0:28:31,2022-03-11 0:28:31,2022-03-11 0:28:31,3414-3421,,2,5,,IEEE Robot. Autom. Lett.,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.03218,,/Users/jacquesthibodeau/Zotero/storage/LG4CHIQ5/Shah et al. - 2020 - Planning With Uncertain Specifications (PUnS).pdf; /Users/jacquesthibodeau/Zotero/storage/BQBFGT5K/1906.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UVBRGSUJ,journalArticle,2021,"Du, Yuqing; Tiomkin, Stas; Kiciman, Emre; Polani, Daniel; Abbeel, Pieter; Dragan, Anca",AvE: Assistance via Empowerment,arXiv:2006.14796 [cs],,,,http://arxiv.org/abs/2006.14796,"One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s). Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.",2021-01-07,2022-03-11 0:28:33,2022-03-11 0:28:33,2022-03-11 0:28:33,,,,,,,AvE,,,,,,,,,,,,arXiv.org,,arXiv: 2006.14796,,/Users/jacquesthibodeau/Zotero/storage/J53YANPG/Du et al. - 2021 - AvE Assistance via Empowerment.pdf; /Users/jacquesthibodeau/Zotero/storage/FRMSL5TI/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQ9TRJU9,journalArticle,2020,"Thomas, Rachel; Uminsky, David",The Problem with Metrics is a Fundamental Problem for AI,arXiv:2002.08512 [cs],,,,http://arxiv.org/abs/2002.08512,"Optimizing a given metric is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences. This poses a fundamental contradiction for AI development. Through a series of real-world case studies, we look at various aspects of where metrics go wrong in practice and aspects of how our online environment and current business practices are exacerbating these failures. Finally, we propose a framework towards mitigating the harms caused by overemphasis of metrics within AI by: (1) using a slate of metrics to get a fuller and more nuanced picture, (2) combining metrics with qualitative accounts, and (3) involving a range of stakeholders, including those who will be most impacted.",2020-02-19,2022-03-11 0:28:36,2022-03-11 0:28:36,2022-03-11 0:28:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.08512,,/Users/jacquesthibodeau/Zotero/storage/MYXUEF2X/Thomas and Uminsky - 2020 - The Problem with Metrics is a Fundamental Problem .pdf; /Users/jacquesthibodeau/Zotero/storage/7GYLEEBA/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAIW3WZX,journalArticle,2019,"Dobbe, Roel; Gilbert, Thomas Krendl; Mintz, Yonatan",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"arXiv:1911.09005 [cs, eess]",,,,http://arxiv.org/abs/1911.09005,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",2019-11-20,2022-03-11 0:28:40,2022-03-11 0:28:40,2022-03-11 0:28:40,,,,,,,Hard Choices in Artificial Intelligence,,,,,,,,,,,,arXiv.org,,arXiv: 1911.09005,,/Users/jacquesthibodeau/Zotero/storage/Y6WWHFR4/Dobbe et al. - 2019 - Hard Choices in Artificial Intelligence Addressin.pdf; /Users/jacquesthibodeau/Zotero/storage/VLRQSA58/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JI9QN779,journalArticle,2020,"McGregor, Sean",Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,arXiv:2011.08512 [cs],,,,http://arxiv.org/abs/2011.08512,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.",2020-11-17,2022-03-11 0:29:15,2022-03-11 0:29:15,2022-03-11 0:29:15,,,,,,,Preventing Repeated Real World AI Failures by Cataloging Incidents,,,,,,,,,,,,arXiv.org,,arXiv: 2011.08512,,/Users/jacquesthibodeau/Zotero/storage/4IKPVPQF/McGregor - 2020 - Preventing Repeated Real World AI Failures by Cata.pdf; /Users/jacquesthibodeau/Zotero/storage/QWR24NYR/2011.html,,,Computer Science - Computers and Society; Computer Science - Software Engineering; I.2.0; K.4.0; K.4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5JQCG2KN,journalArticle,2020,"Juric, Mislav; Sandic, Agneza; Brcic, Mario",AI safety: state of the field through quantitative lens,arXiv:2002.05671 [cs],,,,http://arxiv.org/abs/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.",2020-07-09,2022-03-11 0:29:20,2022-03-11 0:29:20,2022-03-11 0:29:20,,,,,,,AI safety,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05671,,/Users/jacquesthibodeau/Zotero/storage/MQUVP8RM/Juric et al. - 2020 - AI safety state of the field through quantitative.pdf; /Users/jacquesthibodeau/Zotero/storage/ZXNJAUER/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JD5VURSB,journalArticle,2021,"Agarwal, Sandhini; Krueger, Gretchen; Clark, Jack; Radford, Alec; Kim, Jong Wook; Brundage, Miles",Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications,arXiv:2108.02818 [cs],,,,http://arxiv.org/abs/2108.02818,"Recently, there have been breakthroughs in computer vision (""CV"") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.",2021-08-05,2022-03-11 0:29:23,2022-03-11 0:29:23,2022-03-11 0:29:22,,,,,,,Evaluating CLIP,,,,,,,,,,,,arXiv.org,,arXiv: 2108.02818,,/Users/jacquesthibodeau/Zotero/storage/9RD9AH54/Agarwal et al. - 2021 - Evaluating CLIP Towards Characterization of Broad.pdf; /Users/jacquesthibodeau/Zotero/storage/ALLRUCF9/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7JZBY3HQ,journalArticle,2019,"Everitt, Tom; Kumar, Ramana; Krakovna, Victoria; Legg, Shane",Modeling AGI Safety Frameworks with Causal Influence Diagrams,arXiv:1906.08663 [cs],,,,http://arxiv.org/abs/1906.08663,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.",2019-06-20,2022-03-11 0:29:25,2022-03-11 0:29:25,2022-03-11 0:29:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.08663,,/Users/jacquesthibodeau/Zotero/storage/CQYTJDAI/Everitt et al. - 2019 - Modeling AGI Safety Frameworks with Causal Influen.pdf; /Users/jacquesthibodeau/Zotero/storage/NEE7D275/1906.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD9FFH2X,journalArticle,2017,"Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane",Reinforcement Learning with a Corrupted Reward Channel,"arXiv:1705.08417 [cs, stat]",,,,http://arxiv.org/abs/1705.08417,"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.",2017-08-19,2022-03-11 0:30:11,2022-03-11 0:30:11,2022-03-11 0:30:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.08417,,/Users/jacquesthibodeau/Zotero/storage/WI7P9VB4/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/EZ3TIG45/1705.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2.8; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ANIXJHD,journalArticle,2016,"Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","arXiv:1602.04938 [cs, stat]",,,,http://arxiv.org/abs/1602.04938,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016-08-09,2022-03-11 0:30:12,2022-03-11 0:30:12,2022-03-11 0:30:11,,,,,,,"""Why Should I Trust You?",,,,,,,,,,,,arXiv.org,,arXiv: 1602.04938,,/Users/jacquesthibodeau/Zotero/storage/Z48JVI4D/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf; /Users/jacquesthibodeau/Zotero/storage/AG2K5IYQ/1602.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P9MZ98YH,journalArticle,2021,"Everitt, Tom; Hutter, Marcus; Kumar, Ramana; Krakovna, Victoria",Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,arXiv:1908.04734 [cs],,,,http://arxiv.org/abs/1908.04734,"Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.",2021-03-26,2022-03-11 0:30:17,2022-03-11 0:30:17,2022-03-11 0:30:17,,,,,,,Reward Tampering Problems and Solutions in Reinforcement Learning,,,,,,,,,,,,arXiv.org,,arXiv: 1908.04734,,/Users/jacquesthibodeau/Zotero/storage/ZNYDQEC2/Everitt et al. - 2021 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/GX8RR8QS/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTZGCY88,journalArticle,2018,"Kusner, Matt J.; Loftus, Joshua R.; Russell, Chris; Silva, Ricardo",Counterfactual Fairness,"arXiv:1703.06856 [cs, stat]",,,,http://arxiv.org/abs/1703.06856,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",2018-03-08,2022-03-11 0:30:20,2022-03-11 0:30:20,2022-03-11 0:30:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1703.06856,,/Users/jacquesthibodeau/Zotero/storage/8UNC2Q22/Kusner et al. - 2018 - Counterfactual Fairness.pdf; /Users/jacquesthibodeau/Zotero/storage/HAZ3ZDVQ/1703.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZSBDWYX,journalArticle,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,arXiv:2009.09071 [cs],,,,http://arxiv.org/abs/2009.09071,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-03-11 0:30:24,2022-03-11 0:30:24,2022-03-11 0:30:23,,,,,,,Measurement in AI Policy,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09071,,/Users/jacquesthibodeau/Zotero/storage/UEBGQ7GJ/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/VPKHXFGW/2009.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8M59IRFP,journalArticle,2020,"Hämäläinen, Perttu; Babadi, Amin; Ma, Xiaoxiao; Lehtinen, Jaakko",PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,"arXiv:1810.02541 [cs, stat]",,,,http://arxiv.org/abs/1810.02541,"Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, we observe that in a continuous action space, PPO can prematurely shrink the exploration variance, which leads to slow progress and may make the algorithm prone to getting stuck in local optima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization method designed for robustness in similar situations, we propose PPO-CMA, a proximal policy optimization approach that adaptively expands the exploration variance to speed up progress. With only minor changes to PPO, our algorithm considerably improves performance in Roboschool continuous control benchmarks. Our results also show that PPO-CMA, as opposed to PPO, is significantly less sensitive to the choice of hyperparameters, allowing one to use it in complex movement optimization tasks without requiring tedious tuning.",2020-11-03,2022-03-11 0:30:30,2022-03-11 0:30:30,2022-03-11 0:30:30,,,,,,,PPO-CMA,,,,,,,,,,,,arXiv.org,,arXiv: 1810.02541,,/Users/jacquesthibodeau/Zotero/storage/B4VY2NZA/Hämäläinen et al. - 2020 - PPO-CMA Proximal Policy Optimization with Covaria.pdf; /Users/jacquesthibodeau/Zotero/storage/HWWDBFTB/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AI78XF9B,journalArticle,2020,"Garcez, Artur d'Avila; Lamb, Luis C.",Neurosymbolic AI: The 3rd Wave,arXiv:2012.05876 [cs],,,,http://arxiv.org/abs/2012.05876,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.",2020-12-16,2022-03-11 0:30:33,2022-03-11 0:30:33,2022-03-11 0:30:33,,,,,,,Neurosymbolic AI,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05876,,/Users/jacquesthibodeau/Zotero/storage/W74BGLU4/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf; /Users/jacquesthibodeau/Zotero/storage/5IKSJA2A/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.4; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9UP6XT5,journalArticle,2019,"Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina",BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,arXiv:1810.04805 [cs],,,,http://arxiv.org/abs/1810.04805,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",2019-05-24,2022-03-11 0:30:37,2022-03-11 0:30:37,2022-03-11 0:30:37,,,,,,,BERT,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04805,,/Users/jacquesthibodeau/Zotero/storage/W7MUYFB6/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf; /Users/jacquesthibodeau/Zotero/storage/ABQFIA7D/1810.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRHRJGD8,journalArticle,2019,"Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon",Stand-Alone Self-Attention in Vision Models,arXiv:1906.05909 [cs],,,,http://arxiv.org/abs/1906.05909,"Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.",2019-06-13,2022-03-11 0:30:42,2022-03-11 0:30:42,2022-03-11 0:30:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.05909,,/Users/jacquesthibodeau/Zotero/storage/SNCTRMZ4/Ramachandran et al. - 2019 - Stand-Alone Self-Attention in Vision Models.pdf; /Users/jacquesthibodeau/Zotero/storage/SAFN3FL6/1906.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSF2D2HR,journalArticle,2021,"Tsividis, Pedro A.; Loula, Joao; Burga, Jake; Foss, Nathan; Campero, Andres; Pouncy, Thomas; Gershman, Samuel J.; Tenenbaum, Joshua B.","Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning",arXiv:2107.12544 [cs],,,,http://arxiv.org/abs/2107.12544,"Reinforcement learning (RL) studies how an agent comes to achieve reward in an environment through interactions over time. Recent advances in machine RL have surpassed human expertise at the world's oldest board games and many classic video games, but they require vast quantities of experience to learn successfully -- none of today's algorithms account for the human ability to learn so many different tasks, so quickly. Here we propose a new approach to this challenge based on a particularly strong form of model-based RL which we call Theory-Based Reinforcement Learning, because it uses human-like intuitive theories -- rich, abstract, causal models of physical objects, intentional agents, and their interactions -- to explore and model an environment, and plan effectively to achieve task goals. We instantiate the approach in a video game playing agent called EMPA (the Exploring, Modeling, and Planning Agent), which performs Bayesian inference to learn probabilistic generative models expressed as programs for a game-engine simulator, and runs internal simulations over these models to support efficient object-based, relational exploration and heuristic planning. EMPA closely matches human learning efficiency on a suite of 90 challenging Atari-style video games, learning new games in just minutes of game play and generalizing robustly to new game situations and new levels. The model also captures fine-grained structure in people's exploration trajectories and learning dynamics. Its design and behavior suggest a way forward for building more general human-like AI systems.",2021-07-26,2022-03-11 0:33:04,2022-03-11 0:33:04,2022-03-11 0:33:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.12544,,/Users/jacquesthibodeau/Zotero/storage/ICGUHKRE/Tsividis et al. - 2021 - Human-Level Reinforcement Learning through Theory-.pdf; /Users/jacquesthibodeau/Zotero/storage/23XJFZNC/2107.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z23IGLPG,journalArticle,2019,"Achiam, Joshua; Knight, Ethan; Abbeel, Pieter",Towards Characterizing Divergence in Deep Q-Learning,arXiv:1903.08894 [cs],,,,http://arxiv.org/abs/1903.08894,"Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",2019-03-21,2022-03-11 0:33:07,2022-03-11 0:33:07,2022-03-11 0:33:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.08894,,/Users/jacquesthibodeau/Zotero/storage/GRMW4YVF/Achiam et al. - 2019 - Towards Characterizing Divergence in Deep Q-Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/EDVKUZRH/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WJVN8UJY,journalArticle,2019,"Lynch, Corey; Khansari, Mohi; Xiao, Ted; Kumar, Vikash; Tompson, Jonathan; Levine, Sergey; Sermanet, Pierre",Learning Latent Plans from Play,arXiv:1903.01973 [cs],,,,http://arxiv.org/abs/1903.01973,"Acquiring a diverse repertoire of general-purpose skills remains an open challenge for robotics. In this work, we propose self-supervising control on top of human teleoperated play data as a way to scale up skill learning. Play has two properties that make it attractive compared to conventional task demonstrations. Play is cheap, as it can be collected in large quantities quickly without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, covering ~4x more interaction space than task demonstrations for the same amount of collection time. To learn control from play, we introduce Play-LMP, a self-supervised method that learns to organize play behaviors in a latent space, then reuse them at test time to achieve specific goals. Combining self-supervised control with a diverse play dataset shifts the focus of skill learning from a narrow and discrete set of tasks to the full continuum of behaviors available in an environment. We find that this combination generalizes well empirically---after self-supervising on unlabeled play, our method substantially outperforms individual expert-trained policies on 18 difficult user-specified visual manipulation tasks in a simulated robotic tabletop environment. We additionally find that play-supervised models, unlike their expert-trained counterparts, are more robust to perturbations and exhibit retrying-till-success behaviors. Finally, we find that our agent organizes its latent plan space around functional tasks, despite never being trained with task labels. Videos, code and data are available at learning-from-play.github.io",2019-12-20,2022-03-11 0:33:30,2022-03-11 0:33:30,2022-03-11 0:33:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01973,,/Users/jacquesthibodeau/Zotero/storage/8XLXFE7B/Lynch et al. - 2019 - Learning Latent Plans from Play.pdf; /Users/jacquesthibodeau/Zotero/storage/B2RIBE7N/1903.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQNM4NVC,journalArticle,2020,"Schrittwieser, Julian; Antonoglou, Ioannis; Hubert, Thomas; Simonyan, Karen; Sifre, Laurent; Schmitt, Simon; Guez, Arthur; Lockhart, Edward; Hassabis, Demis; Graepel, Thore; Lillicrap, Timothy; Silver, David","Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",Nature,,"0028-0836, 1476-4687",10.1038/s41586-020-03051-4,http://arxiv.org/abs/1911.08265,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",2020-12-24,2022-03-11 0:33:33,2022-03-11 0:33:33,2022-03-11 0:33:33,604-609,,7839,588,,Nature,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.08265,,"/Users/jacquesthibodeau/Zotero/storage/EXSBLA3L/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf; /Users/jacquesthibodeau/Zotero/storage/W42DTMHZ/1911.html",,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VWL3E59U,journalArticle,2018,"McAleer, Stephen; Agostinelli, Forest; Shmakov, Alexander; Baldi, Pierre",Solving the Rubik's Cube Without Human Knowledge,arXiv:1805.07470 [cs],,,,http://arxiv.org/abs/1805.07470,"A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have achieved superhuman proficiency in Go, Chess, and Shogi without human data or domain knowledge. In these environments, a reward is always received at the end of the game, however, for many combinatorial optimization environments, rewards are sparse and episodes are not guaranteed to terminate. We introduce Autodidactic Iteration: a novel reinforcement learning algorithm that is able to teach itself how to solve the Rubik's Cube with no human assistance. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves -- less than or equal to solvers that employ human domain knowledge.",2018-05-18,2022-03-11 0:33:34,2022-03-11 0:33:34,2022-03-11 0:33:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07470,,/Users/jacquesthibodeau/Zotero/storage/TR9F8GMC/McAleer et al. - 2018 - Solving the Rubik's Cube Without Human Knowledge.pdf; /Users/jacquesthibodeau/Zotero/storage/5VNLMHPN/1805.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9R86ZKTI,journalArticle,2019,"OpenAI; Berner, Christopher; Brockman, Greg; Chan, Brooke; Cheung, Vicki; Dębiak, Przemysław; Dennison, Christy; Farhi, David; Fischer, Quirin; Hashme, Shariq; Hesse, Chris; Józefowicz, Rafal; Gray, Scott; Olsson, Catherine; Pachocki, Jakub; Petrov, Michael; Pinto, Henrique P. d O.; Raiman, Jonathan; Salimans, Tim; Schlatter, Jeremy; Schneider, Jonas; Sidor, Szymon; Sutskever, Ilya; Tang, Jie; Wolski, Filip; Zhang, Susan",Dota 2 with Large Scale Deep Reinforcement Learning,"arXiv:1912.06680 [cs, stat]",,,,http://arxiv.org/abs/1912.06680,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",2019-12-13,2022-03-11 0:33:36,2022-03-11 0:33:36,2022-03-11 0:33:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.06680,,/Users/jacquesthibodeau/Zotero/storage/MFT65EEP/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/4CYUQFXY/1912.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A6CXTKX7,journalArticle,2018,"Aytar, Yusuf; Pfaff, Tobias; Budden, David; Paine, Tom Le; Wang, Ziyu; de Freitas, Nando",Playing hard exploration games by watching YouTube,"arXiv:1805.11592 [cs, stat]",,,,http://arxiv.org/abs/1805.11592,"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",2018-11-30,2022-03-11 0:33:38,2022-03-11 0:33:38,2022-03-11 0:33:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.11592,,/Users/jacquesthibodeau/Zotero/storage/KZAEZ5ET/Aytar et al. - 2018 - Playing hard exploration games by watching YouTube.pdf; /Users/jacquesthibodeau/Zotero/storage/Z8ERTZA8/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS3KHCNI,journalArticle,2021,"Evans, Charles; Kasirzadeh, Atoosa",User Tampering in Reinforcement Learning Recommender Systems,arXiv:2109.04083 [cs],,,,http://arxiv.org/abs/2109.04083,"This paper provides the first formalisation and empirical demonstration of a particular safety concern in reinforcement learning (RL)-based news and social media recommendation algorithms. This safety concern is what we call ""user tampering"" -- a phenomenon whereby an RL-based recommender system may manipulate a media user's opinions, preferences and beliefs via its recommendations as part of a policy to increase long-term user engagement. We provide a simulation study of a media recommendation problem constrained to the recommendation of political content, and demonstrate that a Q-learning algorithm consistently learns to exploit its opportunities to 'polarise' simulated 'users' with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. Finally, we argue that given our findings, designing an RL-based recommender system which cannot learn to exploit user tampering requires making the metric for the recommender's success independent of observable signals of user engagement, and thus that a media recommendation system built solely with RL is necessarily either unsafe, or almost certainly commercially unviable.",2021-09-09,2022-03-11 0:33:41,2022-03-11 0:33:41,2022-03-11 0:33:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.04083,,/Users/jacquesthibodeau/Zotero/storage/SYFSTEJ4/Evans and Kasirzadeh - 2021 - User Tampering in Reinforcement Learning Recommend.pdf; /Users/jacquesthibodeau/Zotero/storage/93SPQH6C/2109.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVZAHE3A,journalArticle,2021,"Stray, Jonathan",Designing Recommender Systems to Depolarize,arXiv:2107.04953 [cs],,,,http://arxiv.org/abs/2107.04953,"Polarization is implicated in the erosion of democracy and the progression to violence, which makes the polarization properties of large algorithmic content selection systems (recommender systems) a matter of concern for peace and security. While algorithm-driven social media does not seem to be a primary driver of polarization at the country level, it could be a useful intervention point in polarized societies. This paper examines algorithmic depolarization interventions with the goal of conflict transformation: not suppressing or eliminating conflict but moving towards more constructive conflict. Algorithmic intervention is considered at three stages: which content is available (moderation), how content is selected and personalized (ranking), and content presentation and controls (user interface). Empirical studies of online conflict suggest that the exposure diversity intervention proposed as an antidote to ""filter bubbles"" can be improved and can even worsen polarization under some conditions. Using civility metrics in conjunction with diversity in content selection may be more effective. However, diversity-based interventions have not been tested at scale and may not work in the diverse and dynamic contexts of real platforms. Instead, intervening in platform polarization dynamics will likely require continuous monitoring of polarization metrics, such as the widely used ""feeling thermometer."" These metrics can be used to evaluate product features, and potentially engineered as algorithmic objectives. It may further prove necessary to include polarization measures in the objective functions of recommender algorithms to prevent optimization processes from creating conflict as a side effect.",2021-07-10,2022-03-11 0:33:43,2022-03-11 0:33:43,2022-03-11 0:33:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.04953,,/Users/jacquesthibodeau/Zotero/storage/9HJRYVZQ/Stray - 2021 - Designing Recommender Systems to Depolarize.pdf; /Users/jacquesthibodeau/Zotero/storage/865PUTQ9/2107.html,,,Computer Science - Computers and Society; Computer Science - Information Retrieval; Computer Science - Social and Information Networks; J.4; K.4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7MGE4MN6,journalArticle,2021,"Milli, Smitha; Belli, Luca; Hardt, Moritz",From Optimizing Engagement to Measuring Value,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445933,http://arxiv.org/abs/2008.12623,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of ""value"" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".",2021-03-03,2022-03-11 0:33:46,2022-03-11 0:33:46,2022-03-11 0:33:45,714-722,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.12623,,/Users/jacquesthibodeau/Zotero/storage/IDAKGWMV/Milli et al. - 2021 - From Optimizing Engagement to Measuring Value.pdf; /Users/jacquesthibodeau/Zotero/storage/R9VPMV64/2008.html,,,Computer Science - Machine Learning; Computer Science - Social and Information Networks; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S6MGPEED,journalArticle,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),arXiv:1901.00064 [cs],,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2022-03-11 0:33:47,2022-03-11 0:33:47,2022-03-11 0:33:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.00064,,/Users/jacquesthibodeau/Zotero/storage/SNSESZY4/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf; /Users/jacquesthibodeau/Zotero/storage/PRUXRYYW/1901.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2CNAP7E4,journalArticle,2020,"Ecoffet, Adrien; Clune, Jeff; Lehman, Joel",Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,arXiv:2006.07495 [cs],,,,http://arxiv.org/abs/2006.07495,"Artificial life originated and has long studied the topic of open-ended evolution, which seeks the principles underlying artificial systems that innovate continually, inspired by biological evolution. Recently, interest has grown within the broader field of AI in a generalization of open-ended evolution, here called open-ended search, wherein such questions of open-endedness are explored for advancing AI, whatever the nature of the underlying search algorithm (e.g. evolutionary or gradient-based). For example, open-ended search might design new architectures for neural networks, new reinforcement learning algorithms, or most ambitiously, aim at designing artificial general intelligence. This paper proposes that open-ended evolution and artificial life have much to contribute towards the understanding of open-ended AI, focusing here in particular on the safety of open-ended search. The idea is that AI systems are increasingly applied in the real world, often producing unintended harms in the process, which motivates the growing field of AI safety. This paper argues that open-ended AI has its own safety challenges, in particular, whether the creativity of open-ended systems can be productively and predictably controlled. This paper explains how unique safety problems manifest in open-ended search, and suggests concrete contributions and research questions to explore them. The hope is to inspire progress towards creative, useful, and safe open-ended search algorithms.",2020-06-12,2022-03-11 0:33:50,2022-03-11 0:33:50,2022-03-11 0:33:50,,,,,,,Open Questions in Creating Safe Open-ended AI,,,,,,,,,,,,arXiv.org,,arXiv: 2006.07495,,/Users/jacquesthibodeau/Zotero/storage/4UQVZ9RP/Ecoffet et al. - 2020 - Open Questions in Creating Safe Open-ended AI Ten.pdf; /Users/jacquesthibodeau/Zotero/storage/Q9EDWC2Z/2006.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAMYM89F,journalArticle,2021,"Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik; Buch, Shyamal; Card, Dallas; Castellon, Rodrigo; Chatterji, Niladri; Chen, Annie; Creel, Kathleen; Davis, Jared Quincy; Demszky, Dora; Donahue, Chris; Doumbouya, Moussa; Durmus, Esin; Ermon, Stefano; Etchemendy, John; Ethayarajh, Kawin; Fei-Fei, Li; Finn, Chelsea; Gale, Trevor; Gillespie, Lauren; Goel, Karan; Goodman, Noah; Grossman, Shelby; Guha, Neel; Hashimoto, Tatsunori; Henderson, Peter; Hewitt, John; Ho, Daniel E.; Hong, Jenny; Hsu, Kyle; Huang, Jing; Icard, Thomas; Jain, Saahil; Jurafsky, Dan; Kalluri, Pratyusha; Karamcheti, Siddharth; Keeling, Geoff; Khani, Fereshte; Khattab, Omar; Koh, Pang Wei; Krass, Mark; Krishna, Ranjay; Kuditipudi, Rohith; Kumar, Ananya; Ladhak, Faisal; Lee, Mina; Lee, Tony; Leskovec, Jure; Levent, Isabelle; Li, Xiang Lisa; Li, Xuechen; Ma, Tengyu; Malik, Ali; Manning, Christopher D.; Mirchandani, Suvir; Mitchell, Eric; Munyikwa, Zanele; Nair, Suraj; Narayan, Avanika; Narayanan, Deepak; Newman, Ben; Nie, Allen; Niebles, Juan Carlos; Nilforoshan, Hamed; Nyarko, Julian; Ogut, Giray; Orr, Laurel; Papadimitriou, Isabel; Park, Joon Sung; Piech, Chris; Portelance, Eva; Potts, Christopher; Raghunathan, Aditi; Reich, Rob; Ren, Hongyu; Rong, Frieda; Roohani, Yusuf; Ruiz, Camilo; Ryan, Jack; Ré, Christopher; Sadigh, Dorsa; Sagawa, Shiori; Santhanam, Keshav; Shih, Andy; Srinivasan, Krishnan; Tamkin, Alex; Taori, Rohan; Thomas, Armin W.; Tramèr, Florian; Wang, Rose E.; Wang, William; Wu, Bohan; Wu, Jiajun; Wu, Yuhuai; Xie, Sang Michael; Yasunaga, Michihiro; You, Jiaxuan; Zaharia, Matei; Zhang, Michael; Zhang, Tianyi; Zhang, Xikun; Zhang, Yuhui; Zheng, Lucia; Zhou, Kaitlyn; Liang, Percy",On the Opportunities and Risks of Foundation Models,arXiv:2108.07258 [cs],,,,http://arxiv.org/abs/2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",2021-08-18,2022-03-11 0:33:53,2022-03-11 0:33:53,2022-03-11 0:33:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.07258,,/Users/jacquesthibodeau/Zotero/storage/63IM3WKS/Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf; /Users/jacquesthibodeau/Zotero/storage/C694NVYB/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6ZA42JDT,journalArticle,2021,"Lin, Stephanie; Hilton, Jacob; Evans, Owain",TruthfulQA: Measuring How Models Mimic Human Falsehoods,arXiv:2109.07958 [cs],,,,http://arxiv.org/abs/2109.07958,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",2021-09-08,2022-03-11 0:33:55,2022-03-11 0:33:55,2022-03-11 0:33:55,,,,,,,TruthfulQA,,,,,,,,,,,,arXiv.org,,arXiv: 2109.07958,,/Users/jacquesthibodeau/Zotero/storage/ULUFHY9N/Lin et al. - 2021 - TruthfulQA Measuring How Models Mimic Human False.pdf; /Users/jacquesthibodeau/Zotero/storage/CSSSUHWN/2109.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ZSKPUBV,journalArticle,2021,"Bae, Ho; Jang, Jaehee; Jung, Dahuin; Jang, Hyemi; Ha, Heonseok; Lee, Hyungyu; Yoon, Sungroh",Security and Privacy Issues in Deep Learning,"arXiv:1807.11655 [cs, stat]",,,,http://arxiv.org/abs/1807.11655,"To promote secure and private artificial intelligence (SPAI), we review studies on the model security and data privacy of DNNs. Model security allows system to behave as intended without being affected by malicious external influences that can compromise its integrity and efficiency. Security attacks can be divided based on when they occur: if an attack occurs during training, it is known as a poisoning attack, and if it occurs during inference (after training) it is termed an evasion attack. Poisoning attacks compromise the training process by corrupting the data with malicious examples, while evasion attacks use adversarial examples to disrupt entire classification process. Defenses proposed against such attacks include techniques to recognize and remove malicious data, train a model to be insensitive to such data, and mask the model's structure and parameters to render attacks more challenging to implement. Furthermore, the privacy of the data involved in model training is also threatened by attacks such as the model-inversion attack, or by dishonest service providers of AI applications. To maintain data privacy, several solutions that combine existing data-privacy techniques have been proposed, including differential privacy and modern cryptography techniques. In this paper, we describe the notions of some of methods, e.g., homomorphic encryption, and review their advantages and challenges when implemented in deep-learning models.",2021-03-09,2022-03-11 0:34:01,2022-03-11 0:34:01,2022-03-11 0:33:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.11655,,/Users/jacquesthibodeau/Zotero/storage/QMQ3Q4YR/Bae et al. - 2021 - Security and Privacy Issues in Deep Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/Y2CFJN2W/1807.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AK9DRADR,journalArticle,2018,"Ma, Lei; Juefei-Xu, Felix; Xue, Minhui; Hu, Qiang; Chen, Sen; Li, Bo; Liu, Yang; Zhao, Jianjun; Yin, Jianxiong; See, Simon",Secure Deep Learning Engineering: A Software Quality Assurance Perspective,arXiv:1810.04538 [cs],,,,http://arxiv.org/abs/1810.04538,"Over the past decades, deep learning (DL) systems have achieved tremendous success and gained great popularity in various applications, such as intelligent machines, image processing, speech processing, and medical diagnostics. Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. This brings up many open safety and security issues with enormous and urgent demands on rigorous methodologies and engineering practice for quality enhancement. A plethora of studies have shown that the state-of-the-art DL systems suffer from defects and vulnerabilities that can lead to severe loss and tragedies, especially when applied to real-world safety-critical applications. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning. We, from a software quality assurance perspective, pinpoint challenges and future opportunities towards universal secure deep learning engineering. We hope this work and the accompanied paper repository can pave the path for the software engineering community towards addressing the pressing industrial demand of secure intelligent applications.",2018-10-10,2022-03-11 0:34:02,2022-03-11 0:34:02,2022-03-11 0:34:02,,,,,,,Secure Deep Learning Engineering,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04538,,/Users/jacquesthibodeau/Zotero/storage/2RRF6S76/Ma et al. - 2018 - Secure Deep Learning Engineering A Software Quali.pdf; /Users/jacquesthibodeau/Zotero/storage/DV7EUBKW/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FA57VAL6,journalArticle,2018,"Milli, Smitha; Schmidt, Ludwig; Dragan, Anca D.; Hardt, Moritz",Model Reconstruction from Model Explanations,"arXiv:1807.05185 [cs, stat]",,,,http://arxiv.org/abs/1807.05185,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",2018-07-13,2022-03-11 0:34:04,2022-03-11 0:34:04,2022-03-11 0:34:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.05185,,/Users/jacquesthibodeau/Zotero/storage/U74LY37T/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/NWBIQB53/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GVBD3UR6,journalArticle,2018,"Papernot, Nicolas",A Marauder's Map of Security and Privacy in Machine Learning,arXiv:1811.01134 [cs],,,,http://arxiv.org/abs/1811.01134,"There is growing recognition that machine learning (ML) exposes new security and privacy vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited but expanding. In this talk, we explore the threat model space of ML algorithms through the lens of Saltzer and Schroeder's principles for the design of secure computer systems. This characterization of the threat space prompts an investigation of current and future research directions. We structure our discussion around three of these directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. Key insights resulting from these three directions pursued both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by systematizing best practices in our community.",2018-11-02,2022-03-11 0:34:05,2022-03-11 0:34:05,2022-03-11 0:34:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01134,,/Users/jacquesthibodeau/Zotero/storage/XRIBUKJX/Papernot - 2018 - A Marauder's Map of Security and Privacy in Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/G392P8MA/1811.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6RWQ2TWN,journalArticle,2021,"Carlini, Nicholas; Tramer, Florian; Wallace, Eric; Jagielski, Matthew; Herbert-Voss, Ariel; Lee, Katherine; Roberts, Adam; Brown, Tom; Song, Dawn; Erlingsson, Ulfar; Oprea, Alina; Raffel, Colin",Extracting Training Data from Large Language Models,arXiv:2012.07805 [cs],,,,http://arxiv.org/abs/2012.07805,"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",2021-06-15,2022-03-11 0:34:16,2022-03-11 0:34:16,2022-03-11 0:34:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.07805,,/Users/jacquesthibodeau/Zotero/storage/XVPFZRYY/Carlini et al. - 2021 - Extracting Training Data from Large Language Model.pdf; /Users/jacquesthibodeau/Zotero/storage/SXBX6TTA/2012.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VC6RD9SS,journalArticle,2021,"Pittman, Jason M.; Espinoza, Jesus P.; Crosby, Courtney",Stovepiping and Malicious Software: A Critical Review of AGI Containment,arXiv:1811.03653 [cs],,,,http://arxiv.org/abs/1811.03653,"Awareness of the possible impacts associated with artificial intelligence has risen in proportion to progress in the field. While there are tremendous benefits to society, many argue that there are just as many, if not more, concerns related to advanced forms of artificial intelligence. Accordingly, research into methods to develop artificial intelligence safely is increasingly important. In this paper, we provide an overview of one such safety paradigm: containment with a critical lens aimed toward generative adversarial networks and potentially malicious artificial intelligence. Additionally, we illuminate the potential for a developmental blindspot in the stovepiping of containment mechanisms.",2021-08-01,2022-03-11 0:34:19,2022-03-11 0:34:19,2022-03-11 0:34:19,,,,,,,Stovepiping and Malicious Software,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03653,,/Users/jacquesthibodeau/Zotero/storage/QMM8NDK3/Pittman et al. - 2021 - Stovepiping and Malicious Software A Critical Rev.pdf; /Users/jacquesthibodeau/Zotero/storage/3Q5PKANJ/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GVBY8D7W,journalArticle,2021,"Jain, Arushi; Khetarpal, Khimya; Precup, Doina",Safe Option-Critic: Learning Safety in the Option-Critic Architecture,The Knowledge Engineering Review,,"0269-8889, 1469-8005",10.1017/S0269888921000035,http://arxiv.org/abs/1807.08060,"Designing hierarchical reinforcement learning algorithms that exhibit safe behaviour is not only vital for practical applications but also, facilitates a better understanding of an agent's decisions. We tackle this problem in the options framework, a particular way to specify temporally abstract actions which allow an agent to use sub-policies with start and end conditions. We consider a behaviour as safe that avoids regions of state-space with high uncertainty in the outcomes of actions. We propose an optimization objective that learns safe options by encouraging the agent to visit states with higher behavioural consistency. The proposed objective results in a trade-off between maximizing the standard expected return and minimizing the effect of model uncertainty in the return. We propose a policy gradient algorithm to optimize the constrained objective function. We examine the quantitative and qualitative behaviour of the proposed approach in a tabular grid-world, continuous-state puddle-world, and three games from the Arcade Learning Environment: Ms.Pacman, Amidar, and Q*Bert. Our approach achieves a reduction in the variance of return, boosts performance in environments with intrinsic variability in the reward structure, and compares favorably both with primitive actions as well as with risk-neutral options.",2021,2022-03-11 0:34:22,2022-03-11 0:34:22,2022-03-11 0:34:22,e4,,,36,,The Knowledge Engineering Review,Safe Option-Critic,,,,,,,,,,,,arXiv.org,,arXiv: 1807.08060,,/Users/jacquesthibodeau/Zotero/storage/2SSUE59U/Jain et al. - 2021 - Safe Option-Critic Learning Safety in the Option-.pdf; /Users/jacquesthibodeau/Zotero/storage/7723WXZE/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VJI56G4K,journalArticle,2020,"Miret, Santiago; Majumdar, Somdeb; Wainwright, Carroll",Safety Aware Reinforcement Learning (SARL),arXiv:2010.02846 [cs],,,,http://arxiv.org/abs/2010.02846,"As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.",2020-10-06,2022-03-11 0:34:24,2022-03-11 0:34:24,2022-03-11 0:34:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.02846,,/Users/jacquesthibodeau/Zotero/storage/CE3FJP9R/Miret et al. - 2020 - Safety Aware Reinforcement Learning (SARL).pdf; /Users/jacquesthibodeau/Zotero/storage/KIDAAJI7/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CA64YTYY,journalArticle,2020,"Turner, Alexander Matt; Ratzlaff, Neale; Tadepalli, Prasad",Avoiding Side Effects in Complex Environments,arXiv:2006.06547 [cs],,,,http://arxiv.org/abs/2006.06547,"Reward function specification can be difficult. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at https://avoiding-side-effects.github.io/.",2020-10-22,2022-03-11 0:34:26,2022-03-11 0:34:26,2022-03-11 0:34:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.06547,,/Users/jacquesthibodeau/Zotero/storage/KN2YSGU8/Turner et al. - 2020 - Avoiding Side Effects in Complex Environments.pdf; /Users/jacquesthibodeau/Zotero/storage/HJ662BZN/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4LZJLPCI,journalArticle,2021,"Saisubramanian, Sandhya; Zilberstein, Shlomo; Kamar, Ece",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,arXiv:2008.12146 [cs],,,,http://arxiv.org/abs/2008.12146,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model -- handcrafted or machine acquired -- is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of an agent's actions is critical to improve the safety and reliability of autonomous systems. Mitigating negative side effects is an emerging research topic that is attracting increased attention due to the rapid growth in the deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. The article concludes with a discussion of open questions and suggestions for future research directions.",2021-10-18,2022-03-11 0:34:29,2022-03-11 0:34:29,2022-03-11 0:34:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.12146,,/Users/jacquesthibodeau/Zotero/storage/NJWN9TV6/Saisubramanian et al. - 2021 - Avoiding Negative Side Effects due to Incomplete K.pdf; /Users/jacquesthibodeau/Zotero/storage/HDW668C6/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHPSD9YD,journalArticle,2019,"Jansen, Nils; Könighofer, Bettina; Junges, Sebastian; Serban, Alexandru C.; Bloem, Roderick",Safe Reinforcement Learning via Probabilistic Shields,arXiv:1807.06096 [cs],,,,http://arxiv.org/abs/1807.06096,"This paper targets the efficient construction of a safety shield for decision making in scenarios that incorporate uncertainty. Markov decision processes (MDPs) are prominent models to capture such planning problems. Reinforcement learning (RL) is a machine learning technique to determine near-optimal policies in MDPs that may be unknown prior to exploring the model. However, during exploration, RL is prone to induce behavior that is undesirable or not allowed in safety- or mission-critical contexts. We introduce the concept of a probabilistic shield that enables decision-making to adhere to safety constraints with high probability. In a separation of concerns, we employ formal verification to efficiently compute the probabilities of critical decisions within a safety-relevant fragment of the MDP. We use these results to realize a shield that is applied to an RL algorithm which then optimizes the actual performance objective. We discuss tradeoffs between sufficient progress in exploration of the environment and ensuring safety. In our experiments, we demonstrate on the arcade game PAC-MAN and on a case study involving service robots that the learning efficiency increases as the learning needs orders of magnitude fewer episodes.",2019-11-25,2022-03-11 0:34:36,2022-03-11 0:34:36,2022-03-11 0:34:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06096,,/Users/jacquesthibodeau/Zotero/storage/9XWY8QLJ/Jansen et al. - 2019 - Safe Reinforcement Learning via Probabilistic Shie.pdf; /Users/jacquesthibodeau/Zotero/storage/GGVG8MNT/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PW5CB7B2,journalArticle,2018,"Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane",Scalable agent alignment via reward modeling: a research direction,"arXiv:1811.07871 [cs, stat]",,,,http://arxiv.org/abs/1811.07871,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",2018-11-19,2022-03-11 0:42:06,2022-03-11 0:42:06,2022-03-11 0:42:05,,,,,,,Scalable agent alignment via reward modeling,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07871,,/Users/jacquesthibodeau/Zotero/storage/AIECHVYQ/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/NNLDE4SI/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VA98D7T8,journalArticle,2016,"Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan",Concrete Problems in AI Safety,arXiv:1606.06565 [cs],,,,http://arxiv.org/abs/1606.06565,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",2016-07-25,2022-03-11 0:42:26,2022-03-11 0:42:26,2022-03-11 0:42:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.06565,,/Users/jacquesthibodeau/Zotero/storage/R4EVMHFA/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/V4DD97CN/1606.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4EWDEG7I,journalArticle,2022,"Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,arXiv:1902.09980 [cs],,,,http://arxiv.org/abs/1902.09980,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",2022-01-20,2022-03-11 0:42:33,2022-03-11 0:42:33,2022-03-11 0:42:33,,,,,,,Understanding Agent Incentives using Causal Influence Diagrams. Part I,,,,,,,,,,,,arXiv.org,,arXiv: 1902.09980,,/Users/jacquesthibodeau/Zotero/storage/E5RDDPHQ/Everitt et al. - 2022 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/RSCHXQRY/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FRMHYJ6G,journalArticle,2020,"Krakovna, Victoria; Orseau, Laurent; Ngo, Richard; Martic, Miljan; Legg, Shane",Avoiding Side Effects By Considering Future Tasks,arXiv:2010.07877 [cs],,,,http://arxiv.org/abs/2010.07877,"Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.",2020-10-15,2022-03-11 0:42:35,2022-03-11 0:42:35,2022-03-11 0:42:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.07877,,/Users/jacquesthibodeau/Zotero/storage/33MTNX49/Krakovna et al. - 2020 - Avoiding Side Effects By Considering Future Tasks.pdf; /Users/jacquesthibodeau/Zotero/storage/VH5L2V4Q/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6GHG39AE,journalArticle,2017,"Eysenbach, Benjamin; Gu, Shixiang; Ibarz, Julian; Levine, Sergey",Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,arXiv:1711.06782 [cs],,,,http://arxiv.org/abs/1711.06782,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",2017-11-17,2022-03-11 0:42:46,2022-03-11 0:42:46,2022-03-11 0:42:45,,,,,,,Leave no Trace,,,,,,,,,,,,arXiv.org,,arXiv: 1711.06782,,/Users/jacquesthibodeau/Zotero/storage/QA657MFR/Eysenbach et al. - 2017 - Leave no Trace Learning to Reset for Safe and Aut.pdf; /Users/jacquesthibodeau/Zotero/storage/LDBYGCQ9/1711.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NYNU6YJM,journalArticle,2017,"Armstrong, Stuart; Levinstein, Benjamin",Low Impact Artificial Intelligences,arXiv:1705.10720 [cs],,,,http://arxiv.org/abs/1705.10720,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",2017-05-30,2022-03-11 0:42:48,2022-03-11 0:42:48,2022-03-11 0:42:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.10720,,/Users/jacquesthibodeau/Zotero/storage/G36NEVU8/Armstrong and Levinstein - 2017 - Low Impact Artificial Intelligences.pdf; /Users/jacquesthibodeau/Zotero/storage/5J3V26QV/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KNICDEFZ,journalArticle,2021,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad",Optimal Policies Tend to Seek Power,arXiv:1912.01683 [cs],,,,http://arxiv.org/abs/1912.01683,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",2021-12-03,2022-03-11 0:42:51,2022-03-11 0:42:51,2022-03-11 0:42:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/73K4TGAT/Turner et al. - 2021 - Optimal Policies Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/SQQ73FKH/1912.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63NDGA9I,journalArticle,2017,"Mhamdi, El Mahdi El; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre",Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"arXiv:1704.02882 [cs, stat]",,,,http://arxiv.org/abs/1704.02882,"In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",2017-05-22,2022-03-11 0:42:57,2022-03-11 0:42:57,2022-03-11 0:42:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1704.02882,,/Users/jacquesthibodeau/Zotero/storage/HR9VLUER/Mhamdi et al. - 2017 - Dynamic Safe Interruptibility for Decentralized Mu.pdf; /Users/jacquesthibodeau/Zotero/storage/KDS44NFT/1704.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGB25IIX,journalArticle,2017,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",The Off-Switch Game,arXiv:1611.08219 [cs],,,,http://arxiv.org/abs/1611.08219,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",2017-06-15,2022-03-11 0:42:59,2022-03-11 0:42:59,2022-03-11 0:42:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1611.08219,,/Users/jacquesthibodeau/Zotero/storage/7WJ7VJ7Y/Hadfield-Menell et al. - 2017 - The Off-Switch Game.pdf; /Users/jacquesthibodeau/Zotero/storage/L2J85TR7/1611.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HNCC3GW,journalArticle,2019,"Lehman, Joel; Clune, Jeff; Misevic, Dusan; Adami, Christoph; Altenberg, Lee; Beaulieu, Julie; Bentley, Peter J.; Bernard, Samuel; Beslon, Guillaume; Bryson, David M.; Chrabaszcz, Patryk; Cheney, Nick; Cully, Antoine; Doncieux, Stephane; Dyer, Fred C.; Ellefsen, Kai Olav; Feldt, Robert; Fischer, Stephan; Forrest, Stephanie; Frénoy, Antoine; Gagné, Christian; Goff, Leni Le; Grabowski, Laura M.; Hodjat, Babak; Hutter, Frank; Keller, Laurent; Knibbe, Carole; Krcah, Peter; Lenski, Richard E.; Lipson, Hod; MacCurdy, Robert; Maestre, Carlos; Miikkulainen, Risto; Mitri, Sara; Moriarty, David E.; Mouret, Jean-Baptiste; Nguyen, Anh; Ofria, Charles; Parizeau, Marc; Parsons, David; Pennock, Robert T.; Punch, William F.; Ray, Thomas S.; Schoenauer, Marc; Shulte, Eric; Sims, Karl; Stanley, Kenneth O.; Taddei, François; Tarapore, Danesh; Thibault, Simon; Weimer, Westley; Watson, Richard; Yosinski, Jason",The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities,arXiv:1803.03453 [cs],,,,http://arxiv.org/abs/1803.03453,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.",2019-11-21,2022-03-11 0:43:07,2022-03-11 0:43:07,2022-03-11 0:43:07,,,,,,,The Surprising Creativity of Digital Evolution,,,,,,,,,,,,arXiv.org,,arXiv: 1803.03453,,/Users/jacquesthibodeau/Zotero/storage/69SMNNDL/Lehman et al. - 2019 - The Surprising Creativity of Digital Evolution A .pdf; /Users/jacquesthibodeau/Zotero/storage/5FZ57XHE/1803.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ75MUDN,journalArticle,2016,"Everitt, Tom; Hutter, Marcus",Avoiding Wireheading with Value Reinforcement Learning,arXiv:1605.03143 [cs],,,,http://arxiv.org/abs/1605.03143,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",2016-05-10,2022-03-11 0:43:17,2022-03-11 0:43:17,2022-03-11 0:43:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1605.03143,,/Users/jacquesthibodeau/Zotero/storage/US83UTF9/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/JJVC8DQC/1605.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WQRJXT3M,journalArticle,2018,"Fisac, Jaime F.; Gates, Monica A.; Hamrick, Jessica B.; Liu, Chang; Hadfield-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv; Sastry, S. Shankar; Griffiths, Thomas L.; Dragan, Anca D.",Pragmatic-Pedagogic Value Alignment,arXiv:1707.06354 [cs],,,,http://arxiv.org/abs/1707.06354,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.",2018-02-05,2022-03-11 0:43:31,2022-03-11 0:43:31,2022-03-11 0:43:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1707.06354,,/Users/jacquesthibodeau/Zotero/storage/X6H3CUGX/Fisac et al. - 2018 - Pragmatic-Pedagogic Value Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/JG5LLJD7/1707.html,,,68T05; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; I.2.0; I.2.6; I.2.8; I.2.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHKCNLGW,journalArticle,2017,"Milli, Smitha; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Should Robots be Obedient?,arXiv:1705.09990 [cs],,,,http://arxiv.org/abs/1705.09990,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",2017-05-28,2022-03-11 0:43:34,2022-03-11 0:43:34,2022-03-11 0:43:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.09990,,/Users/jacquesthibodeau/Zotero/storage/39JYHBQ3/Milli et al. - 2017 - Should Robots be Obedient.pdf; /Users/jacquesthibodeau/Zotero/storage/NZVTKACD/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5CVN6W5,journalArticle,2016,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",Cooperative Inverse Reinforcement Learning,arXiv:1606.03137 [cs],,,,http://arxiv.org/abs/1606.03137,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",2016-11-12,2022-03-11 0:43:37,2022-03-11 0:43:37,2022-03-11 0:43:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.03137,,/Users/jacquesthibodeau/Zotero/storage/GM9JBIWP/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/HDDUUGTH/1606.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L23N649D,journalArticle,2015,"Evans, Owain; Stuhlmueller, Andreas; Goodman, Noah D.","Learning the Preferences of Ignorant, Inconsistent Agents",arXiv:1512.05832 [cs],,,,http://arxiv.org/abs/1512.05832,"An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.",2015-12-17,2022-03-11 0:43:55,2022-03-11 0:43:55,2022-03-11 0:43:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1512.05832,,"/Users/jacquesthibodeau/Zotero/storage/E8RI8QP6/Evans et al. - 2015 - Learning the Preferences of Ignorant, Inconsistent.pdf; /Users/jacquesthibodeau/Zotero/storage/YEKCHHXP/1512.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YCKXJDSL,journalArticle,2018,"Christiano, Paul; Shlegeris, Buck; Amodei, Dario",Supervising strong learners by amplifying weak experts,"arXiv:1810.08575 [cs, stat]",,,,http://arxiv.org/abs/1810.08575,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",2018-10-19,2022-03-11 0:43:58,2022-03-11 0:43:58,2022-03-11 0:43:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08575,,/Users/jacquesthibodeau/Zotero/storage/W4VA7R8S/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/89JT3M6T/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJGNXPNN,journalArticle,2017,"Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario",Deep reinforcement learning from human preferences,"arXiv:1706.03741 [cs, stat]",,,,http://arxiv.org/abs/1706.03741,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",2017-07-13,2022-03-11 0:44:19,2022-03-11 0:44:19,2022-03-11 0:44:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1706.03741,,/Users/jacquesthibodeau/Zotero/storage/MI43DNB4/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/8R8GAI69/1706.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WT89LZU,journalArticle,2016,"Critch, Andrew","Parametric Bounded L\""ob's Theorem and Robust Cooperation of Bounded Agents",arXiv:1602.04184 [cs],,,,http://arxiv.org/abs/1602.04184,"L\""ob's theorem and G\""odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.",2016-08-24,2022-03-11 0:44:28,2022-03-11 0:44:28,2022-03-11 0:44:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1602.04184,,/Users/jacquesthibodeau/Zotero/storage/C3IBEIN7/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/WY8SWMEB/1602.html,,,Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJMNAAYK,journalArticle,2017,"Babcock, James; Kramar, Janos; Yampolskiy, Roman V.",Guidelines for Artificial Intelligence Containment,arXiv:1707.08476 [cs],,,,http://arxiv.org/abs/1707.08476,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",2017-07-24,2022-03-11 0:44:34,2022-03-11 0:44:34,2022-03-11 0:44:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1707.08476,,/Users/jacquesthibodeau/Zotero/storage/G4Q86FPE/Babcock et al. - 2017 - Guidelines for Artificial Intelligence Containment.pdf; /Users/jacquesthibodeau/Zotero/storage/3ZXKW6RX/1707.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KEJHL6CV,journalArticle,2017,"Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane",AI Safety Gridworlds,arXiv:1711.09883 [cs],,,,http://arxiv.org/abs/1711.09883,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",2017-11-28,2022-03-11 0:44:42,2022-03-11 0:44:42,2022-03-11 0:44:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.09883,,/Users/jacquesthibodeau/Zotero/storage/UL2GIZZX/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/VZLHCUDR/1711.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9NQ6YV92,journalArticle,2020,"Hernandez, Danny; Brown, Tom B.",Measuring the Algorithmic Efficiency of Neural Networks,"arXiv:2005.04305 [cs, stat]",,,,http://arxiv.org/abs/2005.04305,"Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.",2020-05-08,2022-03-11 0:44:54,2022-03-11 1:36:58,2022-03-11 0:44:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.04305,,/Users/jacquesthibodeau/Zotero/storage/JC7RS7J2/Hernandez and Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf; /Users/jacquesthibodeau/Zotero/storage/Z3HDZ9IE/2005.html; /Users/jacquesthibodeau/Zotero/storage/BY8J4GU4/Hernandez and Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf; /Users/jacquesthibodeau/Zotero/storage/6WIJZCB8/2005.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EL6UNISZ,journalArticle,2014,"Kingma, Diederik P.; Welling, Max",Auto-Encoding Variational Bayes,"arXiv:1312.6114 [cs, stat]",,,,http://arxiv.org/abs/1312.6114,"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",2014-05-01,2022-03-11 0:46:12,2022-03-11 0:46:12,2022-03-11 0:46:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1312.6114,,/Users/jacquesthibodeau/Zotero/storage/UZPG9FA5/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf; /Users/jacquesthibodeau/Zotero/storage/CAIAKN8Q/1312.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NA8XG89R,journalArticle,2013,"Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey",Distributed Representations of Words and Phrases and their Compositionality,"arXiv:1310.4546 [cs, stat]",,,,http://arxiv.org/abs/1310.4546,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",2013-10-16,2022-03-11 0:46:13,2022-03-11 0:46:13,2022-03-11 0:46:13,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1310.4546,,/Users/jacquesthibodeau/Zotero/storage/2N29Q6YH/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf; /Users/jacquesthibodeau/Zotero/storage/BLK42UCN/1310.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ8ELTQC,journalArticle,2014,"Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua",Generative Adversarial Networks,"arXiv:1406.2661 [cs, stat]",,,,http://arxiv.org/abs/1406.2661,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",2014-06-10,2022-03-11 0:46:15,2022-03-11 0:46:15,2022-03-11 0:46:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1406.2661,,/Users/jacquesthibodeau/Zotero/storage/HVWCFA35/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/SP78GDXY/1406.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WHTA2X9,journalArticle,2016,"Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua",Neural Machine Translation by Jointly Learning to Align and Translate,"arXiv:1409.0473 [cs, stat]",,,,http://arxiv.org/abs/1409.0473,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",2016-05-19,2022-03-11 0:46:17,2022-03-11 0:46:17,2022-03-11 0:46:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.0473,,/Users/jacquesthibodeau/Zotero/storage/5P4IAMPV/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf; /Users/jacquesthibodeau/Zotero/storage/HJ65LWP9/1409.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUX5IU2E,journalArticle,2017,"Kingma, Diederik P.; Ba, Jimmy",Adam: A Method for Stochastic Optimization,arXiv:1412.6980 [cs],,,,http://arxiv.org/abs/1412.6980,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",2017-01-29,2022-03-11 0:46:19,2022-03-11 0:46:19,2022-03-11 0:46:19,,,,,,,Adam,,,,,,,,,,,,arXiv.org,,arXiv: 1412.6980,,/Users/jacquesthibodeau/Zotero/storage/NC9XZNEX/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/VQKWXJ43/1412.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJU95QAX,journalArticle,2017,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia",Attention Is All You Need,arXiv:1706.03762 [cs],,,,http://arxiv.org/abs/1706.03762,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",2017-12-05,2022-03-11 0:46:22,2022-03-11 0:46:22,2022-03-11 0:46:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1706.03762,,/Users/jacquesthibodeau/Zotero/storage/JZJ8T8EL/Vaswani et al. - 2017 - Attention Is All You Need.pdf; /Users/jacquesthibodeau/Zotero/storage/8UIKQ785/1706.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV84CAEJ,journalArticle,2016,"Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg; Hughes, Macduff; Dean, Jeffrey",Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,arXiv:1609.08144 [cs],,,,http://arxiv.org/abs/1609.08144,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2016-10-08,2022-03-11 0:46:23,2022-03-11 0:46:23,2022-03-11 0:46:23,,,,,,,Google's Neural Machine Translation System,,,,,,,,,,,,arXiv.org,,arXiv: 1609.08144,,/Users/jacquesthibodeau/Zotero/storage/NE29WJV2/Wu et al. - 2016 - Google's Neural Machine Translation System Bridgi.pdf; /Users/jacquesthibodeau/Zotero/storage/UWQ7F2HF/1609.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VGKIB34A,journalArticle,2017,"Zoph, Barret; Le, Quoc V.",Neural Architecture Search with Reinforcement Learning,arXiv:1611.01578 [cs],,,,http://arxiv.org/abs/1611.01578,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",2017-02-15,2022-03-11 0:46:25,2022-03-11 0:46:25,2022-03-11 0:46:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1611.01578,,/Users/jacquesthibodeau/Zotero/storage/HE8SNP2I/Zoph and Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/ZKD6S2JZ/1611.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
STARDH43,journalArticle,2017,"Chollet, François",Xception: Deep Learning with Depthwise Separable Convolutions,arXiv:1610.02357 [cs],,,,http://arxiv.org/abs/1610.02357,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2017-04-04,2022-03-11 0:46:28,2022-03-11 0:46:28,2022-03-11 0:46:28,,,,,,,Xception,,,,,,,,,,,,arXiv.org,,arXiv: 1610.02357,,/Users/jacquesthibodeau/Zotero/storage/QAZ54WG6/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf; /Users/jacquesthibodeau/Zotero/storage/EFBWMIG5/1610.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EDJYWEF7,journalArticle,2015,"Amodei, Dario; Anubhai, Rishita; Battenberg, Eric; Case, Carl; Casper, Jared; Catanzaro, Bryan; Chen, Jingdong; Chrzanowski, Mike; Coates, Adam; Diamos, Greg; Elsen, Erich; Engel, Jesse; Fan, Linxi; Fougner, Christopher; Han, Tony; Hannun, Awni; Jun, Billy; LeGresley, Patrick; Lin, Libby; Narang, Sharan; Ng, Andrew; Ozair, Sherjil; Prenger, Ryan; Raiman, Jonathan; Satheesh, Sanjeev; Seetapun, David; Sengupta, Shubho; Wang, Yi; Wang, Zhiqian; Wang, Chong; Xiao, Bo; Yogatama, Dani; Zhan, Jun; Zhu, Zhenyao",Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,arXiv:1512.02595 [cs],,,,http://arxiv.org/abs/1512.02595,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",2015-12-08,2022-03-11 0:46:31,2022-03-11 0:46:31,2022-03-11 0:46:30,,,,,,,Deep Speech 2,,,,,,,,,,,,arXiv.org,,arXiv: 1512.02595,,/Users/jacquesthibodeau/Zotero/storage/GCW6SYTS/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf; /Users/jacquesthibodeau/Zotero/storage/R524APC3/1512.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9MWJA2PZ,journalArticle,2015,"Simonyan, Karen; Zisserman, Andrew",Very Deep Convolutional Networks for Large-Scale Image Recognition,arXiv:1409.1556 [cs],,,,http://arxiv.org/abs/1409.1556,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",2015-04-10,2022-03-11 0:46:42,2022-03-11 0:46:42,2022-03-11 0:46:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.1556,,/Users/jacquesthibodeau/Zotero/storage/IU8LRRIT/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf; /Users/jacquesthibodeau/Zotero/storage/GJ3DHSB9/1409.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BNWXRCHF,journalArticle,2014,"Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V.",Sequence to Sequence Learning with Neural Networks,arXiv:1409.3215 [cs],,,,http://arxiv.org/abs/1409.3215,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",2014-12-14,2022-03-11 0:46:45,2022-03-11 0:46:45,2022-03-11 0:46:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.3215,,/Users/jacquesthibodeau/Zotero/storage/XIY9LPCW/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/R2YMG2T3/1409.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2CM3NXAW,journalArticle,2013,"Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Graves, Alex; Antonoglou, Ioannis; Wierstra, Daan; Riedmiller, Martin",Playing Atari with Deep Reinforcement Learning,arXiv:1312.5602 [cs],,,,http://arxiv.org/abs/1312.5602,"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",2013-12-19,2022-03-11 0:46:48,2022-03-11 0:46:48,2022-03-11 0:46:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1312.5602,,/Users/jacquesthibodeau/Zotero/storage/8RGV3UHT/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/6P98LQKJ/1312.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DYKT55PJ,journalArticle,2013,"Zeiler, Matthew D.; Fergus, Rob",Visualizing and Understanding Convolutional Networks,arXiv:1311.2901 [cs],,,,http://arxiv.org/abs/1311.2901,"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",2013-11-28,2022-03-11 0:46:49,2022-03-11 0:46:49,2022-03-11 0:46:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1311.2901,,/Users/jacquesthibodeau/Zotero/storage/RKFMSM5J/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/URZ59DSU/1311.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PPY7NANL,journalArticle,2012,"Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.",Improving neural networks by preventing co-adaptation of feature detectors,arXiv:1207.0580 [cs],,,,http://arxiv.org/abs/1207.0580,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",2012-07-03,2022-03-11 0:46:51,2022-03-11 0:46:51,2022-03-11 0:46:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1207.0580,,/Users/jacquesthibodeau/Zotero/storage/RKKHRNR9/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf; /Users/jacquesthibodeau/Zotero/storage/MQSGQ5TF/1207.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R87A9SW2,journalArticle,2020,"Hubinger, Evan",An overview of 11 proposals for building safe advanced AI,arXiv:2012.07532 [cs],,,,http://arxiv.org/abs/2012.07532,"This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.",2020-12-04,2022-03-11 0:47:00,2022-03-11 0:47:00,2022-03-11 0:46:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.07532,,/Users/jacquesthibodeau/Zotero/storage/ATHV7RAM/Hubinger - 2020 - An overview of 11 proposals for building safe adva.pdf; /Users/jacquesthibodeau/Zotero/storage/HBRXGPYV/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DPTBSH82,journalArticle,2022,"Gathani, Sneha; Hulsebos, Madelon; Gale, James; Haas, Peter J.; Demiralp, Çağatay",Augmenting Decision Making via Interactive What-If Analysis,arXiv:2109.06160 [cs],,,,http://arxiv.org/abs/2109.06160,"The fundamental goal of business data analysis is to improve business decisions using data. Business users often make decisions to achieve key performance indicators (KPIs) such as increasing customer retention or sales, or decreasing costs. To discover the relationship between data attributes hypothesized to be drivers and those corresponding to KPIs of interest, business users currently need to perform lengthy exploratory analyses. This involves considering multitudes of combinations and scenarios and performing slicing, dicing, and transformations on the data accordingly, e.g., analyzing customer retention across quarters of the year or suggesting optimal media channels across strata of customers. However, the increasing complexity of datasets combined with the cognitive limitations of humans makes it challenging to carry over multiple hypotheses, even for simple datasets. Therefore mentally performing such analyses is hard. Existing commercial tools either provide partial solutions or fail to cater to business users altogether. Here we argue for four functionalities to enable business users to interactively learn and reason about the relationships between sets of data attributes thereby facilitating data-driven decision making. We implement these functionalities in SystemD, an interactive visual data analysis system enabling business users to experiment with the data by asking what-if questions. We evaluate the system through three business use cases: marketing mix modeling, customer retention analysis, and deal closing analysis, and report on feedback from multiple business users. Users find the SystemD functionalities highly useful for quick testing and validation of their hypotheses around their KPIs of interest, addressing their unmet analysis needs. The feedback also suggests that the UX design can be enhanced to further improve the understandability of these functionalities.",2022-02-08,2022-03-11 0:54:22,2022-03-11 0:54:22,2022-03-11 0:54:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.06160,,/Users/jacquesthibodeau/Zotero/storage/AX2U4F9X/Gathani et al. - 2022 - Augmenting Decision Making via Interactive What-If.pdf; /Users/jacquesthibodeau/Zotero/storage/HBYRB7W9/2109.html,,,Computer Science - Databases; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXJ7BGT7,journalArticle,2021,"Pearce, Hammond; Ahmad, Baleegh; Tan, Benjamin; Dolan-Gavitt, Brendan; Karri, Ramesh",Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions,arXiv:2108.09293 [cs],,,,http://arxiv.org/abs/2108.09293,"There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described `AI pair programmer', GitHub Copilot, a language model trained over open-source GitHub code. However, code often contains bugs - and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot's code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those from MITRE's ""Top 25"" list). We explore Copilot's performance on three distinct code generation axes -- examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.",2021-12-16,2022-03-11 0:54:25,2022-03-11 0:54:25,2022-03-11 0:54:24,,,,,,,Asleep at the Keyboard?,,,,,,,,,,,,arXiv.org,,arXiv: 2108.09293,,/Users/jacquesthibodeau/Zotero/storage/BHUSEKZ7/Pearce et al. - 2021 - Asleep at the Keyboard Assessing the Security of .pdf; /Users/jacquesthibodeau/Zotero/storage/BIAL94TY/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FVBGZZD,journalArticle,2022,"Rahtz, Matthew; Varma, Vikrant; Kumar, Ramana; Kenton, Zachary; Legg, Shane; Leike, Jan",Safe Deep RL in 3D Environments using Human Feedback,arXiv:2201.08102 [cs],,,,http://arxiv.org/abs/2201.08102,"Agents should avoid unsafe behaviour during both training and deployment. This typically requires a simulator and a procedural specification of unsafe behaviour. Unfortunately, a simulator is not always available, and procedurally specifying constraints can be difficult or impossible for many real-world tasks. A recently introduced technique, ReQueST, aims to solve this problem by learning a neural simulator of the environment from safe human trajectories, then using the learned simulator to efficiently learn a reward model from human feedback. However, it is yet unknown whether this approach is feasible in complex 3D environments with feedback obtained from real humans - whether sufficient pixel-based neural simulator quality can be achieved, and whether the human data requirements are viable in terms of both quantity and quality. In this paper we answer this question in the affirmative, using ReQueST to train an agent to perform a 3D first-person object collection task using data entirely from human contractors. We show that the resulting agent exhibits an order of magnitude reduction in unsafe behaviour compared to standard reinforcement learning.",2022-01-21,2022-03-11 0:54:50,2022-03-11 0:54:50,2022-03-11 0:54:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.08102,,/Users/jacquesthibodeau/Zotero/storage/9B34QRJG/Rahtz et al. - 2022 - Safe Deep RL in 3D Environments using Human Feedba.pdf; /Users/jacquesthibodeau/Zotero/storage/K8AJLZ9H/2201.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NRYCFH8M,journalArticle,2022,"Pan, Alexander; Bhatia, Kush; Steinhardt, Jacob",The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"arXiv:2201.03544 [cs, stat]",,,,http://arxiv.org/abs/2201.03544,"Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",2022-02-14,2022-03-11 0:56:03,2022-03-11 0:56:03,2022-03-11 0:56:03,,,,,,,The Effects of Reward Misspecification,,,,,,,,,,,,arXiv.org,,arXiv: 2201.03544,,/Users/jacquesthibodeau/Zotero/storage/53BDMGWR/Pan et al. - 2022 - The Effects of Reward Misspecification Mapping an.pdf; /Users/jacquesthibodeau/Zotero/storage/EGP4DIZ6/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADBMHHNP,journalArticle,2022,"Du, Xuefeng; Wang, Zhaoning; Cai, Mu; Li, Yixuan",VOS: Learning What You Don't Know by Virtual Outlier Synthesis,arXiv:2202.01197 [cs],,,,http://arxiv.org/abs/2202.01197,"Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos.",2022-02-04,2022-03-11 0:56:21,2022-03-11 1:37:55,2022-03-11 0:56:21,,,,,,,VOS,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01197,,/Users/jacquesthibodeau/Zotero/storage/Z3WME7GW/Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf; /Users/jacquesthibodeau/Zotero/storage/KP6NEVTF/Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf; /Users/jacquesthibodeau/Zotero/storage/NKYHICJN/2202.html; /Users/jacquesthibodeau/Zotero/storage/R5WB6WPU/2202.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQXFH99W,journalArticle,2021,"Meinke, Alexander; Bitterwolf, Julian; Hein, Matthias",Provably Robust Detection of Out-of-distribution Data (almost) for free,arXiv:2106.04260 [cs],,,,http://arxiv.org/abs/2106.04260,"When applying machine learning in safety-critical systems, a reliable assessment of the uncertainy of a classifier is required. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data and even if trained to be non-confident on OOD data one can still adversarially manipulate OOD data so that the classifer again assigns high confidence to the manipulated samples. In this paper we propose a novel method where from first principles we combine a certifiable OOD detector with a standard classifier into an OOD aware classifier. In this way we achieve the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in prediction accuracy and close to state-of-the-art OOD detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks.",2021-06-08,2022-03-11 0:56:43,2022-03-11 0:56:43,2022-03-11 0:56:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04260,,/Users/jacquesthibodeau/Zotero/storage/DZCQQRR8/Meinke et al. - 2021 - Provably Robust Detection of Out-of-distribution D.pdf; /Users/jacquesthibodeau/Zotero/storage/DMYN2EYN/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HK525QH9,journalArticle,2021,"Yoon, Jinsung; Sohn, Kihyuk; Li, Chun-Liang; Arik, Sercan O.; Lee, Chen-Yu; Pfister, Tomas","Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection",,,,,https://openreview.net/forum?id=Nct9j3BVswZ,"Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective...",2021-09-29,2022-03-11 0:57:29,2022-03-11 0:57:29,2022-03-11 0:57:29,,,,,,,"Self-Supervise, Refine, Repeat",,,,,,,en,,,,,openreview.net,,,,"/Users/jacquesthibodeau/Zotero/storage/ANY6VSD7/Yoon et al. - 2021 - Self-Supervise, Refine, Repeat Improving Unsuperv.pdf; /Users/jacquesthibodeau/Zotero/storage/GTIQ5XJ2/forum.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PU8QPVJ2,journalArticle,2022,"Wortsman, Mitchell; Ilharco, Gabriel; Kim, Jong Wook; Li, Mike; Kornblith, Simon; Roelofs, Rebecca; Lopes, Raphael Gontijo; Hajishirzi, Hannaneh; Farhadi, Ali; Namkoong, Hongseok; Schmidt, Ludwig",Robust fine-tuning of zero-shot models,arXiv:2109.01903 [cs],,,,http://arxiv.org/abs/2109.01903,"Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.",2022-02-24,2022-03-11 0:58:29,2022-03-11 0:58:29,2022-03-11 0:58:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.01903,,/Users/jacquesthibodeau/Zotero/storage/ZPIXAEPG/Wortsman et al. - 2022 - Robust fine-tuning of zero-shot models.pdf; /Users/jacquesthibodeau/Zotero/storage/74PFL6FD/2109.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LHDSLAVF,journalArticle,2020,"Guo, Weiyu; Ouyang, Yidong",Robust Learning with Frequency Domain Regularization,"arXiv:2007.03244 [cs, stat]",,,,http://arxiv.org/abs/2007.03244,"Convolution neural networks have achieved remarkable performance in many tasks of computing vision. However, CNN tends to bias to low frequency components. They prioritize capturing low frequency patterns which lead them fail when suffering from application scenario transformation. While adversarial example implies the model is very sensitive to high frequency perturbations. In this paper, we introduce a new regularization method by constraining the frequency spectra of the filter of the model. Different from band-limit training, our method considers the valid frequency range probably entangles in different layers rather than continuous and trains the valid frequency range end-to-end by backpropagation. We demonstrate the effectiveness of our regularization by (1) defensing to adversarial perturbations; (2) reducing the generalization gap in different architecture; (3) improving the generalization ability in transfer learning scenario without fine-tune.",2020-07-07,2022-03-11 0:59:35,2022-03-11 0:59:35,2022-03-11 0:59:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2007.03244,,/Users/jacquesthibodeau/Zotero/storage/TXPXRMAB/Guo and Ouyang - 2020 - Robust Learning with Frequency Domain Regularizati.pdf; /Users/jacquesthibodeau/Zotero/storage/3ZLQBLGC/2007.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6G2KFV3,journalArticle,2021,"Salman, Hadi; Jain, Saachi; Wong, Eric; Mądry, Aleksander",Certified Patch Robustness via Smoothed Vision Transformers,arXiv:2110.07719 [cs],,,,http://arxiv.org/abs/2110.07719,"Certified patch defenses can guarantee robustness of an image classifier to arbitrary changes within a bounded contiguous region. But, currently, this robustness comes at a cost of degraded standard accuracies and slower inference times. We demonstrate how using vision transformers enables significantly better certified patch robustness that is also more computationally efficient and does not incur a substantial drop in standard accuracy. These improvements stem from the inherent ability of the vision transformer to gracefully handle largely masked images. Our code is available at https://github.com/MadryLab/smoothed-vit.",2021-10-11,2022-03-11 0:59:50,2022-03-11 0:59:50,2022-03-11 0:59:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.07719,,/Users/jacquesthibodeau/Zotero/storage/24FM6UTF/Salman et al. - 2021 - Certified Patch Robustness via Smoothed Vision Tra.pdf; /Users/jacquesthibodeau/Zotero/storage/E69HPRYS/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9X3L95V,journalArticle,2021,"Zhang, Marvin Mengxin; Levine, Sergey; Finn, Chelsea",Test Time Robustification of Deep Models via Adaptation and Augmentation,,,,,https://openreview.net/forum?id=J1uOGgf-bP,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the...",2021-09-29,2022-03-11 1:00:09,2022-03-11 1:00:09,2022-03-11 1:00:09,,,,,,,,,,,,,,en,,,,,openreview.net,,,,/Users/jacquesthibodeau/Zotero/storage/FMDPIXDJ/Zhang et al. - 2021 - Test Time Robustification of Deep Models via Adapt.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPQFF5J7,journalArticle,2022,"Zhang, Marvin; Levine, Sergey; Finn, Chelsea",MEMO: Test Time Robustness via Adaptation and Augmentation,arXiv:2110.09506 [cs],,,,http://arxiv.org/abs/2110.09506,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the domain, or other sources of distribution shift. We study the problem of test time robustification, i.e., using the test input to improve model robustness. Recent prior works have proposed methods for test time adaptation, however, they each introduce additional assumptions, such as access to multiple test points, that prevent widespread adoption. In this work, we aim to study and devise methods that make no assumptions about the model training process and are broadly applicable at test time. We propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model's average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations, while also maintaining confidence in its predictions. In our experiments, we evaluate two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and we demonstrate that this approach achieves accuracy gains of 1-8\% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, we achieve state-of-the-art results on the ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A distribution shift benchmarks.",2022-01-24,2022-03-11 1:00:14,2022-03-11 1:00:14,2022-03-11 1:00:14,,,,,,,MEMO,,,,,,,,,,,,arXiv.org,,arXiv: 2110.09506,,/Users/jacquesthibodeau/Zotero/storage/XHMLXPLS/Zhang et al. - 2022 - MEMO Test Time Robustness via Adaptation and Augm.pdf; /Users/jacquesthibodeau/Zotero/storage/NNWE2FCS/2110.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SKVTSGIG,journalArticle,2021,"Mao, Chengzhi; Jiang, Lu; Dehghani, Mostafa; Vondrick, Carl; Sukthankar, Rahul; Essa, Irfan",Discrete Representations Strengthen Vision Transformer Robustness,arXiv:2111.10493 [cs],,,,http://arxiv.org/abs/2111.10493,"Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (e.g., nuisances and texture) and fail to make adequate use of global context (e.g., shape and structure). As a result, ViTs fail to generalize to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",2021-11-19,2022-03-11 1:00:31,2022-03-11 1:00:31,2022-03-11 1:00:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.10493,,/Users/jacquesthibodeau/Zotero/storage/H25YPYY2/Mao et al. - 2021 - Discrete Representations Strengthen Vision Transfo.pdf; /Users/jacquesthibodeau/Zotero/storage/QS6L6XT6/2111.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VJ37T7YV,journalArticle,2021,"Tang, Leonard; Ke, Elizabeth; Singh, Nikhil; Verma, Nakul; Drori, Iddo",Solving Probability and Statistics Problems by Program Synthesis,arXiv:2111.08267 [cs],,,,http://arxiv.org/abs/2111.08267,"We solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.",2021-11-16,2022-03-11 1:00:35,2022-03-11 1:00:35,2022-03-11 1:00:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.08267,,/Users/jacquesthibodeau/Zotero/storage/SY7Y8EE9/Tang et al. - 2021 - Solving Probability and Statistics Problems by Pro.pdf; /Users/jacquesthibodeau/Zotero/storage/57AFMQM2/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R5VFGPBS,journalArticle,2021,"Trabucco, Brandon; Kumar, Aviral; Geng, Xinyang; Levine, Sergey",Conservative Objective Models for Effective Offline Model-Based Optimization,arXiv:2107.06882 [cs],,,,http://arxiv.org/abs/2107.06882,"Computational design problems arise in a number of settings, from synthetic biology to computer architectures. In this paper, we aim to solve data-driven model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function provided access to only a static dataset of prior experiments. Such data-driven optimization procedures are the only practical methods in many real-world domains where active data collection is expensive (e.g., when optimizing over proteins) or dangerous (e.g., when optimizing over aircraft designs). Typical methods for MBO that optimize the design against a learned model suffer from distributional shift: it is easy to find a design that ""fools"" the model into predicting a high value. To overcome this, we propose conservative objective models (COMs), a method that learns a model of the objective function that lower bounds the actual value of the ground-truth objective on out-of-distribution inputs, and uses it for optimization. Structurally, COMs resemble adversarial training methods used to overcome adversarial examples. COMs are simple to implement and outperform a number of existing methods on a wide range of MBO problems, including optimizing protein sequences, robot morphologies, neural network weights, and superconducting materials.",2021-07-14,2022-03-11 1:00:39,2022-03-11 1:00:39,2022-03-11 1:00:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.06882,,/Users/jacquesthibodeau/Zotero/storage/C5EGTGJS/Trabucco et al. - 2021 - Conservative Objective Models for Effective Offlin.pdf; /Users/jacquesthibodeau/Zotero/storage/GNWT39N2/2107.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KF6A3HRJ,journalArticle,2021,"Grinsztajn, Nathan; Ferret, Johan; Pietquin, Olivier; Preux, Philippe; Geist, Matthieu",There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning,arXiv:2106.04480 [cs],,,,http://arxiv.org/abs/2106.04480,"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.",2021-10-29,2022-03-11 1:00:53,2022-03-11 1:00:53,2022-03-11 1:00:53,,,,,,,There Is No Turning Back,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04480,,/Users/jacquesthibodeau/Zotero/storage/LHDD4ENB/Grinsztajn et al. - 2021 - There Is No Turning Back A Self-Supervised Approa.pdf; /Users/jacquesthibodeau/Zotero/storage/EH4DTHSK/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9KHBCIAM,journalArticle,2022,"Papantoniou, Katerina; Papadakos, Panagiotis; Flouris, Giorgos; Plexousakis, Dimitris",Linguistic Cues of Deception in a Multilingual April Fools' Day Context,arXiv:2111.03913 [cs],,,,http://arxiv.org/abs/2111.03913,"In this work we consider the collection of deceptive April Fools' Day(AFD) news articles as a useful addition in existing datasets for deception detection tasks. Such collections have an established ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current research thread, we also discuss the individualism/collectivism dimension in deception with respect to these two datasets. Lastly, we build classifiers by testing various monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection studies, and are in alignment with the observations of other deception detection works.",2022-02-28,2022-03-11 1:02:16,2022-03-11 1:02:16,2022-03-11 1:02:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.03913,,/Users/jacquesthibodeau/Zotero/storage/RY7TMNC4/Papantoniou et al. - 2022 - Linguistic Cues of Deception in a Multilingual Apr.pdf; /Users/jacquesthibodeau/Zotero/storage/SUXTGAER/2111.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P4BJVT2D,journalArticle,2021,"Jung, Sanghun; Lee, Jungsoo; Gwak, Daehoon; Choi, Sungha; Choo, Jaegul",Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation,arXiv:2107.11264 [cs],,,,http://arxiv.org/abs/2107.11264,"Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leaderboard with a large margin. Our code is publicly available at this $\href{https://github.com/shjung13/Standardized-max-logits}{link}$.",2021-10-11,2022-03-11 1:02:18,2022-03-11 1:02:18,2022-03-11 1:02:18,,,,,,,Standardized Max Logits,,,,,,,,,,,,arXiv.org,,arXiv: 2107.11264,,/Users/jacquesthibodeau/Zotero/storage/PZIWZIZU/Jung et al. - 2021 - Standardized Max Logits A Simple yet Effective Ap.pdf; /Users/jacquesthibodeau/Zotero/storage/APA8JUSF/2107.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D696L88R,journalArticle,2021,"Besnier, Victor; Bursuc, Andrei; Picard, David; Briot, Alexandre",Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation,arXiv:2108.01634 [cs],,,,http://arxiv.org/abs/2108.01634,"In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.",2021-08-03,2022-03-11 1:02:20,2022-03-11 1:02:20,2022-03-11 1:02:20,,,,,,,Triggering Failures,,,,,,,,,,,,arXiv.org,,arXiv: 2108.01634,,/Users/jacquesthibodeau/Zotero/storage/GLNSWEXX/Besnier et al. - 2021 - Triggering Failures Out-Of-Distribution detection.pdf; /Users/jacquesthibodeau/Zotero/storage/632WXETD/2108.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UW2MWSM2,journalArticle,2021,"Zimmermann, Roland S.; Borowski, Judy; Geirhos, Robert; Bethge, Matthias; Wallis, Thomas S. A.; Brendel, Wieland",How Well do Feature Visualizations Support Causal Understanding of CNN Activations?,arXiv:2106.12447 [cs],,,,http://arxiv.org/abs/2106.12447,"A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm 4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better ""causal understanding"" of unit activations than simple alternative visualizations.",2021-11-12,2022-03-11 1:02:22,2022-03-11 1:02:22,2022-03-11 1:02:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.12447,,/Users/jacquesthibodeau/Zotero/storage/NJ7L54YY/Zimmermann et al. - 2021 - How Well do Feature Visualizations Support Causal .pdf; /Users/jacquesthibodeau/Zotero/storage/IGG727ZB/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7SXHW6NB,journalArticle,2021,"Zhao, Bingchen; Yu, Shaozuo; Ma, Wufei; Yu, Mingxin; Mei, Shenxiao; Wang, Angtian; He, Ju; Yuille, Alan; Kortylewski, Adam",ROBIN : A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts,arXiv:2111.14341 [cs],,,,http://arxiv.org/abs/2111.14341,"Enhancing the robustness in real-world scenarios has been proven very challenging. One reason is that existing robustness benchmarks are limited, as they either rely on synthetic data or they simply measure robustness as generalization between datasets and hence ignore the effects of individual nuisance factors. In this work, we introduce ROBIN, a benchmark dataset for diagnosing the robustness of vision algorithms to individual nuisances in real-world images. ROBIN builds on 10 rigid categories from the PASCAL VOC 2012 and ImageNet datasets and includes out-of-distribution examples of the objects 3D pose, shape, texture, context and weather conditions. ROBIN is richly annotated to enable benchmark models for image classification, object detection, and 3D pose estimation. We provide results for a number of popular baselines and make several interesting observations: 1. Some nuisance factors have a much stronger negative effect on the performance compared to others. Moreover, the negative effect of an OODnuisance depends on the downstream vision task. 2. Current approaches to enhance OOD robustness using strong data augmentation have only marginal effects in real-world OOD scenarios, and sometimes even reduce the OOD performance. 3. We do not observe any significant differences between convolutional and transformer architectures in terms of OOD robustness. We believe our dataset provides a rich testbed to study the OOD robustness of vision algorithms and will help to significantly push forward research in this area.",2021-12-02,2022-03-11 1:02:24,2022-03-11 1:02:24,2022-03-11 1:02:24,,,,,,,ROBIN,,,,,,,,,,,,arXiv.org,,arXiv: 2111.14341,,/Users/jacquesthibodeau/Zotero/storage/5LTTPXN7/Zhao et al. - 2021 - ROBIN  A Benchmark for Robustness to Individual N.pdf; /Users/jacquesthibodeau/Zotero/storage/ACW5XG3M/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BBSEGLAK,journalArticle,2022,"Wang, Boxin; Xu, Chejian; Wang, Shuohang; Gan, Zhe; Cheng, Yu; Gao, Jianfeng; Awadallah, Ahmed Hassan; Li, Bo",Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models,arXiv:2111.02840 [cs],,,,http://arxiv.org/abs/2111.02840,"Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.",2022-01-10,2022-03-11 1:02:45,2022-03-11 1:02:45,2022-03-11 1:02:45,,,,,,,Adversarial GLUE,,,,,,,,,,,,arXiv.org,,arXiv: 2111.02840,,/Users/jacquesthibodeau/Zotero/storage/ICNPCDMG/Wang et al. - 2022 - Adversarial GLUE A Multi-Task Benchmark for Robus.pdf; /Users/jacquesthibodeau/Zotero/storage/BYPTVT8I/2111.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7VARQKSN,journalArticle,2021,"Wallace, Eric; Williams, Adina; Jia, Robin; Kiela, Douwe",Analyzing Dynamic Adversarial Training Data in the Limit,arXiv:2110.08514 [cs],,,,http://arxiv.org/abs/2110.08514,"To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.",2021-10-16,2022-03-11 1:02:47,2022-03-11 1:02:47,2022-03-11 1:02:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08514,,/Users/jacquesthibodeau/Zotero/storage/MVBXYNDE/Wallace et al. - 2021 - Analyzing Dynamic Adversarial Training Data in the.pdf; /Users/jacquesthibodeau/Zotero/storage/R3MSXIM9/2110.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQQSUQMM,journalArticle,2021,"Herrmann, Charles; Sargent, Kyle; Jiang, Lu; Zabih, Ramin; Chang, Huiwen; Liu, Ce; Krishnan, Dilip; Sun, Deqing",Pyramid Adversarial Training Improves ViT Performance,arXiv:2111.15121 [cs],,,,http://arxiv.org/abs/2111.15121,"Aggressive data augmentation is a key component of the strong generalization capabilities of Vision Transformer (ViT). One such data augmentation technique is adversarial training; however, many prior works have shown that this often results in poor clean accuracy. In this work, we present Pyramid Adversarial Training, a simple and effective technique to improve ViT's overall performance. We pair it with a ""matched"" Dropout and stochastic depth regularization, which adopts the same Dropout and stochastic depth configuration for the clean and adversarial samples. Similar to the improvements on CNNs by AdvProp (not directly applicable to ViT), our Pyramid Adversarial Training breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. It leads to $1.82\%$ absolute improvement on ImageNet clean accuracy for the ViT-B model when trained only on ImageNet-1K data, while simultaneously boosting performance on $7$ ImageNet robustness metrics, by absolute numbers ranging from $1.76\%$ to $11.45\%$. We set a new state-of-the-art for ImageNet-C (41.4 mCE), ImageNet-R ($53.92\%$), and ImageNet-Sketch ($41.04\%$) without extra data, using only the ViT-B/16 backbone and our Pyramid Adversarial Training. Our code will be publicly available upon acceptance.",2021-11-29,2022-03-11 1:02:49,2022-03-11 1:02:49,2022-03-11 1:02:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.15121,,/Users/jacquesthibodeau/Zotero/storage/EJIU9S9U/Herrmann et al. - 2021 - Pyramid Adversarial Training Improves ViT Performa.pdf; /Users/jacquesthibodeau/Zotero/storage/YPLHXMN9/2111.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TFUIX2ZG,journalArticle,2021,"Yang, Tsung-Yen; Hu, Michael; Chow, Yinlam; Ramadge, Peter J.; Narasimhan, Karthik",Safe Reinforcement Learning with Natural Language Constraints,arXiv:2010.05150 [cs],,,,http://arxiv.org/abs/2010.05150,"While safe reinforcement learning (RL) holds great promise for many practical applications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such specifications demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we first introduce HazardWorld, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HazardWorld, we show that our method achieves higher rewards (up to11x) and fewer constraint violations (by 1.8x) compared to existing approaches. However, in terms of absolute performance, HazardWorld still poses significant challenges for agents to learn efficiently, motivating the need for future work.",2021-08-03,2022-03-11 1:03:02,2022-03-11 1:03:02,2022-03-11 1:03:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.05150,,/Users/jacquesthibodeau/Zotero/storage/PAB7GCUJ/Yang et al. - 2021 - Safe Reinforcement Learning with Natural Language .pdf; /Users/jacquesthibodeau/Zotero/storage/WDEUWHPV/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IRSRXTZA,journalArticle,2022,"Yu, Haonan; Xu, Wei; Zhang, Haichao",Towards Safe Reinforcement Learning with a Safety Editor Policy,arXiv:2201.12427 [cs],,,,http://arxiv.org/abs/2201.12427,"We consider the safe reinforcement learning (RL) problem of maximizing utility while satisfying provided constraints. Since we do not assume any prior knowledge or pre-training of the safety concept, we are interested in asymptotic constraint satisfaction. A popular approach in this line of research is to combine the Lagrangian method with a model-free RL algorithm to adjust the weight of the constraint reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. Inspired by the safety layer design (Dalal et al., 2018), we propose to separately learn a safety editor policy that transforms potentially unsafe actions output by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility Q values of actions before and after the edit. On 12 custom Safety Gym (Ray et al., 2019) tasks and 2 safe racing tasks with very harsh constraint thresholds, our approach demonstrates outstanding utility performance while complying with the constraints. Ablation studies reveal that our two-policy design is critical. Simply doubling the model capacity of typical single-policy approaches will not lead to comparable results. The Q hinge loss is also important in certain circumstances, and replacing it with the usual L2 distance could fail badly.",2022-01-28,2022-03-11 1:03:18,2022-03-11 1:38:19,2022-03-11 1:03:18,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.12427,,/Users/jacquesthibodeau/Zotero/storage/9C9VJF2I/Yu et al. - 2022 - Towards Safe Reinforcement Learning with a Safety .pdf; /Users/jacquesthibodeau/Zotero/storage/IK4X6IGS/Yu et al. - 2022 - Towards Safe Reinforcement Learning with a Safety .pdf; /Users/jacquesthibodeau/Zotero/storage/LIDV2Y77/2201.html; /Users/jacquesthibodeau/Zotero/storage/SL4D3MH8/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIW2SID7,journalArticle,2022,"Thomas, Garrett; Luo, Yuping; Ma, Tengyu",Safe Reinforcement Learning by Imagining the Near Future,arXiv:2202.07789 [cs],,,,http://arxiv.org/abs/2202.07789,"Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",2022-02-15,2022-03-11 1:03:43,2022-03-11 1:03:43,2022-03-11 1:03:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.07789,,/Users/jacquesthibodeau/Zotero/storage/BPZXUX3A/Thomas et al. - 2022 - Safe Reinforcement Learning by Imagining the Near .pdf; /Users/jacquesthibodeau/Zotero/storage/93SZGJUK/2202.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNVV4SR2,journalArticle,2022,"Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan",Training language models to follow instructions with human feedback,arXiv:2203.02155 [cs],,,,http://arxiv.org/abs/2203.02155,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",2022-03-04,2022-03-11 1:03:58,2022-03-11 1:03:58,2022-03-11 1:03:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2203.02155,,/Users/jacquesthibodeau/Zotero/storage/PADEXKSX/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf; /Users/jacquesthibodeau/Zotero/storage/TWCAF3MA/2203.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G4KC3SAZ,journalArticle,2021,"Lee, Kimin; Smith, Laura; Dragan, Anca; Abbeel, Pieter",B-Pref: Benchmarking Preference-Based Reinforcement Learning,arXiv:2111.03026 [cs],,,,http://arxiv.org/abs/2111.03026,"Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",2021-11-04,2022-03-11 1:04:21,2022-03-11 1:04:21,2022-03-11 1:04:21,,,,,,,B-Pref,,,,,,,,,,,,arXiv.org,,arXiv: 2111.03026,,/Users/jacquesthibodeau/Zotero/storage/LZZIN2MY/Lee et al. - 2021 - B-Pref Benchmarking Preference-Based Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/7R2FJCXT/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4QFVRC4R,journalArticle,2022,"Meng, Kevin; Bau, David; Andonian, Alex; Belinkov, Yonatan",Locating and Editing Factual Knowledge in GPT,arXiv:2202.05262 [cs],,,,http://arxiv.org/abs/2202.05262,"We investigate the mechanisms underlying factual knowledge recall in autoregressive transformer language models. First, we develop a causal intervention for identifying neuron activations capable of altering a model's factual predictions. Within large GPT-style models, this reveals two distinct sets of neurons that we hypothesize correspond to knowing an abstract fact and saying a concrete word, respectively. This insight inspires the development of ROME, a novel method for editing facts stored in model weights. For evaluation, we assemble CounterFact, a dataset of over twenty thousand counterfactuals and tools to facilitate sensitive measurements of knowledge editing. Using CounterFact, we confirm the distinction between saying and knowing neurons, and we find that ROME achieves state-of-the-art performance in knowledge editing compared to other methods. An interactive demo notebook, full code implementation, and the dataset are available at https://rome.baulab.info/.",2022-02-10,2022-03-11 1:04:40,2022-03-11 1:04:40,2022-03-11 1:04:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.05262,,/Users/jacquesthibodeau/Zotero/storage/FVG2NMDT/Meng et al. - 2022 - Locating and Editing Factual Knowledge in GPT.pdf; /Users/jacquesthibodeau/Zotero/storage/FIPFTGUT/2202.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; I.2.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIQ5IJQ8,journalArticle,2020,"Liu, Hanmeng; Cui, Leyang; Liu, Jian; Zhang, Yue",Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts,arXiv:2011.04864 [cs],,,,http://arxiv.org/abs/2011.04864,"Natural language inference (NLI) is a fundamental NLP task, investigating the entailment relationship between two texts. Popular NLI datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce ConTRoL, a new dataset for ConTextual Reasoning over Long texts. Consisting of 8,325 expert-designed ""context-hypothesis"" pairs with gold labels, ConTRoL is a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous NLI benchmarks, the materials in ConTRoL are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.",2020-11-09,2022-03-11 1:04:42,2022-03-11 1:04:42,2022-03-11 1:04:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.04864,,/Users/jacquesthibodeau/Zotero/storage/6J29LTVY/Liu et al. - 2020 - Natural Language Inference in Context -- Investiga.pdf; /Users/jacquesthibodeau/Zotero/storage/JMU6K4XF/2011.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG42Y6E,journalArticle,2021,"Minderer, Matthias; Djolonga, Josip; Romijnders, Rob; Hubis, Frances; Zhai, Xiaohua; Houlsby, Neil; Tran, Dustin; Lucic, Mario",Revisiting the Calibration of Modern Neural Networks,arXiv:2106.07998 [cs],,,,http://arxiv.org/abs/2106.07998,"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.",2021-10-26,2022-03-11 1:04:52,2022-03-11 1:04:52,2022-03-11 1:04:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.07998,,/Users/jacquesthibodeau/Zotero/storage/V74X8RRV/Minderer et al. - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/Z9XNYP9T/2106.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4WMRJ5M,journalArticle,2021,"Karandikar, Archit; Cain, Nicholas; Tran, Dustin; Lakshminarayanan, Balaji; Shlens, Jonathon; Mozer, Michael C.; Roelofs, Becca",Soft Calibration Objectives for Neural Networks,arXiv:2108.00106 [cs],,,,http://arxiv.org/abs/2108.00106,"Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.",2021-12-07,2022-03-11 1:05:04,2022-03-11 1:05:04,2022-03-11 1:05:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.00106,,/Users/jacquesthibodeau/Zotero/storage/R5PT9IM3/Karandikar et al. - 2021 - Soft Calibration Objectives for Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/CQRA3RSD/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRRZ4CQE,journalArticle,2022,"Yu, Yaodong; Yang, Zitong; Wei, Alexander; Ma, Yi; Steinhardt, Jacob",Predicting Out-of-Distribution Error with the Projection Norm,"arXiv:2202.05834 [cs, stat]",,,,http://arxiv.org/abs/2202.05834,"We propose a metric -- Projection Norm -- to predict a model's performance on out-of-distribution (OOD) data without access to ground truth labels. Projection Norm first uses model predictions to pseudo-label test samples and then trains a new model on the pseudo-labels. The more the new model's parameters differ from an in-distribution model, the greater the predicted OOD error. Empirically, our approach outperforms existing methods on both image and text classification tasks and across different network architectures. Theoretically, we connect our approach to a bound on the test error for overparameterized linear models. Furthermore, we find that Projection Norm is the only approach that achieves non-trivial detection performance on adversarial examples. Our code is available at https://github.com/yaodongyu/ProjNorm.",2022-02-11,2022-03-11 1:05:16,2022-03-11 1:05:16,2022-03-11 1:05:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.05834,,/Users/jacquesthibodeau/Zotero/storage/2588K5QW/Yu et al. - 2022 - Predicting Out-of-Distribution Error with the Proj.pdf; /Users/jacquesthibodeau/Zotero/storage/2HT6DBJK/2202.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R5DUJUZS,journalArticle,2022,"Ypsilantis, Nikolaos-Antonios; Garcia, Noa; Han, Guangxing; Ibrahimi, Sarah; Van Noord, Nanne; Tolias, Giorgos",The Met Dataset: Instance-level Recognition for Artworks,arXiv:2202.01747 [cs],,,,http://arxiv.org/abs/2202.01747,"This work introduces a dataset for large-scale instance-level recognition in the domain of artworks. The proposed benchmark exhibits a number of different challenges such as large inter-class similarity, long tail distribution, and many classes. We rely on the open access collection of The Met museum to form a large training set of about 224k classes, where each class corresponds to a museum exhibit with photos taken under studio conditions. Testing is primarily performed on photos taken by museum guests depicting exhibits, which introduces a distribution shift between training and testing. Testing is additionally performed on a set of images not related to Met exhibits making the task resemble an out-of-distribution detection problem. The proposed benchmark follows the paradigm of other recent datasets for instance-level recognition on different domains to encourage research on domain independent approaches. A number of suitable approaches are evaluated to offer a testbed for future comparisons. Self-supervised and supervised contrastive learning are effectively combined to train the backbone which is used for non-parametric classification that is shown as a promising direction. Dataset webpage: http://cmp.felk.cvut.cz/met/",2022-02-03,2022-03-11 1:05:29,2022-03-11 1:05:29,2022-03-11 1:05:29,,,,,,,The Met Dataset,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01747,,/Users/jacquesthibodeau/Zotero/storage/DRGJVC3M/Ypsilantis et al. - 2022 - The Met Dataset Instance-level Recognition for Ar.pdf; /Users/jacquesthibodeau/Zotero/storage/SYB6FK9M/2202.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ICPFS4N,journalArticle,2021,"Sun, Yiyou; Guo, Chuan; Li, Yixuan",ReAct: Out-of-distribution Detection With Rectified Activations,arXiv:2111.12797 [cs],,,,http://arxiv.org/abs/2111.12797,"Out-of-distribution (OOD) detection has received much attention lately due to its practical importance in enhancing the safe deployment of neural networks. One of the primary challenges is that models often produce highly confident predictions on OOD data, which undermines the driving principle in OOD detection that the model should only be confident about in-distribution samples. In this work, we propose ReAct--a simple and effective technique for reducing model overconfidence on OOD data. Our method is motivated by novel analysis on internal activations of neural networks, which displays highly distinctive signature patterns for OOD distributions. Our method can generalize effectively to different network architectures and different OOD detection scores. We empirically demonstrate that ReAct achieves competitive detection performance on a comprehensive suite of benchmark datasets, and give theoretical explication for our method's efficacy. On the ImageNet benchmark, ReAct reduces the false positive rate (FPR95) by 25.05% compared to the previous best method.",2021-11-24,2022-03-11 1:05:39,2022-03-11 1:05:39,2022-03-11 1:05:39,,,,,,,ReAct,,,,,,,,,,,,arXiv.org,,arXiv: 2111.12797,,/Users/jacquesthibodeau/Zotero/storage/UR9UMZSS/Sun et al. - 2021 - ReAct Out-of-distribution Detection With Rectifie.pdf; /Users/jacquesthibodeau/Zotero/storage/JX4WRFTR/2111.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKMB48ME,journalArticle,2022,"Hendrycks, Dan; Basart, Steven; Mazeika, Mantas; Zou, Andy; Kwon, Joe; Mostajabi, Mohammadreza; Steinhardt, Jacob; Song, Dawn",Scaling Out-of-Distribution Detection for Real-World Settings,arXiv:1911.11132 [cs],,,,http://arxiv.org/abs/1911.11132,"Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.",2022-02-07,2022-03-11 1:05:54,2022-03-11 1:05:54,2022-03-11 1:05:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.11132,,/Users/jacquesthibodeau/Zotero/storage/CBAXQ3U5/Hendrycks et al. - 2022 - Scaling Out-of-Distribution Detection for Real-Wor.pdf; /Users/jacquesthibodeau/Zotero/storage/GV4HFVE2/1911.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D42L7Z7U,journalArticle,2022,"Foley, Harrison; Fowl, Liam; Goldstein, Tom; Taylor, Gavin",Execute Order 66: Targeted Data Poisoning for Reinforcement Learning,arXiv:2201.00762 [cs],,,,http://arxiv.org/abs/2201.00762,"Data poisoning for reinforcement learning has historically focused on general performance degradation, and targeted attacks have been successful via perturbations that involve control of the victim's policy and rewards. We introduce an insidious poisoning attack for reinforcement learning which causes agent misbehavior only at specific target states - all while minimally modifying a small fraction of training observations without assuming any control over policy or reward. We accomplish this by adapting a recent technique, gradient alignment, to reinforcement learning. We test our method and demonstrate success in two Atari games of varying difficulty.",2022-01-03,2022-03-11 1:06:03,2022-03-11 1:06:03,2022-03-11 1:06:03,,,,,,,Execute Order 66,,,,,,,,,,,,arXiv.org,,arXiv: 2201.00762,,/Users/jacquesthibodeau/Zotero/storage/DCWYVM9Q/Foley et al. - 2022 - Execute Order 66 Targeted Data Poisoning for Rein.pdf; /Users/jacquesthibodeau/Zotero/storage/DR45MNB4/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JQG3F593,journalArticle,2022,"Shi, Bowen; Hsu, Wei-Ning; Mohamed, Abdelrahman",Robust Self-Supervised Audio-Visual Speech Recognition,"arXiv:2201.01763 [cs, eess]",,,,http://arxiv.org/abs/2201.01763,"Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by ~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75% (25.8% vs. 5.8%) on average.",2022-01-05,2022-03-11 1:06:23,2022-03-11 1:06:23,2022-03-11 1:06:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.01763,,/Users/jacquesthibodeau/Zotero/storage/59BG2ZWZ/Shi et al. - 2022 - Robust Self-Supervised Audio-Visual Speech Recogni.pdf; /Users/jacquesthibodeau/Zotero/storage/P73PM5N8/2201.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35ZECVN7,journalArticle,2022,"Kar, Oğuzhan Fatih; Yeo, Teresa; Atanov, Andrei; Zamir, Amir",3D Common Corruptions and Data Augmentation,arXiv:2203.01441 [cs],,,,http://arxiv.org/abs/2203.01441,"We introduce a set of image transformations that can be used as `corruptions' to evaluate the robustness of models as well as `data augmentation' mechanisms for training neural networks. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions, the geometry of the scene is incorporated in the transformations -- thus leading to corruptions that are more likely to occur in the real world. We show these transformations are `efficient' (can be computed on-the-fly), `extendable' (can be applied on most datasets of real images), expose vulnerability of existing models, and can effectively make models more robust when employed as `3D data augmentation' mechanisms. Our evaluations performed on several tasks and datasets suggest incorporating 3D information into robustness benchmarking and training opens up a promising direction for robustness research.",2022-03-02,2022-03-11 1:06:33,2022-03-11 1:06:33,2022-03-11 1:06:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2203.01441,,/Users/jacquesthibodeau/Zotero/storage/5JKJMPKB/Kar et al. - 2022 - 3D Common Corruptions and Data Augmentation.pdf; /Users/jacquesthibodeau/Zotero/storage/7AXV3UB7/2203.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TW5ZP9IV,journalArticle,2022,"Long, Alexander; Yin, Wei; Ajanthan, Thalaiyasingam; Nguyen, Vu; Purkait, Pulak; Garg, Ravi; Blair, Alan; Shen, Chunhua; Hengel, Anton van den",Retrieval Augmented Classification for Long-Tail Visual Recognition,arXiv:2202.11233 [cs],,,,http://arxiv.org/abs/2202.11233,"We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.",2022-02-22,2022-03-11 1:06:44,2022-03-11 1:06:44,2022-03-11 1:06:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.11233,,/Users/jacquesthibodeau/Zotero/storage/GNXXXSFR/Long et al. - 2022 - Retrieval Augmented Classification for Long-Tail V.pdf; /Users/jacquesthibodeau/Zotero/storage/GRHJ4W6F/2202.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BM2QHXN,journalArticle,2022,"Kaplun, Gal; Ghosh, Nikhil; Garg, Saurabh; Barak, Boaz; Nakkiran, Preetum",Deconstructing Distributions: A Pointwise Framework of Learning,"arXiv:2202.09931 [cs, stat]",,,,http://arxiv.org/abs/2202.09931,"In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ""compatible"" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts ""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021)",2022-02-20,2022-03-11 1:06:55,2022-03-11 1:06:55,2022-03-11 1:06:55,,,,,,,Deconstructing Distributions,,,,,,,,,,,,arXiv.org,,arXiv: 2202.09931,,/Users/jacquesthibodeau/Zotero/storage/M4MFP3EB/Kaplun et al. - 2022 - Deconstructing Distributions A Pointwise Framewor.pdf; /Users/jacquesthibodeau/Zotero/storage/KEFGW4PR/2202.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EWMKLQRL,journalArticle,2022,"Weber, Maurice; Li, Linyi; Wang, Boxin; Zhao, Zhikuan; Li, Bo; Zhang, Ce",Certifying Out-of-Domain Generalization for Blackbox Functions,arXiv:2202.01679 [cs],,,,http://arxiv.org/abs/2202.01679,"Certifying the robustness of model performance under bounded data distribution shifts has recently attracted intensive interests under the umbrella of distributional robustness. However, existing techniques either make strong assumptions on the model class and loss functions that can be certified, such as smoothness expressed via Lipschitz continuity of gradients, or require to solve complex optimization problems. As a result, the wider application of these techniques is currently limited by its scalability and flexibility -- these techniques often do not scale to large-scale datasets with modern deep neural networks or cannot handle loss functions which may be non-smooth, such as the 0-1 loss. In this paper, we focus on the problem of certifying distributional robustness for black box models and bounded losses, without other assumptions. We propose a novel certification framework given bounded distance of mean and variance of two distributions. Our certification technique scales to ImageNet-scale datasets, complex models, and a diverse range of loss functions. We then focus on one specific application enabled by such scalability and flexibility, i.e., certifying out-of-domain generalization for large neural networks and loss functions such as accuracy and AUC. We experimentally validate our certification method on a number of datasets, ranging from ImageNet, where we provide the first non-vacuous certified out-of-domain generalization, to smaller classification tasks where we are able to compare with the state-of-the-art and show that our method performs considerably better.",2022-02-03,2022-03-11 1:07:06,2022-03-11 1:07:06,2022-03-11 1:07:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01679,,/Users/jacquesthibodeau/Zotero/storage/5CJR9VJI/Weber et al. - 2022 - Certifying Out-of-Domain Generalization for Blackb.pdf; /Users/jacquesthibodeau/Zotero/storage/QUDAFBNS/2202.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EYMUMTND,journalArticle,2022,"Kumar, Aounon; Levine, Alexander; Goldstein, Tom; Feizi, Soheil",Certifying Model Accuracy under Distribution Shifts,arXiv:2201.12440 [cs],,,,http://arxiv.org/abs/2201.12440,"Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts and changes in brightness and saturation, (ii) certify robustness against adversarial shifts of the input distribution, and (iii) show provable lower bounds (hardness results) on the performance of models trained on so-called ""unlearnable"" datasets that have been poisoned to interfere with model training.",2022-01-28,2022-03-11 1:07:17,2022-03-11 1:07:17,2022-03-11 1:07:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.12440,,/Users/jacquesthibodeau/Zotero/storage/FPGPLJLI/Kumar et al. - 2022 - Certifying Model Accuracy under Distribution Shift.pdf; /Users/jacquesthibodeau/Zotero/storage/57NNXGYH/2201.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BCHQQL7G,journalArticle,2021,"Sun, Jiachen; Mehra, Akshay; Kailkhura, Bhavya; Chen, Pin-Yu; Hendrycks, Dan; Hamm, Jihun; Mao, Z. Morley",Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines,arXiv:2112.00659 [cs],,,,http://arxiv.org/abs/2112.00659,"Certified robustness guarantee gauges a model's robustness to test-time attacks and can assess the model's readiness for deployment in the real world. In this work, we critically examine how the adversarial robustness guarantees from randomized smoothing-based certification methods change when state-of-the-art certifiably robust models encounter out-of-distribution (OOD) data. Our analysis demonstrates a previously unknown vulnerability of these models to low-frequency OOD data such as weather-related corruptions, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We find that FourierMix augmentations help eliminate the spectral bias of certifiably robust models enabling them to achieve significantly better robustness guarantees on a range of OOD benchmarks. Our evaluation also uncovers the inability of current OOD benchmarks at highlighting the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite highlights their spectral biases and establishes the superiority of FourierMix trained models at achieving better-certified robustness guarantees under OOD shifts over the entire frequency spectrum.",2021-12-01,2022-03-11 1:07:27,2022-03-11 1:07:27,2022-03-11 1:07:27,,,,,,,Certified Adversarial Defenses Meet Out-of-Distribution Corruptions,,,,,,,,,,,,arXiv.org,,arXiv: 2112.00659,,/Users/jacquesthibodeau/Zotero/storage/6HF2MDV3/Sun et al. - 2021 - Certified Adversarial Defenses Meet Out-of-Distrib.pdf; /Users/jacquesthibodeau/Zotero/storage/CJWSQXUZ/2112.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U4N2XYSI,journalArticle,2022,"Perez, Ethan; Huang, Saffron; Song, Francis; Cai, Trevor; Ring, Roman; Aslanides, John; Glaese, Amelia; McAleese, Nat; Irving, Geoffrey",Red Teaming Language Models with Language Models,arXiv:2202.03286 [cs],,,,http://arxiv.org/abs/2202.03286,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",2022-02-07,2022-03-11 1:07:38,2022-03-11 1:07:38,2022-03-11 1:07:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.03286,,/Users/jacquesthibodeau/Zotero/storage/JHI3DJ3D/Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/XQL524J2/2202.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AN7HZJTD,journalArticle,2020,"Li, Linyang; Ma, Ruotian; Guo, Qipeng; Xue, Xiangyang; Qiu, Xipeng",BERT-ATTACK: Adversarial Attack Against BERT Using BERT,arXiv:2004.09984 [cs],,,,http://arxiv.org/abs/2004.09984,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.",2020-10-01,2022-03-11 1:07:50,2022-03-11 1:07:50,2022-03-11 1:07:49,,,,,,,BERT-ATTACK,,,,,,,,,,,,arXiv.org,,arXiv: 2004.09984,,/Users/jacquesthibodeau/Zotero/storage/5PYEYRB4/Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf; /Users/jacquesthibodeau/Zotero/storage/XK76QFYP/2004.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CF4WKV4F,journalArticle,2022,"Xie, Zhouhang; Brophy, Jonathan; Noack, Adam; You, Wencong; Asthana, Kalyani; Perkins, Carter; Reis, Sabrina; Singh, Sameer; Lowd, Daniel",Identifying Adversarial Attacks on Text Classifiers,arXiv:2201.08555 [cs],,,,http://arxiv.org/abs/2201.08555,"The landscape of adversarial attacks against text classifiers continues to grow, with new attacks developed every year and many of them available in standard toolkits, such as TextAttack and OpenAttack. In response, there is a growing body of work on robust learning, which reduces vulnerability to these attacks, though sometimes at a high cost in compute time or accuracy. In this paper, we take an alternate approach -- we attempt to understand the attacker by analyzing adversarial text to determine which methods were used to create it. Our first contribution is an extensive dataset for attack detection and labeling: 1.5~million attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. As our second contribution, we use this dataset to develop and benchmark a number of classifiers for attack identification -- determining if a given text has been adversarially manipulated and by which attack. As a third contribution, we demonstrate the effectiveness of three classes of features for these tasks: text properties, capturing content and presentation of text; language model properties, determining which tokens are more or less probable throughout the input; and target model properties, representing how the text classifier is influenced by the attack, including internal node activations. Overall, this represents a first step towards forensics for adversarial attacks against text classifiers.",2022-01-21,2022-03-11 1:08:10,2022-03-11 1:08:10,2022-03-11 1:08:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.08555,,/Users/jacquesthibodeau/Zotero/storage/VNQ3JHFR/Xie et al. - 2022 - Identifying Adversarial Attacks on Text Classifier.pdf; /Users/jacquesthibodeau/Zotero/storage/U6D5XGZ8/2201.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YPD6EZNY,journalArticle,2021,"Guo, Chuan; Sablayrolles, Alexandre; Jégou, Hervé; Kiela, Douwe",Gradient-based Adversarial Attacks against Text Transformers,arXiv:2104.13733 [cs],,,,http://arxiv.org/abs/2104.13733,"We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.",2021-04-15,2022-03-11 1:08:10,2022-03-11 1:08:10,2022-03-11 1:08:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.13733,,/Users/jacquesthibodeau/Zotero/storage/VDXYSSZU/Guo et al. - 2021 - Gradient-based Adversarial Attacks against Text Tr.pdf; /Users/jacquesthibodeau/Zotero/storage/DMMIPIUP/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BW76HKK7,journalArticle,2021,"Rebuffi, Sylvestre-Alvise; Gowal, Sven; Calian, Dan A.; Stimberg, Florian; Wiles, Olivia; Mann, Timothy",Data Augmentation Can Improve Robustness,"arXiv:2111.05328 [cs, stat]",,,,http://arxiv.org/abs/2111.05328,"Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$, our model reaches 60.07% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.",2021-11-09,2022-03-11 1:08:28,2022-03-11 1:08:28,2022-03-11 1:08:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.05328,,/Users/jacquesthibodeau/Zotero/storage/7JIV2ZL9/Rebuffi et al. - 2021 - Data Augmentation Can Improve Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/Q9ZA9DBT/2111.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8L7V5878,journalArticle,2021,"Hendrycks, Dan; Zou, Andy; Mazeika, Mantas; Tang, Leonard; Li, Bo; Song, Dawn; Steinhardt, Jacob",PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures,arXiv:2112.05135 [cs],,,,http://arxiv.org/abs/2112.05135,"In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.",2021-12-11,2022-03-11 1:08:31,2022-03-11 1:08:31,2022-03-11 1:08:31,,,,,,,PixMix,,,,,,,,,,,,arXiv.org,,arXiv: 2112.05135,,/Users/jacquesthibodeau/Zotero/storage/ZKTIUZ67/Hendrycks et al. - 2021 - PixMix Dreamlike Pictures Comprehensively Improve.pdf; /Users/jacquesthibodeau/Zotero/storage/T5KZ92QK/2112.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JUMWFE49,journalArticle,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,,,,10.48550/arXiv.1805.01109,https://arxiv.org/abs/1805.01109v2,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-03,2022-03-11 1:24:37,2022-03-11 1:24:37,2022-03-11 1:24:37,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/DKMM7ZFQ/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/LQ33DGVC/1805.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QTBCAPFF,journalArticle,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,,,,10.48550/arXiv.2109.13916,https://arxiv.org/abs/2109.13916v3,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing deployment hazards (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-09-28,2022-03-11 1:24:59,2022-03-11 1:24:59,2022-03-11 1:24:59,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/5G22C7B7/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/BZ7SK5PB/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LYYL3QVX,journalArticle,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),,,,10.48550/arXiv.2006.04948,https://arxiv.org/abs/2006.04948v1,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",2020-05-30,2022-03-11 1:25:00,2022-03-11 1:25:00,2022-03-11 1:25:00,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CRAVZTQY/Critch and Krueger - 2020 - AI Research Considerations for Human Existential S.pdf; /Users/jacquesthibodeau/Zotero/storage/ZPDQZXMM/2006.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73ZHAJ4U,journalArticle,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,,,,10.48550/arXiv.2102.05008,https://arxiv.org/abs/2102.05008v1,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-03-11 1:25:05,2022-03-11 1:25:05,2022-03-11 1:25:05,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QNF83N3L/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/W5ITAFL9/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ7TJV39,journalArticle,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,,,,10.48550/arXiv.2012.08630,https://arxiv.org/abs/2012.08630v1,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-03-11 1:25:06,2022-03-11 1:25:06,2022-03-11 1:25:06,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QY8XGVLI/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/LFH6FRBJ/2012.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SYS2URXD,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,,,,10.48550/arXiv.2102.08686,https://arxiv.org/abs/2102.08686v1,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-03-11 1:25:07,2022-03-11 1:25:07,2022-03-11 1:25:07,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/7HUD5XGM/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/6TXX3RFN/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM37CFF7,journalArticle,2021,"Klinova, Katya; Korinek, Anton",AI and Shared Prosperity,,,,10.1145/3461702.3462619,https://arxiv.org/abs/2105.08475v1,"Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",2021-05-18,2022-03-11 1:25:14,2022-03-11 1:36:20,2022-03-11 1:25:14,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/APLZI8BN/Klinova and Korinek - 2021 - AI and Shared Prosperity.pdf; /Users/jacquesthibodeau/Zotero/storage/NSDK44MG/Klinova and Korinek - 2021 - AI and Shared Prosperity.pdf; /Users/jacquesthibodeau/Zotero/storage/CVL2FGE4/2105.html; /Users/jacquesthibodeau/Zotero/storage/8GKBMWRN/2105.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WHA6VWZL,journalArticle,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,,,,10.48550/arXiv.2103.03386,https://arxiv.org/abs/2103.03386v1,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-03-11 1:25:21,2022-03-11 1:25:21,2022-03-11 1:25:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QBKW4JPH/Filan et al. - 2021 - Clusterability in Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/VDE8SL52/2103.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K5PD6SSU,journalArticle,2021,"Zhuang, Simon; Hadfield-Menell, Dylan",Consequences of Misaligned AI,,,,10.48550/arXiv.2102.03896,https://arxiv.org/abs/2102.03896v1,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-03-11 1:26:50,2022-03-11 1:26:50,2022-03-11 1:26:50,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/6D3TAWRW/Zhuang and Hadfield-Menell - 2021 - Consequences of Misaligned AI.pdf; /Users/jacquesthibodeau/Zotero/storage/AB3SPPGB/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2D3UTUMI,journalArticle,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,,,,10.48550/arXiv.2107.01969,https://arxiv.org/abs/2107.01969v1,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-03-11 1:26:52,2022-03-11 1:26:52,2022-03-11 1:26:52,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/YCQJBT2C/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/G287MGGG/2107.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WY2UBYEQ,journalArticle,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do by Simulating the Past,,,,10.48550/arXiv.2104.03946,https://arxiv.org/abs/2104.03946v2,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021-04-08,2022-03-11 1:27:04,2022-03-11 1:27:04,2022-03-11 1:27:04,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/9Q8T8D43/Lindner et al. - 2021 - Learning What To Do by Simulating the Past.pdf; /Users/jacquesthibodeau/Zotero/storage/4TS3RJ62/2104.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQWAINS4,journalArticle,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,,,,10.48550/arXiv.1902.04198,https://arxiv.org/abs/1902.04198v2,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-02-12,2022-03-11 1:27:05,2022-03-11 1:27:05,2022-03-11 1:27:05,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/NB55YFZS/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/PKKXP2Q5/1902.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69WEZGBQ,journalArticle,2021,"Lee, Kimin; Smith, Laura; Abbeel, Pieter",PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training,,,,10.48550/arXiv.2106.05091,https://arxiv.org/abs/2106.05091v1,"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",2021-06-09,2022-03-11 1:27:08,2022-03-11 1:27:08,2022-03-11 1:27:08,,,,,,,PEBBLE,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/KIY5YDLG/Lee et al. - 2021 - PEBBLE Feedback-Efficient Interactive Reinforceme.pdf; /Users/jacquesthibodeau/Zotero/storage/9BGQI398/2106.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGUDILHF,journalArticle,2021,"Andrus, McKane; Dean, Sarah; Gilbert, Thomas Krendl; Lambert, Nathan; Zick, Tom",AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks,,,,10.48550/arXiv.2102.04255,https://arxiv.org/abs/2102.04255v1,"Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI.",2021-02-04,2022-03-11 1:27:19,2022-03-11 1:36:22,2022-03-11 1:27:19,,,,,,,AI Development for the Public Interest,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QKNPJEBP/Andrus et al. - 2021 - AI Development for the Public Interest From Abstr.pdf; /Users/jacquesthibodeau/Zotero/storage/UZW8GAVV/Andrus et al. - 2021 - AI Development for the Public Interest From Abstr.pdf; /Users/jacquesthibodeau/Zotero/storage/HT327V55/2102.html; /Users/jacquesthibodeau/Zotero/storage/6TX9QIFH/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FWCKT7XU,journalArticle,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,,,,10.48550/arXiv.2109.11513,https://arxiv.org/abs/2109.11513v1,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-03-11 1:27:21,2022-03-11 1:27:21,2022-03-11 1:27:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/3BX2P4WW/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/6745K6FU/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZMFN98H,journalArticle,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,,,,10.48550/arXiv.2107.03374,https://arxiv.org/abs/2107.03374v2,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-07,2022-03-11 1:27:24,2022-03-11 1:27:24,2022-03-11 1:27:24,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/GLFVN6RB/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/G8J7P6ZA/2107.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZM73QCS,journalArticle,2021,"Welbl, Johannes; Glaese, Amelia; Uesato, Jonathan; Dathathri, Sumanth; Mellor, John; Hendricks, Lisa Anne; Anderson, Kirsty; Kohli, Pushmeet; Coppin, Ben; Huang, Po-Sen",Challenges in Detoxifying Language Models,,,,10.48550/arXiv.2109.07445,https://arxiv.org/abs/2109.07445v1,"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.",2021-09-15,2022-03-11 1:27:27,2022-03-11 1:27:27,2022-03-11 1:27:27,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/YVVZ3C32/Welbl et al. - 2021 - Challenges in Detoxifying Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/RUFXR8I7/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VEMYGUZD,journalArticle,2021,"Gabriel, Iason",Towards a Theory of Justice for Artificial Intelligence,,,,10.48550/arXiv.2110.14419,https://arxiv.org/abs/2110.14419v1,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society.",2021-10-27,2022-03-11 1:27:30,2022-03-11 1:27:30,2022-03-11 1:27:30,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/GLV244ND/Gabriel - 2021 - Towards a Theory of Justice for Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/6RN8GS45/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28FUQKDY,journalArticle,2022,"Ganguli, Deep; Hernandez, Danny; Lovitt, Liane; DasSarma, Nova; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Kernion, Jackson; Mann, Ben; Askell, Amanda; Bai, Yuntao; Chen, Anna; Conerly, Tom; Drain, Dawn; Elhage, Nelson; Showk, Sheer El; Fort, Stanislav; Hatfield-Dodds, Zac; Johnston, Scott; Kravec, Shauna; Nanda, Neel; Ndousse, Kamal; Olsson, Catherine; Amodei, Daniela; Amodei, Dario; Brown, Tom; Kaplan, Jared; McCandlish, Sam; Olah, Chris; Clark, Jack",Predictability and Surprise in Large Generative Models,,,,10.48550/arXiv.2202.07785,https://arxiv.org/abs/2202.07785v1,"Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their ""scaling laws""), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.",2022-02-15,2022-03-11 1:28:44,2022-03-11 1:28:44,2022-03-11 1:28:44,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/8J6ZWACU/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf; /Users/jacquesthibodeau/Zotero/storage/PJB8A842/2202.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RA7RKHJW,journalArticle,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,,,,10.48550/arXiv.2009.09071,https://arxiv.org/abs/2009.09071v1,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-03-11 1:28:55,2022-03-11 1:28:55,2022-03-11 1:28:55,,,,,,,Measurement in AI Policy,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/D28GLISV/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/NUXCBT74/2009.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2X9B74NK,journalArticle,2021,"Zhang, Daniel; Mishra, Saurabh; Brynjolfsson, Erik; Etchemendy, John; Ganguli, Deep; Grosz, Barbara; Lyons, Terah; Manyika, James; Niebles, Juan Carlos; Sellitto, Michael; Shoham, Yoav; Clark, Jack; Perrault, Raymond",The AI Index 2021 Annual Report,,,,10.48550/arXiv.2103.06312,https://arxiv.org/abs/2103.06312v1,"Welcome to the fourth edition of the AI Index Report. This year we significantly expanded the amount of data available in the report, worked with a broader set of external organizations to calibrate our data, and deepened our connections with the Stanford Institute for Human-Centered Artificial Intelligence (HAI). The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop intuitions about the complex field of AI. The report aims to be the most credible and authoritative source for data and insights about AI in the world.",2021-03-09,2022-03-11 1:28:58,2022-03-11 1:28:58,2022-03-11 1:28:58,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/Y6EL6VPQ/Zhang et al. - 2021 - The AI Index 2021 Annual Report.pdf; /Users/jacquesthibodeau/Zotero/storage/MJ99X8U3/2103.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PQNRR2B6,journalArticle,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,,,,10.48550/arXiv.2110.06674,https://arxiv.org/abs/2110.06674v1,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-03-11 1:29:04,2022-03-11 1:29:04,2022-03-11 1:29:04,,,,,,,Truthful AI,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/KFWCLI5U/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/A32SJDCF/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS2LIKSF,journalArticle,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,,,,10.48550/arXiv.2105.14111,https://arxiv.org/abs/2105.14111v2,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-05-28,2022-03-11 1:29:10,2022-03-11 1:29:10,2022-03-11 1:29:10,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CQHLL55C/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/CGDL2DKR/2105.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S68RSNVJ,journalArticle,2021,"Jiang, Liwei; Hwang, Jena D.; Bhagavatula, Chandra; Bras, Ronan Le; Forbes, Maxwell; Borchardt, Jon; Liang, Jenny; Etzioni, Oren; Sap, Maarten; Choi, Yejin",Delphi: Towards Machine Ethics and Norms,,,,10.48550/arXiv.2110.07574,https://arxiv.org/abs/2110.07574v1,"What would it take to teach a machine to behave ethically? While broad ethical rules may seem straightforward to state (""thou shalt not kill""), applying such rules to real-world situations is far more complex. For example, while ""helping a friend"" is generally a good thing to do, ""helping a friend spread fake news"" is not. We identify four underlying challenges towards machine ethics and norms: (1) an understanding of moral precepts and social norms; (2) the ability to perceive real-world situations visually or by reading natural language descriptions; (3) commonsense reasoning to anticipate the outcome of alternative actions in different contexts; (4) most importantly, the ability to make ethical judgments given the interplay between competing values and their grounding in different contexts (e.g., the right to freedom of expression vs. preventing the spread of fake news). Our paper begins to address these questions within the deep learning paradigm. Our prototype model, Delphi, demonstrates strong promise of language-based commonsense moral reasoning, with up to 92.1% accuracy vetted by humans. This is in stark contrast to the zero-shot performance of GPT-3 of 52.3%, which suggests that massive scale alone does not endow pre-trained neural language models with human values. Thus, we present Commonsense Norm Bank, a moral textbook customized for machines, which compiles 1.7M examples of people's ethical judgments on a broad spectrum of everyday situations. In addition to the new resources and baseline performances for future research, our study provides new insights that lead to several important open research questions: differentiating between universal human values and personal values, modeling different moral frameworks, and explainable, consistent approaches to machine ethics.",2021-10-14,2022-03-11 1:29:13,2022-03-11 1:29:13,2022-03-11 1:29:13,,,,,,,Delphi,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CDRKZDXA/Jiang et al. - 2021 - Delphi Towards Machine Ethics and Norms.pdf; /Users/jacquesthibodeau/Zotero/storage/N5CP6QQ9/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y2X87DB2,journalArticle,2020,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,,,,10.48550/arXiv.2008.02275,https://arxiv.org/abs/2008.02275v5,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2020-08-05,2022-03-11 1:29:15,2022-03-11 1:29:15,2022-03-11 1:29:15,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/BBJJGAUR/Hendrycks et al. - 2020 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/X45A9WE8/2008.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZLKJKLU,journalArticle,2020,"Fischer, Ian",The Conditional Entropy Bottleneck,Entropy,,,,http://arxiv.org/abs/2002.05379,"Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.",2020-02-13,2020-09-05 18:45,2020-12-21 18:06,2020-09-05 18:45,,,9,22,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2002.05379,,/Users/angelica/Zotero/storage/AT4UIMH9/Fischer - 2020 - The Conditional Entropy Bottleneck.pdf; /Users/angelica/Zotero/storage/CBDF6ZG3/2002.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While I've categorized this paper under robustness because it can apply to most forms of training, I'll talk about it specifically in the context of unsupervised learning (and in particular its relation to Contrastive Predictive Coding (CPC), summarized in the highlights).

One potential problem with deep learning is that there might be too _much_ information in the input, causing the model to learn spurious correlations that do not actually generalize well (see <@Causal Confusion in Imitation Learning@> as an example). The idea with CEB is to penalize the model for learning irrelevant information, using a form of _information bottleneck_.

We consider a setting where we want to learn a representation **Z** of some input data **X** in order to predict some downstream data **Y**. In CPC, **X** would be the inputs from time 1 to t, **Z** would be the latent representation **z_t**, and **Y** would be the future data **x_{t+k}**. Then, we want **Z** to capture the **minimum necessary information** needed for **Z** to predict **Y** as best as possible. The _necessary_ information is **I(Y; Z)**, that is, the mutual information between **Z** and **Y**: we want to maximize this to maximize our accuracy at predicting **Y**. Since **Y** depends on **X** and **Z** is computed from **X**, any information about **Y** must come through mutual information between **X** and **Z**. Maximizing just this **I(Y; Z)** term gives us Contrastive Predictive Coding.

However, we don't want to capture any extra irrelevant information (the minimality criterion), which means that **Z** shouldn't capture any _more_ information about **X** beyond what it captured to maximize **I(Y; Z)**. In information-theoretic terms, we want to _minimize_ **I(X; Z | Y)**. Thus, we have the CEB objective: minimizing **I(X; Z | Y) - γ I(Y; Z)**, where **γ** is a hyperparameter controlling the tradeoff between the two terms. The authors then use some fairly straightforward math to reduce the objective to simpler terms which can be bounded using variational approximations, leading to an algorithm that can work in practice.

The authors perform experiments on Fashion MNIST and CIFAR10 (where Y corresponds to the labels for the images, so we're in the supervised learning setting). Since the main benefit of CEB is to remove unnecessary information from the model, they evaluate adversarial robustness and out-of-distribution detection in addition to standard performance checks. They find that models trained with CEB perform better than ones trained with a variational information bottleneck, or ones trained with vanilla SGD."
VCQ53PLA,journalArticle,2020,"Li, Mingwei; Zhao, Zhenge; Scheidegger, Carlos",Visualizing Neural Networks with the Grand Tour,Distill,,2476-0757,10.23915/distill.00025,https://distill.pub/2020/grand-tour,"By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.",2020-03-16,2020-09-05 17:37,2020-12-21 18:17,2020-09-05 17:37,e25,,3,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000000,,/Users/angelica/Zotero/storage/RRJ646FX/grand-tour.html,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Visualizing a complete dataset instead of single input examples is helpful when we want to analyze the relationships between different input examples and how their classification changes during training, as we can do so by looking at a single video.

The authors use an example on MNIST in which the network learns to classify the numbers 1 and 7 in an almost discrete fashion during particular epochs to compare different methods for visualizing how the dataset is classified. They find that one problem with nonlinear dimensionality reduction like t-SNE and UMAPs is that changes to a subset of the dataset can strongly affect how unchanged data points are represented. Then they compare this to the Grand Tour, a classical technique that projects the data into two dimensions from varying points of view. As projections are linear in the input variables, it is rather easy to reason about how changes in the data affect this visualization and the times the classes 1 and 7 are learnt are indeed quite salient in their example. Another advantage of this method is that confusion between two specific classes can be identified more easily, as the corresponding data points will be projected onto the line connecting the clusters for these classes. A similar approach can be taken on a network's hidden layers to identify the layer in which different classes become clearly distinguishable. They find that they can identify adversarial examples generated by FGSM by looking at the second to last layer, where the adversarial examples form a cluster distinct from the real images.

As the Grand Tour involves varying rotations, it is basically unaffected by rotations of the data. The authors argue that this is a feature, as rotations are small changes to the data and should not have a large effect on the visualization."
DXSJMY7V,journalArticle,2020,"Hilton, Jacob; Cammarata, Nick; Carter, Shan; Goh, Gabriel; Olah, Chris",Understanding RL Vision,Distill,,2476-0757,10.23915/distill.00029,https://distill.pub/2020/understanding-rl-vision,"With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",2020-11-17,2020-12-18 15:18,2020-12-20 15:20,2020-12-18 15:18,e29,,11,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000000,,/Users/angelica/Zotero/storage/BLZ7VWI8/understanding-rl-vision.html,,NotSafety; AmbiguosSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This work presents an interface for interpreting the vision of a reinforcement learning agent trained with PPO on the CoinRun game. This game is procedurally generated, which means the levels are different in every episode of playing. The interface primarily uses attribution from a hidden layer to the output of the value function. This interface is used in several ways.

First, they use the interface to dissect failed trajectories of the policy (it fails in 1 out of 200 levels). They're able to understand why the failures occurred using their interface: for example, in one case the view of the agent at the top of its jump means it can't see any platforms below it, so doesn't move to the right fast enough to reach the platform it was jumping for, leading it to miss the platform and fail the level. Second, they use the interface to discover ""hallucinations"", where the value function mistakes one element of the environment for another, causing its value to drop or rise significantly. Often these hallucinations only last a single time-step, so they don't affect performance.

Finally, they use the attributions specifically to hand-edit the weights of the model to make it ""blind"" to buzzsaws (one of the hazards) by zeroing the feature which recognises them. After doing this, they show that the edited agent fails a lot more from buzzsaw failures but no more from other types of failures, which gives a quantitative justification for their interpretation of the feature as buzzsaw-recognising.

From using this interface, they propose the **diversity hypothesis:** _Interpretable features tend to arise (at a given level of abstraction) if and only if the training distribution is diverse enough (at that level of abstraction)._ This is based on the fact that interpretable features arise more when the agent is trained on a wider variety of levels. There also seems to be a qualitative link to generalisation - a wider distribution of training levels leads to better interpretability (measured qualitatively) and better generalisation (measured quantitatively)."
,journalArticle,2020,"Mordvintsev, Alexander; Randazzo, Ettore; Niklasson, Eyvind; Levin, Michael",Growing Neural Cellular Automata,Distill,,2476-0757,10.23915/distill.00023,https://distill.pub/2020/growing-ca,"Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.",2020-02-11,2020-08-31 18:44,2020-12-21 18:21,2020-08-31 18:44,e23,,2,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000002,,/Users/angelica/Zotero/storage/XE438DGP/growing-ca.html,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The process of an organism's shape development (morphogensis) is an active area of research. One central problem is determining how cells decide how to grow and when to stop. One popular model for investigating this is Cellular Automata (CA). These model cells as living on a grid and interacting with each other via rules generated by looking at their nearest neighbors. The authors contribute to this research direction by introducing rule-sets that depend continuously on their local surroundings. The central insight connecting CA and deep learning is that because the rule-sets are constant the update rules work similarly to a convolutional filter. This allows the authors to take advantage of methods available to train neural networks to simulate CA. Using this insight, the authors train CA that can form into images that are resistant to perturbations and deletions. In other words, the CA are capable of regeneration."
GHMMCFVM,magazineArticle,2018,"Arkin, Ronald; Russell, Stuart; Min-Seok, Kim",The new weapons of mass destruction?,The Security Times,,,,,,2018,2022-01-30 4:51:10,2022-01-30 4:51:10,,1,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XXVH9ATC/Arkin et al. - The new weapons of mass destruction.pdf,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4HM6IEQ3,magazineArticle,2016,"Russell, Stuart",Robots in war: the next weapons of mass destruction?,World Economic Forum,,,,https://www.weforum.org/agenda/2016/01/robots-in-war-the-next-weapons-of-mass-destruction/,"Davos 2016: There is no doubt that as the technology improves, autonomous weapons will be highly effective. But does that necessarily mean they’re a good idea?",2016,2022-01-30 4:51:08,2022-01-30 4:51:08,2019-12-18 1:17:33,,,,,,,Robots in war,,,,,,,,,,,,,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/UCHEJG2V/robots-in-war-the-next-weapons-of-mass-destruction.html,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,magazineArticle,2018,"Piper, Kelsey",The case for taking AI seriously as a threat to humanity,Vox,,,,https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment,"Why some people fear AI, explained.",2018-12-21,2020-11-21 16:50,2020-12-20 15:54,2020-11-21 16:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/E5RJBYQA/ai-artificial-intelligence-machine-learning-safety-alignment.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is an introduction to the problem of AI safety, from the perspective that it is hard to specify the ""right"" goal, and that goal-driven behavior leads to convergent instrumental subgoals that will likely be dangerous. It also addresses several common initial reactions that people have."
2C8U9GC5,manuscript,2020,"Garcez, Artur d'Avila; Lamb, Luis C.",Neurosymbolic AI: The 3rd Wave,,,,,http://arxiv.org/abs/2012.05876,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.",2020-12-16,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-13 21:56:59,,,,,,,Neurosymbolic AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029  arXiv: 2012.05876,,/Users/jacquesthibodeau/Zotero/storage/4EQI3ZIW/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf; /Users/jacquesthibodeau/Zotero/storage/JISGMD7C/2012.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.4; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XXTW2U4,manuscript,2020,"Srinivasan, Krishnan; Eysenbach, Benjamin; Ha, Sehoon; Tan, Jie; Finn, Chelsea",Learning to be Safe: Deep RL with a Safety Critic,,,,,http://arxiv.org/abs/2010.14603,"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.",2020-10-27,2022-01-30 4:48:46,2022-01-30 4:48:46,2021-11-13 14:06:18,,,,,,,Learning to be Safe,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2010.14603,,/Users/jacquesthibodeau/Zotero/storage/GDXUUMCR/Srinivasan et al. - 2020 - Learning to be Safe Deep RL with a Safety Critic.pdf,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SX36BZ9U,manuscript,2019,"Chandra, Kartik; Meijer, Erik; Andow, Samantha; Arroyo-Fang, Emilio; Dea, Irene; George, Johann; Grueter, Melissa; Hosmer, Basil; Stumpos, Steffi; Tempest, Alanna; Yang, Shannon",Gradient Descent: The Ultimate Optimizer,,,,,http://arxiv.org/abs/1909.13371,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimization, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad infinitum. As these towers of gradient-based optimizers grow, they become significantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.",2019-09-29,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-13 13:43:50,,,,,,,Gradient Descent,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 1909.13371,,/Users/jacquesthibodeau/Zotero/storage/TWKCH5RQ/Chandra et al. - 2019 - Gradient Descent The Ultimate Optimizer.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5PAQDW3C,manuscript,2020,"Shuster, Kurt; Urbanek, Jack; Dinan, Emily; Szlam, Arthur; Weston, Jason",Deploying Lifelong Open-Domain Dialogue Learning,,,,,http://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",2020-08-19,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-07 14:31:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2008.08076,,/Users/jacquesthibodeau/Zotero/storage/8R93AX2Q/Shuster et al. - 2020 - Deploying Lifelong Open-Domain Dialogue Learning.pdf,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8NPI7H29,manuscript,2020,"Lohn, Andrew J.",Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,,,,,http://arxiv.org/abs/2009.00802,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.",2020-09-01,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-07 17:00:41,,,,,,,Estimating the Brittleness of AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2009.00802,,/Users/jacquesthibodeau/Zotero/storage/VU4UZTXJ/Lohn - 2020 - Estimating the Brittleness of AI Safety Integrity.pdf; /Users/jacquesthibodeau/Zotero/storage/MS2FRD92/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Software Engineering; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZADUIUIP,manuscript,2020,"Nakkiran, Preetum; Bansal, Yamini",Distributional Generalization: A New Kind of Generalization,,,,,http://arxiv.org/abs/2009.08092,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",2020-10-14,2022-01-30 4:48:45,2022-01-30 4:48:45,2021-11-14 18:11:08,,,,,,,Distributional Generalization,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2009.08092,,/Users/jacquesthibodeau/Zotero/storage/GUN2A4W9/Nakkiran and Bansal - 2020 - Distributional Generalization A New Kind of Gener.pdf; /Users/jacquesthibodeau/Zotero/storage/RR243E5K/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ5G3WJ,manuscript,2020,"Sharma, Utkarsh; Kaplan, Jared",A Neural Scaling Law from the Dimension of the Data Manifold,,,,,http://arxiv.org/abs/2004.10802,"When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",2020-04-22,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 23:00:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 2004.10802,,/Users/jacquesthibodeau/Zotero/storage/4IPPBR63/Sharma and Kaplan - 2020 - A Neural Scaling Law from the Dimension of the Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/TEW3HG34/2004.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DTDRV7MR,manuscript,2020,"Yuan, Li; Xiao, Will; Kreiman, Gabriel; Tay, Francis E. H.; Feng, Jiashi; Livingstone, Margaret S.",Adversarial images for the primate brain,,,,,http://arxiv.org/abs/2011.05623,"Deep artificial neural networks have been proposed as a model of primate vision. However, these networks are vulnerable to adversarial attacks, whereby introducing minimal noise can fool networks into misclassifying images. Primate vision is thought to be robust to such adversarial images. We evaluated this assumption by designing adversarial images to fool primate vision. To do so, we first trained a model to predict responses of face-selective neurons in macaque inferior temporal cortex. Next, we modified images, such as human faces, to match their model-predicted neuronal responses to a target category, such as monkey faces. These adversarial images elicited neuronal responses similar to the target category. Remarkably, the same images fooled monkeys and humans at the behavioral level. These results challenge fundamental assumptions about the similarity between computer and primate vision and show that a model of neuronal activity can selectively direct primate visual behavior.",2020-11-11,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 22:51:54,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.05623,,/Users/jacquesthibodeau/Zotero/storage/SG5SCXZA/Yuan et al. - 2020 - Adversarial images for the primate brain.pdf; /Users/jacquesthibodeau/Zotero/storage/CVAPB22W/2011.html,,UnsortedSafety,Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3VIZNQR,manuscript,2020,"Klinger, Joel; Mateos-Garcia, Juan; Stathoulopoulos, Konstantinos",A narrowing of AI research?,,,,,http://arxiv.org/abs/2009.10385,"Artificial Intelligence (AI) is being hailed as the latest example of a General Purpose Technology that could transform productivity and help tackle important societal challenges. This outcome is however not guaranteed: a myopic focus on short-term benefits could lock AI into technologies that turn out to be sub-optimal in the longer-run. Recent controversies about the dominance of deep learning methods and private labs in AI research suggest that the field may be getting narrower, but the evidence base is lacking. We seek to address this gap with an analysis of the thematic diversity of AI research in arXiv, a widely used pre-prints site. Having identified 110,000 AI papers in this corpus, we use hierarchical topic modelling to estimate the thematic composition of AI research, and this composition to calculate various metrics of research diversity. Our analysis suggests that diversity in AI research has stagnated in recent years, and that AI research involving private sector organisations tends to be less diverse than research in academia. This appears to be driven by a small number of prolific and narrowly-focused technology companies. Diversity in academia is bolstered by smaller institutions and research groups that may have less incentives to race and lower levels of collaboration with the private sector. We also find that private sector AI researchers tend to specialise in data and computationally intensive deep learning methods at the expense of research involving other (symbolic and statistical) AI methods, and of research that considers the societal and ethical implications of AI or applies it in domains like health. Our results suggest that there may be a rationale for policy action to prevent a premature narrowing of AI research that could reduce its societal benefits, but we note the incentive, information and scale hurdles standing in the way of such interventions.",2020-11-17,2022-01-30 4:48:43,2022-01-30 4:48:43,2021-11-13 21:54:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 2009.10385,,/Users/jacquesthibodeau/Zotero/storage/R7VVT4NZ/Klinger et al. - 2020 - A narrowing of AI research.pdf; /Users/jacquesthibodeau/Zotero/storage/KZJD9H3Z/2009.html,,UnsortedSafety,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32WKIH7F,manuscript,2011,"Tomasik, Brian",Risks of Astronomical Future Suﬀering,,,,,,"It’s far from clear that human values will shape an Earth-based space-colonization wave, but even if they do, it seems more likely that space colonization will increase total suﬀering rather than decrease it. That said, other people care a lot about humanity’s survival and spread into the cosmos, so I think suﬀering reducers should let others pursue their spacefaring dreams in exchange for stronger safety measures against future suﬀering. In general, I encourage people to focus on making an intergalactic future more humane if it happens rather than making sure there will be an intergalactic future.",2011,2022-01-30 4:51:36,2022-01-30 4:51:36,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000018,,/Users/jacquesthibodeau/Zotero/storage/RENG9486/Tomasik - Risks of Astronomical Future Suﬀering.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIW23ZE3,manuscript,2020,"Michaud, Eric J.; Gleave, Adam; Russell, Stuart",Understanding Learned Reward Functions,,,,,http://arxiv.org/abs/2012.05862,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.",2020-12-10,2022-01-30 4:51:11,2022-01-30 4:51:11,2020-12-18 0:37:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2012.05862,,/Users/jacquesthibodeau/Zotero/storage/7RS4HDDE/Michaud et al. - 2020 - Understanding Learned Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/58H3Z5GF/2012.html,,CHAI; TechSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9QCI7FK,manuscript,2020,"Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus",Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,,,,,http://arxiv.org/abs/2004.07213,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",2020-04-20,2022-01-30 4:51:11,2022-01-30 4:51:11,2020-08-18 21:36:21,,,,,,,Toward Trustworthy AI Development,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213,,/Users/jacquesthibodeau/Zotero/storage/7V57AEI3/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/JWN7NA8T/2004.html; /Users/jacquesthibodeau/Zotero/storage/J44EXJ2E/2004.html,,MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GZZWAE9K,manuscript,2017,"Critch, Andrew",Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making,,,,,http://arxiv.org/abs/1701.01302,"Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\""{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.",2017-01-05,2022-01-30 4:51:10,2022-01-30 4:51:10,2018-12-09 18:04:21,,,,,,,Toward negotiable reinforcement learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1701.01302,,/Users/jacquesthibodeau/Zotero/storage/G65SGP7V/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/X9ZHKTCD/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/ARS283R8/1701.html; /Users/jacquesthibodeau/Zotero/storage/XSPJ5PMX/1701.html; /Users/jacquesthibodeau/Zotero/storage/K885DNUJ/1701.html,,CHAI; TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S5KR49MS,manuscript,2017,"Critch, Andrew; Russell, Stuart",Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making,,,,,http://arxiv.org/abs/1711.00363,"It is often argued that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a {\em Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one agent without making sacrifices for another. A famous theorem of Harsanyi shows that, when the principals have a common prior on the outcome distributions of all policies, a Pareto-optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. In this paper, we show that Harsanyi's theorem does not hold for principals with different priors, and derive a more precise generalization which does hold, which constitutes our main result. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. The result has implications for the design of contracts, treaties, joint ventures, and robots.",2017-10-31,2022-01-30 4:51:09,2022-01-30 4:51:09,2018-12-09 18:04:24,,,,,,,Servant of Many Masters,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1711.00363,,/Users/jacquesthibodeau/Zotero/storage/P2ISQ5FB/Critch and Russell - 2017 - Servant of Many Masters Shifting priorities in Pa.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJ7FA6BB/1711.html; /Users/jacquesthibodeau/Zotero/storage/SNVKFDTA/1711.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4W6H9NQQ,manuscript,2017,"Dragan, Anca D.",Robot Planning with Mathematical Models of Human State and Action,,,,,http://arxiv.org/abs/1705.04226,"Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.",2017-05-11,2022-01-30 4:51:08,2022-01-30 4:51:08,2019-07-22 21:49:18,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000027  arXiv: 1705.04226,,,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTQZCG32,manuscript,2020,"Filan, Daniel; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Pruned Neural Networks are Surprisingly Modular,,,,,https://arxiv.org/abs/2003.04881v4,"The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.",2020-03-10,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-12 1:54:56,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/MWESESHU/Filan et al. - 2020 - Pruned Neural Networks are Surprisingly Modular.pdf; /Users/jacquesthibodeau/Zotero/storage/MN3V4CEA/2003.html; /Users/jacquesthibodeau/Zotero/storage/IDPDA9W6/2003.html,,CHAI; TechSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG3RHP5E,manuscript,2021,"Stastny, Julian; Treutlein, Johannes; Riché, Maxime; Clifton, Jesse",Multi-agent learning in mixed-motive coordination problems,,,,,https://longtermrisk.org/files/stastny_et_al_implicit_bargaining.pdf,"Cooperation in settings where agents have diﬀerent but overlapping preferences (mixed-motive settings) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied are simplistic in that they have a single cooperative outcome on which all agents can agree. Multi-agent systems in general may exhibit many payoﬀ proﬁles which might be called cooperative, but which agents have diﬀerent preferences over. This causes problems for independently trained agents that do not arise in the case of that there is a unique cooperative payoﬀ proﬁle. In this note, we illustrate this problem with a class of games called mixed-motive coordination problems (MCPs). We demonstrate the failure of several methods for achieving cooperation in sequential social dilemmas when used to independently train policies in a simple MCP. We discuss some possible directions for ameliorating MCPs.",2021-03-08,2022-01-30 4:51:08,2022-01-30 4:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/K84ENAGC/Stastny et al. - 2021 - Multi-agent learning in mixed-motive coordination .pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D4HJK6A2,manuscript,2017,"Oesterheld, Caspar",Multiverse-wide Cooperation via Correlated Decision Making,,,,,,"Some decision theorists argue that when playing a prisoner’s dilemma-type game against a suﬃciently similar opponent, we should cooperate to make it more likely that our opponent also cooperates. This idea, which Hofstadter calls superrationality, has strong implications when combined with the insight from modern physics that we probably live in a large universe or multiverse of some sort. If we care about what happens in civilizations located elsewhere in the multiverse, we can superrationally cooperate with some of their inhabitants. That is, if we take their values into account, this makes it more likely that they do the same for us. In this paper, I attempt to assess the practical implications of this idea. I argue that to reap the full gains from trade, everyone should maximize the same impartially weighted sum of the utility functions of all collaborators. I also argue that we can obtain at least weak evidence about the content of these utility functions. In practice, the application of superrationality implies that we should promote causal cooperation, moral pluralism, moral reﬂection, and ensure that our descendants, who will be smarter and thus better at ﬁnding out how to beneﬁt other superrationalists in the universe, engage in superrational cooperation.",2017-08-10,2022-01-30 4:51:08,2022-01-30 4:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/WSAM3743/Oesterheld - Multiverse-wide Cooperation via Correlated Decisio.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8M48CF7J,manuscript,2020,"Hutter, Adrian",Learning in two-player games between transparent opponents,,,,,http://arxiv.org/abs/2012.02671,"We consider a scenario in which two reinforcement learning agents repeatedly play a matrix game against each other and update their parameters after each round. The agents' decision-making is transparent to each other, which allows each agent to predict how their opponent will play against them. To prevent an infinite regress of both agents recursively predicting each other indefinitely, each agent is required to give an opponent-independent response with some probability at least epsilon. Transparency also allows each agent to anticipate and shape the other agent's gradient step, i.e. to move to regions of parameter space in which the opponent's gradient points in a direction favourable to them. We study the resulting dynamics experimentally, using two algorithms from previous literature (LOLA and SOS) for opponent-aware learning. We find that the combination of mutually transparent decision-making and opponent-aware learning robustly leads to mutual cooperation in a single-shot prisoner's dilemma. In a game of chicken, in which both agents try to manoeuvre their opponent towards their preferred equilibrium, converging to a mutually beneficial outcome turns out to be much harder, and opponent-aware learning can even lead to worst-case outcomes for both agents. This highlights the need to develop opponent-aware learning algorithms that achieve acceptable outcomes in social dilemmas involving an equilibrium selection problem.",2020-12-04,2022-01-30 4:51:08,2022-01-30 4:51:08,2020-12-12 15:01:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2012.02671,,/Users/jacquesthibodeau/Zotero/storage/EFFXSWJA/Hutter - 2020 - Learning in two-player games between transparent o.pdf; /Users/jacquesthibodeau/Zotero/storage/U9M27NJU/2012.html,,CLR; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4P4VEEUM,manuscript,2016,"Tomasik, Brian",How the Simulation Argument Dampens Future Fanaticism,,,,,,"Some eﬀective altruists assume that most of the expected impact of our actions comes from how we inﬂuence the very long-term future of Earthoriginating intelligence over the coming ∼billions of years. According to this view, helping humans and animals in the short term matters, but it mainly only matters via eﬀects on far-future outcomes.",2016,2022-01-30 4:51:08,2022-01-30 4:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CEX9FT56/Tomasik - How the Simulation Argument Dampens Future Fanatic.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7IV657SA,manuscript,2019,"Plonsky, Ori; Apel, Reut; Ert, Eyal; Tennenholtz, Moshe; Bourgin, David; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.; Carter, Evan C.; Cavanagh, James F.; Erev, Ido",Predicting human decisions with behavioral theories and machine learning,,,,,http://arxiv.org/abs/1904.06866,"Behavioral decision theories aim to explain human behavior. Can they help predict it? An open tournament for prediction of human choices in fundamental economic decision tasks is presented. The results suggest that integration of certain behavioral theories as features in machine learning systems provides the best predictions. Surprisingly, the most useful theories for prediction build on basic properties of human and animal learning and are very different from mainstream decision theories that focus on deviations from rational choice. Moreover, we find that theoretical features should be based not only on qualitative behavioral insights (e.g. loss aversion), but also on quantitative behavioral foresights generated by functional descriptive models (e.g. Prospect Theory). Our analysis prescribes a recipe for derivation of explainable, useful predictions of human decisions.",2019-04-15,2022-01-30 4:51:07,2022-01-30 4:51:07,2019-12-18 2:16:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 34  J: 10 arXiv: 1904.06866,,/Users/jacquesthibodeau/Zotero/storage/ZJE7ABQH/Plonsky et al. - 2019 - Predicting human decisions with behavioral theorie.pdf; /Users/jacquesthibodeau/Zotero/storage/BFKM6MGZ/1904.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NXDBDRKX,manuscript,2020,"Zhan, Albert; Tiomkin, Stas; Abbeel, Pieter",Preventing Imitation Learning with Adversarial Policy Ensembles,,,,,http://arxiv.org/abs/2002.01059,"Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of ""non-clonable"" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.",2020-08-02,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-21 18:40:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2002.01059,,/Users/jacquesthibodeau/Zotero/storage/EUFJR46P/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/THDFQB7N/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/6WBXIJZN/2002.html; /Users/jacquesthibodeau/Zotero/storage/RR6ER6D9/2002.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD2HXECW,manuscript,2016,"Oesterheld, Caspar",Backup utility functions as a fail-safe AI technique,,,,,https://longtermrisk.org/files/backup-utility-functions.pdf,"Many experts believe that AIs will, within the not-too-distant future, become powerful enough for their decisions to have tremendous impact. Unfortunately, setting up AI goal systems in a way that results in benevolent behavior is expected to be diﬃcult, and we cannot be certain to get it completely right on the ﬁrst attempt. We should therefore account for the possibility that the goal systems fail to implement our values the intended way. In this paper, we propose the idea of backup utility functions: Secondary utility functions that are used in case the primary ones “fail”. We also describe how this approach can be generalized to the use of multi-layered utility functions, some of which can fail without aﬀecting the ﬁnal outcome as badly as without the backup mechanism.",2016-10,2022-01-30 4:51:06,2022-01-30 4:51:06,2020-12-18,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/FWH7VQ85/Oesterheld - Backup utility functions as a fail-safe AI techniq.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AMXU6SC,manuscript,2020,"Andrychowicz, Marcin; Raichuk, Anton; Stańczyk, Piotr; Orsini, Manu; Girgin, Sertan; Marinier, Raphael; Hussenot, Léonard; Geist, Matthieu; Pietquin, Olivier; Michalski, Marcin; Gelly, Sylvain; Bachem, Olivier",What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,,,,,http://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",2020-06-10,2022-01-30 4:48:55,2022-01-30 4:48:55,2021-11-07 22:59:39,,,,,,,What Matters In On-Policy Reinforcement Learning?,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 44  arXiv: 2006.05990,,/Users/jacquesthibodeau/Zotero/storage/PMZ4SVSJ/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FN5EFZJ8,manuscript,2020,"D'Amour, Alexander; Heller, Katherine; Moldovan, Dan; Adlam, Ben; Alipanahi, Babak; Beutel, Alex; Chen, Christina; Deaton, Jonathan; Eisenstein, Jacob; Hoffman, Matthew D.; Hormozdiari, Farhad; Houlsby, Neil; Hou, Shaobo; Jerfel, Ghassen; Karthikesalingam, Alan; Lucic, Mario; Ma, Yian; McLean, Cory; Mincu, Diana; Mitani, Akinori; Montanari, Andrea; Nado, Zachary; Natarajan, Vivek; Nielson, Christopher; Osborne, Thomas F.; Raman, Rajiv; Ramasamy, Kim; Sayres, Rory; Schrouff, Jessica; Seneviratne, Martin; Sequeira, Shannon; Suresh, Harini; Veitch, Victor; Vladymyrov, Max; Wang, Xuezhi; Webster, Kellie; Yadlowsky, Steve; Yun, Taedong; Zhai, Xiaohua; Sculley, D.",Underspecification Presents Challenges for Credibility in Modern Machine Learning,,,,,http://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020-11-24,2022-01-30 4:48:54,2022-01-30 4:48:54,2021-11-13 19:45:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 163  arXiv: 2011.03395,,/Users/jacquesthibodeau/Zotero/storage/X3D7X8IC/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf; /Users/jacquesthibodeau/Zotero/storage/VP4HBEMV/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSEVCJIG,manuscript,2019,"Gruetzemacher, Ross; Whittlestone, Jess",The Transformative Potential of Artificial Intelligence,,,,,http://arxiv.org/abs/1912.00747,"Recently the concept of transformative AI (TAI) has begun to receive attention in the AI policy space. TAI is often framed as an alternative formulation to notions of strong AI (e.g. artificial general intelligence or superintelligence) and reflects increasing consensus that advanced AI which does not fit these definitions may nonetheless have extreme and long-lasting impacts on society. However, the term TAI is poorly defined and often used ambiguously. Some use the notion of TAI to describe levels of societal transformation associated with previous 'general purpose technologies' (GPTs) such as electricity or the internal combustion engine. Others use the term to refer to more drastic levels of transformation comparable to the agricultural or industrial revolutions. The notion has also been used much more loosely, with some implying that current AI systems are already having a transformative impact on society. This paper unpacks and analyses the notion of TAI, proposing a distinction between narrowly transformative AI (NTAI), TAI and radically transformative AI (RTAI), roughly corresponding to associated levels of societal change. We describe some relevant dimensions associated with each and discuss what kinds of advances in capabilities they might require. We further consider the relationship between TAI and RTAI and whether we should necessarily expect a period of TAI to precede the emergence of RTAI. This analysis is important as it can help guide discussions among AI policy researchers about how to allocate resources towards mitigating the most extreme impacts of AI and it can bring attention to negative TAI scenarios that are currently neglected.",2019,2022-01-30 4:50:26,2022-01-30 4:50:26,2020-11-14 0:55:29,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 6  arXiv: 1912.00747,,/Users/jacquesthibodeau/Zotero/storage/SD4BQGGF/Gruetzemacher and Whittlestone - 2020 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/F5KGIKR2/1912.html,,MetaSafety; CFI; CSER; AmbiguosSafety; BERI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7CSE8NXS,manuscript,2019,"Ovadya, Aviv; Whittlestone, Jess",Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning,,,,,http://arxiv.org/abs/1907.11274,"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.",2019-07-28,2022-01-30 4:50:25,2022-01-30 4:50:25,2019-12-16 22:39:38,,,,,,,Reducing malicious use of synthetic media research,,,,,,,,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 1907.11274,,/Users/jacquesthibodeau/Zotero/storage/9JS2GGPU/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/DX2WMWTT/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/R2ETWS25/1907.html; /Users/jacquesthibodeau/Zotero/storage/ADSVSN46/1907.html,,MetaSafety; CFI; CSER; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAFKPQNH,manuscript,2018,"Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José",Accounting for the neglected dimensions of AI progress,,,,,https://arxiv.org/abs/1806.00610,,2018,2022-01-30 4:50:23,2022-01-30 4:50:23,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 19,,/Users/jacquesthibodeau/Zotero/storage/RDSFDZK3/Martínez-Plumed et al. - 2018 - Accounting for the neglected dimensions of ai prog.pdf; /Users/jacquesthibodeau/Zotero/storage/XMI73HC7/1806.html,,MetaSafety; CFI; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3ND6DSMU,manuscript,2018,"Manheim, David",Oversight of Unsafe Systems via Dynamic Safety Envelopes,,,,,https://arxiv.org/abs/1811.09246v1,"This paper reviews the reasons that Human-in-the-Loop is both critical for preventing widely-understood failure modes for machine learning, and not a practical solution. Following this, we review two current heuristic methods for addressing this. The first is provable safety envelopes, which are possible only when the dynamics of the system are fully known, but can be useful safety guarantees when optimal behavior is based on machine learning with poorly-understood safety characteristics. The second is the simpler circuit breaker model, which can forestall or prevent catastrophic outcomes by stopping the system, without any specific model of the system. This paper proposes using heuristic, dynamic safety envelopes, which are a plausible halfway point between these approaches that allows human oversight without some of the more difficult problems faced by Human-in-the-Loop systems. Finally, the paper concludes with how this approach can be used for governance of systems where otherwise unsafe systems are deployed.",2018-11-22,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-12 2:12:35,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/GR2JUKSK/Manheim - 2018 - Oversight of Unsafe Systems via Dynamic Safety Env.pdf; /Users/jacquesthibodeau/Zotero/storage/UQH5ZDBA/1811.html,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BBXGS4Q8,manuscript,2019,"Maltinsky, Baeo; Gallagher, Jack; Taylor, Jessica",Feasibility of Training an AGI using Deep RL: A Very Rough Estimate,,,,,"http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf",,2019,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-21,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/32AHTZBN/Maltinsky et al. - Feasibility of Training an AGI using Deep RL A Ve.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6GX3WIP,manuscript,,"McKenzie, Colleen; Hidysmith, J Bryce",AI Insights Dataset Analysis,,,,,http://mediangroup.org/docs/insights-analysis.pdf,,unknown,2022-01-30 4:50:07,2022-01-30 4:50:07,2020-12-21,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/AHG7R97Z/McKenzie and Hidysmith - AI Insights Dataset Analysis.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KKT6G9GE,manuscript,,"Rade, Luca",A Framework for the Safety of Agent-Environment Systems,,,,,,"Ensuring the safety of an autonomous artiﬁcial agent in a complex environment represents a formidable problem across multiple domains. However, there is little interaction between these domains, and no unifying theoretical framework forcing the delineation of assumptions. I propose such a framework in terms of agent-environment systems, in which an agent and environment co-evolve according to a modiﬁed statespace nonlinear system. The agent gathers limited information from the environment and itself to perform an action, operating implicitly on the basis of a coarse-grained model of the systems dynamics. To ensure the systems safety, it is minimally necessary ﬁrst to translate the set of undesirable states from human terms into an adequately precise deﬁnition within the system; then to identify a set of universal markers necessary to the system being in a pre-state to an undesirable state, which transfer with ﬁdelity from the systems state to the agents information; and ﬁnally to have a set of actions by the agent for each pre-state which keep the system out of the set of undesirable states, with the exception of agent-independent dynamics. Incomplete information, information distortion, and coarse-grained models make this a particularly difﬁcult challenge. I conclude by proposing three threads of a research agenda: reducing the possibility space of safe agents by demonstrating the failure of certain methods and identifying problems with particular agent-environment system classes; developing and verifying techniques which address those problems; and matching real systems to agentenvironment systems.",unknown,2022-01-30 4:50:06,2022-01-30 4:50:06,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/Q5GH7SZQ/Mapping existing AI safety.pdf; /Users/jacquesthibodeau/Zotero/storage/CHU7DKNZ/Rade - A Framework for the Safety of Agent-Environment Sy.pdf,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8VGP6QH,manuscript,2019,"Hidysmith, J Bryce",A Descending Veil of Maya,,,,,,,2019,2022-01-30 4:50:06,2022-01-30 4:50:06,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/EWG32KDF/Hidysmith - A Descending Veil of Maya.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WI8THJZV,manuscript,2020,"Tětek, Jakub; Sklenka, Marek; Gavenčiak, Tomáš",Performance of Bounded-Rational Agents With the Ability to Self-Modify,,,,,http://arxiv.org/abs/2011.06275,"Self-modification of agents embedded in complex environments is hard to avoid, whether it happens via direct means (e.g. own code modification) or indirectly (e.g. influencing the operator, exploiting bugs or the environment). While it has been argued that intelligent agents have an incentive to avoid modifying their utility function so that their future instances will work towards the same goals, it is not clear whether this also applies in non-dualistic scenarios, where the agent is embedded in the environment. The problem of self-modification safety is raised by Bostrom in Superintelligence (2014) in the context of safe AGI deployment. In contrast to Everitt et al. (2016), who formally show that providing an option to self-modify is harmless for perfectly rational agents, we show that for agents with bounded rationality, self-modification may cause exponential deterioration in performance and gradual misalignment of a previously aligned agent. We investigate how the size of this effect depends on the type and magnitude of imperfections in the agent's rationality (1-4 below). We also discuss model assumptions and the wider problem and framing space. Specifically, we introduce several types of a bounded-rational agent, which either (1) doesn't always choose the optimal action, (2) is not perfectly aligned with human values, (3) has an innacurate model of the environment, or (4) uses the wrong temporal discounting factor. We show that while in the cases (2)-(4) the misalignment caused by the agent's imperfection does not worsen over time, with (1) the misalignment may grow exponentially.",2020-11-12,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-11-21 18:15:00,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.06275,,/Users/jacquesthibodeau/Zotero/storage/ZG9PUMJQ/Tětek et al. - 2020 - Performance of Bounded-Rational Agents With the Ab.pdf,,TechSafety; AI-Safety-Camp; AISRP2019,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H8VNSUZZ,manuscript,2020,"Gruetzemacher, Ross; Dorner, Florian; Bernaola-Alvarez, Niko; Giattino, Charlie; Manheim, David",Forecasting AI Progress: A Research Agenda,,,,,http://arxiv.org/abs/2008.01848,"Forecasting AI progress is essential to reducing uncertainty in order to appropriately plan for research efforts on AI safety and AI governance. While this is generally considered to be an important topic, little work has been conducted on it and there is no published document that gives and objective overview of the field. Moreover, the field is very diverse and there is no published consensus regarding its direction. This paper describes the development of a research agenda for forecasting AI progress which utilized the Delphi technique to elicit and aggregate experts' opinions on what questions and methods to prioritize. The results of the Delphi are presented; the remainder of the paper follow the structure of these results, briefly reviewing relevant literature and suggesting future work for each topic. Experts indicated that a wide variety of methods should be considered for forecasting AI progress. Moreover, experts identified salient questions that were both general and completely unique to the problem of forecasting AI progress. Some of the highest priority topics include the validation of (partially unresolved) forecasts, how to make forecasting action-guiding and the quality of different performance metrics. While statistical methods seem more promising, there is also recognition that supplementing judgmental techniques can be quite beneficial.",2020-08-04,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-08-24 20:25:42,,,,,,,Forecasting AI Progress,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000[s0]  arXiv: 2008.01848,,/Users/jacquesthibodeau/Zotero/storage/C7CAJQ97/Gruetzemacher et al. - 2020 - Forecasting AI Progress A Research Agenda.pdf; /Users/jacquesthibodeau/Zotero/storage/5B7HP3TB/2008.html,,MetaSafety; AI-Safety-Camp; AISRP2019,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U2DT5V9B,manuscript,2020,"Kovarik, Vojta",AI Services: Introduction v1.3,,,,,https://docs.google.com/document/d/1SYgvWBe1ruDl9dQnxmjll-8COUHPycGOlLvTI68xtLA/edit?pli=1&usp=embed_facebook,"This document aims to serve as an introduction for researchers who want to study the long-term impact of AI through the lens of AI services. It introduces basic concepts related to these systems and gives initial observations to enhance their initial study. It points to several relevant research fields that could be leveraged to study AI services, mentions a number of problems that seem specific to this setting, and makes suggestions for future work.",2020-03-31,2022-01-30 4:49:30,2022-01-30 4:49:30,2020-08-14 19:44:36,,,,,,,AI Services,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/QFJBG8U2/edit.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SCB4T6S5,manuscript,2018,"Armstrong, Stuart; O'Rorke, Xavier",Good and safe uses of AI Oracles,,,,,http://arxiv.org/abs/1711.05541,"It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.",2018-06-05,2022-01-30 4:53:17,2022-01-30 4:53:17,2020-11-22 4:11:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 20  arXiv: 1711.05541,,/Users/jacquesthibodeau/Zotero/storage/J86CWCCN/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/U7F3VQ4J/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/JX48I6WQ/1711.html; /Users/jacquesthibodeau/Zotero/storage/RDZSWUWX/1711.html,,TechSafety; FHI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59AKR2E2,manuscript,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,,,,,http://arxiv.org/abs/2102.08686,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-01-30 4:53:10,2022-01-30 4:53:10,2021-10-31 19:13:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2102.08686,,/Users/jacquesthibodeau/Zotero/storage/BDP2UPND/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/8T8JCEB2/2102.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DI833FP9,manuscript,2016,"Leike, Jan",Exploration Potential,,,,,https://arxiv.org/abs/1609.04994v3,"We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.",2016-09-16,2022-01-30 4:53:10,2022-01-30 4:53:10,2019-12-19 1:45:54,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/Q33MXUNJ/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IK85485C/1609.html; /Users/jacquesthibodeau/Zotero/storage/V7QHF9K7/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IM6V7HDQ/1609.html,,TechSafety; FHI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ST9XJA8B,manuscript,2018,"Armstrong, Stuart","Counterfactual equivalence for POMDPs, and underlying deterministic environments",,,,,https://arxiv.org/abs/1801.03737,,2018,2022-01-30 4:53:09,2022-01-30 4:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000001,,"/Users/jacquesthibodeau/Zotero/storage/GJBHW4QW/Armstrong - 2018 - Counterfactual equivalence for POMDPs, and underly.pdf; /Users/jacquesthibodeau/Zotero/storage/5D5PX64R/1801.html",,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NIZKQPB2,manuscript,2018,"Sandberg, Anders; Drexler, Eric; Ord, Toby",Dissolving the Fermi Paradox,,,,,https://arxiv.org/abs/1806.02404,,2018,2022-01-30 4:53:09,2022-01-30 4:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000034,,/Users/jacquesthibodeau/Zotero/storage/QXFHXM8D/Sandberg et al. - 2018 - Dissolving the Fermi Paradox.pdf; /Users/jacquesthibodeau/Zotero/storage/7WUZBHUW/1806.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W3EIIGEB,manuscript,2020,"O’Keefe, Cullen",Antitrust-Compliant AI Industry Self-Regulation,,,,,https://cullenokeefe.com/blog/antitrust-compliant-ai-industry-self-regulation,"The touchstone of antitrust compliance is competition. To be legally permissible, any industrial restraint on trade must have sufficient countervailing procompetitive justifications. Usually, anticompetitive horizontal agreements like boycotts (including a refusal to produce certain products) are per se illegal.",2020-07-07,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-08-28,,15,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 1,,/Users/jacquesthibodeau/Zotero/storage/ZBGCJCWU/O’Keefe - Antitrust-Compliant AI Industry Self-Regulation.pdf,,MetaSafety; FHI; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMQEGVQE,manuscript,2018,"Schulze, Sebastian; Evans, Owain",Active reinforcement learning with monte-carlo tree search,,,,,https://arxiv.org/abs/1803.04926,,2018,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/VRI3QKNQ/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf; /Users/jacquesthibodeau/Zotero/storage/9BXHK4PU/1803.html; /Users/jacquesthibodeau/Zotero/storage/J9AC5ZVX/Schulze and Evans - 2018 - Active reinforcement learning with monte-carlo tre.pdf; /Users/jacquesthibodeau/Zotero/storage/H347X5TR/1803.html,,TechSafety; FHI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IRHXH7N,manuscript,2019,"Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,,,,,http://arxiv.org/abs/1902.09980,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",2019-09-06,2022-01-30 4:52:49,2022-01-30 4:52:49,2019-12-16 20:27:00,,,,,,,Understanding Agent Incentives using Causal Influence Diagrams. Part I,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1902.09980,,/Users/jacquesthibodeau/Zotero/storage/FJWTC444/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/KKZ6SCGC/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/AX23IEDU/1902.html; /Users/jacquesthibodeau/Zotero/storage/H934KB2V/1902.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27BEKE99,manuscript,2018,"Dvijotham, Krishnamurthy; Gowal, Sven; Stanforth, Robert; Arandjelovic, Relja; O'Donoghue, Brendan; Uesato, Jonathan; Kohli, Pushmeet",Training verified learners with learned verifiers,,,,,http://arxiv.org/abs/1805.10265,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.",2018-05-29,2022-01-30 4:52:49,2022-01-30 4:52:49,2019-12-16 20:32:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000115  arXiv: 1805.10265,,/Users/jacquesthibodeau/Zotero/storage/7QDXM7DA/Dvijotham et al. - 2018 - Training verified learners with learned verifiers.pdf; /Users/jacquesthibodeau/Zotero/storage/V79K38GB/1805.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNV94I7V,manuscript,2018,"Martic, Miljan; Leike, Jan; Trask, Andrew; Hessel, Matteo; Legg, Shane; Kohli, Pushmeet",Scaling shared model governance via model splitting,,,,,http://arxiv.org/abs/1812.05979,"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.",2018-12-14,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:33:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1812.05979,,/Users/jacquesthibodeau/Zotero/storage/TXQ2A3M4/Martic et al. - 2018 - Scaling shared model governance via model splittin.pdf; /Users/jacquesthibodeau/Zotero/storage/9R7NEPEF/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBPVTXNN,manuscript,2018,"Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane",Scalable agent alignment via reward modeling: a research direction,,,,,http://arxiv.org/abs/1811.07871,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",2018-11-19,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:33:45,,,,,,,Scalable agent alignment via reward modeling,,,,,,,,,,,,arXiv.org,,ZSCC: 0000083  arXiv: 1811.07871,,/Users/jacquesthibodeau/Zotero/storage/USVJ3JSI/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/5M854I6Q/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/SSZXRUAI/1811.html; /Users/jacquesthibodeau/Zotero/storage/Z3J54XGK/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XKZTJI95,manuscript,2018,"Uesato, Jonathan; Kumar, Ananya; Szepesvari, Csaba; Erez, Tom; Ruderman, Avraham; Anderson, Keith; Dvijotham, Krishmamurthy; Heess, Nicolas; Kohli, Pushmeet",Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,,,,,http://arxiv.org/abs/1812.01647,"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",2018-12-04,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:26:47,,,,,,,Rigorous Agent Evaluation,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 46  J: 16 arXiv: 1812.01647,,/Users/jacquesthibodeau/Zotero/storage/V96F8BPW/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/XQFBG5UT/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/ZSCSD84M/1812.html; /Users/jacquesthibodeau/Zotero/storage/RVGFMRR9/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FXQMIVSU,manuscript,2020,"Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane",The Incentives that Shape Behaviour,,,,,http://arxiv.org/abs/2001.07118,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",2020-01-20,2022-01-30 4:52:48,2022-01-30 4:52:48,2020-08-18 21:24:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2001.07118,,/Users/jacquesthibodeau/Zotero/storage/JBVM9X3R/Carey et al. - 2020 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/VE49AUW5/2001.html,,TechSafety; FHI; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7MAZDCTT,manuscript,2018,"Dalal, Gal; Dvijotham, Krishnamurthy; Vecerik, Matej; Hester, Todd; Paduraru, Cosmin; Tassa, Yuval",Safe Exploration in Continuous Action Spaces,,,,,http://arxiv.org/abs/1801.08757,"We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.",2018-01-26,2022-01-30 4:52:48,2022-01-30 4:52:48,2019-12-16 20:35:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000173  arXiv: 1801.08757,,/Users/jacquesthibodeau/Zotero/storage/HM9VIAT3/Dalal et al. - 2018 - Safe Exploration in Continuous Action Spaces.pdf; /Users/jacquesthibodeau/Zotero/storage/3FWRT7KM/1801.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U8Z3P3R3,manuscript,2020,"Kumar, Ramana; Uesato, Jonathan; Ngo, Richard; Everitt, Tom; Krakovna, Victoria; Legg, Shane",REALab: An Embedded Perspective on Tampering,,,,,http://arxiv.org/abs/2011.08820,"This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",2020-11-17,2022-01-30 4:52:47,2022-01-30 4:52:47,2020-12-12 15:36:25,,,,,,,REALab,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2011.08820,,/Users/jacquesthibodeau/Zotero/storage/5549P5DE/Kumar et al. - 2020 - REALab An Embedded Perspective on Tampering.pdf; /Users/jacquesthibodeau/Zotero/storage/2SPGRMZ3/2011.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WTI4ZW58,manuscript,2018,"Perolat, Julien; Malinowski, Mateusz; Piot, Bilal; Pietquin, Olivier",Playing the Game of Universal Adversarial Perturbations,,,,,http://arxiv.org/abs/1809.07802,"We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.",2018-09-25,2022-01-30 4:52:47,2022-01-30 4:52:47,2019-12-16 20:36:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 1809.07802,,/Users/jacquesthibodeau/Zotero/storage/9Z9532E6/Perolat et al. - 2018 - Playing the Game of Universal Adversarial Perturba.pdf; /Users/jacquesthibodeau/Zotero/storage/3DZF8UJT/1809.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GD4JV3IM,manuscript,2016,"Dewey, Daniel; Russell, Stuart J; Tegmark, Max",A survey of research questions for robust and beneficial AI,,,,,https://futureoflife.org/data/documents/research_survey.pdf?x96845,,2016-01-25,2022-01-30 4:53:38,2022-01-30 4:53:38,2020-11-21 17:06:15,,,,,,,,,,,,,Future of Life Institute,,,,,,,,ZSCC: 0000002[s0],,/Users/jacquesthibodeau/Zotero/storage/ZZC972RC/research_survey.pdf,,TechSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4VTKAFVS,manuscript,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,,,,,http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-01-30 4:53:37,2022-01-30 4:53:37,2021-11-18 23:51:54,,,,,,,Truthful AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2110.06674,,/Users/jacquesthibodeau/Zotero/storage/N89FGRG3/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/RGF8PQC9/2110.html,,TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BHNFUZ5M,manuscript,2017,"Sandberg, Anders; Armstrong, Stuart; Cirkovic, Milan M.",That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox,,,,,https://arxiv.org/abs/1705.03394v1,"If a civilization wants to maximize computation it appears rational to aestivate until the far future in order to exploit the low temperature environment: this can produce a $10^{30}$ multiplier of achievable computation. We hence suggest the ""aestivation hypothesis"": the reason we are not observing manifestations of alien civilizations is that they are currently (mostly) inactive, patiently waiting for future cosmic eras. This paper analyzes the assumptions going into the hypothesis and how physical law and observational evidence constrain the motivations of aliens compatible with the hypothesis.",2017-04-27,2022-01-30 4:53:36,2022-01-30 4:53:36,2019-12-19 1:36:54,,,,,,,That is not dead which can eternal lie,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/HIATFPH7/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/RUSE883X/1705.html; /Users/jacquesthibodeau/Zotero/storage/97694V8V/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/2WWEMZ7N/1705.html,,MetaSafety; FHI,Physics - Popular Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FM6JR8HH,manuscript,2019,"Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott",Risks from Learned Optimization in Advanced Machine Learning Systems,,,,,http://arxiv.org/abs/1906.01820,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",2019-06-11,2022-01-30 4:53:35,2022-01-30 4:53:35,2019-12-16 2:27:32,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 1906.01820,,/Users/jacquesthibodeau/Zotero/storage/MURNKGU7/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/CFIC3DIX/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/GCV386SM/1906.html,,TechSafety; FHI; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N84Z6VKW,manuscript,2017,"Garfinkel, Ben; Brundage, Miles; Filan, Daniel; Flynn, Carrick; Luketina, Jelena; Page, Michael; Sandberg, Anders; Snyder-Beattie, Andrew; Tegmark, Max",On the Impossibility of Supersized Machines,,,,,https://arxiv.org/abs/1703.10987,,2017,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/3F5AE7EV/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/EIAR3DMW/1703.html; /Users/jacquesthibodeau/Zotero/storage/2QFF7UF5/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/H9N3TUKD/1703.html; /Users/jacquesthibodeau/Zotero/storage/I9J3PC5N/1703.html,,TechSafety; FHI,Computer Science - Computers and Society; Physics - Popular Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SDJWMEQZ,manuscript,2015,"Armstrong, Stuart",Oﬀ-policy Monte Carlo agents with variable behaviour policies,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/monte_carlo_arXiv.pdf,"This paper looks at the convergence property of oﬀ-policy Monte Carlo agents with variable behaviour policies. It presents results about convergence and lack of convergence. Even if the agent generates every possible episode history inﬁnitely often, the algorithm can fail to converge on the correct Q-values. On the other hand, it can converge on the correct Q-values under certain conditions. For instance, if, during the n-th episode, the agent has an independent probability of 1/ log(n) of following the original policy at any given state, then it will converge on the right Q-values for that policy.",2015,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000  J: 0,,/Users/jacquesthibodeau/Zotero/storage/SDACGAB8/Armstrong - Oﬀ-policy Monte Carlo agents with variable behavio.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIRXPN9E,manuscript,2017,"Armstrong, Stuart; Levinstein, Benjamin",Low impact artificial intelligences,,,,,https://arxiv.org/abs/1705.10720,,2017,2022-01-30 4:53:18,2022-01-30 4:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/VT7URXJ6/Armstrong and Levinstein - 2017 - Low impact artificial intelligences.pdf; /Users/jacquesthibodeau/Zotero/storage/6VWVU36V/1705.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F3ICPAQP,manuscript,2005,"Tegmark, Max; Bostrom, Nick",How unlikely is a doomsday catastrophe?,,,,,https://arxiv.org/abs/astro-ph/0512204v2,"Numerous Earth-destroying doomsday scenarios have recently been analyzed, including breakdown of a metastable vacuum state and planetary destruction triggered by a ""strangelet'' or microscopic black hole. We point out that many previous bounds on their frequency give a false sense of security: one cannot infer that such events are rare from the the fact that Earth has survived for so long, because observers are by definition in places lucky enough to have avoided destruction. We derive a new upper bound of one per 10^9 years (99.9% c.l.) on the exogenous terminal catastrophe rate that is free of such selection bias, using planetary age distributions and the relatively late formation time of Earth.",2005-12-08,2022-01-30 4:53:18,2022-01-30 4:53:18,2019-12-19 1:44:09,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/92I96SAN/Tegmark and Bostrom - 2005 - How unlikely is a doomsday catastrophe.pdf; /Users/jacquesthibodeau/Zotero/storage/69V52QGG/0512204.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZM4AF64,manuscript,2017,"Armstrong, Stuart; O'Rourke, Xavier",Indifference' methods for managing agent rewards,,,,,https://arxiv.org/abs/1712.06365,,2017,2022-01-30 4:53:18,2022-01-30 4:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000012,,/Users/jacquesthibodeau/Zotero/storage/KX5XHD8P/Armstrong and O'Rourke - 2018 - 'Indifference' methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/K98E6NZA/1712.html; /Users/jacquesthibodeau/Zotero/storage/GVC3FDWX/Armstrong and O'Rourke - 2017 - 'Indifference'methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/3R2JSKSJ/1712.html,,TechSafety; FHI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S55X7DHC,manuscript,2016,"Critch, Andrew",Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents,,,,,http://arxiv.org/abs/1602.04184,"Löb's theorem and Gödel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.",2016-08-24,2022-01-30 4:50:56,2022-01-30 4:50:56,2019-12-16 2:30:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005[s0]  arXiv: 1602.04184,,/Users/jacquesthibodeau/Zotero/storage/W9BVMS95/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/UVHR7XHR/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/XWNIBX84/1602.html; /Users/jacquesthibodeau/Zotero/storage/43T9ZUX6/1602.html; /Users/jacquesthibodeau/Zotero/storage/ZXV2JJQK/1602.html,,CHAI; TechSafety; MIRI,Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AD9HNBFJ,manuscript,2017,"Hadﬁeld, Gillian K; Hadﬁeld-Menell, Dylan",Pervasive Spurious Normativity,,,,,,This paper proposes a mathematical model for a simpliﬁed version of the game deﬁned in Hadﬁeld and Weingast [2012] which proposes that legal order can be described as an equilibrium in thirdparty decentralized enforcement coordinated by a centralized classiﬁcation institution. We explore the attractiveness of joining a new group (which is assumed to have settled on an enforcement equilibrium already) where groups differ in terms of the frequency of interactions in which norm violation is possible (normative interactions) and thus punishment is called for. We show that groups in which normative interactions are frequent but involve relatively unimportant rules may achieve higher value for participants.,2017,2022-01-30 4:50:56,2022-01-30 4:50:56,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/JAS5344V/Hadﬁeld and Hadﬁeld-Menell - Pervasive Spurious Normativity.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46HGF7X4,manuscript,2020,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Tadepalli, Prasad",Optimal Farsighted Agents Tend to Seek Power,,,,,http://arxiv.org/abs/1912.01683,"Some researchers have speculated that capable reinforcement learning (RL) agents pursuing misspecified objectives are often incentivized to seek resources and power in pursuit of those objectives. An agent seeking power is incentivized to behave in undesirable ways, including rationally preventing deactivation and correction. Others have voiced skepticism: humans seem idiosyncratic in their urges to power, which need not be present in the agents we design. We formalize a notion of power within the context of finite Markov decision processes (MDPs). With respect to a neutral class of reward function distributions, our results suggest that farsighted optimal policies tend to seek power over the environment.",2020-06-05,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-11-21 17:26:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/P5BE9NS6/Turner et al. - 2020 - Optimal Farsighted Agents Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/4HCESMWM/1912.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2T8FD8D,manuscript,2020,"Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha",Multi-agent Social Reinforcement Learning Improves Generalization,,,,,http://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can use social learning to improve their performance using cues from other agents. We find that in most circumstances, vanilla model-free RL agents do not use social learning, even in environments in which individual exploration is expensive. We analyze the reasons for this deficiency, and show that by introducing a model-based auxiliary loss we are able to train agents to lever-age cues from experts to solve hard exploration tasks. The generalized social learning policy learned by these agents allows them to not only outperform the experts with which they trained, but also achieve better zero-shot transfer performance than solo learners when deployed to novel environments with experts. In contrast, agents that have not learned to rely on social learning generalize poorly and do not succeed in the transfer task. Further,we find that by mixing multi-agent and solo training, we can obtain agents that use social learning to out-perform agents trained alone, even when experts are not avail-able. This demonstrates that social learning has helped improve agents' representation of the task itself. Our results indicate that social learning can enable RL agents to not only improve performance on the task at hand, but improve generalization to novel environments.",2020-10-01,2022-01-30 4:50:55,2022-01-30 4:50:55,2020-11-14 0:37:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2010.00581,,/Users/jacquesthibodeau/Zotero/storage/RX2T5UXT/Ndousse et al. - 2020 - Multi-agent Social Reinforcement Learning Improves.pdf; /Users/jacquesthibodeau/Zotero/storage/UJZ4W3NC/2010.html,,CHAI; TechSafety; Open-AI; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7EWBZBCW,manuscript,2018,"Tucker, Aaron; Gleave, Adam; Russell, Stuart",Inverse reinforcement learning for video games,,,,,http://arxiv.org/abs/1810.10593,"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efﬁciency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",2018-10-24,2022-01-30 4:50:53,2022-01-30 4:50:53,2019-12-18 2:19:50,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1810.10593,,/Users/jacquesthibodeau/Zotero/storage/WPQAXM4I/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf; /Users/jacquesthibodeau/Zotero/storage/65P44U7Q/1810.html; /Users/jacquesthibodeau/Zotero/storage/BZ6IQPH2/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIUG993E,manuscript,2021,"Rashidinejad, Paria; Zhu, Banghua; Ma, Cong; Jiao, Jiantao; Russell, Stuart",Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism,,,,,http://arxiv.org/abs/2103.12021,"Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.",2021-03-22,2022-01-30 4:50:43,2022-01-30 4:50:43,2021-10-30 21:30:30,,,,,,,Bridging Offline Reinforcement Learning and Imitation Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2103.12021,,/Users/jacquesthibodeau/Zotero/storage/AUSU3VZ6/Rashidinejad et al. - 2021 - Bridging Offline Reinforcement Learning and Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/U5A563JV/2103.html,,TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Statistics Theory; Mathematics - Optimization and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CJ245SU4,manuscript,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,,,,,http://arxiv.org/abs/2103.03386,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-01-30 4:50:43,2022-01-30 4:50:43,2021-10-30 22:44:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2103.03386,,/Users/jacquesthibodeau/Zotero/storage/2PQM9XHE/Filan et al. - 2021 - Clusterability in Neural Networks.pdf,,TechSafety,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ME3AQUAM,manuscript,2018,"Mindermann, Sören; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan",Active Inverse Reward Design,,,,,https://arxiv.org/abs/1809.03060,"Reward design, the problem of selecting an appropriate reward function for an AI system, is both critically important, as it encodes the task the system should perform, and challenging, as it requires reasoning about and understanding the agent’s environment in detail. AI practitioners often iterate on the reward function for their systems in a trial-and-error process to get their desired behavior. Inverse reward design (IRD) is a preference inference method that infers a true reward function from an observed, possibly misspeciﬁed, proxy reward function. This allows the system to determine when it should trust its observed reward function and respond appropriately. This has been shown to avoid problems in reward design such as negative side-effects (omitting a seemingly irrelevant but important aspect of the task) and reward hacking (learning to exploit unanticipated loopholes). In this paper, we actively select the set of proxy reward functions available to the designer. This improves the quality of inference and simpliﬁes the associated reward design problem. We present two types of queries: discrete queries, where the system designer chooses from a discrete set of reward functions, and feature queries, where the system queries the designer for weights on a small set of features. We evaluate this approach with experiments in a personal shopping assistant domain and a 2D navigation domain. We ﬁnd that our approach leads to reduced regret at test time compared with vanilla IRD. Our results indicate that actively selecting the set of available reward functions is a promising direction to improve the efﬁciency and effectiveness of reward design.",2018,2022-01-30 4:50:42,2022-01-30 4:50:42,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000015,,/Users/jacquesthibodeau/Zotero/storage/S59BK5N6/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/9KM9HA7F/1809.html; /Users/jacquesthibodeau/Zotero/storage/MTNGDAC9/Mindermann et al. - Active Inverse Reward Design.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68WZKQQ4,manuscript,2018,"Ortega, Pedro A.; Legg, Shane",Modeling Friends and Foes,,,,,http://arxiv.org/abs/1807.00196,"How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.",2018-06-30,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:35:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  J: 2 arXiv: 1807.00196,,/Users/jacquesthibodeau/Zotero/storage/VWQC9GCA/Ortega and Legg - 2018 - Modeling Friends and Foes.pdf; /Users/jacquesthibodeau/Zotero/storage/IUKTABAW/1807.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3GKZHK9F,manuscript,2019,"Chow, Yinlam; Nachum, Ofir; Faust, Aleksandra; Duenez-Guzman, Edgar; Ghavamzadeh, Mohammad",Lyapunov-based Safe Policy Optimization for Continuous Control,,,,,http://arxiv.org/abs/1901.10031,"We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",2019-02-11,2022-01-30 4:52:39,2022-01-30 4:52:39,2019-12-16 20:32:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000083  arXiv: 1901.10031,,/Users/jacquesthibodeau/Zotero/storage/IN5MJ7VZ/Chow et al. - 2019 - Lyapunov-based Safe Policy Optimization for Contin.pdf; /Users/jacquesthibodeau/Zotero/storage/85W7HCZP/1901.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JBZW9WNV,manuscript,2020,"Krueger, David; Maharaj, Tegan; Leike, Jan",Hidden Incentives for Auto-Induced Distributional Shift,,,,,http://arxiv.org/abs/2009.09153,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",2020-09-18,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-11-21 17:32:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2009.09153,,/Users/jacquesthibodeau/Zotero/storage/C4IC67EE/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf; /Users/jacquesthibodeau/Zotero/storage/ZHEWPCUB/2009.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQWJG7NP,manuscript,2020,"Hill, Felix; Mokra, Sona; Wong, Nathaniel; Harley, Tim",Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,,,,,http://arxiv.org/abs/2005.09382,"Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instructionfollowing with deep RL typically involves language generated from templates (by an environment simulator), which does not reﬂect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",2020-05-19,2022-01-30 4:52:38,2022-01-30 4:52:38,2020-08-31 18:19:41,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 2005.09382,,/Users/jacquesthibodeau/Zotero/storage/MSQMATUV/Hill et al. - 2020 - Human Instruction-Following with Deep Reinforcemen.pdf,,TechSafety; DeepMind,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6E69ZJCN,manuscript,2019,"Fort, Stanislav; Hu, Huiyi; Lakshminarayanan, Balaji",Deep Ensembles: A Loss Landscape Perspective,,,,,http://arxiv.org/abs/1912.02757,"Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. We demonstrate that while low-loss connectors between modes exist, they are not connected in the space of predictions. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods.",2019-12-05,2022-01-30 4:52:38,2022-01-30 4:52:38,2019-12-16 20:30:39,,,,,,,Deep Ensembles,,,,,,,,,,,,arXiv.org,,ZSCC: 0000155  arXiv: 1912.02757,,/Users/jacquesthibodeau/Zotero/storage/6GHKJWB9/Fort et al. - 2019 - Deep Ensembles A Loss Landscape Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/DU5ECF4A/1912.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNWXKCU9,manuscript,2020,"Uesato, Jonathan; Kumar, Ramana; Krakovna, Victoria; Everitt, Tom; Ngo, Richard; Legg, Shane",Avoiding Tampering Incentives in Deep RL via Decoupled Approval,,,,,http://arxiv.org/abs/2011.08827,"How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible.",2020-11-17,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-12-12 15:36:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.08827,,/Users/jacquesthibodeau/Zotero/storage/D9UAFGVG/Uesato et al. - 2020 - Avoiding Tampering Incentives in Deep RL via Decou.pdf; /Users/jacquesthibodeau/Zotero/storage/9PA82CZF/2011.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26XP8265,manuscript,2020,"Ngo, Richard",AGI Safety From First Principles,,,,,,,2020,2022-01-30 4:52:37,2022-01-30 4:52:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/J2MQFEZQ/Ngo - AGI Safety From First Principles.pdf,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4XFRI2SS,manuscript,2020,"Dulac-Arnold, Gabriel; Levine, Nir; Mankowitz, Daniel J.; Li, Jerry; Paduraru, Cosmin; Gowal, Sven; Hester, Todd",An empirical investigation of the challenges of real-world reinforcement learning,,,,,http://arxiv.org/abs/2003.11881,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.",2020-03-24,2022-01-30 4:52:37,2022-01-30 4:52:37,2020-08-18 21:24:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 2003.11881,,/Users/jacquesthibodeau/Zotero/storage/F4PISH98/Dulac-Arnold et al. - 2020 - An empirical investigation of the challenges of re.pdf; /Users/jacquesthibodeau/Zotero/storage/RIVBCVU2/2003.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSH3KCJW,manuscript,2019,"Gowal, Sven; Uesato, Jonathan; Qin, Chongli; Huang, Po-Sen; Mann, Timothy; Kohli, Pushmeet",An Alternative Surrogate Loss for PGD-based Adversarial Testing,,,,,http://arxiv.org/abs/1910.09338,"Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36% (with $\ell_\infty$ perturbations of $\epsilon = 0.3$) and the accuracy of their CIFAR-10 model to 44.03% (at $\epsilon = 8/255$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07% (with $\ell_\infty$ perturbations of $\epsilon = 0.031$).",2019-10-21,2022-01-30 4:52:37,2022-01-30 4:52:37,2019-12-16 20:31:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  arXiv: 1910.09338,,/Users/jacquesthibodeau/Zotero/storage/ACDBRTUC/Gowal et al. - 2019 - An Alternative Surrogate Loss for PGD-based Advers.pdf; /Users/jacquesthibodeau/Zotero/storage/K9GR5WQX/1910.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DJIA6CVN,manuscript,2021,"Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey",Alignment of Language Agents,,,,,http://arxiv.org/abs/2103.14659,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",2021-03-26,2022-01-30 4:52:37,2022-01-30 4:52:37,2021-11-14 16:31:03,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2103.14659,,/Users/jacquesthibodeau/Zotero/storage/74F9MU53/Kenton et al. - 2021 - Alignment of Language Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/K3PCKCDE/2103.html,,TechSafety; AmbiguousSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AJ25JZ86,manuscript,2017,"Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane",AI Safety Gridworlds,,,,,http://arxiv.org/abs/1711.09883,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",2017-11-28,2022-01-30 4:52:37,2022-01-30 4:52:37,2019-12-16 20:35:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000215  arXiv: 1711.09883,,/Users/jacquesthibodeau/Zotero/storage/S9F534C5/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/2X4PWKEM/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/NF3EHNV5/1711.html; /Users/jacquesthibodeau/Zotero/storage/7587G9PA/1711.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SRQCVSJ8,manuscript,2018,"Orseau, Laurent; McGill, Simon McGregor; Legg, Shane",Agents and Devices: A Relative Definition of Agency,,,,,http://arxiv.org/abs/1805.12387,"According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.",2018-05-31,2022-01-30 4:52:36,2022-01-30 4:52:36,2020-11-14 0:33:25,,,,,,,Agents and Devices,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1805.12387,,/Users/jacquesthibodeau/Zotero/storage/J8BWQUT7/Orseau et al. - 2018 - Agents and Devices A Relative Definition of Agenc.pdf; /Users/jacquesthibodeau/Zotero/storage/DVSN4A5S/1805.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM2U5SSZ,manuscript,2020,"Clifton, Jesse; Riché, Maxime",Towards Cooperation in Learning Games,,,,,,"Suppose that several actors are going to deploy learning agents to act on their behalf. What principles should guide these actors in designing their agents, given that they may have competing goals? An appealing solution concept in this setting is welfareoptimal learning equilibrium. This means that the learning agents should constitute a Nash equilibrium whose payoff proﬁle is optimal according to some measure of total welfare (welfare function). In this work, we construct a class of learning algorithms in this spirit called learning tit-for-tat (L-TFT). L-TFT algorithms maximize a welfare function according to a speciﬁed optimization schedule, and punish their counterpart when they detect that they are deviating from this plan. Because the policies of other agents are not in general fully observed, agents must infer whether their counterpart is following a cooperative learning algorithm. This requires us to develop new techniques for making inferences about counterpart learning algorithms. In two sequential social dilemmas, our L-TFT algorithms successfully cooperate in self-play while effectively avoiding exploitation by and punishing defecting learning algorithms.",2020,2022-01-30 4:51:37,2022-01-30 4:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/Q7RTK23R/Clifton and Riché - Towards Cooperation in Learning Games.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PT6IB3US,manuscript,2016,"Gloor, Lukas","Suffering-focused AI safety: Why ""fail-safe'"" measures might be our top intervention",,,,,,"AI-safety eﬀorts focused on suﬀering reduction should place particular emphasis on avoiding risks of astronomical disvalue. Among the cases where uncontrolled AI destroys humanity, outcomes might still diﬀer enormously in the amounts of suﬀering produced. Rather than concentrating all our eﬀorts on a speciﬁc future we would like to bring about, we should identify futures we least want to bring about and work on ways to steer AI trajectories around these. In particular, a “fail-safe”1 approach to AI safety is especially promising because avoiding very bad outcomes might be much easier than making sure we get everything right. This is also a neglected cause despite there being a broad consensus among diﬀerent moral views that avoiding the creation of vast amounts of suﬀering in our future is an ethical priority.",2016,2022-01-30 4:51:37,2022-01-30 4:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/AIHAKRFD/Gloor - Suffering-focused AI safety Why ``fail-safe'' mea.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79K88D35,manuscript,2019,"Kosoy, Vanessa",Forecasting using incomplete models,,,,,http://arxiv.org/abs/1705.04630,"We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is ""suspected"" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.",2019-05-16,2022-01-30 4:56:48,2022-01-30 4:56:48,2019-12-16 2:29:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 1  J: 1 arXiv: 1705.04630,,/Users/jacquesthibodeau/Zotero/storage/I3DCSTP3/Kosoy - 2019 - Forecasting using incomplete models.pdf; /Users/jacquesthibodeau/Zotero/storage/MEKS9723/1705.html,,TechSafety; MIRI,"Computer Science - Machine Learning; I.2.6; G.3; 68Q32, 62M10, 62G08",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4CGXIREQ,manuscript,2015,"Hibbard, Bill",Ethical Artificial Intelligence,,,,,http://arxiv.org/abs/1411.1373,"This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called ""perverse instantiation"") using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended instrumental actions (sometimes called ""basic AI drives"" or ""instrumental goals"") by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agent's utility function and its definition (one version of this problem is sometimes called ""motivated value selection""). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.",2015-11-17,2022-01-30 4:56:48,2022-01-30 4:56:48,2020-11-22 2:29:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 33  arXiv: 1411.1373,,/Users/jacquesthibodeau/Zotero/storage/DZPJDUCP/Hibbard - 2015 - Ethical Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/7MF8SPGQ/1411.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TXH2RDI8,manuscript,2004,"Yudkowsky, Eliezer",Coherent Extrapolated Volition,,,,,,,2004,2022-01-30 4:56:47,2022-01-30 4:56:47,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000120,,/Users/jacquesthibodeau/Zotero/storage/2CZG225N/Yudkowsky - Coherent Extrapolated Volition.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
659RCK7X,manuscript,2019,"Manheim, David; Garrabrant, Scott",Categorizing Variants of Goodhart's Law,,,,,http://arxiv.org/abs/1803.04585,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",2019-02-24,2022-01-30 4:56:47,2022-01-30 4:56:47,2019-12-16 2:27:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 36  J: 23 arXiv: 1803.04585,,/Users/jacquesthibodeau/Zotero/storage/34365ZKG/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/XSBBKWCC/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/T45ISW4V/1803.html; /Users/jacquesthibodeau/Zotero/storage/JRUQVZ6J/1803.html,,TechSafety; MIRI,Statistics - Machine Learning; Computer Science - Artificial Intelligence; 91E45; Quantitative Finance - General Finance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PGK9NF8P,manuscript,2015,"LaVictoire, Patrick",An Introduction to Löb’s Theorem in MIRI Research,,,,,,,2015,2022-01-30 4:56:47,2022-01-30 4:56:47,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s1]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/STHEKPTX/LaVictoire - An Introduction to Lo¨b’s Theorem in MIRI Research.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RRPKMSTJ,manuscript,2021,"Garrabrant, Scott; Herrmann, Daniel A.; Lopez-Wild, Josiah",Cartesian Frames,,,,,http://arxiv.org/abs/2109.10996,"We introduce a novel framework, the theory of Cartesian frames (CF), that gives powerful tools for manipulating sets of acts. The CF framework takes as its most fundamental building block that an agent can freely choose from a set of available actions. The framework uses the mathematics of Chu spaces to develop a calculus of those sets of actions, how those actions change at various levels of description, and how different agents' actions can combine when agents work in concert. We discuss how this framework might provide an illuminating perspective on issues in decision theory and formal epistemology.",2021-09-22,2022-01-30 4:56:47,2022-01-30 4:56:47,2021-10-31 22:35:31,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2109.10996,,/Users/jacquesthibodeau/Zotero/storage/SUBM35KQ/Garrabrant et al. - 2021 - Cartesian Frames.pdf; /Users/jacquesthibodeau/Zotero/storage/S52XF4ST/2109.html,,TechSafety,Mathematics - Category Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZ2TFKZZ,manuscript,2016,"Garrabrant, Scott; Soares, Nate; Taylor, Jessica",Asymptotic Convergence in Online Learning with Unbounded Delays,,,,,http://arxiv.org/abs/1604.05280,"We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.",2016-09-07,2022-01-30 4:56:47,2022-01-30 4:56:47,2019-12-16 2:30:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 1604.05280,,/Users/jacquesthibodeau/Zotero/storage/FSZFUVAS/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/5GD4TDT9/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/XWQ622KH/1604.html; /Users/jacquesthibodeau/Zotero/storage/5WDRANB4/1604.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WU22E4BX,manuscript,,"Hoffman, Ben",The Professional's Dilemma,,,,,http://mediangroup.org/docs/the_professionals_dilemma.pdf,,unknown,2022-01-30 4:55:38,2022-01-30 4:55:38,2020-11-21 19:50:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/3GQXQPUN/the_professionals_dilemma.pdf,,MetaSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53WPDXKG,manuscript,,"Perry, Miya",Toward A Working Theory of Mind,,,,,http://mediangroup.org/docs/toward_a_working_theory_of_mind.pdf,,unknown,2022-01-30 4:55:38,2022-01-30 4:55:38,2020-11-21 19:49:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/2SZEMX5T/toward_a_working_theory_of_mind.pdf,,TechSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8PSVNP3E,manuscript,2018,"Hwang, Tim",Computational Power and the Social Impact of Artificial Intelligence,,,,,http://arxiv.org/abs/1803.08971,"Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.",2018-03-23,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-11-14 0:34:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000040  arXiv: 1803.08971,,/Users/jacquesthibodeau/Zotero/storage/C9KNXFPA/Hwang - 2018 - Computational Power and the Social Impact of Artif.pdf; /Users/jacquesthibodeau/Zotero/storage/6H9CEWGV/1803.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AMJT8QFM,manuscript,2020,"Chatterjee, Satrajit",Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,,,,,http://arxiv.org/abs/2002.10657,"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",2020-02-24,2022-01-30 4:59:44,2022-01-30 4:59:44,2020-09-05 18:39:31,,,,,,,Coherent Gradients,,,,,,,,,,,,arXiv.org,,ZSCC: 0000018  arXiv: 2002.10657,,/Users/jacquesthibodeau/Zotero/storage/J2MAP4DT/Chatterjee - 2020 - Coherent Gradients An Approach to Understanding G.pdf; /Users/jacquesthibodeau/Zotero/storage/FKT247Z3/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQRWRPSQ,manuscript,2018,"Trazzi, Michaël; Yampolskiy, Roman V.",Building Safer AGI by introducing Artificial Stupidity,,,,,http://arxiv.org/abs/1808.03644,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",2018-08-10,2022-01-30 4:59:43,2022-01-30 4:59:43,2020-11-14 0:55:32,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1808.03644,,/Users/jacquesthibodeau/Zotero/storage/KH2RT4UD/Trazzi and Yampolskiy - 2018 - Building Safer AGI by introducing Artificial Stupi.pdf; /Users/jacquesthibodeau/Zotero/storage/2AIX67AX/1808.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QWU6BGHU,manuscript,2020,"Saisubramanian, Sandhya; Zilberstein, Shlomo; Kamar, Ece",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,,,,,http://arxiv.org/abs/2008.12146,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model---handcrafted or machine acquired---is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of the agent's actions is critical to improving the safety and reliability of autonomous systems. This emerging research topic is attracting increased attention due to the increased deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. We conclude with a discussion of open questions and suggestions for future research directions.",2020-08-28,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-11-14 0:57:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2008.12146,,/Users/jacquesthibodeau/Zotero/storage/F2EV268Z/Saisubramanian et al. - 2020 - Avoiding Negative Side Effects due to Incomplete K.pdf; /Users/jacquesthibodeau/Zotero/storage/BHMQUM67/2008.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38TP8QCZ,manuscript,2016,"Yampolskiy, Roman V.; Spellchecker, M. S.",Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures,,,,,http://arxiv.org/abs/1610.07997,"In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.",2016-10-25,2022-01-30 4:59:36,2022-01-30 4:59:36,2020-12-13 20:19:29,,,,,,,Artificial Intelligence Safety and Cybersecurity,,,,,,,,,,,,arXiv.org,,ZSCC: 0000084  arXiv: 1610.07997,,/Users/jacquesthibodeau/Zotero/storage/XBP68MBT/Yampolskiy and Spellchecker - 2016 - Artificial Intelligence Safety and Cybersecurity .pdf; /Users/jacquesthibodeau/Zotero/storage/GC9TXXSM/1610.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CNJWTPTN,manuscript,2016,"Ziesche, Soenke; Yampolskiy, Roman V.",Artificial Fun: Mapping Minds to the Space of Fun,,,,,http://arxiv.org/abs/1606.07092,"Yampolskiy and others have shown that the space of possible minds is vast, actually infinite (Yampolskiy, 2015). A question of interest is 'Which activities can minds perform during their lifetime?' This question is very broad, thus in this article restricted to 'Which non-boring activities can minds perform?' The space of potential non-boring activities has been called by Yudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the relation between various types of minds and the part of the fun space, which is accessible for them.",2016-06-22,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-12-13 20:23:34,,,,,,,Artificial Fun,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1606.07092,,/Users/jacquesthibodeau/Zotero/storage/X6ZMRXFI/Ziesche and Yampolskiy - 2016 - Artificial Fun Mapping Minds to the Space of Fun.pdf; /Users/jacquesthibodeau/Zotero/storage/AVP4AFV8/1606.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Other Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5B2XBHN,manuscript,2016,"Torres, Phil",Agential Risks: A Comprehensive Introduction,,,,,,"The greatest existential threats to humanity stem from increasingly powerful advanced technologies. Yet the “risk potential” of such tools can only be realized when coupled with a suitable agent who, through error or terror, could use the tool to bring about an existential catastrophe. While the existential risk literature has provided many accounts of how advanced technologies might be misused and abused to cause unprecedented harm, no scholar has yet explored the other half of the agent-tool coupling, namely the agent. This paper aims to correct this failure by offering a comprehensive overview of what we could call “agential riskology.” Only by studying the unique properties of different agential risk types can one acquire an accurate picture of the existential danger before us.",2016,2022-01-30 4:59:33,2022-01-30 4:59:33,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/22B4EZFK/Torres - Agential Risks A Comprehensive Introduction.pdf,,MetaSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35ISE4PP,manuscript,2020,"Hoang, Lê Nguyên",A Roadmap for Robust End-to-End Alignment,,,,,http://arxiv.org/abs/1809.01036,"This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.",2020-02-25,2022-01-30 4:59:33,2022-01-30 4:59:33,2020-11-14 0:52:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1809.01036,,/Users/jacquesthibodeau/Zotero/storage/KS6F4Q7M/Hoang - 2020 - A Roadmap for Robust End-to-End Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/HVT2WEJJ/1809.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QQ7I47Z4,manuscript,2018,"Irving, Geoffrey; Christiano, Paul; Amodei, Dario",AI safety via debate,,,,,http://arxiv.org/abs/1805.00899,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",2018-10-22,2022-01-30 4:58:18,2022-01-30 4:58:18,2019-12-16 20:07:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000044  arXiv: 1805.00899,,/Users/jacquesthibodeau/Zotero/storage/Q8PQWCV7/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/QKP3ZSWZ/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/26MTE6EQ/1805.html; /Users/jacquesthibodeau/Zotero/storage/38FDWSJQ/1805.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS5KIAHS,manuscript,2016,"Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan",Concrete Problems in AI Safety,,,,,http://arxiv.org/abs/1606.06565,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",2016-07-25,2022-01-30 4:57:26,2022-01-30 4:57:26,2019-12-16 20:16:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0001335  arXiv: 1606.06565,,/Users/jacquesthibodeau/Zotero/storage/SB8ZRSGM/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/3RX9H74D/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/UAW8RNPB/1606.html; /Users/jacquesthibodeau/Zotero/storage/9T9F6RZW/1606.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BS9AFFF,manuscript,2019,"Ray, Alex; Achiam, Joshua; Amodei, Dario",Benchmarking Safe Exploration in Deep Reinforcement Learning,,,,,https://arxiv.org/abs/2007.01223,"Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.",2019,2022-01-30 4:57:26,2022-01-30 4:57:26,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000073,,/Users/jacquesthibodeau/Zotero/storage/KEPD9RAS/Ray et al. - Benchmarking Safe Exploration in Deep Reinforcemen.pdf,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IJ4UFE3M,manuscript,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,,,,,http://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-14,2022-01-30 4:57:26,2022-01-30 4:57:26,2021-10-31 22:37:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 36  arXiv: 2107.03374,,/Users/jacquesthibodeau/Zotero/storage/P7CPC96K/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/DDJ2TIQ7/2107.html,,MetaSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QMZS3XGJ,manuscript,2019,"Ziegler, Daniel M.; Stiennon, Nisan; Wu, Jeffrey; Brown, Tom B.; Radford, Alec; Amodei, Dario; Christiano, Paul; Irving, Geoffrey",Fine-tuning language models from human preferences,,,,,,,2019,2022-01-30 4:57:26,2022-01-30 4:57:26,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000091,,/Users/jacquesthibodeau/Zotero/storage/G39X234W/Ziegler et al. - 2019 - Fine-tuning language models from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/G5ISVZVZ/1909.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FZ3JURGJ,manuscript,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,,,,,http://arxiv.org/abs/2009.09071,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-01-30 4:57:25,2022-01-30 4:57:25,2020-11-14 0:54:58,,,,,,,Measurement in AI Policy,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2009.09071,,/Users/jacquesthibodeau/Zotero/storage/RKJ6AFAG/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/GSWE2PCX/2009.html,,MetaSafety; Open-AI; AmbiguosSafety,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JPVIIMSV,manuscript,2021,"Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul",Recursively Summarizing Books with Human Feedback,,,,,http://arxiv.org/abs/2109.10862,"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",2021-09-27,2022-01-30 4:57:25,2022-01-30 4:57:25,2021-10-31 22:37:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2109.10862,,/Users/jacquesthibodeau/Zotero/storage/XN8J3757/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/53895JGP/2109.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VGRU4IVI,manuscript,2021,"Hernandez, Danny; Kaplan, Jared; Henighan, Tom; McCandlish, Sam",Scaling Laws for Transfer,,,,,http://arxiv.org/abs/2102.01293,"We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.",2021-02-01,2022-01-30 4:57:25,2022-01-30 4:57:25,2021-11-13 22:37:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 2102.01293,,/Users/jacquesthibodeau/Zotero/storage/E5B34BKD/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf; /Users/jacquesthibodeau/Zotero/storage/GXQAZ3BT/2102.html,,MetaSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46ZCX4VI,manuscript,2018,"Christiano, Paul; Shlegeris, Buck; Amodei, Dario",Supervising strong learners by amplifying weak experts,,,,,http://arxiv.org/abs/1810.08575,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",2018-10-19,2022-01-30 4:57:25,2022-01-30 4:57:25,2019-12-16 20:06:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000028  arXiv: 1810.08575,,/Users/jacquesthibodeau/Zotero/storage/K9A8KU7E/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/5X96ZJNE/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/JBIIC37H/1810.html; /Users/jacquesthibodeau/Zotero/storage/HUISI8Z7/1810.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9BGAUCZ,manuscript,2019,"Askell, Amanda; Brundage, Miles; Hadfield, Gillian",The Role of Cooperation in Responsible AI Development,,,,,http://arxiv.org/abs/1907.04534,"In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.",2019-07-10,2022-01-30 4:57:24,2022-01-30 4:57:24,2019-12-16 20:04:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1907.04534,,/Users/jacquesthibodeau/Zotero/storage/QQFIEMRW/Askell et al. - 2019 - The Role of Cooperation in Responsible AI Developm.pdf; /Users/jacquesthibodeau/Zotero/storage/9N5T94H5/1907.html,,MetaSafety; Open-AI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; K.1; K.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGTN9FXU,manuscript,2019,"Kang, Daniel; Sun, Yi; Brown, Tom; Hendrycks, Dan; Steinhardt, Jacob",Transfer of Adversarial Robustness Between Perturbation Types,,,,,http://arxiv.org/abs/1905.01034,"We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type may not always imply and may sometimes hurt robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.",2019-05-03,2022-01-30 4:57:24,2022-01-30 4:57:24,2019-12-16 20:04:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1905.01034,,/Users/jacquesthibodeau/Zotero/storage/78UQ6FFU/Kang et al. - 2019 - Transfer of Adversarial Robustness Between Perturb.pdf; /Users/jacquesthibodeau/Zotero/storage/R9H28U6H/1905.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS4BKV9Q,manuscript,2021,"Lin, Stephanie; Hilton, Jacob; Evans, Owain",TruthfulQA: Measuring How Models Mimic Human Falsehoods,,,,,http://arxiv.org/abs/2109.07958,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",2021-09-08,2022-01-30 4:57:24,2022-01-30 4:57:24,2021-11-18 23:35:58,,,,,,,TruthfulQA,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2109.07958,,/Users/jacquesthibodeau/Zotero/storage/G39ZUQDN/Lin et al. - 2021 - TruthfulQA Measuring How Models Mimic Human False.pdf; /Users/jacquesthibodeau/Zotero/storage/GP7663XV/2109.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7RCGS4P,manuscript,2021,"Tamkin, Alex; Brundage, Miles; Clark, Jack; Ganguli, Deep","Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",,,,,http://arxiv.org/abs/2102.02503,"On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.",2021-02-04,2022-01-30 4:57:24,2022-01-30 4:57:24,2021-10-31 22:40:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2102.02503,,"/Users/jacquesthibodeau/Zotero/storage/CUGQGQE9/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and S.pdf",,MetaSafety,Computer Science - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BJJSTE9N,manuscript,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,,,,,http://arxiv.org/abs/2109.13916,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing hazards in deployment (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-10-30,2022-01-30 4:57:23,2022-01-30 4:57:23,2021-11-18 23:47:25,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2109.13916,,/Users/jacquesthibodeau/Zotero/storage/QZGMBZ4T/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/Z3BW94QM/2109.html,,TechSafety; AmbiguousSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8IB8KRA5,manuscript,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,,,,,http://arxiv.org/abs/2109.11513,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-01-30 4:56:59,2022-01-30 4:56:59,2021-10-31 22:35:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2109.11513,,/Users/jacquesthibodeau/Zotero/storage/PIE5EQ45/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/4QRAPMBD/2109.html,,TechSafety,Computer Science - Artificial Intelligence; Mathematics - Probability; Mathematics - Combinatorics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A5HGSXIU,manuscript,2013,"Yudkowsky, Eliezer; Herreshoff, Marcello","Tiling Agents for Self-Modifying AI, and the Löbian Obstacle",,,,,https://intelligence.org/files/TilingAgentsDraft.pdf,,2013-10-07,2022-01-30 4:56:59,2022-01-30 4:56:59,2020-11-21 17:18:36,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/24VG56W8/TilingAgentsDraft.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MU9A79QN,manuscript,2019,"Kosoy, Vanessa; Appel, Alexander",Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm,,,,,http://arxiv.org/abs/1608.04112,"We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of ""optimal polynomial-time estimators."" We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with ""classical"" probability theory.",2019-06-04,2022-01-30 4:56:57,2022-01-30 4:56:57,2019-12-16 2:31:07,,,,,,,Optimal Polynomial-Time Estimators,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 1  J: 1  arXiv: 1608.04112,,/Users/jacquesthibodeau/Zotero/storage/WBCNT5WT/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/8GRDS5M5/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/UQN6CZNA/1608.html; /Users/jacquesthibodeau/Zotero/storage/BWHW5UG4/1608.html,,TechSafety; MIRI,Computer Science - Computational Complexity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6W43B8N7,manuscript,2011,"de Blanc, Peter",Ontological Crises in Artificial Agents' Value Systems,,,,,http://arxiv.org/abs/1105.3821,"Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",2011-05-19,2022-01-30 4:56:57,2022-01-30 4:56:57,2021-01-23 20:43:20,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1105.3821,,/Users/jacquesthibodeau/Zotero/storage/D876G6M2/de Blanc - 2011 - Ontological Crises in Artificial Agents' Value Sys.pdf,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5TDG7QC,manuscript,2016,"Garrabrant, Scott; Fallenstein, Benya; Demski, Abram; Soares, Nate",Inductive Coherence,,,,,http://arxiv.org/abs/1604.05288,"While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.",2016-10-07,2022-01-30 4:56:57,2022-01-30 4:56:57,2019-12-16 2:30:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1604.05288,,/Users/jacquesthibodeau/Zotero/storage/QSFXVV9V/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/BZ229SP3/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/KBHB4XEP/1604.html; /Users/jacquesthibodeau/Zotero/storage/BKI2SSHN/1604.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z23F8ZT9,manuscript,2017,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",Logical Induction,,,,,http://arxiv.org/abs/1609.03543,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]",2017-12-12,2022-01-30 4:56:57,2022-01-30 4:56:57,2019-12-16 2:30:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 32  J: 23 arXiv: 1609.03543,,/Users/jacquesthibodeau/Zotero/storage/EHRSJNQS/Garrabrant et al. - 2017 - Logical Induction.pdf; /Users/jacquesthibodeau/Zotero/storage/QX7KU5KC/1609.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Mathematics - Probability; Mathematics - Logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QPEP6IKI,manuscript,2019,"Demski, Abram; Garrabrant, Scott",Embedded Agency,,,,,http://arxiv.org/abs/1902.09469,"Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts. We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type ``function''; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.",2019-02-25,2022-01-30 4:56:48,2022-01-30 4:56:48,2019-12-16 2:27:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1902.09469,,/Users/jacquesthibodeau/Zotero/storage/F7TPMBEQ/Demski and Garrabrant - 2019 - Embedded Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/T2XX98RN/1902.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WXMKFV5,manuscript,2018,"Yudkowsky, Eliezer; Soares, Nate",Functional Decision Theory: A New Theory of Instrumental Rationality,,,,,http://arxiv.org/abs/1710.05060,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",2018-05-22,2022-01-30 4:56:48,2022-01-30 4:56:48,2019-12-18 4:17:20,,,,,,,Functional Decision Theory,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s3]  ACC: 19  J: 11  arXiv: 1710.05060,,/Users/jacquesthibodeau/Zotero/storage/KQXR2IHP/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/RNCQ7RDX/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/J5XQEFMS/1710.html; /Users/jacquesthibodeau/Zotero/storage/K2U4NXWI/1710.html; /Users/jacquesthibodeau/Zotero/storage/VVPZQTSI/1710.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4H9VQJKN,manuscript,2019,"Evans, Owain; Saunders, William; Stuhlmüller, Andreas",Machine Learning Projects for Iterated Distillation and Ampliﬁcation,,,,,,"Iterated Distillation and Ampliﬁcation (IDA) is a framework for training ML models. IDA is related to existing frameworks like imitation learning and reinforcement learning, but it aims to solve tasks for which humans cannot construct a suitable reward function or solve directly.",2019,2022-01-30 5:00:28,2022-01-30 5:00:28,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 1  J: 0,,/Users/jacquesthibodeau/Zotero/storage/V6BVPGKN/Evans et al. - Machine Learning Projects for Iterated Distillatio.pdf,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I38TP5D8,manuscript,2019,"Roy, Mati",AI Safety Open Problems,,,,,https://docs.google.com/document/d/1J2fOOF-NYiPC0-J3ZGEfE0OhA-QcOInhlvWjr1fAsS0/edit?usp=embed_facebook,Created: 2018-11-08 | Updated: 2019-11-2 | Suggestions: please make suggestions directly in this Doc | List maintainer: Mati Roy (contact@matiroy.com)  AI Safety Open Problems Technical AGI safety research outside AI: https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safet...,2019,2022-01-30 5:00:28,2022-01-30 5:00:28,2019-12-16 19:57:56,,,,,,,,,,,,,,en,,Google Docs,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/73B2JNDW/edit.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IP5NBM2T,manuscript,2017,"Eysenbach, Benjamin; Gu, Shixiang; Ibarz, Julian; Levine, Sergey",Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,,,,,http://arxiv.org/abs/1711.06782,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",2017-11-17,2022-01-30 4:59:59,2022-01-30 4:59:59,2020-11-21 17:26:04,,,,,,,Leave no Trace,,,,,,,,,,,,arXiv.org,,ZSCC: 0000081  arXiv: 1711.06782,,/Users/jacquesthibodeau/Zotero/storage/UIPW3M67/Eysenbach et al. - 2017 - Leave no Trace Learning to Reset for Safe and Aut.pdf; /Users/jacquesthibodeau/Zotero/storage/I3RPW323/1711.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQ3QRMVD,manuscript,2019,"Sevilla, Jaime; Moreno, Pablo",Implications of Quantum Computing for Artificial Intelligence alignment research,,,,,http://arxiv.org/abs/1908.07613,"We explain some key features of quantum computing via three heuristics and apply them to argue that a deep understanding of quantum computing is unlikely to be helpful to address current bottlenecks in Artificial Intelligence Alignment. Our argument relies on the claims that Quantum Computing leads to compute overhang instead of algorithmic overhang, and that the difficulties associated with the measurement of quantum states do not invalidate any major assumptions of current Artificial Intelligence Alignment research agendas. We also discuss tripwiring, adversarial blinding, informed oversight and side effects as possible exceptions.",2019-08-24,2022-01-30 4:59:58,2022-01-30 4:59:58,2019-12-16 22:41:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1908.07613,,/Users/jacquesthibodeau/Zotero/storage/5X947SXX/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/VTG2NVXB/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/AQSE5TCF/1908.html; /Users/jacquesthibodeau/Zotero/storage/VJGCCXHW/1908.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Emerging Technologies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MBKBAF43,manuscript,2018,"Noothigattu, Ritesh; Bouneffouf, Djallel; Mattei, Nicholas; Chandra, Rachita; Madan, Piyush; Varshney, Kush; Campbell, Murray; Singh, Moninder; Rossi, Francesca",Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration,,,,,http://arxiv.org/abs/1809.08343,"Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.",2018-09-21,2022-01-30 4:59:58,2022-01-30 4:59:58,2020-12-13 23:27:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000027  arXiv: 1809.08343,,/Users/jacquesthibodeau/Zotero/storage/BZ3MDGSE/Noothigattu et al. - 2018 - Interpretable Multi-Objective Reinforcement Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/CHJIW45A/1809.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PE2F3NVR,manuscript,2020,"Schneider, Johannes",Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,,,,,http://arxiv.org/abs/2009.09266,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",2020-09-19,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-11-14 0:52:26,,,,,,,Humans learn too,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2009.09266,,/Users/jacquesthibodeau/Zotero/storage/XP5PX8VP/Schneider - 2020 - Humans learn too Better Human-AI Interaction usin.pdf; /Users/jacquesthibodeau/Zotero/storage/JXASWCQG/2009.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7EKAANH7,manuscript,2018,"Baek, Jongmin Jerome",How To Solve Moral Conundrums with Computability Theory,,,,,http://arxiv.org/abs/1805.08347,"Various moral conundrums plague population ethics: The Non-Identity Problem, The Procreation Asymmetry, The Repugnant Conclusion, and more. I argue that the aforementioned moral conundrums have a structure neatly accounted for, and solved by, some ideas in computability theory. I introduce a mathematical model based on computability theory and show how previous arguments pertaining to these conundrums fit into the model. This paper proceeds as follows. First, I do a very brief survey of the history of computability theory in moral philosophy. Second, I follow various papers, and show how their arguments fit into, or don't fit into, our model. Third, I discuss the implications of our model to the question why the human race should or should not continue to exist. Finally, I show that our model ineluctably leads us to a Confucian moral principle.",2018-05-21,2022-01-30 4:59:57,2022-01-30 4:59:57,2020-11-14 1:09:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1805.08347,,/Users/jacquesthibodeau/Zotero/storage/CCQVF6AC/Baek - 2018 - How To Solve Moral Conundrums with Computability T.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH342CDG/1805.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GGN2RAC5,manuscript,2017,"Babcock, James; Kramar, Janos; Yampolskiy, Roman V.",Guidelines for Artificial Intelligence Containment,,,,,http://arxiv.org/abs/1707.08476,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",2017-07-24,2022-01-30 4:59:56,2022-01-30 4:59:56,2020-11-21 17:49:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029[s0]  arXiv: 1707.08476,,/Users/jacquesthibodeau/Zotero/storage/I9CB87G3/Babcock et al. - 2017 - Guidelines for Artificial Intelligence Containment.pdf; /Users/jacquesthibodeau/Zotero/storage/59PWEW8F/1707.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSHKC8QE,manuscript,2019,"Rupprecht, Christian; Ibrahim, Cyril; Pal, Christopher J.",Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,,,,,http://arxiv.org/abs/1904.01318,"As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",2019-04-02,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-09-05 17:02:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 1904.01318,,/Users/jacquesthibodeau/Zotero/storage/KZ3E7C6S/Rupprecht et al. - 2019 - Finding and Visualizing Weaknesses of Deep Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/PJSBJRET/1904.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4TCFB2D9,manuscript,2019,"Gruetzemacher, Ross; Paradice, David; Lee, Kang Bok",Forecasting Transformative AI: An Expert Survey,,,,,http://arxiv.org/abs/1901.08579,"Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.",2019-07-16,2022-01-30 4:59:48,2022-01-30 4:59:48,2020-11-14 0:34:52,,,,,,,Forecasting Transformative AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 1901.08579,,/Users/jacquesthibodeau/Zotero/storage/94FP5N9E/Gruetzemacher et al. - 2019 - Forecasting Transformative AI An Expert Survey.pdf; /Users/jacquesthibodeau/Zotero/storage/N9IVWXEE/1901.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6SSB5PK6,manuscript,2017,"Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.",DropoutDAgger: A Bayesian Approach to Safe Imitation Learning,,,,,http://arxiv.org/abs/1709.06166,"While imitation learning is becoming common practice in robotics, this approach often suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which uses the distribution over actions provided by the novice policy, for a given observation. Our method, which we call DropoutDAgger, uses dropout to train the novice as a Bayesian neural network that provides insight to its confidence. Using the distribution over the novice's actions, we estimate a probabilistic measure of safety with respect to the expert action, tuned to balance exploration and exploitation. The utility of this approach is evaluated on the MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved performance and safety compared to other DAgger variants and classic imitation learning.",2017-09-18,2022-01-30 4:59:46,2022-01-30 4:59:46,2020-12-13 20:55:29,,,,,,,DropoutDAgger,,,,,,,,,,,,arXiv.org,,ZSCC: 0000013  arXiv: 1709.06166,,/Users/jacquesthibodeau/Zotero/storage/IGX8RKK6/Menda et al. - 2017 - DropoutDAgger A Bayesian Approach to Safe Imitati.pdf; /Users/jacquesthibodeau/Zotero/storage/KWHIGHR6/1709.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5E9BGMY8,manuscript,2019,"Quint, Eleanor; Xu, Dong; Flint, Samuel; Scott, Stephen; Dwyer, Matthew",Formal Language Constraints for Markov Decision Processes,,,,,https://arxiv.org/abs/1910.01074v3,"In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.",2019-10-02,2020-11-14 1:21,2020-12-20 21:10,2020-11-14 1:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s0]  ACC: 3,,/Users/angelica/Zotero/storage/WQXN48YU/1910.html; /Users/angelica/Zotero/storage/WPALV4PP/Quint et al. - 2019 - Formal Language Constraints for Markov Decision Pr.pdf,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Within the framework of RL, the authors propose using constraints defined by DFAs (deterministic finite automata) in order to eliminate safety failures, or to prevent agents from exploring clearly ineffective policies (which would accelerate learning). Constraints can be defined on any auxiliary information that can be computed from the ""base"" MDP. A constraint could either restrict the action space, forcing the agent to take an action that doesn't violate the constraint, which they term ""hard"" constraints; or a constraint could impose a penalty on the agent, thus acting as a form of reward shaping, which they term a ""soft"" constraint. They consider two constraints: one that prevents the agent from ""dithering"" (going left, then right, then left, then right), and one that prevents the agent from ""overactuating"" (going in the same direction four times in a row). They evaluate their approach with these constraints on Atari games and Mujoco environments, and show that they lead to increased reward and decreased constraint violations."
ATXQ5FB3,manuscript,2020,"Laskin, Michael; Lee, Kimin; Stooke, Adam; Pinto, Lerrel; Abbeel, Pieter; Srinivas, Aravind",Reinforcement Learning with Augmented Data,,,,,http://arxiv.org/abs/2004.14990,"Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efﬁciency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the ﬁrst extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efﬁciency and ﬁnal performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD signiﬁcantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.",2020-06-23,2020-08-31 19:00,2020-12-20 22:09,2020-08-31 19:00,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000012  ACC: 12  arXiv: 2004.14990,,/Users/angelica/Zotero/storage/GRHL8ZH8/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While CURL (summarized above) applies contrastive learning in order to ensure the network is invariant to specific data augmentations, we can try something even simpler: what if we just run a regular RL algorithm on augmented observations (e.g. observations that have been randomly cropped)? The authors term this approach RAD (RL with Augmented Data), and find that this actually _outperforms_ CURL, despite not using the contrastive learning objective. The authors speculate that CURL is handicapped by using the contrastive loss as an auxiliary objective, and so its representations are forced to be good both for the true task and for the contrastive prediction task, whereas RAD only trains on the true task."
FMNZVCJ6,manuscript,2018,"Achiam, Joshua; Edwards, Harrison; Amodei, Dario; Abbeel, Pieter",Variational Option Discovery Algorithms,,,,,http://arxiv.org/abs/1807.10299,"We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.",2018-07-26,2019-12-16 20:07,2020-12-20 20:22,2019-12-16 20:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 1807.10299,,/Users/angelica/Zotero/storage/FZBGKVVS/1807.html; /Users/angelica/Zotero/storage/ABRLR2QA/1807.html; /Users/angelica/Zotero/storage/XCAD5YFA/Achiam et al. - 2018 - Variational Option Discovery Algorithms.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety; Open-AI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We can hope to do hierarchical reinforcement learning by first discovering several useful simple policies (or ""options"") by just acting in the environment without any reward function, and then using these options as primitive actions in a higher level policy that learns to do some task (using a reward function). How could we learn the options without a reward function though? Intuitively, we would like to learn behaviors that are different from each other. One way to frame this would be to think of this as an encoder-decoder problem. Suppose we want to learn K options. Then, we can give the encoder a number in the range [1, K], have it ""encode"" the number into a trajectory τ (that is, our encoder is a policy), and then have a decoder take τ and recover the original number. We train the encoder/policy and decoder jointly, optimizing them to successfully recover the original number (called a _context_). Intuitively, the encoder/policy wants to have very different behaviors for each option, so that it easy for decoder to figure out the context from the trajectory τ. However, a simple solution would be for the encoder/policy to just take a particular series of actions for each context and then stop, and the decoder learns an exact mapping from final states to contexts. To avoid this, we can decrease the capacity of the decoder (i.e. don't give it too many layers), and we also optimize for the _entropy_ of the encoder/policy, which encourages the encoder/policy to be more stochastic, and so it is more likely to learn overall behaviors that can still have some stochasticity, while still allowing the decoder to decode them. It turns out that this optimization problem has a one-to-one correspondence with variational autoencoders, motivating the name ""variational option discovery"". To stabilize training, they start with a small K, and increase K whenever the decoder becomes powerful enough. They evaluate in Gym environments, a simulated robotic hand, and a new ""Toddler"" environment. They find that the scheme works well (in terms of maximizing the objective) in all environments, but that the learned behaviors no longer look natural in the Toddler environment (which is the most complex). They also show that the learned policies can be used for hierarchical RL in the AntMaze problem.

This is very similar to the recent [Diversity Is All You Need](https://arxiv.org/abs/1802.06070). DIAYN aims to decode the context from _every state_ along a trajectory, which incentivizes it to find behaviors of the form ""go to a goal state"", whereas VALOR (this work) decodes the context from the entire trajectory (without actions, which would make the decoder's job too easy), which allows it to learn behaviors with motion, such as ""go around in a circle""."
72F4X2ZL,manuscript,2020,"Arenz, Oleg; Neumann, Gerhard",Non-Adversarial Imitation Learning and its Connections to Adversarial Methods,,,,,http://arxiv.org/abs/2008.03525,"Many modern methods for imitation learning and inverse reinforcement learning, such as GAIL or AIRL, are based on an adversarial formulation. These methods apply GANs to match the expert's distribution over states and actions with the implicit state-action distribution induced by the agent's policy. However, by framing imitation learning as a saddle point problem, adversarial methods can suffer from unstable optimization, and convergence can only be shown for small policy updates. We address these problems by proposing a framework for non-adversarial imitation learning. The resulting algorithms are similar to their adversarial counterparts and, thus, provide insights for adversarial imitation learning methods. Most notably, we show that AIRL is an instance of our non-adversarial formulation, which enables us to greatly simplify its derivations and obtain stronger convergence guarantees. We also show that our non-adversarial formulation can be used to derive novel algorithms by presenting a method for offline imitation learning that is inspired by the recent ValueDice algorithm, but does not rely on small policy updates for convergence. In our simulated robot experiments, our offline method for non-adversarial imitation learning seems to perform best when using many updates for policy and discriminator at each iteration and outperforms behavioral cloning and ValueDice.",2020-08-08,2020-11-14 0:50,2020-12-20 22:04,2020-11-14 0:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2008.03525,,/Users/angelica/Zotero/storage/7NCUQLDG/2008.html; /Users/angelica/Zotero/storage/SVD29Q5A/Arenz and Neumann - 2020 - Non-Adversarial Imitation Learning and its Connect.pdf,,Other-org; NotSafety,Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Information Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Viewing imitation learning as a distribution matching problem has become more popular in recent years (see <@Value-Dice@>(@Imitation Learning via Off-Policy Distribution Matching@) / <@I2L@>(@State-only Imitation with Transition Dynamics Mismatch@)). However, the authors in this paper argue that such methods are unstable due to their formulation as saddle-point problems which means they have weak convergence guarantees due to the assumption that the policy is slowly updated. In this paper, the authors reformulate <@Adversarial IRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@) as a non-adversarial problem allowing for much stronger convergence guarantees to be proved. In particular, the authors derive a lower-bound on the discrimination reward which allows for larger policy updates and then introduce a method to iteratively tighten this bound. They also build on prior work for value-dice and derive a soft actor-critic algorithm (ONAIL) that they evaluate on a variety of control tasks."
978YC5F5,manuscript,2019,"Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2019-05-25,2019-12-18 1:48,2020-12-20 20:58,2019-07-11 18:47,,,,,,,Adversarial Policies,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1905.10615,,,,BERI; TechSafety; CHAI,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This work demonstrates the existence of _adversarial policies_ of behaviour in high-dimensional, two-player zero-sum games. Specifically, they show that adversarially-trained agents (""Adv""), who can only affect a victim's observations of their (Adv's) states, can act in ways that confuse the victim into behaving suboptimally.

An adversarial policy is trained by reinforcement learning in a single-player paradigm where the victim is a black-box fixed policy that was previously trained via self-play to be robust to adversarial attacks. As a result, the adversarial policies learn to push the observations of the victim outside the training distribution, causing the victim to behave poorly. The adversarial policies do not actually behave intelligently, such as blocking or tackling the victim, but instead do unusual things like spasming in a manner that appears random to humans, curling into a ball or kneeling.

Further experiments showed that if the victim's observations of the adversary were removed, then the adversary was unable to learn such an adversarial policy. In addition, the victim's network activations were very different when playing against an adversarial policy relative to playing against a random or lifeless opponent. By comparing two similar games where the key difference was the number of adversary dimensions being observed, they showed that such policies were easier to learn in higher-dimensional games."
JYPN7CP2,manuscript,2019,"Szlam, Arthur; Gray, Jonathan; Srinet, Kavya; Jernite, Yacine; Joulin, Armand; Synnaeve, Gabriel; Kiela, Douwe; Yu, Haonan; Chen, Zhuoyuan; Goyal, Siddharth; Guo, Demi; Rothermel, Danielle; Zitnick, C. Lawrence; Weston, Jason",Why Build an Assistant in Minecraft?,,,,,http://arxiv.org/abs/1907.09273,"In this document we describe a rationale for a research program aimed at building an open ""assistant"" in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.",2019-07-25,2019-12-16 22:39,2020-12-21 18:33,2019-12-16 22:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1907.09273,,/Users/angelica/Zotero/storage/SEM85CKP/1907.html; /Users/angelica/Zotero/storage/WBGHQ65D/Szlam et al. - 2019 - Why Build an Assistant in Minecraft.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This position paper proposes a new challenge for AI research: building a bot that can provide assistance in [Minecraft](https://www.minecraft.net/en-us/) (creative mode). A [companion paper](https://research.fb.com/wp-content/uploads/2019/07/CraftAssist-A-Framework-for-Dialogue-enabled-Interactive-Agents-v3.pdf) presents an initial setup for such an agent.

The main goal here is to advance natural language understanding, intent inference and instruction following. As a result, there is no formal specification like a reward function -- in their own words, ""the ultimate goal of the bot is to be a useful and fun assistant in a wide variety of tasks specified and evaluated by human players"". They chose Minecraft in particular partly because it has a very rich space of _tasks_, even though the _execution_ of any given task is relatively straightforward. They script many low level policies to automate this execution in order to make learning easier (for example, they have policies to navigate to a location or to build specified structures) and focus the learning challenge on figuring out what the user wants.

The current version of the bot takes dialogue from the user and uses a neural model to parse it into an _action dictionary_ that unambiguously specifies what the agent should do -- I think this neural model is the main thing to be learned. There are a bunch of details on how the rest of the modules work as well. They have also released three datasets: a semantic parsing dataset that associates instructions with action dictionaries, a house dataset that has trajectories where a human builds a house, and a semantic segmentation dataset that labels various parts of houses."
BE6W2FT5,manuscript,2020,"Milli, Smitha; Belli, Luca; Hardt, Moritz",From Optimizing Engagement to Measuring Value,,,,,http://arxiv.org/abs/2008.12623,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of ""value"" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".",2020-08-20,2020-11-14 0:59,2020-12-20 21:11,2020-11-14 0:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2008.12623,,/Users/angelica/Zotero/storage/VPK8L455/2008.html; /Users/angelica/Zotero/storage/VGJCBARX/Milli et al. - 2020 - From Optimizing Engagement to Measuring Value.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Social and Information Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper takes a stab at creating a better objective for existing recommender systems than engagement, in a way that could be applied at existing companies like Twitter. The basic approach is to treat the variable to be optimized (user value) as a latent variable, and use probabilistic inference to infer how likely it is that a particular recommendation was valuable.

Usually a major challenge with such an approach is specifying the _observation model_: how the observed data is caused by the latent variable. In the case of Twitter, this would require you to answer questions like, “if the user does not value a tweet, how likely is a user to hit the like button anyway?” This is a hard question to answer, since perhaps users like tweets in order to stop conversations, or because they are addicting at the moment but are not actually valuable, etc.

One simple heuristic is to take two datasets where we know one dataset has more valuable recommendations than the other. Differences in user behavior between these datasets can then be assumed to be correlations with value. The authors provide a quantitative method for inferring the observation model from such datasets, which I won’t go into here since it is primarily a heuristic baseline. One obvious problem is that if the “better” dataset was produced by optimizing (say) clicks, then the clicks may have increased for reasons other than improved value, but this heuristic approach will attribute the entire increase to improved value.

How can we do better? The key insight of this paper is that if you have a bunch of historical data, then you can get a lot of mileage by identifying an _anchor_: a type of feedback that when given provides unequivocal evidence of the latent value. On Twitter, this is taken to be the “See Less Often” (SLO) button: if this is clicked, then we know with effective certainty that this was not valuable, regardless of any other actions the user took. The connection between value and other behaviors such as liking a tweet can then be inferred by looking at the connection between those behaviors and the anchor, which we can estimate from historical data.

Formally, the authors assume access to a graph describing the relationships between the various possible behaviors (almost all of which have the latent value V as a parent). One of these is identified as the anchor node A, for which P(V = 1 | A = 1) is assumed to be known and independent of all other variables. However, P(V = 1 | A = 0) is not independent of other variables: intuitively, if the SLO button is _not_ clicked, then we need to fall back to looking at other variables to estimate value.

The authors then show that under some reasonable assumptions on the anchor variable, if you have a dataset of historical data to estimate P(A, B) (where B consists of all the other tracked behaviors), then instead of specifying observation models P(B | V) for all behaviors, you only need to specify observation models for the parents of A, that is P(parents(A) | V). Everything else is uniquely determined, allowing us to calculate our final objective P(V | A, B). (There are algorithmic details on how to do this efficiently; see the paper for details.) In this case, they use the heuristic method outlined above to estimate P(parents(A) | V).

They unfortunately don’t have a great way to evaluate their method: they clearly can’t evaluate it by seeing if it leads to higher clicks, since the whole point was to move away from clicks as an optimization target. (I assume a user study on Twitter was infeasible.) Their primary form of evaluation is to run the model and report the learned probabilities, and show that they seem reasonable, whereas those output by a Naive Bayes model do not."
,manuscript,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2019-12-18 1:12,2020-12-20 21:22,2019-12-18 1:12,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1805.08882,,/Users/angelica/Zotero/storage/BSIUB8HB/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf,,TechSafety; CHAI,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper tackles multi-task inverse reinforcement learning, where there are multiple related tasks which have similar rewards, and the goal is to infer the reward for a new task after having already seen demonstrations for previous tasks. In the tabular setting, where we can enumerate all of the states and actions, they adapt Maximum Causal Entropy IRL to the multi-task setting. To make any progress, it is necessary to have some model of how the tasks are related -- in this work, they assume that the reward weights are close to each other, and so they penalize the L2 distance from the reward weights to the mean weights across tasks. Experiments show that this approach can learn a new task reward in 1-2 trajectories, while learning from scratch takes 50+ trajectories. They then consider how to generalize to continuous control environments such as MountainCar. They propose an algorithm that applies <@Reptile@>(@Reptile: A Scalable Meta-Learning Algorithm@) to <@adversarial IRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@), with the goal of learning a good initialization of the reward neural net which can quickly move to the correct reward function given data from a new task. This works well when the policy is unimodal (eg. for MountainCar, a policy that always goes left or always goes right), but not when the policy is multimodal (eg. you have to go either left or right depending on the color of the flag). Experiments suggest that this is because adversarial IRL does not do well with a multimodal policy. Meta-AIRL would require AIRL to produce good results on multiple environments -- if even one of the environments has a bad policy, there's a garbage input to meta-AIRL, which then tanks its performance."
,manuscript,2020,"Marcus, Gary",The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,,,,,http://arxiv.org/abs/2002.06177,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.",2020-02-19,2020-09-05 19:12,2020-12-20 22:15,2020-09-05 19:12,,,,,,,The Next Decade in AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2002.06177,,/Users/angelica/Zotero/storage/FN3CVXUM/Marcus - 2020 - The Next Decade in AI Four Steps Towards Robust A.pdf; /Users/angelica/Zotero/storage/7D6YDGI3/2002.html,,Other-org; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper suggests a few directions which would allow us to build more _robust_ AI systems with better ""understanding"" of the world: specifically, it highlights **symbol manipulation, encoded knowledge, reasoning, and cognitive models** as areas of research for the next decade.

See also [Import AI #187](https://jack-clark.net/2020/03/02/import-ai-187-real-world-robot-tests-at-cvpr-all-hail-the-molecule-transformer-the-four-traits-needed-for-smarter-ai-systems/) and [Matthew Barnett's summary](https://www.lesswrong.com/posts/CeJs4rPgPtJPNqLMt/gary-marcus-four-steps-towards-robust-artificial)."
,manuscript,2020,"Guan, Lin; Verma, Mudit; Kambhampati, Subbarao",Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning,,,,,http://arxiv.org/abs/2006.14804,"Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guidance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. The usual human guidance in HRL is binary evaluative ""good"" or ""bad"" signal for queried states and actions. However, this suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (Explanation Augmented Feedback) which allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback. EXPAND employs a state perturbation approach based on the state salient information to augment the feedback, reducing the number of human feedback signals required. We choose two domains to evaluate this approach, Taxi and Atari-Pong. We demonstrate the effectiveness of our method on three metrics, environment sample efficiency, human feedback sample efficiency, and agent gaze. We show that our method outperforms our baselines. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information gives a boost in performance.",2020-07-16,2020-08-28 17:26,2020-12-21 18:08,2020-08-28 17:26,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.14804,,/Users/angelica/Zotero/storage/BVC7424T/Guan et al. - 2020 - Explanation Augmented Feedback in Human-in-the-Loo.pdf; /Users/angelica/Zotero/storage/EJ73U4WX/2006.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper starts from a similar position as the highlighted paper: that we can improve on algorithms by having humans provide different kinds of feedback that help with learning. They ask humans to provide “explanations” to improve sample efficiency in deep RL, which in this case means asking a human to segment parts of the image observation that are important (similar to a saliency map). They use this to define auxiliary losses that incentivize the agent to be invariant to augmentations of the irrelevant parts of the image. Their empirical evaluation shows improvements in sample efficiency relative to simple good/bad evaluative feedback."
,manuscript,2019,"Edwards, Ashley D.; Isbell, Charles L.",Perceptual Values from Observation,,,,,https://arxiv.org/abs/1905.07861v1,"Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.",2019-05-20,2020-11-14 1:26,2020-12-20 22:06,2020-11-14 1:26,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/angelica/Zotero/storage/MPAIM7HG/Edwards and Isbell - 2019 - Perceptual Values from Observation.pdf; /Users/angelica/Zotero/storage/ESJBB7B7/1905.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a technique for learning from raw expert-trajectory observations by assuming that the last state in the trajectory is the state where the goal was achieved, and that other states have value in proportion to how close they are to a terminal state in demonstration trajectories. They use this as a grounding to train models predicting value and action-value, and then use these estimated values to determine actions. "
,manuscript,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracking Gradient Descent,,,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TrackIn that computes the influence of a training example on a prediction made by the model, by tracking how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TrackIn via a combination of a few key ideas: (a) a first-order approximation to the exact computation, (b) using random projections to speed up the computation of the first-order approximation for large models, (c) using saved checkpoints of standard training procedures, and (d) cherry-picking layers of a deep neural network. An experimental evaluation shows that TrackIn is more effective in identifying mislabelled training examples than other related methods such as influence functions and representer points. We also discuss insights from applying the method on vision, regression and natural language tasks.",2020-07-13,2020-09-05 17:05,2020-12-21 18:25,2020-09-05 17:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2002.08484,,/Users/angelica/Zotero/storage/3TYCLPFP/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracking Gra.pdf; /Users/angelica/Zotero/storage/8XN5TFHS/2002.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents the TrackIn method for tracking the influence of training datapoints on the loss on a test datapoint. The purpose of the method is to discover influential training points for decisions made on the testing set. This is defined (loosely) for a training point **x** and test point **z** as the total change in loss on **z** caused by training on **x**. They present several approximations and methods for calculating this quantity efficiently, *allowing them to scale their method to ResNet 50 models trained on ImageNet*

The standard method of evaluation for these kinds of methods is finding mislabelled examples in the training dataset. Mislabelled examples are likely to have a strong positive influence on their own loss (strong as they're outliers, and positive as they'll reduce their own loss). Sorting the training dataset in decreasing order of this self-influence, we should hence expect to see more mislabelled examples at the beginning of the list. We can measure what proportion of mislabelled examples is present in each different initial segments of the list. The authors perform this experiment on CiFAR, first training a model to convergence, and then mislabelling 10% of the training set as the next highest predicted class, and then retraining a new model on which TrackIn is run. *When compared to the two previous methods from the literature (Influence Functions and Representer Points), TrackIn recovers more than 80% of the mislabelled data in the first 20% of the ranking, whereas the other methods recover less than 50% at the same point. For all segments TrackIn does significantly better.*

They demonstrate the method on a variety of domains, including NLP tasks and vision tasks. The influential examples found seem reasonable, but there's no quantification of these results."
,manuscript,2020,"Rivera, Corban G.; Lyons, Olivia; Summitt, Arielle; Fatima, Ayman; Pak, Ji; Shao, William; Chalmers, Robert; Englander, Aryeh; Staley, Edward W.; Wang, I.-Jeng; Llorens, Ashley J.",TanksWorld: A Multi-Agent Environment for AI Safety Research,,,,,http://arxiv.org/abs/2002.11174,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work.",2020-02-25,2020-09-05 18:48,2020-12-21 18:27,2020-09-05 18:48,,,,,,,TanksWorld,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2002.11174,,/Users/angelica/Zotero/storage/ATW7PHTZ/Rivera et al. - 2020 - TanksWorld A Multi-Agent Environment for AI Safet.pdf; /Users/angelica/Zotero/storage/CXJSRG7S/2002.html,,Other-org; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents TanksWorld, a simulation environment that attempts to illustrate three important aspects of real-world AI safety challenges: competing performance objectives, human-machine learning, and multi-agent competition. TanksWorld consists of two teams of N vs. N tanks. Tanks move and shoot while navigating in a closed arena with obstacles. Tanks are rewarded for killing opponent tanks and penalized for killing neutral and allied tanks according to a specified reward function. Each tank is controlled by either its own AI or a special policy meant to mimic a 'human' teammate. Each individual tank can only see a small portion of its environment, and must communicate with other teammates to gain more information. The following parameters can be varied to emphasize different research challenges:
- The communication range between tanks -- meant to represent environmental uncertainty.
- The number of neutral tanks and obstacles -- meant to represent the extent to which tanks must care about 'safety', i.e. avoid collateral damage.
- The control policies of teammates -- meant to represent the variability of human-machine teams."
,manuscript,2020,"Lynch, Corey; Sermanet, Pierre",Grounding Language in Play,,,,,http://arxiv.org/abs/2005.07648,"Natural language is perhaps the most versatile and intuitive way for humans to communicate tasks to a robot. Prior work on Learning from Play (LfP) [Lynch et al, 2019] provides a simple approach for learning a wide variety of robotic behaviors from general sensors. However, each task must be specified with a goal image---something that is not practical in open-world environments. In this work we present a simple and scalable way to condition policies on human language instead. We extend LfP by pairing short robot experiences from play with relevant human language after-the-fact. To make this efficient, we introduce multicontext imitation, which allows us to train a single agent to follow image or language goals, then use just language conditioning at test time. This reduces the cost of language pairing to less than 1% of collected robot experience, with the majority of control still learned via self-supervised imitation. At test time, a single agent trained in this manner can perform many different robotic manipulation skills in a row in a 3D environment, directly from images, and specified only with natural language (e.g. ""open the drawer...now pick up the block...now press the green button...""). Finally, we introduce a simple technique that transfers knowledge from large unlabeled text corpora to robotic learning. We find that transfer significantly improves downstream robotic manipulation. It also allows our agent to follow thousands of novel instructions at test time in zero shot, in 16 different languages. See videos of our experiments at language-play.github.io",2020-05-15,2020-08-31 18:20,2020-12-21 18:19,2020-08-31 18:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2005.07648,,/Users/angelica/Zotero/storage/W35Q9F4C/Lynch and Sermanet - 2020 - Grounding Language in Play.pdf; /Users/angelica/Zotero/storage/FNXJPVLS/2005.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a new approach to learning to follow natural language human instruction in a robotics setting. It builds on similar ideas to <@Learning Latent Plans from Play@>, in that it uses unsupervised ""play"" data (trajectories of humans playing on the robot with no goal in mind).

The paper combines several ideas to enable training a policy which can follow natural language instructions with only limited human annotations.
* In *Hindsight Instruction Pairing*, human annotators watch small trajectories from the play data, and label them with the instruction which is being completed in the clip. This instruction can take any form, and means we don't need to choose the instructions and ask humans to perform specific tasks.
* *Multicontext Imitation Learning* is a method designed to allow goal-conditioned policies to be learned with multiple different types of goals. For example, we can have lots of example trajectories where the goal is an end state image (as these can be generated automatically without humans), and just a small amount of example trajectories where the goal is a natural language instruction (gathered using *Hindsight Instruction Pairing*). The approach is to learn a goal embedding network for each type of goal specification, and a single shared policy which takes the goal embedding as input.

Combining these two methods enables them to train a policy and embedding networks end to end using imitation learning from a large dataset of (trajectory, image goal) pairs and a small dataset of (trajectory, natural language goal) pairs. The policy can follow very long sequences of natural language instructions in a fairly complex grasping environment with a variety of buttons and objects. Their method performs better than the Learning from Play (LfP) method, even though LfP uses a goal image as the goal conditioning, instead of a natural language instruction.

Further, they propose that instead of learning the goal embedding for the natural language instructions, they use a pretrained large language model to produce the embeddings. This improves the performance of their method over learning the embedding from scratch, which the authors claim is the first example of the knowledge in large language models being transferred and improving performance in a robotics domain. This model also performs well when they create purposefully out of distribution natural language instructions (i.e. with weird synonyms, or google-translated from a different language)."
,manuscript,2020,"Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V.",Towards a Human-like Open-Domain Chatbot,,,,,http://arxiv.org/abs/2001.09977,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2020-02-27,2020-09-07 18:19,2020-12-21 17:53,2020-09-07 18:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000057  arXiv: 2001.09977,,/Users/angelica/Zotero/storage/WDD2CM87/Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf; /Users/angelica/Zotero/storage/BHZLQRPI/2001.html,,Other-org; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a chatbot called Meena that reaches near human-level performance for measures of human likeness. The authors mined social media to find 341 GB of public domain conversations, and trained an [evolved transformer](https://arxiv.org/abs/1901.11117) on those conversations. To test its performance, they devised a metric they call Sensibility and Specificity (SSA) which measures how much sense the chatbot's responses make in context, as well as whether they were specific. SSA was tightly correlated with perplexity and a subjective measure of human likeness, suggesting that optimizing for perplexity will translate to greater conversational ability. Meena substantially improved on the state of the art, including both hand-crafted bots like [Mitsuku](https://en.wikipedia.org/wiki/Mitsuku) and the neural model [DialoGPT](https://arxiv.org/abs/1911.00536), though it still falls short of human performance. You can read some conversation transcrips [here](https://github.com/google-research/google-research/blob/master/meena/meena.txt); many of the responses from Meena are very human-like.

See also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)"
,manuscript,2020,"Henighan, Tom; Kaplan, Jared; Katz, Mor; Chen, Mark; Hesse, Christopher; Jackson, Jacob; Jun, Heewoo; Brown, Tom B.; Dhariwal, Prafulla; Gray, Scott; Hallacy, Chris; Mann, Benjamin; Radford, Alec; Ramesh, Aditya; Ryder, Nick; Ziegler, Daniel M.; Schulman, John; Amodei, Dario; McCandlish, Sam",Scaling Laws for Autoregressive Generative Modeling,,,,,http://arxiv.org/abs/2010.14701,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",2020-11-05,2020-12-19 4:02,2020-12-20 22:11,2020-12-19 4:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 0  arXiv: 2010.14701,,/Users/angelica/Zotero/storage/9YC4HDGP/2010.html; /Users/angelica/Zotero/storage/T6MMZPI9/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf,,NotSafety; AmbiguosSafety; Open-AI,Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper looks at scaling laws for generative Transformer models of images (predicting pixels or parts of image encodings), videos (predicting frames of image encodings), multimodal image <-> text (predicting captions based on images or images based on captions), and mathematical problem solving (predicting answers to auto-generated questions about algebra, arithmetic, calculus, comparisons, integer properties, measurement, polynomials, and probability). The authors find that:

- Cross-entropy loss as a function of compute follows a power law + constant in all these data modalities (just as it does <@in language@>(@Scaling Laws for Neural Language Models@)). Information theoretically, this can be interpreted as scaling a 'reducible loss' which estimates the KL divergence between the true and model distributions, and an 'irreducible loss' which estimates the entropy of the true data distribution.
- Performance on ImageNet classification fine-tuned from their generative image model also follows such a power law, whereas ImageNet classification trained *from scratch* actually gets worse with sufficiently large model sizes. Interestingly, this classification power law continues even past model sizes where the generative cross-entropy loss starts bending as a result of irreducible loss. The authors conclude that approaching the irreducible loss for some dataset does not necessarily indicate diminishing returns for representation quality or semantic content.
- Optimal model size as a function of compute follows a power law with an exponent very close to ~0.7 for all data modalities they've studied so far. This implies that in the current compute regime, as compute budgets grow, it's best to devote a majority of compute towards making models bigger and a minority towards training on more data.
- Larger models perform better on extrapolating to math problems more difficult than those seen in training, but only insofar as they do better on the training distribution (no benefits to 'strong generalization').
- Larger models are able to take advantage of more multimodal information, but the scaling is extremely slow-- a 1-billion-parameter model uses 10% of the information in a caption to define an image, while using 20% of the information would require a 3-trillion-parameter model.

As in the <@language models paper@>(@Scaling Laws for Neural Language Models@), extrapolating the steep power laws found for optimally-used compute seems to eventually paradoxically result in loss lower than the bound given by shallower power laws for optimally-used training data. The authors offer a potential hypothesis for resolving this inconsistency-- in the regime of less compute and smaller model sizes, increasing model size effectively increases the amount of information you extract from each data point you train on, resulting in the steepness of the current compute law. As compute increases past a certain point, however, the amount of information extracted per data point approaches the maximum amount possible, so the curve switches to a shallower regime and marginal compute should be used increasingly on dataset increases rather than model size increases. If this hypothesis is true, we should eventually expect the scaling laws for compute to bend towards laws set by dataset size, and perhaps should think they will ultimately be set by trends for overfitting (see [this post](https://www.alignmentforum.org/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight) for another explanation of this)."
,manuscript,2018,"Arora, Saurabh; Doshi, Prashant; Banerjee, Bikramjit",A Framework and Method for Online Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.07871,"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",2018-05-20,2020-11-14 0:53,2020-12-20 20:55,2020-11-14 0:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1805.07871,,/Users/angelica/Zotero/storage/ZGN9GCXC/1805.html; /Users/angelica/Zotero/storage/3UEZDY95/Arora et al. - 2018 - A Framework and Method for Online Inverse Reinforc.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces Incremental Inverse Reinforcement Learning (I2RL), where the agent continually gets new demonstrations from an expert, and has to update the estimate of the reward function in real time. The running example is a robot that has to navigate to a goal location without being seen by two guards that are patrolling. The robot needs to infer the rewards of the two guards in order to predict what they will do and plan around them. Since the guards are sometimes out of sight, we get demonstrations _with occlusion_, that is, some of the states in the demonstrations are hidden.

In the batch setting, this is solved with Latent Maximum Entropy IRL. To deal with occluded states Z, we define a probability distribution Pr(Z | Y, theta), where Y is the visible states and theta is the reward weights. Then, you can use expectation maximization to find theta -- in the expectation step, you compute feature expectations of the demonstrations (taking an expectation over hidden states Z), and in the maximization step, you compute the reward weights using the feature expectations as in standard maximum entropy IRL. The authors show how to extend this algorithm to the incremental setting where you only keep the reward weights, the feature expectations, and the number of past demonstrations as statistics. They show some convergence guarantees and evaluate on their running example of a robot that must evade guards."
,manuscript,2020,"Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng",GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,,,http://arxiv.org/abs/2006.16668,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2020-06-30,2020-08-28 18:05,2020-12-21 18:17,2020-08-28 18:05,,,,,,,GShard,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2006.16668,,/Users/angelica/Zotero/storage/ZM6Z55RI/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Comp.pdf; /Users/angelica/Zotero/storage/4Q8XLAAU/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces GShard, a module that makes it easy to write parallel computation patterns with minimal changes to existing model code. GShard automatically does a lot of the work of splitting computations across machines, enabling the easy creation of much larger models than before.

The authors use GShard to train a 600 billion parameter multilingual Transformer translation model that's wide, rather than deep (36 layers). They use a ""mixture of experts"" model where some of the individual feed-forward networks in the Transformer are replaced with a set of feed-forward networks-- each one an ""expert"" in some part of the translation. The experts are distributed across different machines, and the function for sending inputs to experts is learned, with each input being sent to the top two most relevant experts. Since each expert only has to process a fraction of all the inputs, the amount of computation needed is dramatically less than if every input were fed through a single, larger network. This decrease in needed computation comes with a decrease in the amount of weight sharing done by the network.

The paper compares the 600 billion parameter model's performance to several other smaller models as well as a 96-layer deep model with only 2.3 billion parameters. For the wide networks, the authors find that in general, larger models do better, but that at some point the larger model starts doing worse for very ""low-resource"" languages-- languages that don't have much training data available. The authors argue that this is because the low-resource languages benefit from ""positive language transfer"", an effect where weights encode knowledge learned from training on other languages that can then be applied to the low-resource ones. As you increase the number of experts in the wide model past a certain point, the amount of training that each expert does decreases, so there's less positive language transfer to low-resource languages within each expert.

They also find that deeper networks are more sample efficient, reaching better test error with the same amount of training examples, but are less computationally efficient (given current constraints). The 600 billion parameter, 36-layer model takes 22.4 TPU core years and 4 days to train, reaching a score on the BLEU benchmark of 44.3. The 2.3 billion parameter, 96-layer model takes 235 TPU core years and 42 days to train, reaching a score on the BLEU benchmark of 36.9."
,manuscript,2018,"Clavera, Ignasi; Rothfuss, Jonas; Schulman, John; Fujita, Yasuhiro; Asfour, Tamim; Abbeel, Pieter",Model-Based Reinforcement Learning via Meta-Policy Optimization,,,,,http://arxiv.org/abs/1809.05214,"Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.",2018-09-13,2019-12-18 2:53,2020-12-20 21:21,2019-12-18 2:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000070  arXiv: 1809.05214,,/Users/angelica/Zotero/storage/WZ7NJUR3/1809.html; /Users/angelica/Zotero/storage/LZR8VRWA/Clavera et al. - 2018 - Model-Based Reinforcement Learning via Meta-Policy.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a new approach to model-based RL, called Model-Based Meta-Policy-Optimisation (MB-MPO), which doesn't require the dynamics models to be as accurate. It does so by learning an ensemble of dynamics models each trained on different subsets of the data, and then using meta-learning (specifically MAML) to find a policy which adapts well to any of these models within one step of gradient descent. This approach is a form of regularisation of policy learning, and achieves much greater sample efficiency without compromising performance: MB-MPO does just as well as top model-free algorithms in various Mujoco continuous-control environments, while requiring between 10 and 100 times fewer samples. Experiments suggest that it does so by having higher plasticity in regions with high dynamics model uncertainty. See also [Import AI](https://jack-clark.net/2018/09/25/import-ai-113-why-satellitesai-gives-us-a-global-eye-industry-pays-academia-to-say-sorry-for-strip-mining-it-and-kindred-researchers-seek-robot-standardization/)."
,manuscript,2017,"Doshi-Velez, Finale; Kim, Been",Towards A Rigorous Science of Interpretable Machine Learning,,,,,http://arxiv.org/abs/1702.08608,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017-03-02,2020-08-31 17:55,2020-12-21 18:04,2020-08-31 17:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000836  arXiv: 1702.08608,,/Users/angelica/Zotero/storage/JT94RUDJ/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf; /Users/angelica/Zotero/storage/2N27VUEW/1702.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper from 2017 discusses the field of interpretability research, and how it can be made more rigorous and well-defined. The authors first highlight the problem of defining interpretability in the first place - they don't have a resolution to this problem, but suggest that we can think of interpretability in terms of what it's used for. They claim that interpretability is used for confirming other important desiderata in ML systems, which stem from an incompleteness in the problem formalization. For example, if we want a system to be unbiased but aren't able to formally specify this in the reward function, or the reward we're optimising for is only a proxy of the true reward, then we could use interpretability to inspect our model and see whether it's reasoning how we want it to.

The authors next move on to discussing how we can evaluate interpretability methods, providing a taxonomy of different evaluation methods: Application-grounded is when the method is evaluated in the context it will actually be used in, by real humans (i.e. doctors getting explanations for AI diagnoses); Human-grounded is about conducting simpler human-subject experiments (who are perhaps not domain experts) using possibly simpler tasks than what the intended purpose of the method is; Functionally-grounded is where no humans are involved in the experiments, and instead some formal notion of interpretability is measured for the method to evaluate its quality. Each of these evaluation methods can be used in different circumstances, depending on the method and the context it will be used in.

Finally, the authors propose a data-driven approach to understanding the factors which are important in interpretability. They propose to try and create a dataset of applications of machine learning models to tasks, and then analyse this dataset to find important factors. They list some possible task- and method- related factors, and then conclude with recommendations to researchers doing interpretability."
,manuscript,2020,"Scheller, Christian; Schraner, Yanick; Vogel, Manfred",Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft,,,,,http://arxiv.org/abs/2003.06066,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",2020-03-12,2020-09-05 18:02,2020-12-21 18:42,2020-09-05 18:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2003.06066,,/Users/angelica/Zotero/storage/XR6TWEI8/Scheller et al. - 2020 - Sample Efficient Reinforcement Learning through Le.pdf; /Users/angelica/Zotero/storage/HX7N352C/2003.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper explains the technique used by the 3rd place team in the MineRL competition (summarized above). They used behavior cloning to train their neural net on human demonstrations, and then used reinforcement learning (specifically, IMPALA) with experience replay and advantage clipping to improve. There are more details about their architecture and design choices in the paper."
,manuscript,2020,"Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario",Scaling Laws for Neural Language Models,,,,,http://arxiv.org/abs/2001.08361,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",2020-01-22,2020-08-21 20:19,2020-12-20 22:11,2020-08-21 20:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2001.08361,,/Users/angelica/Zotero/storage/MGNNQMVW/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/angelica/Zotero/storage/GLKNCSKK/2001.html,,NotSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper empirically measures the effect of scaling model complexity, data, and computation on the cross entropy loss for neural language models. A few results that I would highlight are:

_Performance depends strongly on scale, weakly on model shape:_ Loss depends more strongly on the number of parameters, the size of the dataset, and the amount of compute used for training than on architecture hyperparameters.

_Smooth power laws:_ All three of these show power-law relationships that don’t flatten out even at the highest performance they reached.

_Sample efficiency:_ Larger models are more efficient than small models in both compute and data. For maximum computation efficiency, it is better to train large models and stop before convergence.

There are lots of other interesting conclusions in the paper not included here; section 1.1 provides a very nice one page summary of these conclusions, which I'd recommend you read for more information."
,manuscript,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),,,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2020-11-14 0:58,2020-12-21 18:04,2020-11-14 0:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 16  arXiv: 1901.00064,,/Users/angelica/Zotero/storage/22Y3KWDY/1901.html; /Users/angelica/Zotero/storage/BMB5Y843/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf,,Other-org; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper discusses some impossibility theorems related to the Repugnant conclusion in population ethics (i.e. theorems showing that no moral theory simultaneously satisfies certain sets of intuitively desirable properties). Peter argues that in the context of AI it's best to treat these theorems as uncertainty results, either by allowing incommensurate outcomes or by allowing probabilistic moral judgements. He hypothesises that ""the emergence of instrumental subgoals is deeply connected to moral certainty"", and so implementing uncertain objective functions is a path to making AI safer."
,manuscript,2020,"Chen, Xinlei; Fan, Haoqi; Girshick, Ross; He, Kaiming",Improved Baselines with Momentum Contrastive Learning,,,,,http://arxiv.org/abs/2003.04297,"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework. With simple modiﬁcations to MoCo—namely, using an MLP projection head and more data augmentation—we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",2020-03-09,2020-08-31 18:58,2020-12-21 18:01,2020-08-31 18:58,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000042  ACC: 42  arXiv: 2003.04297,,/Users/angelica/Zotero/storage/AQN4AM47/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper applies the insights from the SimCLR paper to the MoCo framework: it adds an extra hidden layer on top of the representations while training on the contrastive loss, and adds the blur data augmentation. This results in a new SOTA on self-supervised representation learning for images."
,manuscript,2020,"Kostrikov, Ilya; Yarats, Denis; Fergus, Rob",Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels,,,,,http://arxiv.org/abs/2004.13649,"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC) [22], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC’s performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [23, 38, 24] methods and recently proposed contrastive learning [50]. Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN [43] and signiﬁcantly improve its data-efﬁciency on the Atari 100k [31] benchmark. An implementation can be found at https://sites. google.com/view/data-regularized-q.",2020-06-11,2020-08-31 19:00,2020-12-21 18:16,2020-08-31 19:00,,,,,,,Image Augmentation Is All You Need,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000014  ACC: 14  arXiv: 2004.13649,,/Users/angelica/Zotero/storage/SY5MM8DU/Kostrikov et al. - 2020 - Image Augmentation Is All You Need Regularizing D.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper applies data augmentation to Q-learning algorithms, again without a contrastive loss. Specifically, they suggest that the Q-values of states should be invariant to data augmentations (e.g. random translations, which is what they use), and so any time we need to estimate a Q-value, we can reduce the variance of this estimate by sampling multiple data augmentations of the state, and averaging the predicted Q-values for each of them. They apply this to Soft Actor-Critic (SAC) and find that it significantly improves results."
,manuscript,2018,"Pereira, Ramon Fraga; Meneguzzi, Felipe",Heuristic Approaches for Goal Recognition in Incomplete Domain Models,,,,,http://arxiv.org/abs/1804.05917,"Recent approaches to goal recognition have progressively relaxed the assumptions about the amount and correctness of domain knowledge and available observations, yielding accurate and efficient algorithms. These approaches, however, assume completeness and correctness of the domain theory against which their algorithms match observations: this is too strong for most real-world domains. In this paper, we develop goal recognition techniques that are capable of recognizing goals using \textit{incomplete} (and possibly incorrect) domain theories. We show the efficiency and accuracy of our approaches empirically against a large dataset of goal and plan recognition problems with incomplete domains.",2018-04-16,2020-11-14 0:45,2020-12-20 21:13,2020-11-14 0:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1804.05917,,/Users/angelica/Zotero/storage/QSH9H5GP/1804.html; /Users/angelica/Zotero/storage/86ZFTB49/Pereira and Meneguzzi - 2018 - Heuristic Approaches for Goal Recognition in Incom.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The planning community works on algorithms that can plan given a _symbolic_ definition of the environment, how actions affect the environment, and the goal state; analogous to reinforcement learning. The task of inverting the optimal behavior to infer the goal is called goal recognition or plan recognition (analogous to inverse reinforcement learning). This paper looks at goal recognition where the models of the world are incomplete, so that there are _possible_ preconditions and effects of actions. They extract potential _landmarks_ from the plan, which are things (facts or actions) that must happen in order to achieve the goal, and then suggest two heuristics for how to use the landmarks to rank among possible goals."
XRKE5R8P,report,2020,"Flournoy, Michèle A; Haines, Avril; Chefitz, Gabrielle",Building Trust Through Testing,,,,,https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf,,2020,2022-01-30 4:48:44,2022-01-30 4:48:44,,,,,,,,,,,,,WestExec Advisors,,,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/2SCQ6DTV/Flournoy et al. - 2020 - Building Trust Through Testing.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSX2XFIQ,report,2020,"Fitzgerald, McKenna; Boddy, Aaron; Baum, Seth","2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",,,,,https://www.ssrn.com/abstract=3070741,,2020,2022-01-30 4:48:11,2022-01-30 4:48:11,2021-10-31 19:23:44,,,,,,,,,,,,Global Catastrophic Risk Institute,,en,,,,,DOI.org (Crossref),,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/DQQQHZAB/Baum - 2017 - A Survey of Artificial General Intelligence Projec.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9US7JVWW,report,2020,"Lohn, Andrew",Hacking AI,,,,,https://cset.georgetown.edu/publication/hacking-ai/,"Machine learning systems’ vulnerabilities are pervasive. Hackers and adversaries can easily exploit them. As such, managing the risks is too large a task for the technology community to handle alone. In this primer, Andrew Lohn writes that policymakers must understand the threats well enough to assess the dangers that the United States, its military and intelligence services, and its civilians face when they use machine learning.",2020-12,2022-01-30 4:47:49,2022-01-30 4:47:49,2021-10-31 18:59:26,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/5H3AC6WR/hacking-ai.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HUDHI962,report,2020,"Page, Michael; Aiken, Catherine; Murdick, Dewey",Future Indices,,,,,https://cset.georgetown.edu/publication/future-indices/,Foretell is CSET's crowd forecasting pilot project focused on technology and security policy. It connects historical and forecast data on near-term events with the big-picture questions that are most relevant to policymakers. This issue brief uses recent forecast data to illustrate Foretell’s methodology.,2020-10-19,2022-01-30 4:47:49,2022-01-30 4:47:49,2021-11-08 23:47:33,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/SEWFFFFP/future-indices.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75Q2IHR5,report,2020,"Althaus, David; Baumann, Tobias",Reducing long-term risks from malevolent actors,,,,,https://longtermrisk.org/reducing-long-term-risks-from-malevolent-actors/,"Summary Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history.  Malevolent individuals in positions of power could negatively affect humanity’s long-term trajectory by, for example, exacerbating international conflict or other broad risk factors. Malevolent humans with access to advanced technology—such as whole brain emulation […]",2020-07-07,2022-01-30 4:51:36,2022-01-30 4:51:36,2020-08-20 20:10:59,,,,,,,,,,,,Center on Long-Term Risk,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/8TP4B2UC/Althaus and Baumann - 2020 - Reducing long-term risks from malevolent actors.pdf; /Users/jacquesthibodeau/Zotero/storage/7NBW623R/reducing-long-term-risks-from-malevolent-actors.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3JEMBWP9,report,2020,"Clifton, Jesse","Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",,,,,https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf,,2020-03,2022-01-30 4:51:07,2022-01-30 4:51:07,2020-11-22 7:42:34,,,,,,,,,,,,Center on Long-Term Risk,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/8FM6BFXK/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6UT95U9,report,2019,"Kemp, Luke; Cihon, Peter; Maas, Matthijs M; Belfield, Haydn; Ó hÉigeartaigh, Seán; Leung, Jade; Cremer, Zoe",UN High-level Panel on Digital Cooperation: A Proposal for International AI Governance,,,,,https://digitalcooperation.org/wp-content/uploads/2019/02/Luke_Kemp_Submission-to-the-UN-High-Level-Panel-on-Digital-Cooperation-2019-Kemp-et-al.pdf,,2019,2022-01-30 4:50:26,2022-01-30 4:50:26,2020-12-12,,,,,,,,,,,,Centre for the Study of Existential Risk and Leverhulme Centre for the Future of Intelligence,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 3,,/Users/jacquesthibodeau/Zotero/storage/CP3XZSMU/Kemp et al. - 2019 - UN High-level Panel on Digital Cooperation A Prop.pdf,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CG5RTG7G,report,2018,"Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",,,,,http://arxiv.org/abs/1802.07228,"This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.",2018-02-20,2022-01-30 4:50:26,2022-01-30 4:50:26,2019-12-16 20:09:19,,,,,,,The Malicious Use of Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228,,/Users/jacquesthibodeau/Zotero/storage/TPDWWRCW/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/3WMW2XAM/1802.html; /Users/jacquesthibodeau/Zotero/storage/8DSHG3KJ/1802.html; /Users/jacquesthibodeau/Zotero/storage/VSTQKGMW/1802.html,,MetaSafety; CFI; CSER; FHI; Open-AI; BERI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2GPMSVD7,report,2019,"Cihon, Peter",Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development,,,,,,,2019,2022-01-30 4:50:08,2022-01-30 4:50:08,,,,,,,,Standards for AI Governance,,,,,Berkeley Existential Risk Initiative,,,,,,,Google Scholar,,ZSCC: 0000051,,/Users/jacquesthibodeau/Zotero/storage/GSWAIJG2/Cihon - 2019 - Standards for AI Governance International Standar.pdf,,MetaSafety; FHI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QHQHI65S,report,2019,"O’Keefe, Cullen; Candidate, J D",Stable Agreements in Turbulent Times: A Legal Toolkit for Constrained Temporal Decision Transmission,,,,,,,2019,2022-01-30 4:50:07,2022-01-30 4:50:07,,31,,,,,,,,,,,Berkeley Existential Risk Initiative,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6ZHFXKE4/O’Keefe and Candidate - Stable Agreements in Turbulent Times A Legal Tool.pdf,,MetaSafety; FHI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2IKXXPIH,report,2020,"Jayanti, Amritha; Avin, Shahar",It Takes a Village: The Shared Responsibility of 'Raising' an Autonomous Weapon,,,,,,"Expectations around future capabilities of lethal autonomous weapons systems (LAWS) have raised concerns for military risks, ethics, and accountability. The U.K.’s position, as presented among various international voices at the UN’s Convention on Certain Conventional Weapons (CCW) meetings, has attempted to address these concerns through a focused look at the weapons review process, humanmachine teaming or “meaningful human control” (see e.g. JCN1/18), and the ability of autonomous systems to adhere to the Rules of Engagement. Further, the U.K. has stated that the existing governance structures—both domestic and international—around weapons systems are sufficient in dealing with any concerns around the development, deployment, and accountability for emerging LAWS; there is no need for novel agreements on the control of these weapons systems. In an effort to better understand and test the U.K. position on LAWS, the Centre for the Study of Existential Risk has run a research project in which we interviewed experts in multiple relevant organisations, structured around a mock parliamentary inquiry of a hypothetical LAWS-related civilian death. The responses to this scenario have highlighted different, sometimes complementary and sometimes contradicting, conceptions of future systems, challenges, and accountability measures. They have provided rich ""on the ground” perspectives, while also highlighting key gaps that should be addressed by every military that is considering acquisition and deployment of autonomous and semi-autonomous weapon systems.",2020-11-10,2022-01-30 4:50:07,2022-01-30 4:50:07,,,,,,,,,,,,,Cornell University Press,,en,,,,,,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/QDMITJNV/Carpenter - 2014 - Lost Causes Agenda Vetting in Global Issue Networ.pdf,,MetaSafety; CSER; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIHZHVCW,report,2020,"O’Keefe, Cullen",How Will National Security Considerations Affect Antitrust Decisions in AI? An Examination of Historical Precedents,,,,,,,2020-07-07,2022-01-30 4:50:07,2022-01-30 4:50:07,,39,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/CNQBT7ZC/O’Keefe - How Will National Security Considerations Affect A.pdf,,MetaSafety; FHI; Open-AI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RIUZJB7,report,2014,"Bostrom, Nick","Hail mary, value porosity, and utility diversification",,,,,,,2014,2022-01-30 4:53:17,2022-01-30 4:53:17,,,,,,,,,,,,,Future of Humanity Institute,,,,,,,Google Scholar,,ZSCC: 0000018,,"/Users/jacquesthibodeau/Zotero/storage/PIQNHXSA/Bostrom - 2014 - Hail mary, value porosity, and utility diversifica.pdf",,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75IGTK3W,report,2008,"Sandberg, Anders; Bostrom, Nick",Global Catastrophic Risks Survey,,,,,,,2008,2022-01-30 4:53:17,2022-01-30 4:53:17,,,,,,,,,,,,,Future of Humanity Institute,Oxford University,,,,,,,,ZSCC: 0000056,,/Users/jacquesthibodeau/Zotero/storage/IWPM4BCR/gcr-report.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,2008-1,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIF96FJT,report,2016,"Cotton-Barratt, Owen; Farquhar, Sebastian; Halstead, John; Schubert, Stefan; Snyder-Beattie, Andrew",Global Catastrophic Risks 2016,,,,,http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/,"Global catastrophes sometimes strike. In 1918 the Spanish Flu killed as many as one in twenty people. There have been even more devastating pandemics - the Black Death and the 6th century Plague of Justinian may have each killed nearer to one in every six people on this earth. More recently, the Cub",2016-04-28,2022-01-30 4:53:17,2022-01-30 4:53:17,2020-12-13 19:41:24,108,,,,,,,,,,,Global Challenges Foundation,,en-US,,,,,,,ZSCC: 0000034  Section: Policy research,,/Users/jacquesthibodeau/Zotero/storage/MFNIR97H/Cotton-Barratt et al. - 2016 - Global Catastrophic Risks 2016.pdf; /Users/jacquesthibodeau/Zotero/storage/R4KN9F3G/global-catastrophic-risks-2016.html,,MetaSafety; FHI; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NQT3FZE8,report,2021,"Ord, Toby; Mercer, Angus; Dannreuther, Sophie",Future Proof,,,,,,,2021-06,2022-01-30 4:53:10,2022-01-30 4:53:10,,51,,,,,,,,,,,Centre for Long-Term Resilience,,en,,,,,Zotero,,ZSCC: NoCitationData[s1]  ACC: N/F,,"/Users/jacquesthibodeau/Zotero/storage/UV46GKTM/Dannreuther - Angus Mercer, Centre for Long-Term Resilience.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C8MBI7MV,report,2016,"Sandberg, Anders",Energetics of the brain and AI,,,,,https://arxiv.org/abs/1602.04019,"Does the energy requirements for the human brain give energy constraints that give reason to  doubt the feasibility of artificial intelligence? This report will review some relevant estimates of  brain bioenergetics and analyze some of the methods of estimating brain emulation energy re- quirements. Turning to AI, there are reasons to believe the energy requirements for de novo AI  to  have  little  correlation  with  brain  (emulation)  energy  requirements  since  cost  could  depend  merely of the cost of processing higher-level representations rather than billions of neural fir- ings.  Unless  one  thinks  the  human  way  of  thinking  is  the  most  optimal  or  most  easily  imple- mentable  way  of  achieving  software  intelligence,  we  should  expect  de novo  AI  to  make  use  of  different, potentially very compressed and fast, processes.",2016,2022-01-30 4:53:09,2022-01-30 4:53:09,,,,,,,,,,,,,Sapience Project,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/E8476E6C/Sandberg - 2016 - Energetics of the brain and AI.pdf; /Users/jacquesthibodeau/Zotero/storage/6IMBPF7N/1602.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,STR 2016-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD3PXT45,report,2019,"Zhang, Baobao; Dafoe, Allan",Artificial Intelligence: American Attitudes and Trends,,,,,https://www.ssrn.com/abstract=3312874,,2019,2022-01-30 4:53:08,2022-01-30 4:53:08,2019-12-16 22:39:34,,,,,,,Artificial Intelligence,,,,,Center for the Governance of AI and Future of Humanity Institute,,en,,,,,DOI.org (Crossref),,ZSCC: 0000127,,/Users/jacquesthibodeau/Zotero/storage/583N8IMW/papers.html; /Users/jacquesthibodeau/Zotero/storage/7BGC3MI8/papers.html; /Users/jacquesthibodeau/Zotero/storage/E5PBNQQ3/Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Tr.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZJVM7EJ,report,2018,"Dafoe, Allan",AI governance: a research agenda,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf,,2018,2022-01-30 4:53:08,2022-01-30 4:53:08,2020-12-21,,,,,,,AI governance,,,,,Future of Humanity Institute,,,,,,,Google Scholar,,ZSCC: 0000114,,/Users/jacquesthibodeau/Zotero/storage/9WM3BJIG/Dafoe - 2018 - AI governance a research agenda.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5FZ7A6KK,report,2020,"Calvin, Nathan; Leung, Jade",Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-working-paper-Who-owns-AI-Apr2020.pdf,,2020-02,2022-01-30 4:53:45,2022-01-30 4:53:45,2020-09-05,23,,,,,,,,,,,Future of Humanity Institute,,,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/ZPVF27E5/Calvin and Leung - 2020 - Who owns artificial intelligence A preliminary an.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NDFHXVI,report,2010,"Armstrong, Stuart",Utility Indifference,,,,,,"Consider an AI that follows its own motivations. We’re not entirely sure what its motivations are, but we would prefer that the AI cooperate with humanity; or, failing that, that we can destroy it before it defects. We’ll have someone sitting in a room, their finger on a detonator, ready at the slightest hint of defection. Unfortunately as has been noted ([3], [1]), this does not preclude the AI from misbehaving. It just means that the AI must act to take control of the explosives, the detonators or the human who will press the button. For a superlatively intelligence AI, this would represent merely a slight extra difficulty. But now imagine that the AI was somehow indifferent to the explosives going off or not (but that nothing else was changed). Then if ever the AI does decide to defect, it will most likely do so without taking control of the explosives, as that would be easier than otherwise. By “easier ” we mean that the chances of failure are less, since the plan is simpler – recall that under these assumptions, the AI counts getting blown up as an equal value to successfully defecting.",2010,2022-01-30 4:53:37,2022-01-30 4:53:37,,,,,,,,,,,,,Future of Humanity Institute,,,,,,,CiteSeer,,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/N5CJR7JC/2010-1_body.pdf; /Users/jacquesthibodeau/Zotero/storage/VAX4IF4Z/Armstrong - 2010 - Utility Indifference.pdf; /Users/jacquesthibodeau/Zotero/storage/JDJ3ES5A/summary.html; /Users/jacquesthibodeau/Zotero/storage/XNW3UESI/summary.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUZWTKPN,report,2017,"Armstrong, Stuart; Weick, Mario; Sandberg, Anders; Snyder-Beattie, Andrew; Beckstead, Nick",The underwriter and the models-solo dances or pas-de-deux? What policy data can tell us about how underwriters use models,,,,,https://www.msamlin.com/content/dam/ms-amlin/corporate/our-world/Whitepapers/MS%20Amlin%20White%20Paper%20The%20underwriter%20and%20the%20models-%20solo%20dances%20or%20pas-de-deux.pdf.downloadasset.pdf,,2017,2022-01-30 4:53:36,2022-01-30 4:53:36,2020-12-19,,,,,,,The underwriter and the models-solo dances or pas-de-deux?,,,,,MS Amlin,,,,,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/UP7CQVAJ/64873.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W2XDGEWJ,report,2019,"Drexler, K Eric",Reframing Superintelligence: Comprehensive AI Services as General Intelligence,,,,,https://www.fhi.ox.ac.uk/reframing/,,2019,2022-01-30 4:53:20,2022-01-30 4:53:20,,210,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/TAPTRQJC/Drexler - Reframing Superintelligence.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z7HXRRSA,report,2021,"Drexler, K. Eric",QNRs: Toward Language for Intelligent Machines,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/2021/08/QNRs_FHI-TR-2021-3.0.pdf,"Impoverished syntax and nondifferentiable vocabularies make natural language a poor medium for neural representation learning and appli- cations. Learned, quasilinguistic neural representations (QNRs) can upgrade words to embeddings and syntax to graphs to provide a more expressive and computationally tractable medium. Graph-structured, embedding-based quasilinguistic representations can support formal and informal reasoning, human and inter-agent communication, and the development of scalable quasilinguistic corpora with characteristics of both literatures and associative memory. To achieve human-like intellectual competence, machines must be fully literate, able not only to read and learn, but to write things worth retaining as contributions to collective knowledge. In support of this goal, QNR-based systems could translate and process natural language corpora to support the aggregation, refinement, integration, extension, and application of knowledge at scale. Incremental development of QNR- based models can build on current methods in neural machine learning, and as systems mature, could potentially complement or replace today’s opaque, error-prone “foundation models” with systems that are more capable, interpretable, and epistemically reliable. Potential applications and implications are broad.",2021,2022-01-30 4:53:20,2022-01-30 4:53:20,2021-10-31 19:09:57,,,,,,,,,,,,Future of Humanity Institute,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/S242XWXK/QNRs_FHI-TR-2021-3.0.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TX4W9496,report,2018,"Evans, Owain; Stuhlmüller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew",Predicting Human Deliberative Judgments with Machine Learning,,,,,,,2018,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,"Technical report, University of Oxford",,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/FPMK4JWF/Evans et al. - 2018 - Predicting Human Deliberative Judgments with Machi.pdf,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VBFREPX5,report,2014,"Sandberg, Anders",Monte Carlo model of brain emulation development,,,,,,,2014,2022-01-30 4:53:19,2022-01-30 4:53:19,,,,,,,,,,,,,"Working Paper 2014–1 (version 1.2), Future of Humanity Institute. http://www …",,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/K8T8QJJ9/Sandberg - 2014 - Monte Carlo model of brain emulation development.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KWGIDUE8,report,2015,"Cotton-Barratt, Owen",How valuable is movement growth?,,,,,http://globalprioritiesproject.org/wp-content/uploads/2015/05/MovementGrowth.pdf,,2015,2022-01-30 4:53:18,2022-01-30 4:53:18,,,,,,,,,,,,,Centre for Effective Altruism,,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/MW89CNJZ/Cotton-Barratt - 2015 - How valuable is movement growth.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJG4W372,report,2021,"Zaidi, Waqar; Dafoe, Allan",International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Technology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf,"The invention of atomic energy posed a novel global challenge: could the technology be controlled to avoid destructive uses and an existentially dangerous arms race while permitting the broad sharing of its benefits? From 1944 onwards, scientists, policymakers, and other t echnical specialists began to confront this challenge and explored policy options for dealing with the impact of nuclear technology. We focus on the years 1944 to 1951 and review this period for lessons for the governance of powerful technologies, and find the following: Radical schemes for international control can get broad support when confronted by existentially dangerous technologies, but this support can be tenuous and cynical. Secrecy is likely to play an important, and perhaps harmful, role. The public sphere may be an important source of influence, both in general and in particular in favor of cooperation, but also one that is manipulable and poorly informed. Technical experts may play a critical role, but need to be politically savvy. Overall, policymaking may look more like “muddling through” than clear-eyed grand strategy. Cooperation may be risky, and there may be many obstacles to success.",2021-03,2022-01-30 4:53:18,2022-01-30 4:53:18,2021-11-14 18:24:38,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/IX5D2GDZ/Cihon et al. - 2020 - Should Artificial Intelligence Governance be Centr.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6C2SDFXX,report,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),,,,,,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity’s long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks.",2020-05-30,2022-01-30 4:50:42,2022-01-30 4:50:42,,131,,,,,,,,,,,Center for Human-Compatible AI,,en,,,,,Zotero,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/ACREUKJD/Critch - AI Research Considerations for Human Existential S.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VIV37QTM,report,2021,"Gehlhaus, Diana; Koslosky, Luke; Goode, Kayla; Perkins, Claire",U.S. AI Workforce: Policy Recommendations,,,,,https://cset.georgetown.edu/publication/u-s-ai-workforce-policy-recommendations/,"This policy brief addresses the need for a clearly defined artificial intelligence education and workforce policy by providing recommendations designed to grow, sustain, and diversify the U.S. AI workforce. The authors employ a comprehensive definition of the AI workforce—technical and nontechnical occupations—and provide data-driven policy goals. Their recommendations are designed to leverage opportunities within the U.S. education and training system while mitigating its challenges, and prioritize equity in access and opportunity to AI education and AI careers.",2021-10,2022-01-30 4:52:05,2022-01-30 4:52:05,2021-10-31 17:12:04,,,,,,,U.S. AI Workforce,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/NB3W8BMN/u-s-ai-workforce-policy-recommendations.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQZXXA47,report,2021,"Buchanan, Ben; Lohn, Andrew; Musser, Micah; Sedova, Katerina","Truth, Lies, and Automation",,,,,https://cset.georgetown.edu/publication/truth-lies-and-automation/,Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.,2021-05,2022-01-30 4:52:05,2022-01-30 4:52:05,2021-10-31 17:46:29,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/FS8NAUIB/truth-lies-and-automation.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JTKIW98,report,2021,"Konaev, Margarita; Huang, Tina; Chahal, Husanjot",Trusted Partners,,,,,https://cset.georgetown.edu/publication/trusted-partners/,"As the U.S. military integrates artificial intelligence into its systems and missions, there are outstanding questions about the role of trust in human-machine teams. This report examines the drivers and effects of such trust, assesses the risks from too much or too little trust in intelligent technologies, reviews efforts to build trustworthy AI systems, and offers future directions for research on trust relevant to the U.S. military.",2021-02,2022-01-30 4:52:05,2022-01-30 4:52:05,2021-10-31 18:56:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/U2JDP64H/trusted-partners.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3Q9XMGI,report,2020,"Imbrie, Andrew; Kania, Elsa; Laskai, Lorand",The Question of Comparative Advantage in Artificial Intelligence: Enduring Strengths and Emerging Challenges for the United States,,,,,https://cset.georgetown.edu/research/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emerging-challenges-for-the-united-states/,"How do we measure leadership in artificial intelligence, and where does the United States rank? What comparative advantages matter most? As nations embrace AI, answering these questions becomes increasingly critical.",2020-01,2022-01-30 4:52:04,2022-01-30 4:52:04,2020-08-18 21:21:07,,,,,,,The Question of Comparative Advantage in Artificial Intelligence,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: 7,,/Users/jacquesthibodeau/Zotero/storage/VAHM3NKJ/Imbrie et al. - 2020 - The Question of Comparative Advantage in Artificia.pdf; /Users/jacquesthibodeau/Zotero/storage/GDRE4UKP/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emergin.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBPC2N7S,report,2021,"Gehlhaus, Diana; Hodge, Ron; Koslosky, Luke; Goode, Kayla; Rotner, Jonathan",The DOD’s Hidden Artificial Intelligence Workforce,,,,,https://cset.georgetown.edu/publication/the-dods-hidden-artificial-intelligence-workforce/,"This policy brief, authored in collaboration with the MITRE Corporation, provides a new perspective on the U.S. Department of Defense’s struggle to recruit and retain artificial intelligence talent. The authors find that the DOD already has a cadre of AI and related experts, but that this talent remains hidden. Better leveraging this talent could go a long way in meeting the DOD’s AI objectives. The authors argue that this can be done through policies that more effectively identify AI talent and assignment opportunities, processes that incentivize experimentation and changes in career paths, and investing in the necessary technological infrastructure.",2021-09,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:13:13,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/A4AS4MJI/the-dods-hidden-artificial-intelligence-workforce.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KU4UTF7B,report,2021,"Chahal, Husanjot; Toner, Helen; Rahkovsky, Ilya",Small Data’s Big AI Potential,,,,,https://cset.georgetown.edu/publication/small-datas-big-ai-potential/,"Conventional wisdom suggests that cutting-edge artificial intelligence is dependent on large volumes of data. An overemphasis on “big data” ignores the existence—and underestimates the potential—of several AI approaches that do not require massive labeled datasets. This issue brief is a primer on “small data” approaches to AI. It presents exploratory findings on the current and projected progress in scientific research across these approaches, which country leads, and the major sources of funding for this research.",2021-09,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:18:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/I8S77599/small-datas-big-ai-potential.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SCHIPDXW,report,2020,"Hwang, Tim",Shaping the Terrain of AI Competition,,,,,https://cset.georgetown.edu/research/shaping-the-terrain-of-ai-competition/,How should democracies effectively compete against authoritarian regimes in the AI space? This report offers a “terrain strategy” for the United States to leverage the malleability of artificial intelligence to offset authoritarians' structural advantages in engineering and deploying AI.,2020-06,2022-01-30 4:52:04,2022-01-30 4:52:04,2020-08-18 21:12:30,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/E9B7EF92/CSET-Shaping-the-Terrain-of-AI-Competition.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCEI8B5/shaping-the-terrain-of-ai-competition.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7CHZX93M,report,2021,"Cary, Dakota",Robot Hacking Games,,,,,https://cset.georgetown.edu/publication/robot-hacking-games/,"Software vulnerability discovery, patching, and exploitation—collectively known as the vulnerability lifecycle—is time consuming and labor intensive. Automating the process could significantly improve software security and offensive hacking. The Defense Advanced Research Projects Agency’s Cyber Grand Challenge supported teams of researchers from 2014 to 2016 that worked to create these tools. China took notice. In 2017, China hosted its first Robot Hacking Game, seeking to automate the software vulnerability lifecycle. Since then, China has hosted seven such competitions and the People’s Liberation Army has increased its role in hosting the games.",2021-09,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:14:54,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/R33JGR9H/robot-hacking-games.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XQNJ4CAX,report,2021,"Lohn, Andrew",Poison in the Well,,,,,https://cset.georgetown.edu/publication/poison-in-the-well/,"Modern machine learning often relies on open-source datasets, pretrained models, and machine learning libraries from across the internet, but are those resources safe to use? Previously successful digital supply chain attacks against cyber infrastructure suggest the answer may be no. This report introduces policymakers to these emerging threats and provides recommendations for how to secure the machine learning supply chain.",2021-06,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:44:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/N3PM2RS4/poison-in-the-well.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K4XIA668,report,2021,"Stanley-Lockman, Zoe",Military AI Cooperation Toolbox,,,,,https://cset.georgetown.edu/publication/military-ai-cooperation-toolbox/,"The Department of Defense can already begin applying its existing international science and technology agreements, global scientific networks, and role in multilateral institutions to stimulate digital defense cooperation. This issue brief frames this collection of options as a military AI cooperation toolbox, finding that the available tools offer valuable pathways to align policies, advance research, development, and testing, and to connect personnel–albeit in more structured ways in the Euro-Atlantic than in the Indo-Pacific.",2021-08,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:41:07,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/ETNQVAV4/military-ai-cooperation-toolbox.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7FVGJKQD,report,2021,"Musser, Micah; Garriott, Ashton",Machine Learning and Cybersecurity,,,,,https://cset.georgetown.edu/publication/machine-learning-and-cybersecurity/,Cybersecurity operators have increasingly relied on machine learning to address a rising number of threats. But will machine learning give them a decisive advantage or just help them keep pace with attackers? This report explores the history of machine learning in cybersecurity and the potential it has for transforming cyber defense in the near future.,2021-06,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:45:14,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/55E2MHZX/machine-learning-and-cybersecurity.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64I4UBIX,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: Robustness and Adversarial Examples,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples/,"This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.",2021-03,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 18:52:59,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/482VNDJV/key-concepts-in-ai-safety-robustness-and-adversarial-examples.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3S8DDMWK,report,2021,"Stanley-Lockman, Zoe",Responsible and Ethical Military AI,,,,,https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/,"Allies of the United States have begun to develop their own policy approaches to responsible military use of artificial intelligence. This issue brief looks at key allies with articulated, emerging, and nascent views on how to manage ethical risk in adopting military AI. The report compares their convergences and divergences, offering pathways for the United States, its allies, and multilateral institutions to develop common approaches to responsible AI implementation.",2021-08,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:39:09,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/XSV67832/responsible-and-ethical-military-ai.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI7QIJ8V,report,2021,"VerWey, John","No Permits, No Fabs: The Importance of Regulatory Reform for Semiconductor Manufacturing",,,,,https://cset.georgetown.edu/publication/no-permits-no-fabs/,"Congress has advanced legislation to appropriate $52 billion in funding for the CHIPS for America Act, which aims to increase semiconductor manufacturing and supply chain resilience in the United States. But more can be done to improve the resiliency of U.S. access to microelectronics beyond manufacturing incentives. This report outlines infrastructure investments and regulatory reforms that could make the United States a more attractive place to build new chipmaking capacity and ensure continued U.S. access to key inputs for semiconductor manufacturing.",2021-10,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:08:33,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/V5B6V6X7/no-permits-no-fabs.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TBNSHQP2,report,2021,"Daniels, Matthew; Chang, Ben",National Power After AI,,,,,https://cset.georgetown.edu/publication/national-power-after-ai/,"AI technologies will likely alter great power competitions in foundational ways, changing both how nations create power and their motives for wielding it against one another. This paper is a first step toward thinking more expansively about AI & national power and seeking pragmatic insights for long-term U.S. competition with authoritarian governments.",2021-07,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:42:40,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/TWR2J25P/national-power-after-ai.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VKQNA4AA,report,2021,"Luong, Ngor; Gelles, Rebecca; Flagg, Melissa",Mapping the AI Investment Activities of Top Global Defense Companies,,,,,https://cset.georgetown.edu/publication/mapping-the-ai-investment-activities-of-top-global-defense-companies/,"Militaries around the world have often relied on the largest global defense companies to acquire and integrate cutting-edge technologies. This issue brief examines the investment and mergers and acquisition activities in artificial intelligence of the top 50 global defense companies — a key, if limited, approach to accessing AI innovation in the commercial sector — and assesses investment trends of their corporate venture capital subsidiaries and offers a geographic breakdown of defense companies and their AI target companies.",2021-10,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:10:48,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CI8QZ3GJ/mapping-the-ai-investment-activities-of-top-global-defense-companies.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4V99XXR,report,2021,"Daniels, Matthew; Toney, Autumn; Flagg, Melissa; Yang, Charles",Machine Intelligence for Scientific Discovery and Engineering Invention,,,,,https://cset.georgetown.edu/publication/machine-intelligence-for-scientific-discovery-and-engineering-invention/,The advantages of nations depend in part on their access to new inventions—and modern applications of artificial intelligence can help accelerate the creation of new inventions in the years ahead. This data brief is a first step toward understanding how modern AI and machine learning have begun accelerating growth across a wide array of science and engineering disciplines in recent years.,2021-05,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:48:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/VVP6JR7H/machine-intelligence-for-scientific-discovery-and-engineering-invention.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GZTJRH52,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: Interpretability in Machine Learning,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.",2021-03,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 18:52:14,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/Z256SBHM/key-concepts-in-ai-safety-interpretability-in-machine-learning.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VH792VSZ,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: An Overview,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",2021-03,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 18:53:33,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/KDBM6KD4/key-concepts-in-ai-safety-an-overview.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36HRFWME,report,2021,"Goode, Kayla; Kim, Heeu Millie",Indonesia’s AI Promise in Perspective,,,,,https://cset.georgetown.edu/publication/indonesias-ai-promise-in-perspective/,"The United States and China are keeping an eye on Indonesia’s artificial intelligence potential given the country’s innovation-driven national strategy and flourishing AI industry. China views Indonesia as an anchor for its economic, digital, and political inroads in Southeast Asia and has invested aggressively in new partnerships. The United States, with robust political and economic relations rooted in shared democratic ideals, has an opportunity to leverage its comparative advantages and tap into Indonesia’s AI potential through high-level agreements.",2021-08,2022-01-30 4:52:04,2022-01-30 4:52:04,2021-10-31 17:39:58,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/9P8XTSFN/indonesias-ai-promise-in-perspective.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGF6E8W5,report,2021,"Baker, Jamie",Ethics and Artificial Intelligence,,,,,https://cset.georgetown.edu/publication/ethics-and-artificial-intelligence/,"The law plays a vital role in how artificial intelligence can be developed and used in ethical ways. But the law is not enough when it contains gaps due to lack of a federal nexus, interest, or the political will to legislate. And law may be too much if it imposes regulatory rigidity and burdens when flexibility and innovation are required. Sound ethical codes and principles concerning AI can help fill legal gaps. In this paper, CSET Distinguished Fellow James E. Baker offers a primer on the limits and promise of three mechanisms to help shape a regulatory regime that maximizes the benefits of AI and minimizes its potential harms.",2021-04,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 18:49:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/WZE4UHJQ/ethics-and-artificial-intelligence.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R79CMVHN,report,2021,"Imbrie, Andrew; Gelles, Rebecca; Dunham, James; Aiken, Catherine",Contending Frames: Evaluating Rhetorical Dynamics in AI,,,,,https://cset.georgetown.edu/publication/contending-frames/,"The narrative of an artificial intelligence “arms race” among the great powers has become shorthand to describe evolving dynamics in the field. Narratives about AI matter because they reflect and shape public perceptions of the technology. In this issue brief, the second in a series examining rhetorical frames in AI, the authors compare four narrative frames that are prominent in public discourse: AI Competition, Killer Robots, Economic Gold Rush and World Without Work.",2021-05,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 17:49:21,,,,,,,Contending Frames,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/3JV7AUFN/contending-frames.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VWID435G,report,2020,"Murdick, Dewey; Dunham, James; Melot, Jennifer",AI Definitions Affect Policymaking,,,,,https://cset.georgetown.edu/research/ai-definitions-affect-policymaking/,"The task of artificial intelligence policymaking is complex and challenging, made all the more difficult by such a rapidly evolving technology. In order to address the security and economic implications of AI, policymakers must be able to viably define, categorize and assess AI research and technology. In this issue brief, CSET puts forward a functional definition of AI, based on three core principles, that significantly outperforms methods developed over the last decade.",2020-06-02,2022-01-30 4:52:03,2022-01-30 4:52:03,2020-08-18 21:13:44,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/WD8HFI6J/Murdick et al. - 2020 - AI Definitions Affect Policymaking.pdf; /Users/jacquesthibodeau/Zotero/storage/QH8STACD/ai-definitions-affect-policymaking.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JRCH2RU,report,2021,"Hoffman, Wyatt",AI and the Future of Cyber Competition,,,,,https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/,"As states turn to AI to gain an edge in cyber competition, it will change the cat-and-mouse game between cyber attackers and defenders. Embracing machine learning systems for cyber defense could drive more aggressive and destabilizing engagements between states. Wyatt Hoffman writes that cyber competition already has the ingredients needed for escalation to real-world violence, even if these ingredients have yet to come together in the right conditions.",2021-01,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 18:58:12,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/NKNAVMTB/ai-and-the-future-of-cyber-competition.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDKQ9KHV,report,2021,"Arnold, Zachary; Toner, Helen",AI Accidents: An Emerging Threat,,,,,https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/,"As modern machine learning systems become more widely used, the potential costs of malfunctions grow. This policy brief describes how trends we already see today—both in newly deployed artificial intelligence systems and in older technologies—show how damaging the AI accidents of the future could be. It describes a wide range of hypothetical but realistic scenarios to illustrate the risks of AI accidents and offers concrete policy suggestions to reduce these risks.",2021-07,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 17:43:15,,,,,,,AI Accidents,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/6PTVA9UX/ai-accidents-an-emerging-threat.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQSW7FFQ,report,2021,"Konaev, Margarita; Imbrie, Andrew; Fedasiuk, Ryan; Weinstein, Emily; Sedova, Katerina; Dunham, James",Headline or Trend Line?,,,,,https://cset.georgetown.edu/publication/headline-or-trend-line/,"Chinese and Russian government officials are keen to publicize their countries’ strategic partnership in emerging technologies, particularly artificial intelligence. This report evaluates the scope of cooperation between China and Russia as well as relative trends over time in two key metrics of AI development: research publications and investment. The findings expose gaps between aspirations and reality, bringing greater accuracy and nuance to current assessments of Sino-Russian tech cooperation.",2021-08,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 17:19:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/KZNE2FUE/headline-or-trend-line.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGXWUS23,report,2021,"Fedasiuk, Ryan; Melot, Jennifer; Murphy, Ben",Harnessed Lightning,,,,,https://cset.georgetown.edu/publication/harnessed-lightning/,"This report examines nearly 350 artificial intelligence-related equipment contracts awarded by the People’s Liberation Army and state-owned defense enterprises in 2020 to assess how the Chinese military is adopting AI. The report identifies China’s key AI defense industry suppliers, highlights gaps in U.S. export control policies, and contextualizes the PLA’s AI investments within China’s broader strategy to compete militarily with the United States.",2021-10,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 17:07:03,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/XXESBFB3/harnessed-lightning.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NFPE7BBS,report,2021,"Mittelsteadt, Matthew",AI Verification: Mechanisms to Ensure AI Arms Control Compliance,,,,,https://cset.georgetown.edu/publication/ai-verification/,"The rapid integration of artificial intelligence into military systems raises critical questions of ethics, design and safety. While many states and organizations have called for some form of “AI arms control,” few have discussed the technical details of verifying countries’ compliance with these regulations. This brief offers a starting point, defining the goals of “AI verification” and proposing several mechanisms to support arms inspections and continuous verification.",2021-02,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 18:55:15,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/K3TV6PZX/ai-verification.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HRJA33X,report,2021,"Peterson, Dahlia; Goode, Kayla; Gehlhaus, Diana",AI Education in China and the United States,,,,,https://cset.georgetown.edu/publication/ai-education-in-china-and-the-united-states/,"A globally competitive AI workforce hinges on the education, development, and sustainment of the best and brightest AI talent. This issue brief compares efforts to integrate AI education in China and the United States, and what advantages and disadvantages this entails. The authors consider key differences in system design and oversight, as well as strategic planning. They then explore implications for the U.S. national security community.",2021-09,2022-01-30 4:52:03,2022-01-30 4:52:03,2021-10-31 17:16:49,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/TNTW4N5K/ai-education-in-china-and-the-united-states.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZW8JBQD3,report,2015,"Soares, Nate",Formalizing Two Problems of Realistic World-Models,,,,,https://intelligence.org/2015/01/22/new-report-formalizing-two-problems-realistic-world-models/,"An intelligent agent embedded within the real world must reason about an environment which is larger than the agent, and learn how to achieve goals in that environment. We discuss attempts to formalize two problems: one of induction, where an agent must use sensory data to infer a universe which embeds (and computes) the agent, and one of interaction, where an agent must learn to achieve complex goals in the universe. We review related problems formalized by Solomonoﬀ and Hutter, and explore challenges that arise when attempting to formalize analogous problems in a setting where the agent is embedded within the environment.",2015,2022-01-30 4:56:48,2022-01-30 4:56:48,,8,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000015[s0]  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/F254U9E2/Soares - Formalizing Two Problems of Realistic World-Models.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HGCDPKA8,report,2021,"Thorstad, David",The scope of longtermism,,,,,https://globalprioritiesinstitute.org/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford/,"Longtermism holds roughly that in many decision situations, the best thing we can do is what is best for the long-term future. The scope question for longtermism asks: how large is the class of decision situations for which longtermism holds? Although longtermism was initially developed to describe the situation of...",2021-06-22,2022-01-30 4:55:29,2022-01-30 4:55:29,2021-10-31 22:29:16,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/KTVEVVB7/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NSMN5REW,report,2021,"Greaves, Hilary; MacAskill, William",The case for strong longtermism,,,,,https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/,"A striking fact about the history of civilisation is just how early we are in it. There are 5000 years of recorded history behind us, but how many years are still to come? If we merely last as long as the typical mammalian species, we still have over 200,000 years to go (Barnosky et al. 2011); there could be a further one billion years until the Earth is no longer habitable for humans (Wolf and Toon 2015); and trillions of years until the last conventional star formations (Adams and Laughlin 1999:34). Even on the most conservative of these timelines, we have progressed through a tiny fraction of history. If humanity’s saga were a novel, we would be on the very first page.",2021-06-14,2022-01-30 4:55:29,2022-01-30 4:55:29,2021-10-31 22:29:46,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 29,,/Users/jacquesthibodeau/Zotero/storage/HQW64F7C/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RE4PBPVA,report,2021,"Riedener, Stefan",Existential risks from a Thomist Christian perspective,,,,,https://globalprioritiesinstitute.org/stefan-riedener-existential-risks-from-a-thomist-christian-perspective/,"Let’s say with Nick Bostrom that an ‘existential risk’ (or ‘x-risk’) is a risk that ‘threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development’ (2013, 15). There are a number of such risks: nuclear wars, developments in biotechnology or artificial intelligence, climate change, pandemics, supervolcanos, asteroids, and so on (see e.g. Bostrom and Ćirković 2008). ...",2021-01-04,2022-01-30 4:55:29,2022-01-30 4:55:29,2021-10-31 22:32:05,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/TAM2MWDV/stefan-riedener-existential-risks-from-a-thomist-christian-perspective.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPFEIRBC,report,2020,"Trammell, Phillip; Korinek, Anton",Economic growth under transformative AI,,,,,https://globalprioritiesinstitute.org/wp-content/uploads/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf,,2020-10,2022-01-30 4:55:29,2022-01-30 4:55:29,2020-11-21 19:28:29,,,,,,,,,,,,Global Priorities Institute,,,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/6MF4ET6T/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A9VMCT9R,report,2020,"Wilkinson, Hayden",In defence of fanaticism,,,,,https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf,,2020-09,2022-01-30 4:55:29,2022-01-30 4:55:29,2020-11-21 19:28:53,,,,,,,,,,,,Global Priorities Institute,,,,,,,,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/ZJ79B6HC/Hayden-Wilkinson_In-defence-of-fanaticism.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGHGP6FP,report,2021,"Thomas, Teruji",Doomsday and objective chance,,,,,https://globalprioritiesinstitute.org/doomsday-and-objective-chance-teruji-thomas/,"Lewis’s Principal Principle says that one should usually align one’s credences with the known chances. In this paper I develop a version of the Principal Principle that deals well with some exceptional cases related to the distinction between metaphysical and epistemic modal­ity. I explain how this principle gives a unified account of the Sleeping Beauty problem and chance-­based principles of anthropic reasoning. In doing so, I defuse the Doomsday Argument that the end of the world is likely to be nigh.",2021-07-13,2022-01-30 4:55:29,2022-01-30 4:55:29,2021-10-31 22:27:47,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/QZH4S7N9/doomsday-and-objective-chance-teruji-thomas.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W4ESBNR3,report,2020,"MacAskill, William",Are we living at the hinge of history,,,,,,,2020-09,2022-01-30 4:55:28,2022-01-30 4:55:28,,28,,,,,,,,,,,Global Priorities Institute,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/T3DJ73RE/MacAskill - Are we living at the hinge of history.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VN9N5P2C,report,2017,"Baum, Seth; Barrett, Anthony",Global Catastrophes: The Most Extreme Risks,,,,,https://papers.ssrn.com/abstract=3046668,"The most extreme risk are those that threaten the entirety of human civilization, known as global catastrophic risks. The very extreme nature of global catastrophes makes them both challenging to analyze and important to address. They are challenging to analyze because they are largely unprecedented and because they involve the entire global human system. They are important to address because they threaten everyone around the world and future generations. Global catastrophic risks also pose some deep dilemmas. One dilemma occurs when actions to reduce global catastrophic risk could harm society in other ways, as in the case of geoengineering to reduce catastrophic climate change risk. Another dilemma occurs when reducing one global catastrophic risk could increase another, as in the case of nuclear power reducing climate change risk while increasing risks from nuclear weapons. The complex, interrelated nature of global catastrophic risk suggests a research agenda in which the full space of risks are assessed in an integrated fashion in consideration of the deep dilemmas and other challenges they pose. Such an agenda can help identify the best ways to manage these most extreme risks and keep human civilization safe.",2017-10-02,2022-01-30 4:55:19,2022-01-30 4:55:19,2019-12-16 2:43:45,,,,,,,Global Catastrophes,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/38X6ZDSW/papers.html,,MetaSafety; GCRI,risk; catastrophic risk; extreme risk; global catastrophic risk,,,,,,,,,,,,,,,,,,,ID 3046668,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WT78EJX5,report,2017,"Baum, Seth","A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",,,,,https://papers.ssrn.com/abstract=3070741,"Artificial general intelligence (AGI) is AI that can reason across a wide range of domains. It has long been considered the “grand dream” or “holy grail” of AI. It also poses major issues of ethics, risk, and policy due to its potential to transform society: if AGI is built, it could either help solve the world’s problems or cause major catastrophe, possibly even human extinction. This paper presents the first-ever survey of active AGI R&D projects in terms of ethics, risk, and policy. A thorough search identifies 45 projects of diverse sizes, nationalities, ethical goals, and other attributes. Most projects are either academic or corporate. The academic projects tend to express goals of advancing knowledge and are less likely to be active on AGI safety issues. The corporate projects tend to express goals of benefiting humanity and are more likely to be active on safety. Most projects are based in the US, and almost all are in either the US or a US ally, including all of the larger projects. This geographic concentration could simplify policymaking, though most projects publish open-source code, enabling contributions from anywhere in the world. These and other findings of the survey offer an empirical basis for the study of AGI R&D and a guide for policy and other action.",2017-11-12,2022-01-30 4:55:18,2022-01-30 4:55:18,2019-12-16 2:43:48,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,ZSCC: 0000059,,/Users/jacquesthibodeau/Zotero/storage/IQ679H9R/papers.html,,MetaSafety; GCRI,artificial intelligence; risk; ethics; policy,,,,,,,,,,,,,,,,,,,ID 3070741,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GKMKG435,report,2020,Jessica Cussins Newman,Decision Points in AI Governance,,,,,https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf,,2020,2022-01-30 4:59:45,2022-01-30 4:59:45,,58,,,,,,,,,,,Center for Long-Term Cybersecurity,,,,,,,,,ZSCC: 0000006,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GV2AK67,report,2017,"Duettmann, Allison",Artificial General Intelligence: Timeframes & Policy White Paper,,,,,https://foresight.org/publications/AGI-Timeframes&PolicyWhitePaper.pdf,,2017,2022-01-30 4:59:36,2022-01-30 4:59:36,,26,,,,,,,,,,,Foresight Institute,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/7GZFE7Z4/Duettmann - Artificial General Intelligence Timeframes & Poli.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWG743AN,report,2018,"Duettman, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman",Artificial General Intelligence: Coordination and Great Powers,,,,,https://fsone-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf,,2018,2022-01-30 4:59:35,2022-01-30 4:59:35,,,,,,,,,,,,,Foresight Institute,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/7TTB38TG/Duettman et al. - 2018 - Artificial General Intelligence Coordination and .pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T5UVAGFV,report,2020,"McGraw, Gary; Figueroa, Harold; Shepardson, Victor; Bonett, Richie",An Architectural Risk Analysis of Machine Learning Systems: Toward More Secure Machine Learning,,,,,https://www.garymcgraw.com/wp-content/uploads/2020/02/BIML-ARA.pdf,,2020-01-13,2022-01-30 4:59:35,2022-01-30 4:59:35,2020-09-07,42,,,,,,An Architectural Risk Analysis of Machine Learning Systems,,,,,Berryville Institute of Machine Learning,,,,,,,,,ZSCC: 0000003,,,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AHAITFCH,report,2021,"Horowitz, Michael; Scharre, Paul",AI and International Stability: Risks and Confidence-Building Measures,,,,,https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures,Exploring the potential use of confidence-building measures built around the shared interests that all countries have in preventing inadvertent war.,2021-01-12,2022-01-30 4:59:34,2022-01-30 4:59:34,2021-11-14 18:05:37,,,,,,,AI and International Stability,,,,,Center for a New American Security,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/SZ2WC6V5/Horowitz and Scharre - 2021 - AI and International Stability Risks and Confiden.pdf; /Users/jacquesthibodeau/Zotero/storage/VC38NJHI/ai-and-international-stability-risks-and-confidence-building-measures.html,,MetaSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRQB3GAA,report,2015,"Fallenstein, Benja; Soares, Nate",Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents,,,,,https://intelligence.org/files/VingeanReflection.pdf,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.",2015,2022-01-30 4:57:32,2022-01-30 4:57:32,,,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000016  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/PIFEUWZX/Fallenstein and Soares - Vingean Reﬂection Reliable Reasoning for Self-Imp.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4J3EZPG6,report,2014,"Soares, Nate; Fallenstein, Benja",Questions of Reasoning Under Logical Uncertainty,,,,,https://intelligence.org/2015/01/09/new-report-questions-reasoning-logical-uncertainty/,"A logically uncertain reasoner would be able to reason as if they know both a programming language and a program, without knowing what the program outputs. Most practical reasoning involves some logical uncertainty, but no satisfactory theory of reasoning under logical uncertainty yet exists. A better theory of reasoning under logical uncertainty is needed in order to develop the tools necessary to construct highly reliable artiﬁcial reasoners. This paper introduces the topic, discusses a number of historical results, and describes a number of open problems.",2014,2022-01-30 4:56:58,2022-01-30 4:56:58,,8,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000018  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/UQ2E4GWJ/Soares and Fallenstein - Questions of Reasoning Under Logical Uncertainty.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I9GK83X9,report,2013,"Yudkowsky, Eliezer",Intelligence Explosion Microeconomics,,,,,,"I. J. Good’s thesis of the “intelligence explosion” states that a suﬃciently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such.",2013,2022-01-30 4:56:57,2022-01-30 4:56:57,,96,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000054  4 J: 34,,/Users/jacquesthibodeau/Zotero/storage/EINNW7XJ/Yudkowsky - Intelligence Explosion Microeconomics.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,2013-1,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQSC7JWK,report,2019,"Kumar, Ram Shankar Siva; Brien, David O; Albert, Kendra; Viljöen, Salomé; Snover, Jeffrey",Failure Modes in Machine Learning - Security documentation,,,,,https://docs.microsoft.com/en-us/security/failure-modes-in-machine-learning,"In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.",2019,2022-01-30 4:59:48,2022-01-30 4:59:48,2019-12-16 22:40:25,,,,,,,,,,,,"Microsoft Corporation, Berkman Klein Center for Internet and Society at Harvard University",,en-us,,,,,,,ZSCC: NoCitationData[s9]  ACC: 16,,/Users/jacquesthibodeau/Zotero/storage/93TJR5IH/failure-modes-in-machine-learning.html,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I97QI9BH,report,2021,"Gates, Vael; Callaway, Frederick; Ho, Mark K.; Griffiths, Tom",A rational model of people's inferences about others' preferences based on response times,,,,,https://psyarxiv.com/25zfx/,"There's a difference between someone instantaneously saying ""Yes!"" when you ask them on a date compared to ""...yes."" Psychologists and economists have long studied how people can infer preferences from others' choices. However, these models have tended to focus on what people choose and not how long it takes them to make a choice. We present a rational model for inferring preferences from response times, using a Drift Diffusion Model to characterize how preferences influence response time and Bayesian inference to invert this relationship. We test our model's predictions for three experimental questions. Matching model predictions, participants inferred that a decision-maker preferred a chosen item more if the decision-maker spent longer deliberating (Experiment 1), participants predicted a decision-maker's choice in a novel comparison based on inferring the decision-maker's relative preferences from previous response times and choices (Experiment 2), and participants could incorporate information about a decision-maker's mental state of cautious or careless (Experiments 3, 4A, and 4B).",2021-03-15,2022-03-09 23:00:25,2022-03-11 1:37:16,2022-03-09 23:00:25,,,,,,,,,,,,PsyArXiv,,en-us,,,,,OSF Preprints,,DOI: 10.31234/osf.io/25zfx type: article,,/Users/jacquesthibodeau/Zotero/storage/FTBJT79R/Gates et al. - 2021 - A rational model of people's inferences about othe.pdf; /Users/jacquesthibodeau/Zotero/storage/DURF8E83/Gates et al. - 2021 - A rational model of people's inferences about othe.pdf,,,Cognitive Psychology; Computational Modeling; drift diffusion model; Experimental Design and Sample Surveys; inference; Judgment and Decision Making; Quantitative Methods; Reasoning; Social and Behavioral Sciences; social cognition; theory of mind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WUAWT6MC,report,2021,"Maas, Matthijs M.",Aligning AI Regulation to Sociotechnical Change,,,,,https://papers.ssrn.com/abstract=3871635,"How do we regulate a changing technology, with changing uses, in a changing world? This chapter argues that while existing (inter)national AI governance approaches are important, they are often siloed. Technology-centric approaches focus on individual AI applications; law-centric approaches emphasize AI’s effects on pre-existing legal fields or doctrines. This chapter argues that to foster a more systematic, functional and effective AI regulatory ecosystem, policy actors should instead complement these approaches with a regulatory perspective that emphasizes how, when, and why AI applications enable patterns of ‘sociotechnical change’. Drawing on theories from the emerging field of ‘TechLaw’, it explores how this perspective can provide informed, more nuanced, and actionable perspectives on AI regulation. A focus on sociotechnical change can help analyze when and why AI applications actually do create a meaningful rationale for new regulation — and how they are consequently best approached as targets for regulatory intervention, considering not just the technology, but also six distinct ‘problem logics’ that appear around AI issues across domains. The chapter concludes by briefly reviewing concrete institutional and regulatory actions that can draw on this approach in order to improve the regulatory triage, tailoring, timing & responsiveness, and design of AI policy.",2021-06-16,2022-03-10 20:27:54,2022-03-10 20:27:54,2022-03-10 20:27:54,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,DOI: 10.2139/ssrn.3871635,,/Users/jacquesthibodeau/Zotero/storage/89RKKSJR/Maas - 2021 - Aligning AI Regulation to Sociotechnical Change.pdf,,,AI; Artificial Intelligence; Problem Logics; Regulation; Regulatory Rationale; Regulatory Target; Sociotechnical Change; Techlaw,,,,,,,,,,,,,,,,,,,ID 3871635,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B58AD7HY,report,2021,"Liu, Hin-Yan; Maas, Matthijs M.",Solving for X?' Towards a Problem-Finding Framework to Ground Long-Term Governance Strategies for Artificial Intelligence,,,,,https://papers.ssrn.com/abstract=3761623,"Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between- and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.",2021-01-07,2022-03-10 20:36:06,2022-03-10 20:36:06,2022-03-10 20:36:06,,,,,,,Solving for X?,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,DOI: 10.2139/ssrn.3761623,,/Users/jacquesthibodeau/Zotero/storage/BKR6VUWD/Liu and Maas - 2021 - 'Solving for X' Towards a Problem-Finding Framewo.pdf,,,AI; Artificial Intelligence; Futures; Governance; Long-term; Macrostrategy; Problem-Finding; Problem-Solving,,,,,,,,,,,,,,,,,,,ID 3761623,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,report,2019,"Drexler, K Eric",Reframing Superintelligence: Comprehensive AI Services as General Intelligence,,,,,,,2019,2019-12-16 2:15,2020-12-19 23:32,,210,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: 0000009,,/Users/angelica/Zotero/storage/PEXGT3JJ/Drexler - Reframing Superintelligence.pdf,,FHI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is a huge document; rather than summarize it all in this newsletter, I wrote up my summary in [this post](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services). For this newsletter, I've copied over the description of the model, but left out all of the implications and critiques.

The core idea is to look at the pathway by which we will develop general intelligence, rather than assuming that at some point we will get a superintelligent AGI agent. To predict how AI will progress in the future, we can look at how AI progresses currently -- through research and development (R&D) processes. AI researchers consider a problem, define a search space, formulate an objective, and use an optimization technique in order to obtain an AI system, called a service, that performs the task.

A service is an AI system that delivers bounded results for some task using bounded resources in bounded time. Superintelligent language translation would count as a service, even though it requires a very detailed understanding of the world, including engineering, history, science, etc. Episodic RL agents also count as services.

While each of the AI R&D subtasks is currently performed by a human, as AI progresses we should expect that we will automate these tasks as well. At that point, we will have automated R&D, leading to recursive technological improvement. This is not recursive self-improvement, because the improvement comes from R&D services creating improvements in basic AI building blocks, and those improvements feed back into the R&D services. All of this should happen before we get any powerful AGI agents that can do arbitrary general reasoning."
W8F6VI9I,thesis,2020,"Shah, Rohin Monish",Extracting and Using Preference Information from the State of the World,,,,,https://www.proquest.com/openview/da8bf63ef343781a5bb552122be1bd6a/1?pq-origsite=gscholar&cbl=18750&diss=y,"Typically when learning about what people want and don’t want, we look to human action as evidence: what reward they specify, how they perform a task, or what preferences they express can all provide useful information about what an agent should do. This is essential in order to build AI systems that do what we intend them to do. However, existing methods require a lot of expensive human feedback in order to learn even simple tasks. This dissertation argues that there is an additional source of information that is rather helpful: the state of the world. The key insight of this dissertation is that when a robot is deployed in an envi- ronment that humans have been acting in, the state of the environment is already optimized for what humans want, and is thus informative about human preferences. We formalize this setting by assuming that a human H has been acting in an environment for some time, and a robot R observes the final state produced. From this final state, R must infer as much as possible about H’s reward function. We analyze this problem formulation theoretically and show that it is particularly well suited to inferring aspects of the state that should not be changed – exactly the aspects of the reward that H is likely to forget to specify. We develop an algorithm using dynamic programming for tabular environments, analogously to value iteration, and demonstrate its behavior on several simple environments. To scale to high-dimensional environments, we use function approximators judiciously to allow the various parts of our algorithm to be trained without needing to enumerate all possible states. Of course, there is no point in learning about H’s reward function unless we use it to guide R’s decision-making. While we could have R simply optimize the inferred reward, this suffers from a “status quo bias”: the inferred reward is likely to strongly prefer the observed state, since by assumption it is already optimized for H’s preferences. To get R to make changes to 2 the environment, we will usually need to integrate the inferred reward with other sources of preference information. In order to support such reward combination, we use a model in which R must maximize an unknown reward function known only to H. Learning from the state of the world arises as an instrumentally useful behavior in such a setting, and can serve to form a prior belief over the reward function that can then be updated after further interaction with H",2020-12-17,2022-01-30 4:47:35,2022-01-30 4:47:35,,,24,,,,,,,,,,"University of California, Berkeley","Berkeley, CA",en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/S96M3KTK/Shah - Extracting and Using Preference Information from t.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H9CGA9V7,thesis,2021,"Maas, Matthijs M.","Artificial Intelligence Governance Under Change: Foundations, Facets, Frameworks",,,,,https://www.ssrn.com/abstract=3833395,"This dissertation explores how we may govern a changing technology, in a changing world, using governance systems that may themselves be left changed. Artificial Intelligence (AI) has made remarkable progress in the past decade, and is anticipated to become an increasingly disruptive, even transformative technology. AI can be functionally understood as a diverse portfolio of computational techniques to improve the accuracy, speed, or scale of machine decision-making, producing capabilities that can support, substitute for-, or improve upon human task performance. The resulting breadth of application makes AI promising—and challenging — in so many domains of life. In recent years diverse AI applications — from facial recognition to automated legal decision-making, and from computational propaganda to Lethal Autonomous Weapons Systems — have raised deep ethical, political, legal and security concerns. With growing public and policymaker attention has come a wave of governance initiatives and proposals. Nonetheless, global governance for AI remains relatively fragmented and incipient. At this cross-roads, this dissertation takes up the research question, “How should global governance for artificial intelligence account for change?” To answer this question, this dissertation draws together scholarship on technology regulation, (international) law, and global governance, in order to unpack three facets of ‘change’ that will prove critical to the global governance of AI. These three facets of change are examined through the conceptual lenses of Sociotechnical Change, Governance Disruption, and Regime Complexity. Sociotechnical Change (Chapter 4) explores how and why technological change in AI produces societal changes that create a rationale for regulatory intervention, and how we can productively characterize the appropriate targets for AI governance. Along with material features, I distinguish six problem logics which highlight different governance solutions and conditions. Governance Disruption (Chapter 5) addresses when, where and why certain AI capabilities might drive or demand change in the substance (Development), tools or processes (Displacement) or political scaffolding (Destruction) of global governance itself, and what are the implications for global regime complexity. Regime Complexity (Chapter 6) helps focus attention on how prospective AI regimes are shaped by underlying changes in the broader global governance architecture. It provides insight into the (1) origins or foundations of AI regimes; the (2) topology of the AI ‘regime complex’; its (3) evolution towards integration or fragmentation; (4) the functional consequences of these paths; and (5) strategies for managing the AI regime complex. Through these three lenses, this dissertation explores key considerations, insights and tradeoffs for AI governance (Chapter 7). It argues that AI governance needs to shift or adopt novel strategies — in conceptual approach, instrument choice, and instrument design — to ensure the efficacy of AI regimes in tracking AI’s sociotechnical impacts, their resilience to future AI-driven disruption to the tools, norms or broader conditions of governance, and their coherence. In this way, AI governance regimes may remain fit for change.",2021,2022-01-30 4:50:24,2022-01-30 4:50:24,2021-12-11 14:45:47,,,,,,,Artificial Intelligence Governance Under Change,,,,,University of Copenhagen,,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/2CR8NPGZ/Maas - 2021 - Artificial Intelligence Governance Under Change F.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHIK9C74,thesis,2015,"Evans, Owain Rhys",Bayesian computational models for inferring preferences,,,,,,,2015,2022-01-30 4:53:08,2022-01-30 4:53:08,,,,,,,,,,,,,Massachusetts Institute of Technology,,,,PhD Thesis,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/ETI5EWEM/Evans - 2015 - Bayesian computational models for inferring prefer.pdf; /Users/jacquesthibodeau/Zotero/storage/S4QH28R6/101522.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
343FMLM8,webpage,,,A Tour of Emerging Cryptographic Technologies | GovAI,,,,,https://www.governance.ai/research-paper/a-tour-of-emerging-cryptographic-technologies,"Historically, progress in the field of cryptography has been enormously consequential. Over the past century, for instance, cryptographic discoveries have played a key role in a world war and made it possible to use the internet..",,2022-03-09 22:49:05,2022-03-09 22:49:05,2022-03-09 22:49:05,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/INLAMK28/a-tour-of-emerging-cryptographic-technologies.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BAHHAZC4,webpage,,,"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation (2018). Brundage and Avin et al.",,,,,https://docs.google.com/document/d/e/2PACX-1vQzbSybtXtYzORLqGhdRYXUqiFsaEOvftMSnhVgJ-jRh6plwkzzJXoQ-sKtej3HW_0pzWTFY7-1eoGf/pub,,,2022-03-09 23:37:56,2022-03-09 23:37:56,2022-03-09 23:37:56,,,,,,,,,,,,,,,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/IQ32QQR3/pub.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CXMQR5MA,webpage,,,Futureproof: Artificial Intelligence Chapter | GovAI,,,,,https://www.governance.ai/research-paper/futureproof-artificial-intelligence-chapter,"Out of the wreckage of the Second World War, the UK transformed itself. It rebuilt its shattered economy. It founded the NHS. It created national insurance. And it helped establish international institutions like the United...",,2022-03-09 23:52:36,2022-03-09 23:52:36,2022-03-09 23:52:36,,,,,,,Futureproof,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ9ISC6C,webpage,,,"AI Policy Levers: A Review of the U.S. Government’s Tools to Shape AI Research, Development, and Deployment | GovAI",,,,,https://www.governance.ai/research-paper/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-deployment,"The U.S. government (USG) has taken increasing interest in the national security implications of artificial intelligence (AI). In this report, we ask: Given its national security concerns, how migh...",,2022-03-09 23:53:29,2022-03-09 23:53:29,2022-03-09 23:53:29,,,,,,,AI Policy Levers,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/JFEB4SSS/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-dep.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIJL5WTX,webpage,,,AI & Antitrust: Reconciling Tensions Between Competition Law and Cooperative AI Development | Yale Journal of Law & Technology,,,,,https://yjolt.org/ai-antitrust-reconciling-tensions-between-competition-law-and-cooperative-ai-development,,,2022-03-10 16:29:40,2022-03-10 16:29:40,2022-03-10 16:29:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBEFB3C2,webpage,,,Highly accurate protein structure prediction with AlphaFold | Nature,,,,,https://www.nature.com/articles/s41586-021-03819-2,,,2022-03-10 20:43:39,2022-03-10 20:43:39,2022-03-10 20:43:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXGXFBJD,webpage,,,The Immigration Preferences of Top AI Researchers: New Survey Evidence | GovAI,,,,,https://www.governance.ai/research-paper/the-immigration-preferences-of-top-ai-researchers-new-survey-evidence,"Artificial intelligence (AI) talent is global. AI researchers and engineers come from, and are in high demand, all over theworld. Countries and companies trying to recruit and retain AI talent thus...",,2022-03-10 20:54:14,2022-03-10 20:54:14,2022-03-10 20:54:14,,,,,,,The Immigration Preferences of Top AI Researchers,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XZ8LNYAX,webpage,2020,,Improving Verifiability in AI Development,OpenAI,,,,https://openai.com/blog/improving-verifiability/,"We’ve contributed to a multi-stakeholder report by 58 co-authors at 30 organizations, including the Centre for the Future of Intelligence, Mila, Schwartz Reisman Institute for Technology and Society, Center for Advanced Study in the Behavioral Sciences, and Center for Security and Emerging Technologies. This report describes 10 mechanisms to",2020-04-16,2022-03-10 21:12:56,2022-03-10 21:12:56,2022-03-10 21:12:56,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/5WMR2334/improving-verifiability.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8V336TAD,webpage,2019,"Zhou, Hattie","Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask",Uber Engineering Blog,,,,https://eng.uber.com/deconstructing-lottery-tickets/,Uber builds upon the Lottery Ticket Hypothesis by proposing explanations behind these mechanisms and deriving a surprising by-product: the Supermask.,2019-05-06,2022-03-10 21:18:16,2022-03-10 21:18:16,2022-03-10 21:18:16,,,,,,,Deconstructing Lottery Tickets,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2JH5DTPD,webpage,2016,,Faulty Reward Functions in the Wild,OpenAI,,,,https://openai.com/blog/faulty-reward-functions/,"Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.",2016-12-22,2022-03-10 22:05:48,2022-03-10 22:05:48,2022-03-10 22:05:48,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F8KYURCP,webpage,2018,,Likelihood of discontinuous progress around the development of AGI,AI Impacts,,,,https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/,"We aren’t convinced by any of the arguments we’ve seen to expect large discontinuity in AI progress above the extremely low base rate for all technologies. However this topic is controversial, and many thinkers on the topic disagree with us, so we consider this an open question. Details Definitions We say a technological discontinuity has...",2018-02-23,2022-03-10 22:07:13,2022-03-10 22:07:13,2022-03-10 22:07:13,,,,,,,,,,,,,,en-US,,,,,,,Section: Featured Articles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VTR4ADB8,webpage,,"Seita, Daniel",Learning Preferences by Looking at the World,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2019/02/11/learning_preferences/,The BAIR Blog,,2022-03-10 22:42:22,2022-03-10 22:42:22,2022-03-10 22:42:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CM7G7ACY,webpage,,"Seita, Daniel",Robots Learning to Move like Animals,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2020/04/03/laikago/,The BAIR Blog,,2022-03-10 22:59:54,2022-03-10 22:59:54,2022-03-10 22:59:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBN7XD3G,webpage,,"Zellers, Rowan",Defending Against Neural Fake News,,,,,https://rowanzellers.com/grover/,,,2022-03-10 23:44:08,2022-03-10 23:44:08,2022-03-10 23:44:08,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UP6S5XPD,webpage,,"Slides, ADAM GAIER Google Brain DAVID HA Google Brain June 12 2019 Download PDF NeurIPS 2019",Weight Agnostic Neural Networks,Weight Agnostic Neural Networks,,,,https://weightagnostic.github.io/,Networks that can already (sort of) perform tasks with random weights.,,2022-03-10 23:47:39,2022-03-10 23:47:39,2022-03-10 23:47:39,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He,Feature Denoising for Improving Adversarial Robustness,,,,,https://arxiv.org/abs/1812.03411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018,,,,,,,,,,,,,,,,"This paper claims to obtain nontrivial adversarial robustness on ImageNet. Assuming an adversary can add perturbations of size 16/255 (l_infinity), previous adversarially trained classifiers could not obtain above 1% adversarial accuracy. Some groups have tried to break the model proposed in this paper, but so far it appears its robustness is close to what it claims, [around](https://github.com/facebookresearch/ImageNet-Adversarial-Training/issues/1#issuecomment-470069171) 40% adversarial accuracy. Vanilla adversarial training is how they obtain said adversarial robustness. There has only been one previous public attempt at applying (multistep) adversarial training to ImageNet, as those at universities simply do not have the GPUs necessary to perform adversarial training on 224x224 images. Unlike the previous attempt, this paper ostensibly uses better hyperparameters, possibly accounting for the discrepancy. If true, this result reminds us that hyperparameter tuning can be critical even in vision, and that improving adversarial robustness on large-scale images may not be possible outside industry for many years."
,,,Yang Song, Rui Shu, Nate Kushman, Stefano Ermon,Constructing Unrestricted Adversarial Examples with Generative Models,,,,,https://arxiv.org/abs/1805.07894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,"This paper predates the [unrestricted adversarial examples challenge](https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html) ([AN #24](https://mailchi.mp/d7b5059d64ed/alignment-newsletter-24)) and shows how to generate such unrestricted adversarial examples using generative models. As a reminder, most adversarial examples research is focused on finding imperceptible perturbations to existing images that cause the model to make a mistake. In contrast, unrestricted adversarial examples allow you to find _any_ image that humans will reliably classify a particular way, where the model produces some other classification.The key idea is simple -- train a GAN to generate images in the domain of interest, and then create adversarial examples by optimizing an image to simultaneously be "realistic" (as evaluated by the generator), while still being misclassified by the model under attack. The authors also introduce another term into the loss function that minimizes deviation from a randomly chosen noise vector -- this allows them to get diverse adversarial examples, rather than always converging to the same one.They also consider a "noise-augmented" attack, where in effect they are running the normal attack they have, and then running a standard attack like FGSM or PGD afterwards. (They do these two things simultaneously, but I believe it's nearly equivalent.)For evaluation, they generate adversarial examples with their method and check that humans on Mechanical Turk reliably classify the examples as a particular class. Unsurprisingly, their adversarial examples "break" all existing defenses, including the certified defenses, though to be clear existing defenses assume a different threat model where an adversarial example must be an imperceptible perturbation to one of a known set of images. You could imagine doing something similar by taking the imperceptible-perturbation attacks and raise the value of ϵ until it is perceptible -- but in this case the generated images are much less realistic."
,,,Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David Andersen, George E. Dahl,Motivating the Rules of the Game for Adversarial Example Research,,,,,https://arxiv.org/abs/1807.06732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems,,,,,,,,,,,,,,,,"In this position paper, the authors argue that many of the threat models which motivate adversarial examples are unrealistic. They enumerate various previously proposed threat models, and then they show their limitations or detachment from reality. For example, it is common to assume that an adversary must create an imperceptible perturbation to an example, but often attackers can input whatever they please. In fact, in some settings an attacker can provide an input from the clean test set that is misclassified. Also, they argue that adversarial robustness defenses which degrade clean test set error are likely to make systems less secure since benign or nonadversarial inputs are vastly more common. They recommend that future papers motivated by adversarial examples take care to define the threat model realistically. In addition, they encourage researchers to establish “content-preserving” adversarial attacks (as opposed to “imperceptible” l_p attacks) and improve robustness to unseen input transformations."
,,,Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba, Seiya Tokui, Motoki Abe,Adversarial Attacks and Defences Competition,,,,,https://arxiv.org/abs/1804.00097,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The NIPS '17 Competition: Building Intelligent Systems,,,,,,,,,,,,,,,,This is a report on a competition held at NIPS 2017 for the best adversarial attacks and defences. It includes a summary of the field and then shows the results from the competition.
,,,Marc Khoury, Dylan Hadfield-Menell,On the Geometry of Adversarial Examples,,,,,https://arxiv.org/abs/1811.00525,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Genetic and Evolutionary Computation Conference '18,,,,,,,,,,,,,,,,"This paper analyzes adversarial examples based off a key idea: even if the data of interest forms a low-dimensional manifold, as we often assume, the _x000f__x000f_ϵ-tube _around_ the manifold is still high-dimensional, and so accuracy in an ϵ-ball around true data points will be hard to learn.For a given L_p norm, we can define the optimal decision boundary to be the one that maximizes the margin from the true data manifold. If there exists some classifier that is adversarially robust, then the optimal decision boundary is as well. Their first result is that the optimal decision boundary can change dramatically if you change p. In particular, for concentric spheres, the optimal L_inf decision boundary provides an L_2 robustness guarantee √d times smaller than the optimal L_2 decision boundary, where d is the dimensionality of the input. This explains why a classifier that is adversarially trained on L_inf adversarial examples does so poorly on L_2 adversarial examples.I'm not sure I understand the point of the next section, but I'll give it a try. They show that a nearest neighbors classifier can achieve perfect robustness if the underlying manifold is sampled sufficiently densely (requiring samples exponential in k, the dimensionality of the manifold). However, a learning algorithm with a particular property that they formalize would require exponentially more samples in at least some cases in order to have the same guarantee. I don't know why they chose the particular property they did -- my best guess is that the property is meant to represent what we get when we train a neural net on L_p adversarial examples. If so, then their theorem suggests that we would need exponentially more training points to achieve perfect robustness with adversarial training compared to a nearest neighbor classifier.They next turn to the fact that the ϵ-tube around the manifold is d-dimensional instead of k-dimensional. If we consider ϵ-balls around the training set X, this covers a very small fraction of the ϵ-tube, approaching 0 as d becomes much larger than k, even if the training set X covers the k-dimensional manifold sufficiently well.Another issue is that if we require adversarial robustness, then we severely restrict the number of possible decision boundaries, and so we may need significantly more expressive models to get one of these decision boundaries. In particular, since feedforward neural nets with Relu activations have "piecewise linear" decision boundaries (in quotes because I might be using the term incorrectly), it is hard for them to separate concentric spheres. Suppose that the spheres are separated by a distance d. Then for accuracy on the manifold, we only need the decision boundary to lie entirely in the shell of width d. However, for ϵ-tube adversarial robustness, the decision boundary must lie in a shell of width d - 2ϵ. They prove a lower bound on the number of linear regions for the decision boundary that grows as τ^(-d), where τ is the width of the shell, suggesting that adversarial robustness would require more parameters in the model.Their experiments show that for simple learning problems (spheres and planes), adversarial examples tend to be in directions orthogonal to the manifold. In addition, if the true manifold has high codimension, then the learned model has poor robustness."
,,,Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu,Towards Deep Learning Models Resistant to Adversarial Attacks,,,,,https://arxiv.org/abs/1706.06083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR,,,,,,,,,,,,,,,,"Madry et al.'s paper is a seminal work which shows that some neural networks can attain more adversarial robustness with a well-designed adversarial training procedure. The key idea is to phrase the adversarial defense problem as minimizing the expected result of the adversarial attack problem, which is maximizing the loss on an input training point when the adversary is allowed to perturb the point anywhere within an L-infinity norm ball. They also start the gradient descent from a random point in the norm ball. Then, given this attack, to optimize the adversarial defense problem, we simply do adversarial training. When trained long enough, some networks will attain more adversarial robustness."
,,,Linh Nguyen, Sky Wang, Arunesh Sinha,A learning and masking approach to secure learning,,,,,https://arxiv.org/abs/1709.04447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Decision and Game Theory for Security 2018,,,,,,,,,,,,,,,,"One way to view the problem of adversarial examples is that adversarial attacks map "good" clean data points that are classified correctly into a nearby "bad" space that is low probability and so is misclassified. This suggests that in order to attack a model, we can use a neural net to _learn_ a transformation from good data points to bad ones. The loss function is easy -- one term encourages similarity to the original data point, and the other term encourages the new data point to have a different class label. Then, for any new input data point, we can simply feed it through the neural net to get an adversarial example.Similarly, in order to defend a model, we can learn a neural net transformation that maps bad data points to good ones. The loss function continues to encourage similarity between the data points, but now encourages that the new data point have the correct label. Note that we need to use some attack algorithm in order to generate the bad data points that are used to train the defending neural net."
,,,Adam Gleave, Michael Dennis, Neel Kant, Cody Wild, Sergey Levine, Stuart Russell,Adversarial Policies: Attacking Deep Reinforcement Learning,,,,,https://arxiv.org/abs/1905.10615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This work demonstrates the existence of _adversarial policies_ of behaviour in high-dimensional, two-player zero-sum games. Specifically, they show that adversarially-trained agents ("Adv"), who can only affect a victim's observations of their (Adv's) states, can act in ways that confuse the victim into behaving suboptimally.An adversarial policy is trained by reinforcement learning in a single-player paradigm where the victim is a black-box fixed policy that was previously trained via self-play to be robust to adversarial attacks. As a result, the adversarial policies learn to push the observations of the victim outside the training distribution, causing the victim to behave poorly. The adversarial policies do not actually behave intelligently, such as blocking or tackling the victim, but instead do unusual things like spasming in a manner that appears random to humans, curling into a ball or kneeling.Further experiments showed that if the victim's observations of the adversary were removed, then the adversary was unable to learn such an adversarial policy. In addition, the victim's network activations were very different when playing against an adversarial policy relative to playing against a random or lifeless opponent. By comparing two similar games where the key difference was the number of adversary dimensions being observed, they showed that such policies were easier to learn in higher-dimensional games."
,,,Markus Kettunen et al,E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles,,,,,https://arxiv.org/abs/1906.03973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Convolutional neural networks are one of the best methods for assessing the perceptual similarity between images. This paper provides evidence that perceptual similarity metrics can be made adversarially robust. Out-of-the-box, network-based perceptual similarity metrics exhibit some adversarial robustness. While classifiers transform a long embedding vector to class scores, perceptual similarity measures compute distances between long and wide embedding tensors, possibly from multiple layers. Thus the attacker must alter far more neural network responses, which makes attacks on perceptual similarity measures harder for adversaries. This paper makes attacks even harder for the adversary by using a barrage of input image transformations and by using techniques such as dropout while computing the embeddings. This forces the adversarial perturbation to be substantially larger."
,,,Chris Finlay, Aram-Alexandre Pooladian, Adam M. Oberman,The LogBarrier adversarial attack: making effective use of decision boundary information,,,,,https://arxiv.org/abs/1903.10396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Rather than maximizing the loss of a model given a perturbation budget, this paper minimizes the perturbation size subject to the constraint that the model misclassify the example. This misclassification constraint is enforced by adding a logarithmic barrier to the objective, which they prevent from causing a loss explosion through through a few clever tricks. Their attack appears to be faster than the Carlini-Wagner attack."
,,,Matt Jordan, Naren Manoj, Surbhi Goel, Alexandros G. Dimakis,Quantifying Perceptual Distortion of Adversarial Examples,,,,,https://arxiv.org/abs/1902.08265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper takes a step toward more general adversarial threat models by combining adversarial additive perturbations small in an l_p sense with [spatially transformed adversarial examples](https://arxiv.org/abs/1801.02612), among other other attacks. In this more general setting, they measure the size of perturbations by computing the [SSIM](https://ece.uwaterloo.ca/~z70wang/research/ssim/#MAD) between clean and perturbed samples, which has limitations but is on the whole better than the l_2 distance. This work shows, along with other concurrent works, that perturbation robustness under some threat models does not yield robustness under other threat models. Therefore the view that l_p perturbation robustness must be achieved before considering other threat models is made more questionable. The paper also contributes a large code library for testing adversarial perturbation robustness."
,,,Carl-Johann Simon-Gabriel, Yann Ollivier, Léon Bottou, Bernhard Schölkopf, David Lopez-Paz,Adversarial Vulnerability of Neural Networks Increases With Input Dimension,,,,,https://arxiv.org/abs/1802.01421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The key idea of this paper is that imperceptible adversarial vulnerability happens when small changes in the input lead to large changes in the output, suggesting that the gradient is large. They first recommend choosing ϵ_p to be proportional to d^(1/p). Intuitively, this is because larger values of p behave more like maxing instead of summing, and so using the same value of ϵ across values of p would lead to more points being considered for larger p. They show a link between adversarial robustness and regularization, which makes sense since both of these techniques aim for better generalization.Their main point is that the norm of the gradient increases with the input dimension d. In particular, a typical initialization scheme will set the variance of the weights to be inversely proportional to d, which means the absolute value of each weight is inversely proportional to √d. For a single-layer neural net (that is, a perceptron), the gradient is exactly the weights. For L_inf adversarial robustness, the relevant norm for the gradient is the L_1 norm. This gives the sum of the d weights, which will be proportional to √d. For L_p adversarial robustness, the corresponding gradient is L_q with q larger than 1, which decreases the size of the gradient. However, this is exactly offset by the increase in the size of ϵ_p that they proposed. Thus, in this simple case the adversarial vulnerability increases with input dimension. They then prove theorems that show that this generalizes to other neural nets, including CNNS (albeit still only at initialization, not after training). They also perform experiments showing that their result also holds after training."
,,,Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, Pascal Frossard,"Robustness via curvature regularization, and vice versa",,,,,https://arxiv.org/abs/1811.09716,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper proposes a distinct way to increase adversarial perturbation robustness. They take an adversarial example generated with the FGSM, compute the gradient of the loss for the clean example and the gradient of the loss for the adversarial example, and they penalize this difference. Decreasing this penalty relates to decreasing the loss surface curvature. The technique works slightly worse than adversarial training."
,,,Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao,Is Robustness [at] the Cost of Accuracy?,,,,,https://arxiv.org/abs/1808.01688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ECCV,,,,,,,,,,,,,,,,This work shows that older architectures such as VGG exhibit more adversarial robustness than newer models such as ResNets. Here they take adversarial robustness to be the average adversarial perturbation size required to fool a network. They use this to show that architecture choice matters for adversarial robustness and that accuracy on the clean dataset is not necessarily predictive of adversarial robustness. A separate observation they make is that adversarial examples created with VGG transfers far better than those created with other architectures. All of these findings are for models without adversarial training.
,,,Nic Ford*, Justin Gilmer*, Nicolas Carlini, Dogus Cubuk,Adversarial Examples Are a Natural Consequence of Test Error in Noise,,,,,https://arxiv.org/abs/1901.10513,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper argues that there is a link between model accuracy on noisy images and model accuracy on adversarial images. They establish this empirically by showing that augmenting the dataset with random additive noise can improve adversarial robustness reliably. To establish this theoretically, they use the Gaussian Isoperimetric Inequality, which directly gives a relation between error rates on noisy images and the median adversarial perturbation size. Given that measuring test error on noisy images is easy, given that claims about adversarial robustness are almost always wrong, and given the relation between adversarial noise and random noise, they suggest that future defense research include experiments demonstrating enhanced robustness on nonadversarial, noisy images."
,,,Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry,Robustness May Be at Odds with Accuracy,,,,,https://arxiv.org/abs/1805.12152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,OpenReview,,,,,,,,,,,,,,,,"Since adversarial training can markedly reduce accuracy on clean images, one may ask whether there exists an inherent trade-off between adversarial robustness and accuracy on clean images. They use a simple model amenable to theoretical analysis, and for this model they demonstrate a trade-off. In the second half of the paper, they show adversarial training can improve feature visualization, which has been shown in several concurrent works."
,,,Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas Papernot, Veliqi, Marcel Salathé, Sharada P. Mohanty, Matthias Bethge,Adversarial Vision Challenge,,,,,http://arxiv.org/abs/1808.01976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,There will be a competition on adversarial examples for vision at NIPS 2018.
,,,Dan Hendrycks, Thomas G. Dietterich,Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations,,,,,http://arxiv.org/abs/1807.01697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/07/09/import-ai-102-testing-ai-robustness-with-imagenet-c-militarycivil-ai-development-in-china-and-how-teamwork-lets-ai-beat-humans/).
,,,Li Yuan, Will Xiao, Gabriel Kreiman, Francis E.H. Tay, Jiashi Feng, Margaret S. Livingstone,Adversarial images for the primate brain,,,,,https://arxiv.org/abs/2011.05623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"It turns out that you can create adversarial examples for monkeys! The task: classifying a given face as coming from a monkey vs. a human. The method is pretty simple: train a neural network to predict what monkeys would do, and then find adversarial examples for monkeys. These examples don’t transfer perfectly, but they transfer enough that it seems reasonable to call them adversarial examples. In fact, these adversarial examples also make humans make the wrong classification reasonably often (though not as often as with monkeys), when given about 1 second to classify (a fairly long amount of time). Still, it is clear that the monkeys and humans are much more behaviorally robust than the neural networks."
,,,Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, and Dawn Song,Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation,,,,,https://arxiv.org/abs/1810.05162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ECCV,,,,,,,,,,,,,,,,"This paper considers adversarial attacks on segmentation systems. They find that segmentation systems behave inconsistently on adversarial images, and they use this inconsistency to detect adversarial inputs. Specifically, they take overlapping crops of the image and segment each crop. For overlapping crops of an adversarial image, they find that the segmentation are more inconsistent. They defend against one adaptive attack."
,,,Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song,Spatially Transformed Adversarial Examples,,,,,https://arxiv.org/abs/1801.02612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR,,,,,,,,,,,,,,,,"Many adversarial attacks perturb pixel values, but the attack in this paper perturbs the pixel locations instead. This is accomplished with a smooth image deformation which has subtle effects for large images. For MNIST images, however, the attack is more obvious and not necessarily content-preserving (see Figure 2 of the paper)."
,,,Laurent Orseau, Simon McGregor McGill, Shane Legg,Agents and Devices: A Relative Definition of Agency,,,,,https://arxiv.org/abs/1805.12387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper considers the problem of modeling other behavior, either as an agent (trying to achieve some goal) or as a device (that reacts to its environment without any clear goal). They use Bayesian IRL to model behavior as coming from an agent optimizing a reward function, and design their own probability model to model the behavior as coming from a device. They then use Bayes rule to decide whether the behavior is better modeled as an agent or as a device. Since they have a uniform prior over agents and devices, this ends up choosing the one that better fits the data, as measured by log likelihood.In their toy gridworld, agents are navigating towards particular locations in the gridworld, whereas devices are reacting to their local observation (the type of cell in the gridworld that they are currently facing, as well as the previous action they took). They create a few environments by hand which demonstrate that their method infers the intuitive answer given the behavior."
,,,Toby Shevlane, Allan Dafoe,The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,,,,,https://arxiv.org/abs/2001.00463,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Since <@GPT-2@>(@Better Language Models and Their Implications@), the AI research community has wrestled with the question of publication of research with malicious applications. On the one hand, publishing such research makes it more likely that those malicious applications arise in reality, but on the other hand, it also allows defenses against the application to be developed. The core of the question is what the _offense-defense balance_ of AI research looks like.In particular, publication is particularly good if attackers are likely to independently develop the knowledge, or would find it hard to translate the research into a real-world attack, or if defenders will put in a lot of effort to finding a solution, and such a solution is likely to be found and deployed. A canonical example is computer security: once a vulnerability is found, it is usually quite easy to develop a patch that fixes the vulnerability, and such patches can be deployed relatively easily via automatic updates. As a result, in computer security, the default is to publicly disclose vulnerabilities after giving vendors some time to develop and deploy a patch.Under the opposite conditions, where attackers are likely to be able to use the research to create a real-world attack, or where defenders would find it hard to find and deploy a good solution, it is better to keep the research secret. For example, in biorisks such as the risk of an engineered pandemic, solutions are not necessarily easy to find and/or deploy, and so it seems better to avoid making public the knowledge of how to create a novel virus.The paper argues that relative to computer security (the default comparison for many AI researchers), publication in AI is more likely to be net negative (specifically from a security standpoint, ignoring beneficial applications of the research), since solutions must often be social (as in e.g. fake news) which are harder to deploy, and publication seems more likely to counterfactually educate attackers rather than defenders (since the defenders are big companies that already have a lot of expertise)."
,,,Owain Evans*, Owen Cotton-Barratt*, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, William Saunders,Truthful AI: Developing and governing AI that does not lie,,,,,https://arxiv.org/abs/2110.06674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper argues that we should develop both the technical capabilities and the governance mechanisms necessary to ensure that AI systems can be made _truthful_. We will primarily think about conversational AI systems here (so not, say, AlphaFold).Some key terms:1. An AI system is **honest** if it only makes statements that it actually believes. (This requires you to have some way of ascribing beliefs to the system.) In contrast, **truthfulness** only checks if statements correspond to reality, without making any claims about the AI system’s beliefs.2. An AI system is **broadly truthful** if it doesn’t lie, volunteers all the relevant information it knows, is well-calibrated and knows the limits of its information, etc.3. An AI system is **narrowly truthful** if it avoids making **negligent suspected-falsehoods**. These are statements that can feasibly be determined by the AI system to be unacceptably likely to be false. Importantly, a narrowly truthful AI is not required to make contentful statements, it can express uncertainty or refuse to answer.This paper argues for narrow truthfulness as the appropriate standard. Broad truthfulness is not very precisely defined, making it challenging to coordinate on.  Honesty does not give us the guarantees we want: in settings in which it is advantageous to say false things, AI systems might end up being honest but **deluded**. They would honestly report their beliefs, but those beliefs might be false.Narrow truthfulness is still a much stronger standard than we impose upon humans. This is desirable because (1) AI systems need not be constrained by social norms the way humans are; consequently they need stronger standards, and (2) it may be less costly to enforce that AI systems are narrowly truthful than to enforce that humans are narrowly truthful, so a higher standard is more feasible.Evaluating the (narrow) truthfulness of a model is non-trivial. There are two parts: first, determining whether a given statement is unacceptably likely to be false, and second, determining whether the model was negligent in uttering such a statement. The former could be done by having human processes that study a wide range of information and determine whether a given statement is unacceptably likely to be false. In addition to all of the usual concerns about the challenges of evaluating a model that might know more than you, there is also the challenge that it is not clear exactly what counts as “unacceptably likely to be false”. For example, if a model utters a false statement but expresses low confidence, how should that be rated? The second part, determining negligence, needs to account for the fact that the AI system might not have had all the necessary information, or that it might not have been capable enough to come to the correct conclusion. One way of handling this is to compare the AI system to other AI systems built in a similar fashion.How might narrow truthfulness be useful? One nice thing it enables is **truthfulness amplification**, in which we can amplify properties of a model by asking a web of related questions and combining the answers appropriately. For example, if we are concerned that the AI system is deceiving us on just this question, we could ask it whether it is deceiving us, or whether an investigation into its statement would conclude that it was deceptive. As another example, if we are worried that the AI system is making a mistake on some question where its statement isn’t _obviously_ false, we can ask it about its evidence for its position and how strong the evidence is (where false statements are more likely to be negligently false).Section 3 is devoted to the potential benefits and costs if we successfully ensure that AI systems are narrowly truthful, with the conclusion that the costs are small relative to the benefits and can be partially mitigated. Section 6 discusses other potential benefits and costs if we attempt to create truthfulness standards to ensure the AI systems are narrowly truthful. (For example, we might try to create a truthfulness standard but instead create an institution that makes sure that AI systems follow a particular agenda (by only rating as true the statements that are consistent with that agenda). Section 4 talks about the governance mechanisms we might use to implement a truthfulness standard. Section 5 describes potential approaches for building truthful AI systems. As I mentioned in the highlighted post, these techniques are general alignment techniques that have been specialized for truthful AI."
,,,Mohamed Abdalla, Moustafa Abdalla,"The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity",,,,,https://arxiv.org/abs/2009.13676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Big tech companies fund a lot of academic research, including on AI ethics. This paper points out that we would not trust research on smoking that was funded by tobacco companies: why should AI ethics research be any different? Enough information has now surfaced (through litigation) for us to see that Big Tobacco’s actions were clearly unacceptable, but it took years for this to be realized. The same thing could be happening again with Big Tech.The paper identifies four goals that drive investment into academia by big industries, and argues that these are consistent with the actions of Big Tobacco and Big Tech. First, funding academic research allows companies to present themselves as socially responsible. For example, some researchers have argued that academic or non-profit institutions like the ACLU and MIT do not have any effective power in the Partnership on AI and their membership ends up serving a legitimating function for the companies in the partnership.Second, companies can influence the events and decisions made by universities. Top conferences in ML receive large sponsorships from companies, and many of the workshops have such sponsorships as well, including ones about AI ethics.Third, companies can influence the research conducted by individual scientists. The authors studied funding of professors at four top universities, and found that of the cases where they could determine funding, over 52% had been funded by Big Tech, and the number rose to 58% when restricting to those who had published in ethics or fairness. There need not be any explicit pressure for this to be an issue: the implicit threat of loss of funding can be enough to prevent some types of research.Fourth, companies can discover academics who can be leveraged in other situations. For example, tobacco companies explicitly searched for academics who would testify in favor of the companies at legislative hearings. In Big Tech, there are similar suggestive stories: for example, in one case a professor who had been funded indirectly by Google criticized antitrust scrutiny of Google. They then joined the FTC, and shortly after the FTC dropped their antitrust suit against Google.The paper concludes with some ideas on how the current situation could be improved."
,,,Aaron D. Tucker, Markus Anderljung, Allan Dafoe,Social and Governance Implications of Improved Data Efficiency,,,,,https://arxiv.org/abs/2001.05068,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Few-shot learning, meta learning, transfer learning, active learning: there's a lot of types of learning that are aiming to improve the data efficiency of ML techniques. What happens if we succeed? This paper propose two effects: an _access effect_, by which smaller actors can start using ML capabilities with their smaller amounts of data, and a _performance effect_, by which existing actors see improvements in the performance of their AI systems (since their existing data goes further than it used to). It then analyzes some societal implications of these effects.By making it easier to reach a given performance with limited data, we will gain access to new applications where data is limited (e.g. machine translation of ancient languages), and for existing applications, more actors will be able to use ML capabilities (this also includes bad actors, who can more easily pursue malicious applications). However, it is not clear how this will affect the competitive advantage of large AI firms: while more actors can access a given level of performance, which might suggest more competition, the large AI firms also gain performance, which could reverse the effect. For example, improved data efficiency makes no difference in a pure winner-take-all situation, and _advantages_ the large firms in cases where the last few miles of performance lead to large gains in utility (e.g. self-driving cars).The paper also makes two comments on the impacts for AI safety: that algorithms based on human oversight will become more competitive (as it will be more reasonable to collect expensive human data), and that distributional shift problems may become worse (since if you train on smaller amounts of data, you are less likely to see "rare" inputs)."
,,,Miljan Martic, Jan Leike, Andrew Trask, Matteo Hessel, Shane Legg, Pushmeet Kohli,Scaling shared model governance via model splitting,,,,,https://arxiv.org/abs/1812.05979,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE Global Communications Conference (GLOBECOM),,,,,,,,,,,,,,,,"Suppose that two organizations want to develop a deep learning model together without allowing either one to unilaterally use the model. This can be done cryptographically using homomorphic encryption or secure multiparty computation, but this introduces several orders of magnitude of slowdown. What about the much simpler solution of letting each organization have half of the parameters, that are not shared with the other organization? For this to be secure, it should be prohibitively difficult to find the other organization's parameters. In the least convenient world where each organization has access to all training data, hyperparameters etc., this is the security of the _model completion problem_, where given all of the normal setup for deep learning as well as half of the trained parameters for a model M, the goal is to create a new model that performs as well as M. Of course, we can simply rerun the training procedure that was used to create M, so the cost is bounded above by the cost to create M in the first place. We might be able to do better by leveraging the trained parameters that we know -- for example, by using those parameters as an initialization for the model instead of whatever initialization we normally use. The paper empirically investigates how well strategies like this can work. They find that it is relatively easy to create a model that achieves good performance (getting 80% of the way to the best performance), but quite difficult to achieve performance as good as that of M, typically requiring 40-100% of the time it took to create M."
,,,Alexander Kott, Luigi V Mancini, Paul Théron, Martin Drašar, Edlira Dushku, Heiko Günther, Markus Kont, Benoît LeBlanc, Agostino Panico, Mauno Pihelgas, Krzysztof Rzadca,Initial Reference Architecture of an Intelligent Autonomous Agent for Cyber Defense,,,,,https://arxiv.org/abs/1803.10664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,See [Import AI's summary](https://jack-clark.net/2018/04/02/importai-88-nato-designs-a-cyber-defense-ai-object-detection-improves-with-yolov3-france-unveils-its-national-ai-strategy/).
,,,Gabriel Dulac-Arnold, Daniel Mankowitz, Todd Hester,Challenges of Real-World Reinforcement Learning,,,,,https://arxiv.org/abs/1904.12901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper is a fairly clear and well-done literature review focusing on the difficulties that will need to be overcome in order to train and deploy reinforcement learning on real-world problems. They describe each of these challenges - which range from slow simulation speeds, to the need to frequently learn off-policy, to the importance of safety in real world systems - and for each propose or refer to an existing metric to capture how well a given RL model addresses the challenge. Finally, they propose a modified version of a humanoid environment with some of these real-world-style challenges baked in, and encourage other researchers to test systems within this framework. "
,,,Joel Klinger, Juan Mateos-Garcia, Konstantinos Stathoulopoulos,A narrowing of AI research?,,,,,https://arxiv.org/abs/2009.10385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Technology development can often be _path-dependent_, where initial poorly-thought-out design choices can persist even after they are recognized as poorly thought out. For example, the QWERTY keyboard persists to this day, because once enough typists had learned to use it, there was too high a cost to switch over to a better-designed keyboard. This suggests that we want to maintain a diversity of approaches to AI so that we can choose amongst the best options, rather than getting locked into a suboptimal approach early on.The paper then argues, based on an analysis of arXiv papers, that thematic diversity in AI has been going down over time, as more and more papers are focused on deep learning. Thus, we may want to have policies that encourage more diversity. It also has a lot of additional analysis of the arXiv dataset for those interested in a big-picture overview of what is happening in the entire field of AI."
,,,Jared Kaplan*, Sam McCandlish*, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei,Scaling Laws for Neural Language Models,,,,,https://arxiv.org/abs/2001.08361,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper empirically measures the effect of scaling model complexity, data, and computation on the cross entropy loss for neural language models. A few results that I would highlight are:_Performance depends strongly on scale, weakly on model shape:_ Loss depends more strongly on the number of parameters, the size of the dataset, and the amount of compute used for training than on architecture hyperparameters._Smooth power laws:_ All three of these show power-law relationships that don’t flatten out even at the highest performance they reached._Sample efficiency:_ Larger models are more efficient than small models in both compute and data. For maximum computation efficiency, it is better to train large models and stop before convergence.There are lots of other interesting conclusions in the paper not included here; section 1.1 provides a very nice one page summary of these conclusions, which I'd recommend you read for more information."
,,,Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit,A Constructive Prediction of the Generalization Error Across Scales,,,,,https://arxiv.org/abs/1909.12673,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This earlier paper also explicitly studies the relationship of test error to various inputs, on language models and image classification (the previous paper studied only language models). The conclusions agree with the previous paper quite well: it finds that smooth power laws are very good predictors for the influence of dataset size and model capacity. (It fixed the amount of compute, and so did not investigate whether there was a power law for compute, as the previous paper did.) Like the previous paper, it found that it basically doesn't matter whether the model size is increased by scaling the width or the depth of the network."
,,,Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le,Towards a Human-like Open-Domain Chatbot,,,,,https://arxiv.org/abs/2001.09977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents a chatbot called Meena that reaches near human-level performance for measures of human likeness. The authors mined social media to find 341 GB of public domain conversations, and trained an [evolved transformer](https://arxiv.org/abs/1901.11117) on those conversations. To test its performance, they devised a metric they call Sensibility and Specificity (SSA) which measures how much sense the chatbot's responses make in context, as well as whether they were specific. SSA was tightly correlated with perplexity and a subjective measure of human likeness, suggesting that optimizing for perplexity will translate to greater conversational ability. Meena substantially improved on the state of the art, including both hand-crafted bots like [Mitsuku](https://en.wikipedia.org/wiki/Mitsuku) and the neural model [DialoGPT](https://arxiv.org/abs/1911.00536), though it still falls short of human performance. You can read some conversation transcrips [here](https://github.com/google-research/google-research/blob/master/meena/meena.txt); many of the responses from Meena are very human-like.See also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)"
,,,David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel,ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring,,,,,https://arxiv.org/abs/1911.09785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"A common criticism of deep learning is that it requires far too much training data. Some view this as a fundamental flaw that suggests we need a new approach. However, considerable data efficiency is possible with a new technique called ReMixMatch. ReMixMatch on CIFAR-10 obtains 84.92% accuracy using only 4 labeled examples per class. Using 250 labeled examples, or around 25 labeled examples per class, a ReMixMatch model on CIFAR-10 has 93.73% accuracy. This is approximately how well a vanilla ResNet does on CIFAR-10 with 50000 labeled examples. Two years ago, special techniques utilizing 250 CIFAR-10 labeled examples could enable an accuracy of approximately [53%](https://paperswithcode.com/sota/semi-supervised-image-classification-on-3). ReMixMatch builds on [MixMatch](https://arxiv.org/abs/1905.02249) and has several seemingly arbitrary design decisions, so I will refrain from describing its design. In short, deep networks do not necessarily require large labeled datasets.And just yesterday, after this summary was first written, the [FixMatch](https://arxiv.org/abs/2001.07685) paper got even better results."
,,,Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu,"Relational inductive biases, deep learning, and graph networks",,,,,http://arxiv.org/abs/1806.01261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2018,,,,,,,,,,,,,,,,""Part position paper, part review, and part unification", this paper emphasises the importance of combinatorial generalisation, which is key to how humans understand the world. It argues for approaches which perform computation over discrete entities and the relations between them, such as graph networks. The authors claim that CNNs and RNNs are so successful due to relational inductive biases - for example, the bias towards local structure induced by convolutional layers. Graph networks are promising because they can express arbitrary relational biases: any nodes can be connected with any others depending on the structure of the problem. Further, since graph networks learn functions which are reused for all nodes and edges, each one can be applied to graphs of any shape and size: a form of combinatorial generalisation.In this paper's framework, each 'graph block' does computations over an input graph and returns an output graph. The relevant part of the output might be the values of edges, or those of nodes, or 'global' properties of the overall graph. Graph blocks can be implemented by standard neural network architectures or more unusual ones such as message-passing neural networks or non-local neural networks. The authors note some major open questions: how to generate the graphs in the first place, and how to adaptively modify them during the course of computation."
,,,Jacob Austin*, Augustus Odena*, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton,Program Synthesis with Large Language Models,,,,,https://arxiv.org/abs/2108.07732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Can we use large language models to solve programming problems? In order to answer this question, this paper builds the Mostly Basic Python Programming (MBPP) dataset. The authors asked crowd workers to provide a short problem statement, a Python function that solves the problem, and three test cases checking correctness. On average across the 974 programs, the reference solution has 7 lines of code, suggesting the problems are fairly simple. (This is partly because you can use library functions.) They also edit a subset of 426 problems to improve their quality, for example by making the problem statement less ambiguous or making the function signature more normal.They evaluate pretrained language models on this dataset across a range of model sizes from 0.244B to 137B parameters. (This largest model is within a factor of 2 of GPT-3.) They consider both few-shot and finetuned models. Since we have test cases that can be evaluated automatically, we can boost performance by generating lots of samples (80 in this case), evaluating them on the test cases, and then keeping the ones that succeed. They count a problem as solved if any sample passes all the test cases, and report as their primary metric the fraction of problems solved according to this definition. Note however that the test cases are not exhaustive: when they wrote more exhaustive tests for 50 of the problems, they found that about 12% of the so-called “solutions” did not pass the new tests (but conversely, 88% did). They also look at the fraction of samples which solve the problem, as a metric of the reliability or confidence of the model for a given problem.Some of their findings:1. Performance increases approximately log-linearly with model size. The trend is clearer and smoother by the primary metric (fraction of problems solved by at least one sample) compared to the secondary metric (fraction of samples that solve their problem).2. Finetuning provides a roughly constant boost across model sizes. An exception: at the largest model size, finetuning provides almost no benefit, though this could just be noise.3. It is important to provide at least one test case to the model (boosts problems solved from 43% to 55%) but after that additional test cases don’t make much of a difference (an additional two examples per problem boosts performance to 59%).4. In few-shot learning, the examples used in the prompt matter a lot. In a test of 15 randomly selected prompts for the few-shot 137B model, the worst one got ~1%, while the best one got ~59%, with the others distributed roughly uniformly between them. Ensembling all 15 prompts boosts performance to 66%.5. In rare cases, the model overfits to the test cases. For example, in a question about checking whether the input is a Woodall number, there is only one test checking an actual Woodall number (383), and the model generates a program that simply checks whether the input is 383.6. When choosing the best of multiple samples, you want a slightly higher temperature, in order to have more diversity of possible programs to check.7. It is important to have high quality problem descriptions as input for the model. The 137B model solves 79% of problems in the edited dataset, but only solves 63% of the original (unedited) versions of those problems. The authors qualitatively analyze the edits on the problems that switched from unsolved to solved and find a variety of things that you would generally expect to help.Now for the controversial question everyone loves to talk about: does the model _understand_ the meaning of the code, or is it “just learning statistical correlations”? One way to check this is to see whether the model can also _execute_ code. Specifically, we provide the ground truth code for one of the problems in the MBPP dataset along with one of the test case inputs and ask the model to predict the output for that test case. Even after finetuning for this task, the 137B model gets only 21% right. This can be boosted to 27% by also providing example test cases for the code before predicting the output for a new test case. Overall, this suggests that the model doesn’t “understand” the code yet.We can take the model finetuned for execution and see how well it does on program synthesis. (We can do this because there are different prompts for execution and synthesis.) For the 8B model, the finetuning makes basically no difference: it’s equivalent to the original few-shot setting. However, for the 137B model, finetuning on execution actually leads to a small but non-trivial improvement in performance (from ~59% to ~63%, I think). This is true relative to either the few-shot or finetuned-for-synthesis setting, since they performed near-identically for the 137B model. So in fact the 137B model finetuned on execution is actually the strongest model, according to synthesis performance.So far we’ve just been looking at how our model performs when taking the best of multiple samples. However, if our goal is to actually use models for program synthesis, we aren’t limited to such simple tricks. Another approach is to have a human provide _feedback_ in natural language when the model’s output is incorrect, and then have the model generate a new program. This feedback is very informal, for example, “Close, but you need to replace the underscore with an empty string”. This provides a huge performance boost: the 137B solves ~31% of problems on its first sample; adding just a single piece of human feedback per problem boosts performance to ~55%, and having four rounds of human feedback gets you to over 65%.The authors also introduce the MathQA-Python dataset, which provides arithmetic word problems and asks models to write programs that would output the correct answer to the problem. They only run a few experiments on this dataset, so I’ve mostly ignored it. The main upshot is that a finetuned 137B parameter model can solve 83.8% of problems with _some_ sample. They don’t report metrics with a single sample, which seems like the more relevant metric for this dataset, but eyeballing other graphs I think it would be around 45%, which you could probably boost a little bit by decreasing the sampling temperature."
,,,Mark Chen*, Jerry Tworek*, Heewoo Jun*, Qiming Yuan*, Henrique Ponde*, Jared Kaplan*, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, Will Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba,Evaluating Large Language Models Trained on Code,,,,,https://arxiv.org/abs/2107.03374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"You’ve probably heard of GitHub Copilot, the programming assistant tool that can provide suggestions while you are writing code. This paper evaluates Codex, a precursor to the model underlying Copilot. There’s a lot of content here; I’m only summarizing what I see as the highlights.The core ingredient for Codex was the many, many public repositories on GitHub, which provided hundreds of millions of lines of training data. With such a large dataset, the authors were able to get good performance by training a model completely from scratch, though in practice they finetuned an existing pretrained GPT model as it converged faster while providing similar performance.Their primary tool for evaluation is HumanEval, a collection of 164 hand-constructed Python programming problems where the model is provided with a docstring explaining what the program should do along with some unit tests, and the model must produce a correct implementation of the resulting function. Problems are not all equally difficult; an easier problem asks Codex to “increment all numbers in a list by 1” while a harder one provides a function that encodes a string of text using a transposition cipher and asks Codex to write the corresponding decryption function.To improve performance even further, they collect a sanitized finetuning dataset of problems formatted similarly to those in HumanEval and train Codex to perform well on such problems. These models are called Codex-S. With this, we see the following results:1. Pretrained GPT models get roughly 0%.2. The largest 12B Codex-S model succeeds on the first try 29% of the time. (A Codex model of the same size only gets roughly 22%.)3. There is a consistent scaling law for reduction in loss. This translates into a less consistent graph for performance on the HumanEval dataset, where once the model starts to solve at least (say) 5% of the tasks, there is a roughly linear increase in the probability of success when doubling the size of the model.4. If instead we generate 100 samples and check whether they pass the unit tests to select the best one, then Codex-S gets 78%. If we still generate 100 samples but select the sample that has the highest mean log probability (perhaps because we don’t have an exhaustive suite of unit tests), then we get 45%.They also probe the model for bad behavior, including misalignment. In this context, they define misalignment as a case where the user wants A, but the model outputs B, and the model is both capable of outputting A and capable of distinguishing between cases where the user wants A and the user wants B.Since Codex is trained primarily to predict the next token, it has likely learned that buggy code should be followed by more buggy code, that insecure code should be followed by more insecure code, and so on. This suggests that if the user accidentally provides examples with subtle bugs, then the model will continue to create buggy code, even though the user would want correct code. They find that exactly this effect occurs, and that the divergence between good and bad performance _increases_ as the model size increases (presumably because larger models are better able to pick up on the correlation between previous buggy code and future buggy code)."
,,,Ethan Perez, Douwe Kiela, Kyunghyun Cho,True Few-Shot Learning with Language Models,,,,,https://arxiv.org/abs/2105.11447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We can get <@GPT-3@>(@Language Models are Few-Shot Learners@) to perform useful tasks using “prompt programming”, in which we design an input sentence such that the most likely continuation of that sentence would involve GPT-3 performing the task of interest. For example, to have GPT-3 answer questions well, we might say something like “The following is a transcript of a dialogue with a helpful, superintelligent, question-answering system:”, followed by a few example question-answer pairs, after which we ask our questions.Since the prompts only contain a few examples, this would seem to be an example of strong _few-shot learning_, in which an AI system can learn how to do a task after seeing a small number of examples of that task. This paper contends that while GPT-3 is capable of such few-shot learning, the results reported in various papers exaggerate this ability. Specifically, while it is true that the prompt only contains a few examples, researchers often tune their choice of prompt by looking at how well it performs on a relatively large validation set -- which of course contains many examples of performing the task, something we wouldn’t expect to have in a true few-shot learning context.To illustrate the point, the authors conduct several experiments where we start with around 12 possible prompts and must choose which to use based only on the examples given (typically 5). They test two methods for doing so:1. Cross-validation: Given a prompt without examples, we attach 4 of the examples to the prompt and evaluate it on the last example, and average this over all possible ways of splitting up the examples.2. Minimum description length: While cross-validation evaluates the final generalization loss on the last example after updating on previous examples, MDL samples an ordering of the examples and then evaluates the average generalization loss as you feed the examples in one-by-one (so more like an online learning setup).On the LAMA-UHN task, the difference between a random prompt and the best prompt looks to be roughly 5-6 percentage points, regardless of model size. Using MDL or cross-validation usually gives 20-40% of the gain, so 1-2 percentage points. This suggests that on LAMA-UHN, typical prompt-based “few-shot” learning results are likely 3-5 percentage points higher than what you would expect if you were in a true few-shot setting where there is no validation set to tune on. This is all on average across tasks -- for any given task, you could recover the entire gain, or you might select a prompt that is actually worse than what you'd get from random chance.But it may actually be worse than that. We’ve talked just about the prompt so far, but the validation set can also be used to improve hyperparameters, network architecture, the design of the learning algorithm etc. This could also lead to inflated results. The authors conduct one experiment with ADAPET on SuperGLUE which suggests that using the validation set to select hyperparameters can also lead to multiple percentage points of inflation."
,,,Henry W. Lin, Max Tegmark, David Rolnick,Why does deep and cheap learning work so well?,,,,,https://arxiv.org/abs/1608.08225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Journal of Statistical Physics,,,,,,,,,,,,,,,,"We know that the success of neural networks must be at least in part due to some inductive bias (presumably towards “simplicity”), based on the following empirical observations:1. Neural networks with mere millions of parameters work well with high-dimensional inputs such as images, despite the fact that, speaking loosely, there are exponentially more functions from images to classifications than there are functions expressible by million-parameter neural networks.2. Neural networks learn solutions that generalize well even in the overparameterized regime, where statistical learning theory would predict that they overfit.3. Relatedly, neural networks learn solutions that generalize well, despite the fact that they can memorize a _randomly_ labeled training dataset of the same size.Can we say more about this inductive bias towards simplicity? This paper tackles this question from the perspective of the first empirical observation: what is it about neural networks and/or reality such that relatively small neural networks can still learn the “correct” function? We can’t appeal to the fact that neural networks are universal function approximators, because that theorem doesn’t put a bound on the size of the neural network. The core idea of this paper is that any function that we care to model with neural networks in practice tends to be quite simple: in particular, it can often be expressed as a polynomial plus a few extra things.Typically, we’re interested in modeling the relationship between some latent class y and some detailed observations or data x. For example, y might be a concept like “cat” or image labels more broadly, while x might be specific natural images. In this case the causal structure in reality looks like y → x. In our example, there is first an actual cat (y), and then via the physics of light and cameras we get the image of the cat (x).Given this setup, why are functions of interest typically “just polynomials”? Well, thanks to Taylor expansions, all (smooth) functions can be expressed as infinite polynomials, so let’s rephrase the question: why are they polynomials with only a few terms?The negative log probability -ln p(x | y) is called the _Hamiltonian_ in statistical physics. There are lots of reasons you might expect that the Hamiltonian is a simple low order polynomial:1. The Hamiltonians of several fundamental physical laws are polynomials of order 2-4. A polynomial of order d can have at most O(n^d) terms (where n is the number of input variables in the polynomial).2. The Gaussian distribution (often created in reality thanks to the Central Limit Theorem) has a quadratic Hamiltonian (i.e. order 2).3. Most functions of interest have a _locality_ property: things only directly affect what is in their immediate vicinity. This causes almost all of the coefficients in the Taylor series to vanish.4. Many functions have symmetry properties that can further reduce the number of parameters needed to specify them.One might respond that while this could be true for simple functions like predicting the sum of independent events, this wouldn’t apply for the complex functions like “cat” → cat image. Here the authors appeal to _hierarchy_: in practice, the world is very hierarchical, and complex functions can usually be broken down into sequences of simpler ones. If we agree that the simple ones can be implemented with simple polynomials, then a deep neural network could simply learn the same sequence of operations (here the depth of the network is used to chain the operations one after the other).So far we’ve argued that generative models p(x | y) tend to be simple polynomials. What about discriminative models p(y | x)? Well, if we can implement the Hamiltonian -ln p(x | y), then there is a simple way to get p(y | x): we simply calculate the Hamiltonian for all possible y, and then add in the prior probabilities -ln p(y) (which can be done through the bias term of the logit layer), and apply a softmax layer to the result. Indeed, the softmax layer at the end is best practice in ML for creating such models. In addition, in the case of a hierarchical sequence of steps, we can invert that sequence of steps and throw away unnecessary information at each step.Okay, so far we’ve argued that the functions we care about learning can be expressed with polynomials with relatively few terms (in particular, not an exponential number of terms). What does this have to do with neural networks? It turns out that neural networks can express polynomials quite easily. In particular, the authors show:1. Multiplication of two real numbers can be approximated arbitrarily well by a neural network with a hidden layer containing 4 neurons.2. As a result, any given multivariate polynomial can be approximated arbitrarily well by a (potentially deep) neural network of size a little larger than 4 times the number of multiplications needed to evaluate the polynomial.The authors also show that depth is required for the second result: for a single-layer neural network to multiply n inputs arbitrarily well, it _must_ have at least 2^n neurons (under the assumption that the nonlinear activation function is smooth)."
,,,Danny Hernandez, Jared Kaplan, Tom Henighan, Sam McCandlish,Scaling Laws for Transfer,,,,,https://arxiv.org/abs/2102.01293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper studies empirical scaling laws for transfer learning in language models. The authors use Transformer-based models to predict Python code by training on three different dataset curricula:- Training from-scratch on Python code- Pre-training on natural language, then fine-tuning on Python code- Pre-training on natural language and non-Python code, then fine-tuning on Python codeThe authors then measure the "effective data transferred" from pre-training-- if we wanted to replace all the pre-training steps with from-scratch training, maintaining the same loss, how much additional from-scratch data would we need?They find that when the amount of data used to train is small, effective data transferred is described by a simple power-law function of **D_F**, the amount of data used for fine-tuning, and **N**, the number of parameters: **k (D_F)^α (N)^β**, for constants k, α, and β.In their experiments, **β** doesn't change between pre-training on natural language and pre-training on a mixture of natural language and non-Python code. They hypothesize that **β** measures how the model architecture generalizes on the target distribution, and doesn't depend on the contents of the pre-training data.The authors think that **α** is a measure of the directed proximity of the pre-training and from-scratch distributions, with smaller **α** indicating closer proximity. Measuring **α** can be done cheaply by changing the finetuning dataset size while holding the pretrained model constant, making it useful for deciding between collecting more fine-tuning data and increasing model size. For pre-training on natural language and fine-tuning on Python, **β** is about **2 * α**, so for decreasing loss, increasing the fine-tuning dataset size by a factor of **C** (e.g., 100x) would be worth approximately the same as increasing the model size by **√C** (e.g. 10x).The authors find that pre-training on a mixture of natural language and non-Python code has a higher **k** but lower **α** than pre-training on natural language alone. The higher **k** indicates that the mixture model has better transfer performance when trained in the low data regime, while the lower **α** value means that benefits of the mixture model diminish as more data is used.The authors also observe that:- Not counting pre-training compute, pre-trained models are generally more compute efficient than from-scratch models when trained in the low data regime, approximately as compute efficient in the medium data regime, and less compute efficient in the high data regime (close to convergence).- Small pre-trained models perform worse than small from-scratch models in the high data regime. The authors call this phenomenon "ossification"-- a term used to suggest that small pre-trained models may have a hard time moving away from bad initializations.- In general, pre-trained models of a given size are compute efficient (on the frontier of loss given compute) for a large portion of their fine-tuning. From-scratch models, by contrast, are only compute efficient for a narrow window of training-- using too little compute for a given model dramatically increases loss and suggests that you should instead be using a smaller model. This makes pre-trained models in some sense "easier" to train."
,,,Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma,Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,,,,,https://arxiv.org/abs/2002.11328,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"A fundamental result in ML theory shows that the squared error loss function can be decomposed into two components: bias and variance. Suppose that we train a model f to predict some ground truth function y. The bias measures how incorrect the model will be _in expectation over the training process_, while the variance measures how different the model’s output can be over different runs of the training process. More concretely, imagine that we run a training process N times, each with a different training set drawn iid from the same underlying training distribution, to get N different models. Bias is like taking the average of these N models, and asking how far away it is from the truth. Meanwhile, variance is like the average distance from each of the N models to the average of all of the N models.Classical ML predicts that larger models have *lower bias* but *higher variance*. This paper shows that instead, the variance of deep NNs first increases but then decreases at larger model sizes. If the bias tends to be much larger than variance, then we see monotonically decreasing total error. If the variance tends to be much larger than the bias, then loss will also look bell-shaped, initially _increasing_ as models get bigger and then decreasing. Finally, if the bias starts high, but over time is overshadowed by the variance, we get <@double descent@>(@Deep Double Descent@) curves; this explains why previous work needed to add label noise to get double descent curves (as higher label noise should lead to higher variance).In order to estimate the variance, the authors split their data into two subsets and use these to create an unbiased estimator of the variance (effectively following a similar procedure to the one described in the first paragraph). The bias estimate can then be determined from the test loss and the estimated variance. They then test how various factors contribute to test loss. As expected, label noise increases variance. Out-of-distribution samples have higher test loss, which is driven by both bias and variance, but most of the increase comes from bias. Deeper networks sharing the same architecture have lower bias but higher variance."
,,,Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei,Language Models are Few-Shot Learners,,,,,https://arxiv.org/abs/2005.14165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The biggest <@GPT-2 model@>(@Better Language Models and Their Implications@) had 1.5 billion parameters, and since its release people have trained language models with up to 17 billion parameters. This paper reports GPT-3 results, where the largest model has _175 billion_ parameters, a 10x increase over the previous largest language model. To get the obvious out of the way, it sets a new state of the art (SOTA) on zero-shot language modeling (evaluated only on Penn Tree Bank, as other evaluation sets were accidentally a part of their training set).The primary focus of the paper is on analyzing the _few-shot learning_ capabilities of GPT-3. In few-shot learning, after an initial training phase, at test time models are presented with a small number of examples of a new task, and then must execute that task for new inputs. Such problems are usually solved using _meta-learning_ or _finetuning_, e.g. at test time [MAML](https://arxiv.org/abs/1703.03400) takes a few gradient steps on the new examples to produce a model finetuned for the test task. In contrast, the key hypothesis with GPT-3 is that language is so diverse, that doing well on it already requires adaptation to the input, and so the learned language model will _already be a meta-learner_. This implies that they can simply "prime" the model with examples of a task they care about, and the model can _learn_ what task is supposed to be performed, and then perform that task well.For example, consider the task of generating a sentence using a newly made-up word whose meaning has been explained. In one notable example, the prompt for GPT-3 is:_A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:__We were traveling in Africa and we saw these very cute whatpus.__To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:_Given this prompt, GPT-3 generates the following example sentence for "farduddle":_One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles._The paper tests on several downstream tasks for which benchmarks exist (e.g. question answering), and reports zero-shot, one-shot, and few-shot performance on all of them. On some tasks, the few-shot version sets a new SOTA, _despite not being finetuned using the benchmark’s training set_; on others, GPT-3 lags considerably behind finetuning approaches.The paper also consistently shows that few-shot performance increases as the number of parameters increases, and the rate of increase is faster than the corresponding rate for zero-shot performance. While they don’t outright say it, we might take this as suggestive evidence that as models get larger, they are more incentivized to learn “general reasoning abilities”.The most striking example of this is in arithmetic, where the smallest 6 models (up to 6.7 billion parameters) have poor performance (< 20% on 2-digit addition), then the next model (13 billion parameters) jumps to > 50% on 2-digit addition and subtraction, and the final model (175 billion parameters) achieves > 80% on 3-digit addition and subtraction and a perfect 100% on 2-digit addition (all in the few-shot regime). They explicitly look for their test problems in the training set, and find very few examples, suggesting that the model really is learning “how to do addition”; further, when it is incorrect, it tends to make mistakes like “forgetting to carry a 1”.On broader impacts, the authors talk about potential misuse, fairness and bias concerns, and energy usage concerns; and say they about these issues what you’d expect. One interesting note: “To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed.” They find that while there was significant discussion of misuse, they found no successful deployments. They also consulted with professional threat analysts about the possibility of well-resourced actors misusing the model. According to the paper: “The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage.”"
,,,Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava,SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems,,,,,http://arxiv.org/abs/1903.03129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MLSys 2020,,,,,,,,,,,,,,,,"This paper presents an algorithmic technique called SLIDE (Sub-LInear Deep learning Engine) which takes advantage of sparsity in inputs and activations to speed up the training of large neural networks.Suppose that activations at layer k are a_k. Then, the ith element of a_{k+1} is given by the dot product of a_k and w_i for some weight vector w_i. Call w_i the ith neuron of layer k + 1. The largest activations in a_{k+1} are the ones for whom w_i has high magnitude and points in the same direction as a_k. The core proposal of SLIDE is to only compute the largest elements of a_{k+1}, which they call the “activated neurons”, and approximate all of the others are zero, allowing us to avoid a lot of computation.In order to do this, we maintain a data structure called a _locality-sensitive hash table_, which when given an activation a_k can tell us which neurons (w_is) are most similar. We can then compute the outputs for just those neurons to get a_{k+1}. In this way, we can effectively ‘sparsify’ the network, calculating the activations and updating the weights of only a small subset of the neurons. This is what gives us our computational gains.SLIDE randomly initializes weights in the network and generates the locality-sensitive hash table that maps activations to activated neurons. To take a gradient step on an input, it calculates the activated neurons in a forward pass, then backpropagates through the activated neurons, and then updates the locality-sensitive hash table. The hash table update is computationally expensive, and SLIDE uses several mechanisms to make it less costly, such as updating hash tables less frequently later in the training process since gradients are likely to change less then. Due to the sparsity, the gradients for different inputs are often changing different neurons, and so SLIDE asynchronously parallelizes gradient updates without worrying about race conditions, allowing for much better scaling with additional cores.The paper evaluates SLIDE on large multi-label classification tasks, which must run on neural networks with extremely wide final layers. It finds that the CPUs running SLIDE are 1.8 times faster in clock-time than the GPU on the Delicious 200k dataset, and 2.7 times faster than the GPU on the Amazon-670K dataset, with an additional ~1.3x speed-up after performing cache optimization on SLIDE. Scalability tests suggest that the SLIDE CPUs beat GPU performance even when using only 8 cores. The paper claims that SLIDE’s computational benefits come because the number of neurons sampled in the wide final layer is extremely small-- fewer than 0.5% of active neurons."
,,,Preetum Nakkiran,More Data Can Hurt for Linear Regression: Sample-wise Double Descent,,,,,https://arxiv.org/abs/1912.07242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper demonstrates the presence of double descent (in the size of the dataset) for _unregularized linear regression_. In particular, we assume that each data point x is a vector in independent samples from Normal(0, σ^2), and the output is y = βx + ε. Given a dataset of (x, y) pairs, we would like to estimate the unknown β, under the mean squared error loss, with no regularization.In this setting, when the dimensionality d of the space (and thus number of parameters in β) is equal to the number of training points n, the training data points are linearly independent almost always / with probability 1, and so there will be exactly one β that solves the n linearly independent equalities of the form βx = y. However, such a β must also be fitting the noise variables ε, which means that it could be drastically overfitted, with very high norm. For example, imagine β = [1, 1], so that y = x1 + x2 + ε, and in our dataset x = (-1, 3) is mapped to y = 3 (i.e. an ε of +1), and x = (0, 1) is mapped to y = 0 (i.e. an ε of -1). Gradient descent will estimate that β = [-3, 0], which is going to generalize very poorly.As we decrease the number of training points n, so that d > n, there are infinitely many settings of the d parameters of β that satisfy the n linearly independent equalities, and gradient descent naturally chooses the one with minimum norm (even without regularization). This limits how bad the test error can be. Similarly, as we increase the number of training points, so that d < n, there are too many constraints for β to satisfy, and so it ends up primarily modeling the signal rather than the noise, and so generalizing well."
,,,Taesung Park, Ming-Yu Liu, Ting-Chun Wang and Jun-Yan Zhu,Semantic Image Synthesis with Spatially-Adaptive Normalization,,,,,https://arxiv.org/pdf/1903.07291.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CVPR 2019,,,,,,,,,,,,,,,,"This paper shows how to create somewhat realistic images specified by semantic segmentation maps. They accomplish this by modifying batch normalization. Batch normalization modifications can be quite powerful for image generation, even enough to [control style](https://arxiv.org/abs/1703.06868). Their modification is that normalization is a direct function of the semantic segmentation map throughout the network, so that the semantic segmentation map is readily available to each ResBlock. Visualizations produced by this method are [here](https://nvlabs.github.io/SPADE/)."
,,,Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, Peter Battaglia,Relational Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1806.01830,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"This paper uses the self-attention mechanism discussed in 'Relational recurrent neural networks' to compute relationships between entities extracted from input data. The system was tested on the Box-World environment, in which an agent needs to use keys to open boxes in a certain order. It generalised very well to test environments which required much longer sequences of actions than any training examples, and improved slightly on a baseline for Starcraft mini-games."
,,,Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, Sonal Gupta,Muppet: Massive Multi-task Representations with Pre-Finetuning,,,,,https://arxiv.org/abs/2101.11038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper proposes pre-finetuning: given a language model pretrained on a large dataset, we do a second stage where we train the model to solve a large variety of tasks (around 50 in this paper), and only after that do we finetune the model on our actual task of interest. The authors show that this leads to improved results, especially on tasks where we only have limited data."
,,,Andrew L. Jones,Scaling Scaling Laws with Board Games,,,,,https://arxiv.org/abs/2104.03113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"While we've seen <@scaling laws@>(@Scaling Laws for Neural Language Models@) for compute, data, and model size, we haven't yet seen scaling laws for the _problem size_. This paper studies this case using the board game Hex, in which difficulty can be increased by scaling up the size of the board. The author applies AlphaZero to a variety of different board sizes, model sizes, RL samples, etc and finds that performance tends to be a logistic function of compute / samples used. The function can be characterized as follows:1. Slope: In the linearly-increasing regime, you will need about 2× as much compute as your opponent to beat them 2/3 of the time.2) Perfect play: The minimum compute needed for perfect play increases 7× for each increment in board size.3) Takeoff: The minimum training compute needed to see any improvement over random play increases by 4× for each increment of board size.These curves fit the data quite well. If the curves are fit to data from small board sizes and then used to predict results for large board sizes, their error is small.Recall that AlphaZero uses MCTS to amplify the neural net policy. The depth of this MCTS determines how much compute is spent on each decision, both at training time and test time. The author finds that a 10x increase in training-time compute allows you to eliminate about 15x of test-time compute while maintaining similar performance."
,,,Marcus Hutter,Learning Curve Theory,,,,,https://arxiv.org/abs/2102.04074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Like [last week’s highlight](https://arxiv.org/abs/2102.06701) ([AN #140](https://mailchi.mp/229fd666e06b/an-140-theoretical-models-that-predict-scaling-laws)), this paper proposes a theoretical model that could predict empirically observable scaling laws. The author considers a very simple online learning model, in which we are given a feature vector and must classify it into one of two categories. We’ll also consider a very simple tabular algorithm that just memorizes the classifications of all previously seen vectors and spits out the correct classification if it has been seen before, and otherwise says “I don’t know”. How does the error incurred by this algorithm scale with data size?The answer of course depends on the data distribution -- if we always see the same feature vector, then we never make an error after the first timestep, whereas if the vector is chosen uniformly at random, we’ll always have maximal error. The author analyzes several possible data distributions in between these extremes.The most interesting case is when the data is drawn from a Zipf distribution. In this case, when you order the feature vectors from most to least likely, the nth vector has probability proportional to n^(-(α+1)). Then we see a power law for the scaling, n^(-β), where β = α / (α+1). This could explain the scaling laws observed in the wild."
,,,Kartik Chandra, Erik Meijer, Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George, Melissa Grueter, Basil Hosmer, Steffi Stumpos, Alanna Tempest, Shannon Yang,Gradient Descent: The Ultimate Optimizer,,,,,https://arxiv.org/abs/1909.13371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Hyperparameter tuning is an important and tedious step for most applications of machine learning. Often this can cause a project to take significantly longer, as you need to have multiple training runs with different hyperparameters in order to identify which ones work best. How can we do better?This paper shows that in some cases, you can make the computation involving your hyperparameters differentiable, such that they too can be optimized using gradient descent _during the actual training run_. They show this for SGD and Adam (where for Adam they optimize all four hyperparameters, not just the learning rate). Since these hyperparameters are then optimized using another instantiation of gradient descent, that new instantiation also has its own hyperparameters that can once again be optimized. They show how to build an arbitrarily high “stack” of hyperparameter optimizers.In practice, building a stack of just 3 or 4 such optimizers makes it very robust to the initial choice of parameters by a human, while only increasing the cost of training by less than 2x."
,,,Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,,,https://arxiv.org/abs/2006.16668,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper introduces GShard, a module that makes it easy to write parallel computation patterns with minimal changes to existing model code. GShard automatically does a lot of the work of splitting computations across machines, enabling the easy creation of much larger models than before.The authors use GShard to train a 600 billion parameter multilingual Transformer translation model that's wide, rather than deep (36 layers). They use a "mixture of experts" model where some of the individual feed-forward networks in the Transformer are replaced with a set of feed-forward networks-- each one an "expert" in some part of the translation. The experts are distributed across different machines, and the function for sending inputs to experts is learned, with each input being sent to the top two most relevant experts. Since each expert only has to process a fraction of all the inputs, the amount of computation needed is dramatically less than if every input were fed through a single, larger network. This decrease in needed computation comes with a decrease in the amount of weight sharing done by the network.The paper compares the 600 billion parameter model's performance to several other smaller models as well as a 96-layer deep model with only 2.3 billion parameters. For the wide networks, the authors find that in general, larger models do better, but that at some point the larger model starts doing worse for very "low-resource" languages-- languages that don't have much training data available. The authors argue that this is because the low-resource languages benefit from "positive language transfer", an effect where weights encode knowledge learned from training on other languages that can then be applied to the low-resource ones. As you increase the number of experts in the wide model past a certain point, the amount of training that each expert does decreases, so there's less positive language transfer to low-resource languages within each expert.They also find that deeper networks are more sample efficient, reaching better test error with the same amount of training examples, but are less computationally efficient (given current constraints). The 600 billion parameter, 36-layer model takes 22.4 TPU core years and 4 days to train, reaching a score on the BLEU benchmark of 44.3. The 2.3 billion parameter, 96-layer model takes 235 TPU core years and 42 days to train, reaching a score on the BLEU benchmark of 36.9."
,,,Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn,Universal Planning Networks,,,,,https://arxiv.org/abs/1804.00645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This is an architecture that has a differentiable planning module, that is, a neural network that takes in (encodings of) states or observations and produces actions. You can use this in conjunction with eg. expert demonstrations (as in imitation learning) in order to learn features that are optimized for the purpose of planning, focusing only on the details relevant to the task, unlike an auto-encoder, which must reconstruct the entire image, including irrelevant details."
,,,Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin,Linear Mode Connectivity and the Lottery Ticket Hypothesis,,,,,https://arxiv.org/abs/1912.05671,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Instability analysis looks at how sensitive neural network training is to noise in SGD. A network is called stable if the test error remains approximately constant along the line connecting network weights obtained by training on differently ordered data. The authors find that most popular networks in image classification are unstable at initialization for more challenging tasks but become stable long before convergence. They also find that <@winning tickets@>(@Understanding the generalization of ‘lottery tickets’ in neural networks@) found by iterative magnitude pruning are usually stable, while unstable subnetworks don't manage to match the original network's performance after training. As the original network, pruned subnetworks become more stable when they are initialized with weights from later stages of the training process. This is consistent with previous results showing that resetting subnetwork weights to states in early training leads to increased performance after retraining, compared to resetting to the initial state. While stability seems to correspond to better accuracy for subnetworks, very sparse subnetworks perform worse than the unpruned network, even if they are stable."
,,,nan,Do Better ImageNet Models Transfer Better?,,,,,https://arxiv.org/abs/1805.08974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/05/29/import-ai-96-seeing-heartbeats-with-deepphys-better-synthetic-images-via-sagan-and-spotting-pedestrians-via-a-trans-european-dataset/)
,,,Guillaume Lample, François Charton,Deep Learning for Symbolic Mathematics,,,,,https://arxiv.org/abs/1912.01412,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper demonstrates the ability of sequence-to-sequence models to outperform [computer algebra systems](https://en.wikipedia.org/wiki/Computer_algebra_system) (CAS) at the tasks of symbolic integration and solving ordinary differential equations. Since finding the derivative of a function is usually easier than integration, the authors generated a large training set by generating random mathematical expressions, and then using these expressions as the labels for their derivatives. The mathematical expressions were formulated as syntax trees, and mapped to sequences by writing them in Polish notation. These sequences were, in turn, used to train a transformer model. While their model outperformed top CAS on the training data set, and could compute answers much more quickly than the CAS could, tests of generalization were mixed: importantly, the model did not generalize extremely well to datasets that were generated using different techniques than the training dataset."
,,,Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le,XLNet: Generalized Autoregressive Pretraining for Language Understanding,,,,,https://arxiv.org/abs/1906.08237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"XLNet sets significantly improved state-of-the-art scores on many NLP tasks, beating out BERT. This was likely due to pretraining on significantly more data, though there are also architectural improvements."
,,,Tom Schaul, Diana Borsa, Joseph Modayil, Razvan Pascanu,Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,,,,,https://arxiv.org/abs/1904.11455,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The authors argue that Deep RL is subject to a particular kind of training pathology called "ray interference", caused by situations where (1) there are multiple sub-tasks within a task, and the gradient update of one can decrease performance on the others, and (2) the ability to learn on a given sub-task is a function of its current performance. Performance interference can happen whenever there are shared components between notional subcomponents or subtasks, and the fact that many RL algorithms learn on-policy means that low performance might lead to little data collection in a region of parameter space, and make it harder to increase performance there in future. "
,,,Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap,Relational recurrent neural networks,,,,,http://arxiv.org/abs/1806.01822,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CVPR 2018,,,,,,,,,,,,,,,,"This paper introduces the Relational Memory Core, which allows interactions between memories stored in memory-based neural networks. It does so using a "self-attention mechanism": each memory updates its contents by attending to all other memories via several "attention heads" which focus on different features. This leads to particularly good performance on the nth-farthest task, which requires the ranking of pairwise distances between a set of vectors (91% accuracy, compared with baseline 30%), and the Mini-Pacman task."
,,,Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, Augustus Odena,Discriminator Rejection Sampling,,,,,https://arxiv.org/abs/1810.06758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,"Under simplifying assumptions, GAN training should converge to the generator modelling the true data distribution while the discriminator always outputs 0.5. In practice, at the end of training the discriminator can still distinguish between images from the generator and images from the dataset. This suggests that we can improve the generated images by only choosing the ones that the discriminator thinks are from the dataset. However, if we use a threshold (rejecting all images where the discriminator is at least X% sure it comes from the generator), then we no longer model the true underlying distribution, since some low probability images could never be generated. They instead propose a rejection sampling algorithm that still recovers the data distribution under strict assumptions, and then relax those assumptions to get a practical algorithm, and show that it improves performance."
,,,Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William E. Byrd, Matthew Might, Raquel Urtasun, Richard Zemel,Neural Guided Constraint Logic Programming for Program Synthesis,,,,,http://arxiv.org/abs/1809.02840,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,,,,,,,,,,,,,,,,"In program synthesis from examples, we want to find a program consistent with a given set of input-output examples. One classic approach is to use logic programming. In logic programming, instead of writing functions that compute output = f(input), we write rules to compute relations. To encode standard functions, we would write the relation (f, i, o), which is interpreted as "computing f(i) gives o". In logic programming, you can let any variable be unknown, and the language will search for a solution. Using this you can eg. invert a function f on a specific output o, using the query (f, ?, o). To apply logic programming to program synthesis, we write an interpreter eval for the language we want to synthesize in, and pose the query (eval, ?, i, o). They consider the lambda calculus with pairs and lists as their language.The algorithm that falls out is a recursive descent search over the possible structure of the program, that generates and checks partial constraints over the partial programs implied by the input-output examples during the search. The search has branching points where it must choose, for some as-yet-unknown part of the program, what language construct it should use (if, cons, variable, etc.) This paper attempts to use a neural net to predict what choice the search should make to find a solution, replacing some simple hand-tuned heuristics. It can be trained either using reinforcement learning (where the search choices are actions, the partial search trees are states, and the goal is to find a complete program), or through supervised learning since they know for training programs what choices are optimal. They also use a curriculum and experience replay. They evaluate against classical symbolic approaches (λ2, Escher, Myth) and RobustFill, and show that their method generalizes better to finding longer programs not seen in the training dataset."
,,,Pedro A. Ortega, Markus Kunesch, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, Tom Everitt, Corentin Tallec, Emilio Parisotto, Tom Erez, Yutian Chen, Scott Reed, Marcus Hutter, Nando de Freitas, Shane Legg,Shaking the foundations: delusions in sequence models for interaction and control,,,,,https://arxiv.org/abs/2110.10819,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"**Delusions** in language models (LMs) like GPT-3 occur when an incorrect generation early on throws the LM off the rails later. Specifically, if there is some unobserved context that influences how humans generate text that the LM is unaware of, then the LM will generate some plausible text -- and then take that text as _evidence_ about what the unobserved context must be. This can be especially likely when the desired context or task for the generation is difficult to infer from the input. In these settings the human generating the text has access to a lot more information than the model, making generation harder for the model and delusions more likely: an incorrect generation will make it more likely that the model infers the task or context incorrectly. This also applies to sequence modelling approaches in RL like <@Decision Transformer@>(@Decision Transformer: Reinforcement Learning via Sequence Modeling@) and <@Trajectory Transformer@>(@Reinforcement Learning as One Big Sequence Modeling Problem@), where incorrectly chosen actions could change the model's beliefs about optimal future actions.This work explains this problem using tools from causality and argues that these models should act as if their previous actions are causal interventions rather than observations. However, training a model in this way requires access to a model of the environment and the expert demonstrating trajectories in an online way, and the authors don't describe a way to do this with purely offline data (it may be fundamentally impossible). The authors do argue that in settings where the context or task information can be easily extracted from the observations so far, then delusions are less likely. This points to the importance of prompt engineering, or providing context information in another way to sequence models, so that they don't delude themselves."
,,,Thilo Stadelmann, Mohammadreza Amirian, Ismail Arabaci, Marek Arnold, Gilbert François Duivesteijn, Ismail Elezi, Melanie Geiger, Stefan Lörwald, Benjamin Bruno Meier, Katharina Rombach, Lukas Tuggener,Deep Learning in the Wild,,,,,http://arxiv.org/abs/1807.04950,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ANNPR 2018: Artificial Neural Networks in Pattern Recognition,,,,,,,,,,,,,,,,Describes how deep learning is used to solve real-world problems (eg. in industry).
,,,Brian Lester, Rami Al-Rfou, Noah Constant,The Power of Scale for Parameter-Efficient Prompt Tuning,,,,,https://arxiv.org/abs/2104.08691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The highlighted paper showed that prompt programming as currently practiced depends on having a dataset on which prompts can be tested. If we have to use a large dataset anyway, then could we do better by using ML techniques like gradient descent to choose the prompt? Now, since prompts are discrete English sentences, you can’t calculate gradients for them, but we know how to deal with this -- the first step of a language model is to _embed_ English words (or syllables, or bytes) into a real-valued vector, after which everything is continuous. So instead of using gradient descent to optimize the English words in the prompt, we instead optimize the embeddings directly. Another way of thinking about this is that we have our “prompt” be a sentence of (say) 50 completely new words, and then we optimize the “meaning” of those words such that the resulting sequence of 50 newly defined words becomes a good prompt for the task of interest.The authors show that this approach significantly outperforms the method of designing prompts by hand. While it does not do as well as finetuning the full model on the task of interest, the gap between the two decreases as the size of the model increases. At ~10 billion parameters, the maximum size tested, prompt tuning and model tuning are approximately equivalent.In addition, using a prompt is as simple as prepending the new prompt embedding to your input and running it through your model. This makes it particularly easy to do ensembling: if you have N prompts in your ensemble, then given a new input, you create a batch of size N where the ith element consists of the ith prompt followed by the input, and run that batch through your model to get your answer. (In contrast, if you had an ensemble of finetuned models, you would have to run N different large language models for each input, which can be significantly more challenging.)"
,,,Daniel Filan*, Stephen Casper*, Shlomi Hod*, Cody Wild, Andrew Critch, Stuart Russell,Clusterability in Neural Networks,,,,,https://arxiv.org/abs/2103.03386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Neural networks are often construed as lacking internal structure. In this paper, the authors challenge the predominant view and hypothesize that neural networks are more clusterable than is suggested by chance. To investigate the claim, the authors partition the network into groups where most of the edge weight is between neurons in the same group. The authors find that the quality of these groups improves after training, as compared to randomly initialized networks. However, this only holds for certain training setups. Despite this limitation, the authors show it's possible to promote clusterability with little to no effect on accuracy.In experiments, the authors compare the clusterability of trained networks to randomly initialized networks and trained networks with shuffled weights. They focus on multi-layer perceptrons (MLPs) and convolutional networks with dropout regularization. They also run experiments with pruned networks or networks where 'unimportant' edges are removed. They find that MLP networks have clusterable neurons at rates higher than chance, but have mixed results for convolutional networks.The authors hypothesize that clusterability is more likely to arise when different features of the input can be computed in parallel without communication between the features (which is very similar to the hypothesis in the previous paper). To test the hypothesis, they combine examples from the datasets into pairs and then train the neural network to make a double-prediction in a side-by-side setup. Intuitively, the network would need to look at each pair separately, without any need to combine information across the two sides. They find that this setup results in increased modularity."
,,,Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, Ard A. Louis,"Is SGD a Bayesian Sampler? Well, almost. ",,,,,https://arxiv.org/abs/2006.15191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Neural networks have been shown empirically to generalize well in the overparameterized setting, which suggests that there is an inductive bias for the final learned function to be simple. The obvious next question: does this inductive bias come from the _architecture_ and _initialization_ of the neural network, or does it come from stochastic gradient descent (SGD)? This paper argues that it is primarily the former.Specifically, if the inductive bias came from SGD, we would expect that bias to go away if we replaced SGD with random sampling. In random sampling, we sample an initialization of the neural network, and if it has zero training error, then we’re done, otherwise we repeat.The authors explore this hypothesis experimentally on the MNIST, Fashion-MNIST, and IMDb movie review databases. They test on variants of SGD, including Adam, Adagrad, and RMSprop. Since actually running rejection sampling for a dataset would take _way_ too much time, the authors approximate it using a Gaussian Process. This is known to be a good approximation in the large width regime.Results show that the two probabilities are correlated over a wide order of magnitudes for different architectures, datasets, and optimization methods. While correlation isn't perfect over all scales, it tends to improve as the frequency of the function increases. In particular, the top few most likely functions tend to have highly correlated probabilities under both generation mechanisms."
,,,Kurt Shuster*, Jack Urbanek*, Emily Dinan, Arthur Szlam, Jason Weston,Deploying Lifelong Open-Domain Dialogue Learning,,,,,https://arxiv.org/abs/2008.08076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Most research in natural language processing (NLP) follows a paradigm in which we first collect a dataset via crowdsourced workers, and then we train a model on this dataset to solve some task. Could we instead have _lifelong learning_, in which a model could continue learning after being deployed, getting better and better the more it is used? This paper shows one instantiation of such an approach, in a fantasy role-playing game.The authors take the previously developed LIGHT role-playing setting, and gamify it. The human player talks to a language model while playing some role, and earns stars and badges for saying realistic things (as evaluated by another language model). Rather than paying crowdsourced workers to provide data, the authors instead merely advertise their game, which people then play for fun, reducing the cost of data acquisition. They find that in addition to reducing costs, this results in a more diverse dataset, and also leads to faster improvements in automated metrics."
,,,Chhavi Yadav, Léon Bottou,Cold Case: The Lost MNIST Digits,,,,,https://arxiv.org/abs/1905.10498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"As the MNIST test set only contains 10,000 samples, concerns that further improvements are essentially overfitting on the test set have been voiced. Interestingly, MNIST was originally meant to have a test set of 60,000, as large as the training set, but the remaining 50,000 digits have been lost. The authors made many attempts to reconstruct the way MNIST was obtained from the NIST handwriting database as closely as possible and present QMNIST(v5) which features an additional 50,000 test images for MNIST, while the rest of the images are very close to the originals from MNIST. They test their dataset using multiple classification methods and find little difference in whether MNIST or QMNIST is used for training, but the test error on the additional 50,000 images is consistently higher than on the original 10,000 test images or their reconstruction of these. While the concerns about overuse of a test set are justified, the measured effects were mostly small and their relevance might be outweighed by the usefulness of paired differences for statistical model selection. "
,,,Gary Marcus,The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,,,,,https://arxiv.org/abs/2002.06177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper suggests a few directions which would allow us to build more _robust_ AI systems with better "understanding" of the world: specifically, it highlights **symbol manipulation, encoded knowledge, reasoning, and cognitive models** as areas of research for the next decade.See also [Import AI #187](https://jack-clark.net/2020/03/02/import-ai-187-real-world-robot-tests-at-cvpr-all-hail-the-molecule-transformer-the-four-traits-needed-for-smarter-ai-systems/) and [Matthew Barnett's summary](https://www.lesswrong.com/posts/CeJs4rPgPtJPNqLMt/gary-marcus-four-steps-towards-robust-artificial)."
,,,Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal,Reconciling modern machine learning practice and the bias-variance trade-off,,,,,https://arxiv.org/abs/1812.11118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper first proposed double descent as a general phenomenon, and demonstrated it in three machine learning models: linear predictors over random Fourier features, fully connected neural networks with one hidden layer, and forests of decision trees. Note that they define the interpolation threshold as the point where the number of parameters equals the number of training points, rather than using something like effective model complexiy.For linear predictors over random Fourier features, their procedure is as follows: they generate a set of random features, and then find the linear predictor that minimizes the squared loss incurred. If there are multiple predictors that achieve zero squared loss, then they choose the one with the minimum L2 norm. The double descent curve for a subset of MNIST is very pronounced and has a huge peak at the point where the number of features equals the number of training points.For the fully connected neural networks on MNIST, they make a significant change to normal training: prior to the interpolation threshold, rather than training the networks from scratch, they train them from the final solution found for the previous (smaller) network, but after the interpolation threshold they train from scratch as normal. With this change, you see a very pronounced and clear double descent curve. However, if you always train from scratch, then it's less clear -- there's a small peak, which the authors describe as "clearly discernible", but to me it looks like it could be noise.For decision trees, if the dataset has n training points, they learn decision trees of size up to n leaves, and then at that point (the interpolation threshold) they switch to having ensembles of decision trees (called forests) to get more expressive function classes. Once again, you can see a clear, pronounced double descent curve."
,,,Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin,The Lottery Ticket Hypothesis at Scale,,,,,https://arxiv.org/abs/1903.01611,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The [lottery ticket hypothesis](https://arxiv.org/abs/1803.03635) is the claim that "dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations". This paper builds on previous work to show that winning tickets can also be found for larger networks (Resnet-50, not just Resnet-18), if those winning tickets are initialised not with their initial weights from the full network, but rather with their weights after a small amount of full-network training."
,,,Deepak Pathak*, Dhiraj Gandhi*, Abhinav Gupta,Self-Supervised Exploration via Disagreement,,,,,http://arxiv.org/abs/1906.04161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2019,,,,,,,,,,,,,,,,"For researchers who want to build a reinforcement learning system that can learn to explore its environment without explicit rewards, a common approach is to have the agent learn a model of the world, and incentivize it to explore places where its model has the highest error, under the theory that these represent places where it needs to interact more to collect more data and improve its world model. However, this approach suffers in cases when the environment is inherently stochastic, since in a stochastic environment  (think: sitting in front of a static TV and trying to predict the next frame), prediction error can never be brought to zero, and the agent will keep interacting even when its world model has collected enough data to converge as much as it can.  This paper proposes an alternative technique: instead of exploring in response to prediction error, learn an ensemble of bootstrapped next-state prediction models and explore in response to variance or disagreement between the models. This has a few nice properties. One is that, in cases of inherent stochasticity, all models will eventually converge to predicting the mean of the stochastic distribution, and so even though they've not brought error down to zero, the variance among models will be low, and will correctly incentivize our agent to not spend more time trying to learn. Another benefit is that since the reward is purely a function of the agent's models, it can be expressed analytically as a function of the agent's choices and trained via direct backpropogation rather than "black box reward" RL, making it more efficient. "
,,,Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly,Episodic Curiosity through Reachability,,,,,http://arxiv.org/abs/1810.02274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2018,,,,,,,,,,,,,,,,"This paper addresses the "couch potato" problem for intrinsic curiousity - the fact that, if you reward an agent for observing novel or surprising states, it prefers to sit in front of a TV and keep changing channels rather than actually exploring. It proposes instead rewarding states which are difficult to reach from already-explored states (stored in episodic memory). Their agent has a separate network to estimate reachability, which is trained based on the agent's experiences (where observations few steps apart are negative examples and those many steps apart are positive examples). This method significantly outperforms the previous state of the art curiousity method on VizDoom and DMLab environments."
,,,Brendon Matusch, Jimmy Ba, Danijar Hafner,Evaluating Agents without Rewards,,,,,https://arxiv.org/abs/2012.11538,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"How can we evaluate algorithms for exploration? This paper suggests that we look at a variety of proxy objectives, such as reward obtained, similarity to human behavior, empowerment, and entropy of the visited state distribution.The authors evaluate two algorithms ([ICM](https://arxiv.org/abs/1705.05363) and <@RND@>(@Reinforcement Learning with Prediction-Based Rewards@)) as well as three baselines (noop agent, random agent, and PPO) on three Atari games and the <@Minecraft TreeChop task@>(@NeurIPS 2019 Competition: The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors@), producing a list of proxy objective values for each combination. Their analysis then concludes that intrinsic objectives correlate with human behavior more strongly than task rewards do."
,,,Sina Fazelpour, Zachary C. Lipton,Algorithmic Fairness from a Non-ideal Perspective,,,,,https://arxiv.org/abs/2001.09773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The field of fairness has aimed to develop objective metrics of fairness, which can then be optimized for in order to produce a just AI system. Unfortunately, many intuitively desirable fairness metrics are fundamentally incompatible, and cannot be simultaneously achieved except in special circumstances. Should we lose all hope for fairness?This paper argues that the problem was that we were building _idealized_ theories, referring to a conception from political philosophy of ideal and non-ideal modes of theorizing. An ideal theory is one that describes an optimal, ideal world, and then identifies injustices by searching for discrepancies between the real world and the idealized one. This leads to three major flaws:1. It can lead to systematic neglect of some injustices and distortions of our understanding of other injustices. For example, group parity metrics of fairness applied to college admissions would identify east Asian students as privileged relative to white students despite historical and institutional discrimination.2. It does not offer sufficient practical guidance about what should be done, sometimes leading to misguided mitigation strategies. Consider college admissions again. A _disparate learning process_ aims to be blind to protected characteristics (like gender) while still achieving demographic parity. This forces the model to penalize features that correlate with being male. As a result, we end up rewarding women who go into female-dominated fields, and penalize women who go into male-dominated fields! This was presumably not what we wanted.3. It does not make clear who among decision-makers is responsible for intervening to correct specific injustices.The authors suggest that the research community move towards a non-ideal mode of theorizing, in which there is more emphasis on having a deep empirical understanding of the problem (including the various causal factors, rather than summary statistics), and using empirically-informed choices of treatments, rather than modifying ML algorithms to optimize a mathematically defined metric."
,,,Dan Hendrycks*, Steven Basart*, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt,Measuring Coding Challenge Competence With APPS,,,,,https://arxiv.org/abs/2105.09938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The APPS dataset measures programming competence by testing models the way humans are tested: we provide them with natural language descriptions of the code to be written and then evaluate whether the code they generate successfully solves the problem by testing the proposed solutions. The authors collect a dataset of 3,639 introductory problems (solvable by humans with 1-2 years of experience), 5,000 interview problems (comparable difficulty to interview questions), and 1,361 competition problems (comparable difficulty to questions in programming competitions). In addition, the test set contains 1,000 introductory problems, 3,000 interview problems, and 1,000 competition problems.They use this benchmark to test four models: two variants of GPT-2 (0.1B params and 1.5B params), GPT-Neo (2.7B params), and GPT-3 (175B params). GPT-3 is prompted with examples; all other models are finetuned on a dataset collected from GitHub. The authors find that:1. Finetuning makes a big difference in performance: GPT-3 only solves 0.2% of introductory problems, while the finetuned GPT-2-0.1B model solves 1% of such problems.2. Model performance increases with size, as you would expect: GPT-Neo performs best, solving 3.9% of problems.3. Syntax errors in generated code drop sharply as model performance improves: for introductory problems, GPT-3 has syntax errors in slightly under 40% of generations, while GPT-Neo has under 1%.4. Performance can be improved by sampling the best of multiple generated programs: a beam search for 5 programs boosts GPT-Neo’s performance from 3.9% to 5.5% on introductory problems.5. While no model synthesizes a correct solution to a competition level program, they do sometimes generate solutions that pass some of the test cases: for example, GPT-Neo passes 6.5% of test cases."
,,,Tom Henighan*, Jared Kaplan*, Mor Katz*, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish,Scaling Laws for Autoregressive Generative Modeling,,,,,https://arxiv.org/abs/2010.14701,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper looks at scaling laws for generative Transformer models of images (predicting pixels or parts of image encodings), videos (predicting frames of image encodings), multimodal image <-> text (predicting captions based on images or images based on captions), and mathematical problem solving (predicting answers to auto-generated questions about algebra, arithmetic, calculus, comparisons, integer properties, measurement, polynomials, and probability). The authors find that:- Cross-entropy loss as a function of compute follows a power law + constant in all these data modalities (just as it does <@in language@>(@Scaling Laws for Neural Language Models@)). Information theoretically, this can be interpreted as scaling a 'reducible loss' which estimates the KL divergence between the true and model distributions, and an 'irreducible loss' which estimates the entropy of the true data distribution.- Performance on ImageNet classification fine-tuned from their generative image model also follows such a power law, whereas ImageNet classification trained *from scratch* actually gets worse with sufficiently large model sizes. Interestingly, this classification power law continues even past model sizes where the generative cross-entropy loss starts bending as a result of irreducible loss. The authors conclude that approaching the irreducible loss for some dataset does not necessarily indicate diminishing returns for representation quality or semantic content.- Optimal model size as a function of compute follows a power law with an exponent very close to ~0.7 for all data modalities they've studied so far. This implies that in the current compute regime, as compute budgets grow, it's best to devote a majority of compute towards making models bigger and a minority towards training on more data.- Larger models perform better on extrapolating to math problems more difficult than those seen in training, but only insofar as they do better on the training distribution (no benefits to 'strong generalization').- Larger models are able to take advantage of more multimodal information, but the scaling is extremely slow-- a 1-billion-parameter model uses 10% of the information in a caption to define an image, while using 20% of the information would require a 3-trillion-parameter model.As in the <@language models paper@>(@Scaling Laws for Neural Language Models@), extrapolating the steep power laws found for optimally-used compute seems to eventually paradoxically result in loss lower than the bound given by shallower power laws for optimally-used training data. The authors offer a potential hypothesis for resolving this inconsistency-- in the regime of less compute and smaller model sizes, increasing model size effectively increases the amount of information you extract from each data point you train on, resulting in the steepness of the current compute law. As compute increases past a certain point, however, the amount of information extracted per data point approaches the maximum amount possible, so the curve switches to a shallower regime and marginal compute should be used increasingly on dataset increases rather than model size increases. If this hypothesis is true, we should eventually expect the scaling laws for compute to bend towards laws set by dataset size, and perhaps should think they will ultimately be set by trends for overfitting (see [this post](https://www.alignmentforum.org/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight) for another explanation of this)."
,,,Jian Liu*, Leyang Cui*, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning ,,,,,https://arxiv.org/abs/2007.08124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2020,,,,,,,,,,,,,,,,"LogiQA is a benchmark that attempts to track models' understanding of logic and reason.It consists of translated questions from the Civil Servants Examination of China, designed to test civil servant candidates.The questions often require thought and deliberation. Two examples are as follows,David knows Mr. Zhang's friend Jack, and Jack knows David's friend Ms. Lin. Everyone of them who knows Jack has a master's degree, and everyone of them who knows Ms. Lin is from Shanghai.Who is from Shanghai and has a master's degree?A. David.B. Jack.C. Mr. Zhang.D. Ms. Lin.Last night, Mark either went to play in the gym or visited his teacher Tony. If Mark drove last night, he didn't go to play in the gym. Mark would go visit his teacher Tony only if he and his teacher had an appointment. In fact, Mark had no appointment with his teacher Tony in advance.Which is true based on the above statements?A. Mark went to the gym with his teacher Tony last night.B. Mark visited his teacher Tony last night.C. Mark didn't drive last night.D. Mark didn't go to the gym last night.See Figure 2 of the paper for the answers to these two questions (I don't want to spoil the answers). In the paper, the authors show that RoBERTa models obtain around 36% accuracy, whereas human-level accuracy is around 86%."
,,,Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt,Measuring Mathematical Problem Solving With the MATH Dataset,,,,,https://arxiv.org/abs/2103.03874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We’ve seen <@GPT-3@>(@Language Models are Few-Shot Learners@) perform well on lots of downstream tasks. What about challenging high school math problems that require intuition to solve? The authors create the MATH dataset and demonstrate that this is in fact challenging for models: models currently get around 5-7%, even when pretraining on a dataset of math-relevant text and finetuning on the MATH training dataset. Note that the models have to get the answer exactly right: there is no partial credit.Not only are current models not very good at the task, but also they scale poorly -- while there isn’t much data to extrapolate from yet, a simple extrapolation suggests that models would need 10^35 parameters to achieve just 40% accuracy. (This is in contrast to easier tasks, which <@might be solved with some more scaling@>(@Extrapolating GPT-N performance@).) In contrast, in a simple study with university students, performance ranged between 40% and 90%, with the best human only making minor arithmetic errors. This suggests we’ll need additional algorithmic improvements for better performance.The authors also consider allowing language models to have “scratch space” to work on the problem: the models are prompted to generate a solution where they explain their work. They find that this actually _decreases_ accuracy, presumably because the poor generations at the beginning end up confusing the model."
,,,Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt,Measuring Massive Multitask Language Understanding,,,,,https://arxiv.org/abs/2009.03300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"With the advent of large language models, there has been a shift to evaluating these models based on the knowledge they have acquired, i.e. evaluating their “common sense”. However, with <@GPT-3@>(@Language Models are Few-Shot Learners@) models have reached approximately human performance even on these benchmarks. What should be next?We’ve <@previously seen@>(@Aligning AI With Shared Human Values@) a benchmark that evaluates models based on their knowledge of ethics. This benchmark (with many of the same authors) goes further by testing models with multiple choice questions on a variety of subjects that humans need to learn. These are not easy: their 57 subjects include advanced topics like Professional Medicine, College Mathematics, and International Law.All but the largest of the GPT-3 models do about as well as random chance (25%). However, the largest 175 billion parameter model does significantly better, reaching an average score of 43.9%. This performance is very lopsided: on US Foreign Policy it gets almost 70%, while on College Chemistry and Moral Scenarios it gets about 25% (i.e. still random chance). The authors note that GPT-3 tends to do worse on subjects that require calculations and thus speculate that it is harder for GPT-3 to acquire procedural knowledge compared to declarative knowledge. The authors also find that GPT-3 is very uncalibrated about its answers in the zero-shot setting, and becomes more calibrated (though still not very good) in the few-shot setting.It isn’t _necessary_ to have huge models in order to do better than chance: in fact, you can do better with a smaller model that is finetuned for question answering. In particular, the UnifiedQA system has an order of magnitude fewer parameters than GPT-3, but outperforms it with a score of 48.9% accuracy. This system was trained on other question answering datasets (but notably was not trained on the questions in this dataset, as this dataset is meant for evaluation rather than training). A small UnifiedQA model with only 60 million parameters (over 3 orders of magnitude smaller than GPT-3) can still do better than chance, achieving 29.3% on the dataset."
,,,Ross Gruetzemacher, Florian Dorner, Niko Bernaola-Alvarez, Charlie Giattino, David Manheim,Forecasting AI Progress: A Research Agenda,,,,,https://arxiv.org/abs/2008.01848,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper develops a research agenda using the Delphi Process. The Delphi process consists of 4 steps:1.  Ask experts a series of open-ended questions to identify interesting research questions and methods.2.  Authors summarize and aggregate results and send back to experts.3.  The experts comment on and discuss the results.4.  The experts score the research questions and methods on importance and feasibility.  This process yields a large list of questions and methods. A few that I am personally interested in are:-   What are the most useful indicators (e.g. compute, talent, economic impact) of AI progress?-   How effective is long-term technological forecasting and how can we best validate near- and mid-term forecasts?-   How do we utilize forecasts to inform decision makers and develop interventions?-   What are the most likely scenarios for the development of TAI?There is already an existing body of work on many of these questions, so their strongest recommendation for future work is for literature reviews."
,,,Dylan Hadfield-Menell, McKane Andrus, Gillian K. Hadfield,Legible Normativity for AI Alignment: The Value of Silly Rules,,,,,https://arxiv.org/abs/1811.01267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One issue we might have with value learning is that our AI system might look at "silly rules" and infer that we care about them deeply. For example, we often enforce dress codes through social punishments. Given that dress codes do not have much functional purpose and yet we enforce them, should an AI system infer that we care about dress codes as much as we care about (say) property rights? This paper claims that these "silly rules" should be interpreted as a coordination mechanism that allows group members to learn whether or not the group rules will be enforced by neutral third parties. For example, if I violate the dress code, no one is significantly harmed but I would be punished anyway -- and this can give everyone confidence that if I were to break an important rule, such as stealing someone's wallet, _bystanders_ would punish me by reporting me to the police, even though they are not affected by my actions and it is a cost to them to report me.They formalize this using a model with a pool of agents that can choose to be part of a group. Agents in the group play "important" games and "silly" games. In any game, there is a scofflaw, a victim, and a bystander. In an important game, if the bystander would punish any rule violations, then the scofflaw follows the rule and the victim gets +1 utility, but if the bystander would not punish the violation, the scofflaw breaks the rule and the victim gets -1 utility. Note that in order to signal that they would punish, bystanders must pay a cost of c. A silly game works the same way, except the victim always gets 0 utility. Given a set of important rules, the main quantity of interest is how many silly rules to add. The authors quantify this by considering the _proportion_ of all games that are silly games, which they call the density. Since we are imagining _adding_ silly rules, all outcomes are measured with respect to the number of _important_ games. We can think of this as a proxy for time, and indeed the authors call the expected number of games till an important game a _timestep_.Now, for important games the expected utility to the victim is positive if the probability that the bystander is a punisher is greater than 0.5. So, each of the agents cares about estimating this probability in order to decide whether or not to stay in the group. Now, if we only had important games, we would have a single game per timestep, and we would only learn whether one particular agent is a punisher. As we add more silly games, we get more games per timestep, and so we can learn much more quickly the proportion of punishers, which leads to more stable groups. However, the silly rules are not free. The authors prove that if they _are_ free, then we keep adding silly rules and the density would approach 1. (More precisely, they show that as density goes to 1, the value of being told the true probability of punishment goes to 0, meaning that the agent already knows everything.)They then show experimental results showing a few things. When the agents are relatively certain of the probability of an agent being a punisher, then silly rules are not very useful and the group is more likely to collapse (since the cost of enforcing the silly rules starts to be important). Second, as long as c is low (so it is easy to signal that you will enforce rules), then groups with more silly rules will be more resilient to shocks in individual's beliefs about the proportion of punishers, since they will very quickly converge to the right belief. If there aren't any silly rules it can take more time and your estimate might be incorrectly low enough that you decide to leave the group even though group membership is still net positive. Finally, if the proportion of punishers drops below 0.5, making group membership net negative, agents in groups with high density will learn this faster, and their groups will disband much sooner."
,,,DJ Strouse, Kevin R. McKee, Matt Botvinick, Edward Hughes, Richard Everett,Collaborating with Humans without Human Data,,,,,https://arxiv.org/abs/2110.08176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We’ve previously seen that if you want to collaborate with humans in the video game Overcooked, <@it helps to train a deep RL agent against a human model@>(@Collaborating with Humans Requires Understanding Them@), so that the agent “expects” to be playing against humans (rather than e.g. copies of itself, as in self-play). We might call this a “human-aware” model. However, since a human-aware model must be trained against a model that imitates human gameplay, we need to collect human gameplay data for training. Could we instead train an agent that is robust enough to play with lots of different agents, including humans as a special case?This paper shows that this can be done with **Fictitious Co-Play** (FCP), in which we train our final agent against a population of self-play agents and their past checkpoints taken throughout training. Such agents get significantly higher rewards when collaborating with humans in Overcooked (relative to the human-aware approach in the previously linked paper).In their ablations, the authors find that it is particularly important to include past checkpoints in the population against which you train. They also test whether it helps to have the self-play agents have a variety of architectures, and find that it mostly does not make a difference (as long as you are using past checkpoints as well)."
,,,Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo, Kate Larson, Thore Graepel,Open Problems in Cooperative AI,,,,,https://arxiv.org/abs/2012.08630,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Cooperation can often lead to better outcomes for everyone involved and some progress in AI, like improved machine translation or smart contracts, can make cooperation easier. On the other hand, as AI agents will likely become more and more important, it is crucial they can cooperate with each other and with humans. The term **cooperative AI** combines these perspectives and refers to "AI research trying to help individuals, humans and machines, to find ways to improve their joint welfare". While previous AI research has focused on individual intelligence of artificial agents, more work on social intelligence and cooperative capabilities is needed. The paper provides an overview of research relevant to cooperative AI from a wide range of disciplines and aims to facilitate more interdisciplinary conversations about cooperative AI by providing a common framework and vocabulary. Research on cooperative opportunities, situations in which gains from cooperation are possible, can differ along four major dimensions: 1) How much do interests overlap or conflict? 2) What kind of agents are involved? (Humans, machines, organizations)3) What perspective is taken: the perspective of an individual trying to cooperate or a social planner trying to incentivize cooperation?4) What is the scope and how interdisciplinary should research be?Four types of capabilities can be crucial for cooperation:  1) Understanding: In cases where cooperation is optimal for all parties, with no incentive to deviate, but agents lack understanding of either the environment or one another, they may still fail to reach a cooperative equilibrium. For example, one agent might have false beliefs about the others' beliefs and preferences and thus their incentive for defection. This is particularly hard to get around, because (1) preferences might not be defined explicitly or might even be incoherent; (2) there might be incentives to misrepresent preferences; and (3) the recursive nature of beliefs about other agents' beliefs may be challenging to handle.2) Communication: In other cases, like the Stag Hunt game, there are multiple equilibria but agents' incentives are mostly aligned. Still, a lack of communicative abilities or common ground to interpret each other's messages can lead to agents converging to a suboptimal equilibrium. This is complicated further by constraints like limited bandwidth, high latency, or compatibility with human forms of communication. 3) Commitment: While communication helps when incentives are mostly aligned, in games like Chicken where some equilibria clearly favour one agent, communication alone is insufficient as agents have incentive to lie. In some cases, these problems can be circumvented using costly signals. However, often some form of credible commitment device to ensure cooperation, or at least truth-telling, is needed. Such commitment devices can enable unconditional ("I won't defect") or conditional ("I won't defect if you won't") and unilateral or multilateral commitments that require multiple actors to consent but bind them all. While unilateral unconditional commitments are most accessible, other forms can be a lot more powerful enablers of cooperation. Mechanisms that could be useful for these more powerful commitments include reputation, delegation to a trusted third party and (smart) contracts.4) Institutions: In some games like the Prisoner's Dilemma, defection is a dominant strategy for all players, even if they would all be better off with mutual cooperation. In such situations, changing the rules of the game to align incentives and facilitate cooperation can make everyone better off, for example by linking different games (as done with the Iterated Prisoner's Dilemma) or introducing institutions. Institutions can be decentralized and entail (sometimes implicit) enforcement by players (norms, conventions, trust and reputation) or involve a centralized enforcing authority. The study of centralized institutions can draw on the literature in social choice theory, fairness, and mechanism design.Lastly, the authors list potential downsides that better cooperative capabilities could have: While they increase the welfare of the cooperating agents, this might be at the cost of other agents. For example, better cooperation between criminals would likely be bad for society. Similarly, cooperation can undermine prosocial competition at the expense of society, as seen in the example of cartels. On the other hand, a better understanding of the world and others' preferences makes it easier to threaten others efficiently and coercive capabilities greatly benefit from credible conditional commitments to carry out a threat. Furthermore, coercive capabilities might be important for stabilizing cooperation as in the case of punishing defectors. Lastly, powerful bad actors can use more efficient institutions to serve their own antisocial goals."
,,,Adam Lerer and Alexander Peysakhovich,Learning Existing Social Conventions via Observationally Augmented Self-Play,,,,,https://arxiv.org/abs/1806.10071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AIES 2019,,,,,,,,,,,,,,,,"This paper starts from the same key insight about self-play not working when it needs to generalize to out-of-distribution agents, but then does something different. They assume that the test-time agents are playing an **equilibrium policy**, that is, each agent plays a best response policy assuming all the other policies are fixed. They train their agent using a combination of imitation learning and self-play: the self-play gets them to learn an equilibrium behavior, while the imitation learning pushes them towards the equilibrium that the test-time agents use. They outperform both vanilla self-play and vanilla imitation learning."
,,,Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine,PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,,,,,https://arxiv.org/abs/1905.01296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper models a multi-agent self driving car scenario by developing a model of future states conditional on both its own action and the action of multiple humans, and picking the latent-space action that balances between the desiderata of reaching its goal and preferring trajectories seen in the expert multi-agent trajectories its shown (where, e.g., two human agents rarely crash into one another)."
,,,Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas,Social Influence as Intrinsic Motivation for Multi-Agent Deep RL,,,,,https://arxiv.org/abs/1810.08647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2019,,,,,,,,,,,,,,,,"An emerging field of common-sum multi-agent research asks how to induce groups of agents to perform complex coordination behavior to increase general reward, and many existing approaches involve centralized training or hardcoding altruistic behavior into the agents. This paper suggests a new technique that rewards agents for having a causal influence over the actions of other agents, in the sense that the actions of the pair of agents agents have high mutual information. The authors empirically find that having even a small number of agents who act as "influencers" can help avoid coordination failures in partial information settings and lead to higher collective reward.  In one sub-experiment, they only add this influence reward to the agents' communication channels, so agents are incentivized to provide information that will impact other agents' actions (this information is presumed to be truthful and beneficial since otherwise it would subsequently be ignored). "
,,,Pedro Fernandes, Francisco C. Santos, Manuel Lopes,Norms for beneficial A.I.: A computational analysis of the societal value alignment problem,,,,,https://arxiv.org/abs/1907.03843,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AI Communications,,,,,,,,,,,,,,,,"This paper presents a simple quantitative model to argue for the following two observations:1. Unless they are willing to “fall behind” others, individual actors will need to use AI systems to stay competitive.2. Those AI systems will optimize for their owner’s goals, even though a better outcome could be achieved if all AI systems optimized for the average welfare across all actors."
,,,Maximilian Igl, Andrew Gambardella, Nantas Nardelli, N. Siddharth, Wendelin Böhmer, Shimon Whiteson,Multitask Soft Option Learning,,,,,https://arxiv.org/abs/1904.01033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper is a mix of variational inference and hierarchical reinforcement learning, in the context of learning skills that can be reused across tasks. Instead of learning a fixed set of options (read: skills/subpolicies), and a master task-specific policy to switch between them, this method learns cross-task priors for each skill, and then learns a task-specific posterior using reward signal from the task, but regularized towards the prior. The hope is that this will allow for an intermediary between cross-task transfer and single-task specificity. "
,,,Janice Lan, Rosanne Liu, Hattie Zhou, Jason Yosinski,LCA: Loss Change Allocation for Neural Network Training,,,,,https://arxiv.org/abs/1909.01440,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This paper introduces the _Loss Change Allocation_ (LCA) method. The method's purpose is to gain insight and understanding into the training process of deep neural networks. The method calculates an allocation of the change in overall loss (on the whole training set) between every parameter at each training iteration, which is iteratively refined until the approximation error is less than 1% overall. This loss change allocation can be either positive or negative; **if it's negative, then the parameter is said to have helped training at that iteration, and if it's positive then the parameter hurt training**. Given this measurement is per-parameter and per-iteration, it can be aggregated to per-layer LCA, or any other summation over parameters and training iterations.The authors use the method to gain a number of insights into the training process of several small neural networks (trained on MNIST and CIFAR-10).First, they validate that learning is very noisy, with **on average only half of the parameters helping at each iteration**. The distribution is heavier-tailed than a normal distribution, and is fairly symmetrical. However, parameters tend to alternate between helping and hurting, and each parameter only tends to help approximately 50% of the time.Second, they look at the LCA aggregated per-layer, summed over the entire training process, and show that in the CIFAR ResNet model **the first and last layers hurt overall** (i.e. have positive LCA). In an attempt to remedy this and understand the causes, the authors try freezing these layers, or reducing their learning rate. The first layer can't be fixed (freezing makes it's LCA 0, but later layers' LCA is increased in turn so the overall final loss stays the same). However, for the last layer, **freezing or reducing the learning rate increases the overall performance of the network**, as the last layer's LCA is decreased more than all the other layer's LCAs are increased. They also hypothesize that by reducing the momentum for the last layer, they can give it fresher information and make it more likely to learn. They find that this does work, though in this setting previous layers’ LCA increases to compensate, leaving overall performance unchanged.Finally, the authors show that **learning seems to be synchronised across layers**; layers get local LCA minima at the same training iterations, in a statistically significant way. They show this must be a combination of parameter motion and the gradient, as neither on their own explains this phenomenon."
,,,Jesse Mu, Jacob Andreas,Compositional Explanations of Neurons,,,,,https://arxiv.org/abs/2006.14032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Network dissection is an interpretability technique introduced in 2017, which uses a dataset of images with dense (i.e. pixel) labels of concepts, objects and textures. The method measures the areas of high activation of specific channels in a convolutional neural network, then compares these areas with the labelled areas in the dataset. If there's a high similarity for a particular channel (measured by the intersection divided by the union of the two areas), then we can say this channel is recognising or responding to this human-interpretable concept. This paper introduces an extension of this idea, where instead of just using the basic concepts (and matching areas in the dataset), they search through logical combinations of concepts (respectively areas) to try and find a compositional concept which matches the channel's activations. For example, a channel might respond to (water OR river) AND NOT blue. This is still a concept humans can understand (bodies of water which aren't blue), but enables us to explain the behaviour of a larger number of neurons than in the original network dissection method. Their work also extends the method to natural language inference (NLI), and they interpret neurons in the penultimate layer of a BiLSTM-based network trained to know whether a sentence entails, contradicts, or is neutral with respect to another. Here they create their own features based on words, lexical similarity between the two sentences, and part-of-speech tags. Using their method, they find that channels in image classifiers do learn compositional concepts that seem useful. Some of these concepts are semantically coherent (i.e. the example above), and some seem to have multiple unrelated concepts entangled together (i.e. operating room OR castle OR bathroom). In the NLI network, they see that many neurons seem to learn shallow heuristics based on bias in the dataset - i.e. the appearance of single words (like nobody) which are highly informative about the classification. Finally, they use their method to create copy-paste adversarial examples (like in Activation Atlas (AN #49)). In the Places365 dataset (where the goal is to classify places), they can crudely add images which appear in compositional concepts aligned with highly contributing neurons, to make that neuron fire more, and hence change the classification. Some of these examples generalise across classifier architectures, implying a bias present in the dataset."
,,,Amirata Ghorbani, James Zou,Neuron Shapley: Discovering the Responsible Neurons,,,,,https://arxiv.org/abs/2002.09815,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents a novel method, Neuron Shapley, that uses the [Shapley value framework](https://en.wikipedia.org/wiki/Shapley_value) to measure the importance of different neurons in determining an arbitrary metric of the neural net output. (Shapley values have been applied to machine learning before to [measure the importance of features to a model's output](https://christophm.github.io/interpretable-ml-book/shapley.html), but here the authors use them to calculate neuron importance.) Due to several novel approaches and optimisations in calculating these Shapley values, **the top k most responsible neurons (k ~ 30) can be feasibly found for large networks such as Inception-v3**.The authors demonstrate that finding these neurons enables the performance of model surgery. Removing the top 30 neurons that contribute to accuracy completely destroys the accuracy, whereas in expectation removing 30 neurons at random from the network barely moves the accuracy at all. Since the method can be applied to an arbitrary metric, this kind of surgery can be performed for other metrics we care about. For example, removing the neurons which are most responsible for vulnerability to adversarial attacks makes the network more robust, and removing the neurons most responsible for the class-accuracy imbalance (a fairness metric) makes the classes much more even, while only reducing the overall accuracy a small amount."
,,,Sophie Hilgard*, Nir Rosenfeld*, Mahzarin R. Banaji, Jack Cao, David C. Parkes,"Learning Representations by Humans, for Humans",,,,,https://arxiv.org/abs/1905.12686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Historically, interpretability approaches have involved machines acting as **experts**, making decisions and generating explanations for their decisions. This paper takes a slightly different approach, instead using machines as **advisers** who are trying to give the best possible advice to humans, the final decision makers. Models are given input data and trained to generate visual representations based on the data that cause humans to take the best possible actions. In the main experiment in this paper, humans are tasked with deciding whether to approve or deny loans based on details of a loan application. Advising networks generate realistic-looking faces whose expressions represent multivariate information that's important for the loan decision. Humans do better when provided the facial expression 'advice', and furthermore can justify their decisions with analogical reasoning based on the faces, e.g. "x will likely be repaid because x is similar to x', and x' was repaid"."
,,,Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan, Hanna Wallach,Manipulating and Measuring Model Interpretability,,,,,https://arxiv.org/abs/1802.07810,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper performs a rigorous, pre-registered experiment investigating to what degree transparent models are more useful for participants. They investigate how well participants can estimate what the _model predicts_, as well as how well the participant can make predictions given access to the model information. The task they consider is prediction of house prices based on 8 features (such as number of bathrooms and square footage). They manipulate two independent variables. First, CLEAR is a presentation of the model where the coefficients for each feature are visible, whereas BB (black box) is the opposite. Second, **-8** is a setting where all 8 features are used and visible, whereas in **-2** only the 2 most important features (number of bathrooms and square footage) are visible. (The model predictions remain the same whether 2 or 8 features are revealed to the human.) This gives 4 conditions: CLEAR-2, CLEAR-8, BB-2, BB-8.They find a significant difference in ability to predict model output in the CLEAR-2 setting vs all other settings, supporting their pre-registered hypothesis that showing the few most important features of a transparent model is the easiest for participants to simulate. However, counter to another pre-registered prediction, they find no significant difference in deviation from model prediction based on transparency or number of features. Finally, they found that participants shown the clear model were less likely to correct the model's inaccurate predictions on "out of distribution" examples than participants with the black box model."
,,,Akanksha Atrey, Kaleigh Clary, David Jensen,Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,,,,,https://arxiv.org/abs/1912.05743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"This paper presents an analysis of the use of saliency maps in deep vision-based reinforcement learning on ATARI. They consider several types of saliency methods, all of which produce heatmaps on the input image. They show that all (46 claims across 11 papers) uses of saliency maps in deep RL literature interpret them as representing the agent's "focus", 87% use the saliency map to generate a claim about the agent's behaviour or reasoning, but only 7% validate their claims with additional or more direct evidence.They go on to present a framework to turn subjective and under-defined claims about agent behaviour generated with saliency maps into falsifiable claims. This framework effectively makes the claim more specific and targeted at specific semantic concepts in the game's state space. Using a fully parameterized version of the ATARI environment, they can alter the game's state in ways which preserve meaning (i.e. the new state is still a valid game state). This allows them to perform interventions in a rigorous way, and falsify the claims made in their framework.Using their framework, they perform 3 experimental case studies on popular claims about agent behaviour backed up by saliency maps, and show that all of them are false (or at least stated more generally than they should be). For example, in the game Breakout, agents tend to build tunnels through the bricks to get a high score. Saliency maps show that the agent attends to these tunnels in natural games. However, shifting the position of the tunnel and/or the agent's paddle and/or the ball all remove the saliency on the tunnel's location. Even flipping the whole screen vertically (which still results in a valid game state) removes the saliency on the tunnel's location. This shows that the agent doesn’t understand the concept of tunnels generally or robustly, which is often what is claimed."
,,,Peter Hase, Mohit Bansal,Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,,,,,https://arxiv.org/abs/2005.01831,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL 2020,,,,,,,,,,,,,,,,"In this paper the authors perform user tests on 5 different model agnostic interpretability methods: LIME, Anchor, Decision Boundary, Prototype Model and a Composite model (LIME Anchor and Decision Boundary). The use cases they test are a tabular dataset predicting income, and a movie-review dataset predicting sentiment of the review from a single sentence.Their experimental setup consists of 2 tests: **forward prediction** and **counterfactual prediction**. In forward prediction, the user is shown 16 examples of inputs and corresponding outputs and explanations, and then must predict the model’s output on new inputs (without the explanation, which often gives away the answer). In counterfactual prediction, after seeing 16 examples, the user is given an input-output-explanation triple, and then must predict how the output changes for a specific perturbation of the input.Throughout the results they use a significance threshold of p < 0.05 (they don't use Bonferroni corrections). Their study has responses from 32 different students who'd taken at least 1 computer science course, with some screened out for outliers or low accuracy during training. There are approximately 200 individual predictions for each method/dataset-type combination, and each method/prediction-type combination.Overall, their results show that **only LIME (Local Interpretable Model-agnostic Explanation) helps improve performance** with statistical significance on the tabular dataset across both prediction settings, and **only the Prototype model in counterfactual prediction across both datasets**. **No other result was statistically significant.** The improvement in accuracy for the statistically significant results is around 10% (from 70% to 80% in the Tabular dataset with LIME, and 63% to 73% for Prototype in counterfactual prediction).They also showed that **user's ratings of the explanation method didn't correlate in a statistically significant way with the improvement the model gave to their predictions.**"
,,,Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal,Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,,,,,https://arxiv.org/abs/1904.01318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"This paper proposes a new visualization tool in order to understand the behaviour of agents trained using deep reinforcement learning. Specifically, they train a generative model which produces game states, and then optimise a distribution over state embeddings according to some target function (such as high reward for taking a specific action). By sampling from the resulting distribution, they create a diverse set of realistic states that score highly according to the target function. They propose a few target cost functions, which allow them to optimise for states in which the agent takes a particular action, states which are high reward (worst Q-value is large), states which are low reward (best Q-value is small), and critical states (large difference in Q value). They demonstrate results on Atari games as well as a simulated driving environment."
,,,Garima Pruthi, Frederick Liu, Mukund Sundararajan, Satyen Kale,Estimating Training Data Influence by Tracking Gradient Descent,,,,,https://arxiv.org/abs/2002.08484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents the TrackIn method for tracking the influence of training datapoints on the loss on a test datapoint. The purpose of the method is to discover influential training points for decisions made on the testing set. This is defined (loosely) for a training point **x** and test point **z** as the total change in loss on **z** caused by training on **x**. They present several approximations and methods for calculating this quantity efficiently, *allowing them to scale their method to ResNet 50 models trained on ImageNet*The standard method of evaluation for these kinds of methods is finding mislabelled examples in the training dataset. Mislabelled examples are likely to have a strong positive influence on their own loss (strong as they're outliers, and positive as they'll reduce their own loss). Sorting the training dataset in decreasing order of this self-influence, we should hence expect to see more mislabelled examples at the beginning of the list. We can measure what proportion of mislabelled examples is present in each different initial segments of the list. The authors perform this experiment on CiFAR, first training a model to convergence, and then mislabelling 10% of the training set as the next highest predicted class, and then retraining a new model on which TrackIn is run. *When compared to the two previous methods from the literature (Influence Functions and Representer Points), TrackIn recovers more than 80% of the mislabelled data in the first 20% of the ranking, whereas the other methods recover less than 50% at the same point. For all segments TrackIn does significantly better.*They demonstrate the method on a variety of domains, including NLP tasks and vision tasks. The influential examples found seem reasonable, but there's no quantification of these results."
,,,Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David E. Smith, Subbarao Kambhampati2,Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior,,,,,https://arxiv.org/abs/1811.09722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Author's Website,,,,,,,,,,,,,,,,"This paper reviews and discusses definitions of concepts of interpretable behaviour. The first concept, **explicability** measures how close an agent's behaviour is to the observer's expectations. An agent that takes a turn while its goal is straight ahead does not behave explicably by this definition, even if it has good reasons for its behaviour, as long as these reasons are not captured in the observer's model. **Predictable** behaviour reduces the observer's uncertainty about the agent's future behaviour. For example, an agent that is tasked to wait in a room behaves more predictably if it shuts itself off temporarily than if it paced around the room. Lastly, **legibility** or **transparency** reduces observer's uncertainty about an agent's goal. This can be achieved by preferentially taking actions that do not help with other goals. For example, an agent tasked with collecting apples can increase its legibility by actively avoiding pears, even if it could collect them without any additional costs. These definitions do not always assume correctness of the observer's model. In particular, an agent can explicably and predictably achieve the observer's task in a specific context while actually trying to do something else. Furthermore, these properties are dynamic. If the observer's model is imperfect and evolves from observing the agent, formerly inexplicable behaviour can become explicable as the agent's plans unfold."
,,,Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri,Programmatically Interpretable Reinforcement Learning,,,,,https://arxiv.org/abs/1804.02477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,"This work uses program synthesis in order to get interpretable reinforcement learning policies. Some of you can probably guess that I'm very excited by this paper :P As with most program synthesis techniques, they define a space of possible programs (policies), and then search through the space for the program that achieves the highest reward. Since they are using program synthesis, they can take advantage of standard tricks such as sketching. They also train a deep RL agent and use the agent to give feedback to the program synthesis algorithm, so that the algorithm produces the program whose outputs are closest to the outputs of the deep RL policy. They evaluate on TORCS (a racecar simulator) and find that the policy does almost as well as deep RL. However, it has a few major advantages over deep RL. Since it is a program, it is much more interpretable -- a human can actually look at the resulting program and understand it (and hence the title of the paper). It is also possible to use formal verification methods to prove properties about the program (whereas neural nets are often too large for these techniques to work). But perhaps most importantly, restricting your class of functions to the space of (small) programs is often a very useful inductive bias, and it is no different in this case -- the learned programs perform much better than deep RL when run on a new unseen track, showing good generalization."
,,,Mengnan Du, Ninghao Liu, Xia Hu,Techniques for Interpretable Machine Learning,,,,,http://arxiv.org/abs/1808.00033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AGI 2018: Artificial General Intelligence,,,,,,,,,,,,,,,,"This paper summarizes work on interpretability, providing a classification of different ways of achieving interpretability. There are two main axes -- first, whether you are trying to gain insight into the entire model, or its classification of a particular example; and second, whether you try to create a new model that is inherently interpretable, or whether you are post-hoc explaining the decision made by an uninterpretable model. The whole paper is a summary of techniques, so I'm not going to summarize it even further."
,,,Finale Doshi-Velez, Been Kim,Towards A Rigorous Science of Interpretable Machine Learning,,,,,https://arxiv.org/abs/1702.08608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper from 2017 discusses the field of interpretability research, and how it can be made more rigorous and well-defined. The authors first highlight the problem of defining interpretability in the first place - they don't have a resolution to this problem, but suggest that we can think of interpretability in terms of what it's used for. They claim that interpretability is used for confirming other important desiderata in ML systems, which stem from an incompleteness in the problem formalization. For example, if we want a system to be unbiased but aren't able to formally specify this in the reward function, or the reward we're optimising for is only a proxy of the true reward, then we could use interpretability to inspect our model and see whether it's reasoning how we want it to.The authors next move on to discussing how we can evaluate interpretability methods, providing a taxonomy of different evaluation methods: Application-grounded is when the method is evaluated in the context it will actually be used in, by real humans (i.e. doctors getting explanations for AI diagnoses); Human-grounded is about conducting simpler human-subject experiments (who are perhaps not domain experts) using possibly simpler tasks than what the intended purpose of the method is; Functionally-grounded is where no humans are involved in the experiments, and instead some formal notion of interpretability is measured for the method to evaluate its quality. Each of these evaluation methods can be used in different circumstances, depending on the method and the context it will be used in.Finally, the authors propose a data-driven approach to understanding the factors which are important in interpretability. They propose to try and create a dataset of applications of machine learning models to tasks, and then analyse this dataset to find important factors. They list some possible task- and method- related factors, and then conclude with recommendations to researchers doing interpretability."
,,,Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim,A Benchmark for Interpretability Methods in Deep Neural Networks,,,,,https://arxiv.org/abs/1806.10758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This paper presents an automatic benchmark for *feature importance* methods (otherwise known as saliency maps) called *RemOve And Retrain* (ROAR). The benchmark follows the following procedure:1. Train an image classifier on a dataset (they use ResNet-50s on ImageNet, and get about 77% accuracy)2. Measure the test-set accuracy at convergence3. Using the feature importance method, find the most important features in the dataset, and remove them (by greying out the pixels)4. Train another model on this new dataset, and measure the new test-set accuracy 5. **The difference between the accuracy in (4) and in (2) is the measure of how effective the feature importance method is at finding important features**The idea behind retraining is that giving the original classifier images where many pixels have been greyed out will obviously result in lower accuracy, as they're out of the training distribution. Retraining solves this problem.They benchmark a variety of feature importance methods (Gradient heatmap, Guided backprop, Integrated gradients, Classic SmoothGrad, SmoothGrad^2, VarGrad) on their benchmark, and compare to a random baseline, and a Sobel Edge detector (a hard-coded algorithm for finding edges in images). **Only SmoothGrad^2 and VarGrad (which are both methods which ensemble other feature importance methods) do better than random.** They can't explain why these methods perform better than other methods. They also note that even when removing 90% of the pixels in every image (i.e. the random baseline), the accuracy only drops from 77% to 63%, which shows how correlated pixels in images are."
,,,Jasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, Mark Neerincx,Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences,,,,,http://arxiv.org/abs/1807.08706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,XAI workshop on the IJCAI conference 2018,,,,,,,,,,,,,,,,"This paper aims to provide contrastive explanations for the behavior of an RL agent, meaning that they contrast why the RL agent used one policy instead of another policy. They do this by computing the expected outcomes under the alternate policy, and then describing the difference between the two. (An outcome is a human-interpretable event -- they assume that they are given a function that maps states to outcomes.)"
,,,Ethan Perez, Douwe Kiela, Kyunghyun Cho,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,,,,,https://arxiv.org/abs/2103.03872,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We are often interested in estimating how useful a particular capability might be for a model. For example, for <@Factored Cognition@> we're interested in how useful the "decomposition" ability is, that is, how useful it is to decompose the original question into subquestions (as in <@this paper@>(@Unsupervised Question Decomposition for Question Answering@)). This paper proposes a simple methodology: give the model oracle access to the capability in question, and see how much it improves its predictions. This is measured in an online learning setup (rather than in one fell swoop at the end of training), in order to evaluate how useful the capability is in both low and high data regimes.(The paper frames this as asking how much better you can compress the labels when you have access to the capability, relative to not having the capability. This can be seen as an upper bound on the minimum description length, which in turn is one way of operationalizing Occam's razor. I find the prediction view more intuitive, and as far as I can tell the two views are equivalent in the context of this paper.)They then use this framework to investigate a bunch of empirical questions:1. For question answering models trained from scratch, both ML decompositions and human decompositions are helpful, though ML still has a long way to go to catch up to human decompositions.2. One way to evaluate gender bias in a dataset is to ask, "how useful is the "capability" of seeing the male-gendered words", relative to the same question for female-gendered words. This confirms the general male-gendered bias, even in a dataset that has more female-gendered words.3. Some papers have claimed that neural nets are effectively "bag-of-words" models, i.e. they don't pay attention to the ordering of words in a sentence. They evaluate how useful the capability of "getting the correct order" is, and find that it does lead to significantly better results."
,,,Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, Douwe Kiela,Unsupervised Question Decomposition for Question Answering,,,,,https://arxiv.org/abs/2002.09758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Existing methods are proficient at simple question and answering (QA). These simple questions are called single-hop and can be answered with a single yes/no or underlined passage in the text. However, progress on the more difficult task of multi-hop QA lags behind. **This paper introduces a method that can decompose hard multi-hop questions into easier single-hop questions that existing QA systems can answer.** Since collecting labeled decompositions is hard, the authors introduce a pseudo-decomposition where multi-hop questions are matched with similar single-hop questions while making sure the single-hop questions are diverse. Following this, the model is trained to map multi-hop questions to simpler subquestions using _unsupervised_ sequence-to-sequence learning (as they found the supervised version performed worse). They show large improvement on the popular HotPot QA baseline with large improvement on out-of-domain questions due to the ability of sub-questions to help gather supporting facts that can be used to answer questions. "
,,,Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, Kyunghyun Cho,Finding Generalizable Evidence by Learning to Convince Q&A Models,,,,,https://arxiv.org/abs/1909.05863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper tries to improve performance on multiple-choice questions about text passages using a technique similar to <@AI safety via debate@>. The set-up consists of a **judge model** and one or more **evidence agents**. First, the judge model is pretrained on samples consisting of a passage, a multiple-choice question about that passage, and the correct answer to that question. Then, in the experimental portion of the set-up, instead of looking at a full passage, the judge model looks at a subsequence of the passage created by combining the outputs from several evidence agents. Each evidence agent has been given the same passage and assigned a particular answer to the question, and must select a limited number of sentences from the passage to present to the judge model to convince it of that answer.The paper varies several parameters in its setup, including the training process for the judge model, the questions used, the process evidence agents use to select sentences, etc. It finds that for many settings of these parameters, when judge models are tasked with generalizing from shorter passages to longer passages, or easier passages to harder passages, they do better with the new passages when assisted by the evidence agents. It also finds that the sentences given as evidence by the evidence agents are convincing to humans as well as the judge model."
,,,Hong Jun Jeon*, Smitha Milli*, Anca D. Dragan,Reward-rational (implicit) choice: A unifying formalism for reward learning,,,,,https://arxiv.org/abs/2002.04833,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We've got algorithms for learning preferences from <@demonstrations@>(@Modeling Interaction via the Principle of Maximum Causal Entropy@) (possibly <@ranked@>(@Ranking-Based Reward Extrapolation without Rankings@)), <@comparisons@>(@Fine-Tuning GPT-2 from Human Preferences@), <@proxy rewards@>(@Inverse Reward Design@), and even the <@observed state@>(@Learning Preferences by Looking at the World@). The insight of this paper is that these are all instances of a simple underlying formalism.Specifically, these forms of preference learning can be described by two properties: (1) the set of choices that the human picks from and (2) how each choice corresponds to a distribution over agent trajectories. Given these properties, we assume that the human makes their choice according to a Boltzmann-rational model (where the human is more likely to choose an option if it leads to higher expected reward). We have now specified a likelihood over the choice given the reward, and we can use Bayes rule to infer a distribution over the reward given the human's choice.Consider more exotic types of feedback, such as the human's decision to <@turn the agent off@>(@The Off-Switch Game@). Here, the human has two options: turning the agent off (corresponding to the agent staying still forever), or letting it continue (corresponding to the agent taking the trajectory that maximizes its current expected reward). If the agent has the right reward function, then the Boltzmann rational human would let it continue; as a result, if the human instead tries to turn the agent off, Bayes Rule allows the agent to infer that its belief about the reward must be wrong. Thus, even this decision of whether to turn the agent off can be captured in this framework.The paper then shows two examples of new feedback types that can be generated from this framework: first, credit assignment, in which the human identifies a subset of the trajectory that had maximal reward, and second, meta-choice, where the choice of which _type_ of feedback to give can itself give information about the reward function."
,,,Siddharth Reddy, Anca D. Dragan, Sergey Levine,Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,,,,,https://arxiv.org/abs/1805.08010,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Inverse reinforcement learning algorithms typically assume that the demonstrations come from an expert who is approximately optimal. However, this is often not the case, at least when the experts are fallible humans. This paper considers the case where the expert has an incorrect model of the dynamics (transition function) of the environment, and proposes learning the expert's model of the dynamics to improve reward function inference. However, this leads to severe unidentifiability problems, where many models of the dynamics are compatible with the observed behavior. To overcome this, they assume that they have multiple tasks with known reward functions, which they use to infer the expert's dynamics. This is then used to infer the reward function in a new task using an adaptation of max causal entropy IRL. The dynamics can be an arbitrary neural net while the reward function is a weighted linear combination of features. They evaluate the inference of the dynamics model with real humans on Lunar Lander. Given transcripts of humans playing Lunar Lander, they infer the underlying (incorrect) dynamics model. Then, when the human takes an action, they predict which next state the human wanted to achieve, and replace the human's action with the action that would actually get close to the state the human wanted."
,,,Lawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, Anca Dragan,The Assistive Multi-Armed Bandit,,,,,https://arxiv.org/abs/1901.08654,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HRI 2019,,,,,,,,,,,,,,,,"Standard approaches for inverse reinforcement learning assume that humans are acting optimally according to their preferences, rather than learning about their preferences as time goes on. This paper tries to model the latter by introducing the _assistive multi-armed bandit_ problem. In the standard _multi-armed bandit_ problem, a player repeatedly chooses one of several “arms” to pull, where each arm provides reward according to some unknown distribution. Imagine getting 1000 free plays on your choice of 10 different, unknown slot machines. This is a hard problem since the player must trade off between exploration (learning about some arm) and exploitation (pulling the best arm so far). In _assistive multi-armed bandit_, a robot is given the opportunity to intercept the player every round and pull an arm of its choice. If it does not intercept, it can see the arm pulled by the player but not the reward the player receives. This formalizes the notion of an AI with only partial information trying to help a learning agent optimize their reward.The paper does some theoretical analysis of this problem as well as an experimental set-up involving a neural network and players acting according to a variety of different policies. It makes several observations about the problem:- A player better at learning does not necessarily lead to the player-robot team performing better-- the robot can help a suboptimal player do better in accordance with how much information the player's arm pulls convey about the reward of the arm. - A robot is best at assisting when it has the right model for how the player is learning.- A robot that models the player as learning generally does better than a robot that does not, even if the robot has the wrong model for the player's learning.- The problem is very sensitive to which learning model the player uses and which learning model the robot assumes. Some player learning models can only be effectively assisted when they are correctly modeled. Some robot-assumed learning models effectively assist for a variety of actual player learning models."
,,,Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell,Cooperative Inverse Reinforcement Learning,,,,,https://arxiv.org/abs/1606.03137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2016,,,,,,,,,,,,,,,,"This paper provides a formalization of the three principles from the book, in the case where there is a single human H and a single robot R. H and R are trying to optimize the same reward function. Since both H and R are represented in the environment, it can be the _human's_ reward: that is, it is possible to reward the state where the human drinks coffee, without also rewarding the state where the robot drinks coffee. This corresponds to the first principle: that machines should optimize _our_ objectives. The second principle, that machines should initially be uncertain about our objectives, is incorporated by assuming that _only H knows the reward_, requiring R to maintain a belief over the reward. Finally, for the third principle, R needs to get information about the reward from H's behavior, and so R assumes that H will choose actions that best optimize the reward (taking into account the fact that R doesn't know the reward).This defines a two-player game, originally called a CIRL game but now called an _assistance game_. We can compute optimal joint strategies for H and R. Since this is an _interactive_ process, H can do better than just acting optimally as if R did not exist (the assumption typically made in IRL): H can _teach_ R what the reward is. In addition, R does not simply passively listen and then act, but interleaves learning and acting, and so must manage the explore-exploit tradeoff.See also <@Learning to Interactively Learn and Assist@>, which is inspired by this paper and does a similar thing with deep RL."
,,,Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell,The Off-Switch Game,,,,,https://arxiv.org/abs/1611.08219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2017,,,,,,,,,,,,,,,,"This paper studies theoretically the impact of uncertainty over the reward on R's incentives around potential off switches. It proposes the simplest model that the authors expect to lead to generalizable results. R and H are in an assistance game, in which R goes first. R may either take an action a, getting utility u, or shut itself down, getting utility 0. In either case, the game ends immediately. Alternatively, R can choose to wait, in which case H can either shut down R, getting utility 0, or allow R to go ahead with action a, getting utility u.If H is perfectly rational, then waiting is always an optimal action for R, since H will ensure that the team gets max(u, 0) utility. There can be other optimal actions: if R is sure that u >= 0, then taking action a is also optimal, and similarly if R is sure that u <= 0, then shutting down is also optimal. However, if H is not rational, and sometimes fails to take the utility-maximizing action (in a way R can't predict), then things get murkier. If R is sure about the value of u, then it is never optimal to wait, better to just take the action a (if u >= 0) or shut down (if u < 0) rather than let H screw it up. If R is pretty confident that u is positive, it may still decide to take action a, rather than risk that H makes the wrong decision. However, if R is very uncertain about the sign of u, then waiting becomes optimal again. In general, more uncertainty over the reward leads to more deferential behavior (allowing H to shut it off), but at a cost: R is much less able to help H when it is very uncertain about the reward."
,,,Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan,Inverse Reward Design,,,,,https://arxiv.org/abs/1711.02827,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2017,,,,,,,,,,,,,,,,"Usually, in RL, the reward function is treated as the _definition_ of optimal behavior, but this conflicts with the third principle, which says that human behavior is the ultimate source of information about human preferences. Nonetheless, reward functions clearly have some information about our preferences: how do we make it compatible with the third principle? We need to connect the reward function to human behavior somehow.This paper proposes a simple answer: since reward designers usually make reward functions through a process of trial-and-error where they test their reward functions and see what they incentivize, the reward function _tells us about optimal behavior in the **training** environment(s)_. The authors formalize this using a Boltzmann rationality model, where the reward designer is more likely to pick a _proxy reward_ when it gives higher _true reward_ in the _training environment_ (but it doesn't matter if the proxy reward becomes decoupled from the true reward in some test environment). With this assumption connecting the human behavior (i.e. the proxy reward function) to the human preferences (i.e. the true reward function), they can then perform Bayesian inference to get a posterior distribution over the _true_ reward function.They demonstrate that by using risk-averse planning with respect to this posterior distribution, the agent can avoid negative side effects that it has never seen before and has no information about. For example, if the agent was trained to collect gold in an environment with dirt and grass, and then it is tested in an environment with lava, the agent will know that even though the specified reward was indifferent about lava, this doesn't mean much, since _any_ weight on lava would have led to the same behavior in the training environment. Due to risk aversion, it conservatively assumes that the lava is bad, and so successfully avoids it.See also <@Active Inverse Reward Design@>, which builds on this work."
,,,Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal, Demi Guo, Danielle Rothermel, C. Lawrence Zitnick, Jason Weston,Why Build an Assistant in Minecraft?,,,,,https://arxiv.org/abs/1907.09273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FAIR Website,,,,,,,,,,,,,,,,"This position paper proposes a new challenge for AI research: building a bot that can provide assistance in [Minecraft](https://www.minecraft.net/en-us/) (creative mode). A [companion paper](https://research.fb.com/wp-content/uploads/2019/07/CraftAssist-A-Framework-for-Dialogue-enabled-Interactive-Agents-v3.pdf) presents an initial setup for such an agent.The main goal here is to advance natural language understanding, intent inference and instruction following. As a result, there is no formal specification like a reward function -- in their own words, "the ultimate goal of the bot is to be a useful and fun assistant in a wide variety of tasks specified and evaluated by human players". They chose Minecraft in particular partly because it has a very rich space of _tasks_, even though the _execution_ of any given task is relatively straightforward. They script many low level policies to automate this execution in order to make learning easier (for example, they have policies to navigate to a location or to build specified structures) and focus the learning challenge on figuring out what the user wants.The current version of the bot takes dialogue from the user and uses a neural model to parse it into an _action dictionary_ that unambiguously specifies what the agent should do -- I think this neural model is the main thing to be learned. There are a bunch of details on how the rest of the modules work as well. They have also released three datasets: a semantic parsing dataset that associates instructions with action dictionaries, a house dataset that has trajectories where a human builds a house, and a semantic segmentation dataset that labels various parts of houses."
,,,Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang,No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,,,,,https://arxiv.org/abs/1804.09160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper tackles visual story-telling, the task of generating a story that matches a sequence of photos. It proposes learning a reward function from the labeled dataset that can then be optimized with reinforcement learning, with the hope that the reward function is a good compression of what we want and so leads to more generalizable behavior. They show that the standard automated techniques for evaluating visual stories are not very good, and so they perform a Mechanical Turk study that shows very good results compared to prior work. MTurk workers are often unable to tell whether the stories were generated by their algorithm or a human!How does it work? Their architecture has a policy network that creates the stories and a reward network that provides the supervision, which are trained adversarially. We can think of the reward function as inducing a probability distribution over stories, where stories with higher reward are more probable. Then, the reward network acts as a discriminator, trying to make its implied probability distribution similar to the empirical data distribution and dissimilar to the policy network distribution, while the policy network acts as a generator, creating a policy that tries to match the implied probability distribution of the reward network. (This is equivalent to maximizing the expected reward from the reward network.)"
,,,Rohan Choudhury, Gokul Swamy, Dylan Hadfield-Menell, Anca Dragan,On the Utility of Model Learning in HRI,,,,,https://arxiv.org/abs/1901.01291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In human-robot interaction (HRI), we often require a model of the human that we can plan against. Should we use a specific model of the human (a so-called "theory of mind", where the human is approximately optimizing some unknown reward), or should we simply learn a model of the human from data? This paper presents empirical evidence comparing three algorithms in an autonomous driving domain, where a robot must drive alongside a human.The first algorithm, called Theory of Mind based learning, models the human using a theory of mind, infers a human reward function, and uses that to predict what the human will do, and plans around those actions. The second algorithm, called Black box model-based learning, trains a neural network to directly predict the actions the human will take, and plans around those actions. The third algorithm, model-free learning, simply applies Proximal Policy Optimization (PPO), a deep RL algorithm, to directly predict what action the robot should take, given the current state.Quoting from the abstract, they "find that there is a significant sample complexity advantage to theory of mind methods and that they are more robust to covariate shift, but that when enough interaction data is available, black box approaches eventually dominate". They also find that when the ToM assumptions are significantly violated, then the black-box model-based algorithm will vastly surpass ToM. The model-free learning algorithm did not work at all, probably because it cannot take advantage of knowledge of the dynamics of the system and so the learning problem is much harder."
,,,Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei,Reward learning from human preferences and demonstrations in Atari,,,,,https://arxiv.org/abs/1811.06521,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"We have had lots of work on learning from preferences, demonstrations, proxy rewards, natural language, rankings etc. However, most such work focuses on one of these modes of learning, sometimes combined with an explicit reward function. This work learns to play Atari games using both preference and demonstration information. They start out with a set of expert demonstrations which are used to initialize a policy using behavioral cloning. They also use the demonstrations to train a reward model using the DQfD algorithm. They then continue training the reward and policy simultaneously, where the policy is trained on rewards from the reward model, while the reward model is trained using preference information (collected and used in the same way as Deep RL from Human Preferences) and the expert demonstrations. They then present a _lot_ of experimental results. The main thing I got out of the experiments is that when demonstrations are good (near optimal), they convey a lot of information about how to perform the task, leading to high reward, but when they are not good, they will actively hurt performance, since the algorithm assumes that the demonstrations are high quality and the demonstrations "override" the more accurate information collected via preferences. They also show results on efficiency, the quality of the reward model, and the reward hacking that can occur if you don't continue training the reward model alongside the policy."
,,,Andreea Bobu*, Marius Wiggert*, Claire Tomlin, Anca D. Dragan,Feature Expansive Reward Learning: Rethinking Human Input,,,,,https://arxiv.org/abs/2006.13208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One goal we might have with our algorithms is that _after_ training, when the AI system is deployed with end users, the system would be personalized to those end users. You might hope that we could use deep inverse RL algorithms like <@AIRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@), but unfortunately they require a lot of data, which isn’t feasible for end users. You could use earlier IRL algorithms like <@MCEIRL@>(@Modeling Interaction via the Principle of Maximum Causal Entropy@) that require you to specify what features of the environment you care about, but in practice you’ll never successfully write down all of these features. Can we somehow get the best of both worlds?<@Past work@>(@Learning under Misspecified Objective Spaces@) made progress on this front, by allowing the agent to at least _detect_ when it is missing some feature, by checking whether the human feedback is surprisingly inefficient given the existing features. But what do you do once you detect it? The key insight of this paper is that applying a deep IRL algorithm here would be inefficient because it has to implicitly learn the unknown feature, and we can do much better by explicitly querying the human for the unknown feature.In particular, their method Feature Expansive Reward Learning (FERL) asks the human for a few _feature traces_: demonstrations in which the new feature’s value monotonically decreases. For example, suppose a robot arm carrying a cup of water gets too close to a laptop, but the arm doesn’t know the feature “close to a laptop”. Then a feature trace would start with the arm close to the laptop, and move it successively further away. Given a set of feature traces, we can convert this into a dataset of noisy comparisons, where earlier states are more likely to have higher feature values than later states, and use this to train a neural net to predict the feature value (similarly to the reward model in [Deep RL from Human Preferences](https://deepmind.com/blog/learning-through-human-feedback/)). We can then add this to our set of features, and learn rewards over the new set of features.They evaluate their method with a few human-robot interaction scenarios (though without a user study due to COVID), comparing it against deep MaxEnt IRL, and find that their method does better on a variety of metrics."
,,,Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, Edward Grefenstette,Learning to Follow Language Instructions with Adversarial Reward Induction,,,,,https://arxiv.org/abs/1806.01946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Adversarial Goal-Induced Learning from Examples (AGILE) is a way of training an agent to follow instructions. The authors consider a 5x5 gridworld environment with colored shapes that the agent can manipulate. The agent is given an instruction in a structured domain-specific language. Each instruction can correspond to many goal states -- for example, the instruction corresponding to "red square south of the blue circle" has many different goal states, since only the relative orientation of the shapes matters, not their absolute positions.The key idea is to learn two things simultaneously -- an encoding of _what_ the agent needs to do, and a policy that encodes _how_ to do it, and to use these two modules to train each other. The "what" is encoded by a discriminator that can classify (state, instruction) pairs as either being a correct goal state or not, and the "how" is encoded by a policy. They assume they have some human-annotated goal states for instructions. The discriminator is then trained with supervised learning, where the positive examples are the human-annotated goal states, and the negative examples are states that the policy achieves during training (which are usually failures). The policy is trained using A3C with a reward function that is 1 if the discriminator says the state is more likely than not to be a goal state, and 0 otherwise. Of course, if the policy actually achieves the goal state, there is no way of knowing this apart from the discriminator -- so by default _all_ of the states that the policy achieves (including goal states) are treated as negative examples for the dsicriminator. This leads to the discriminator getting slightly worse over time as the policy becomes better, since it is incorrectly told that certain states are not goal states. To fix this issue, the authors drop the top 25% of states achieved by the policy that have the highest probability of being a goal state (according to the discriminator).The authors compare AGILE against A3C with the true reward function (i.e. the reward function implied by a perfect discriminator) and found that AGILE actually performed _better_, implying that the inaccuracy of the discriminator actually _helped_ with learning. The authors hypothesize that this is because when the discriminator incorrectly rewards non-goal states, it is actually providing useful reward shaping that rewards progress towards the goal, leading to faster learning. Note though that A3C with an auxiliary reward prediction objective performed best. They have several other experiments that look at individual parts of the system."
,,,Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, Jan Leike,Quantifying Differences in Reward Functions,,,,,https://arxiv.org/abs/2006.13900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Current work on reward learning typically evaluates the learned reward models by training a policy to optimize the learned reward, and seeing how well that policy performs according to the true reward. However, this only tests how well the reward works in the particular environment you test in, and doesn’t tell you how well the reward will generalize. For example, suppose the user loves apricots, likes plums, but hates durians. A reward that has apricots > durians > plums works perfectly -- until the store runs out of apricots, in which case it buys the hated durian.So, it seems like we should evaluate reward functions directly, rather than looking at their optimal policies. This paper proposes Equivalent-Policy Invariant Comparison (EPIC), which can compare two reward functions while ignoring any potential shaping that doesn’t affect the optimal policy.EPIC is parameterized by a distribution of states and actions DS and DA, as well as a distribution DT over transitions (s, a, s’). The first step is to find canonical versions of the two rewards to be compared, such that they have expected zero reward over DS and DA, and any potential shaping is removed. Then, we look at the reward each of these would assign to transitions in DT, and compute the Pearson correlation. This is transformed to be in the range [0, 1], giving the EPIC distance.The authors prove that EPIC is a pseudometric, that is, it behaves like a distance function, except that it is possible for EPIC(R1, R2) to be zero even if R1 and R2 are different. This is desirable, since if R1 and R2 differ by a potential shaping function, then their optimal policies are guaranteed to be the same _regardless_ of transition dynamics, and so we should report the “distance” between them to be zero.The authors show how to approximately compute the EPIC distance in high dimensional environments, and run experiments to showcase EPIC’s properties. Their first experiment demonstrates that EPIC is able to correctly detect that a densely shaped reward for various MuJoCo environments is equivalent to a sparse reward, whereas other baseline methods are not able to do so. The second experiment compares reward models learned from preferences, demonstrations, and direct regression, and finds that the EPIC distance for the rewards learned from demonstrations are much higher than those for preferences and regression. Indeed, when the rewards are reoptimized in a new test environment, the new policies work when using the preference or regression reward models, but not when using the demonstration reward model. The final experiment shows that EPIC is robust to variations in the visitation distribution DT, while baseline methods are not."
,,,Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley,Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,,,,,https://arxiv.org/abs/2005.09382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper proposes the Simulation-to-Human Instruction Following via Transfer from Text (SHIFTT) method for training an RL agent to receive commands from humans in natural language. One approach to this problem is to train an RL agent to respond to commands based on a template; however, this is not robust to small changes in how humans phrase the commands. In SHIFTT, you instead begin with a pretrained language model such as BERT and first feed the templated commands through the language model. This is then combined with vision inputs to produce a policy. The human commands are later fed through the same language model, and they find that the model has zero-shot transfer to the human commands even if they differ in structure. "
,,,Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine,Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition,,,,,http://arxiv.org/abs/1805.11686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"For reinforcement learning, we can create a probabilistic model in which there are events for the state the agent is in and the action the agent takes. We can also add events e_t corresponding roughly to "the agent achieved something good in timestep t". We set P(e_t = 1 | s_t, a_t) to be exp(R(s_t, a_t)). Then, we can simply set all of the e_t to 1, and infer the likely state-action pairs that would have led to that. This leads to maximum entropy reinforcement learning, which in the setting of deterministic dynamics is equivalent to soft Q-learning. The authors then note that in this setup, the reward corresponds to the log probability of event e_t happening. So, instead of specifying a reward function, we can instead define binary events that we care about, model their probability of occurring, and then find the actions that maximize the likelihood of the event occurring. The authors derive backup equations for three kinds of queries -- ALL (the event must happen every timestep), AT (the event happens at a particular timestep), and ANY (the event happens on some timestep).In this setup, specifying a reward function corresponds to explicitly writing down probabilities P(e | s, a). Of course, we can learn these probabilities from data using standard ML techniques, and this now corresponds to learning a reward function! If we use the ALL query, this corresponds to inverse reinforcement learning. However, by using the AT or ANY query instead, we only require examples of the event e_t for a single s_t and a_t -- for example, images that represent a goal state. They derive an algorithm for this query and show experimentally that this framework can learn event probabilities that lead to good behavior on Mujoco environments."
,,,Shervin Javdani, Siddhartha S. Srinivasa, J. Andrew Bagnell,Shared Autonomy via Hindsight Optimization,,,,,https://arxiv.org/abs/1503.07619,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper considers a shared autonomy task in which a user controls a robot to achieve some goal, and the robot learns to assist the user, without knowing the goal in advance. They formalize this as a POMDP in which the state includes the user's goal, which the robot does not get to observe. However, the POMDP observation model assigns higher probability to user actions that better achieve the goal (a standard Boltzmann rationality model), and this allows the agent to reason about what the goal must be. In practice, for computational tractability, rather than choosing optimal actions in the overall POMDP, the robot chooses optimal actions using a technique called hindsight optimization, which _assumes that the robot will never learn more information about the user's goal_."
,,,Andreea Bobu*, Dexter R.R. Scobee*, Jaime F. Fisac, S. Shankar Sastry, Anca D. Dragan,LESS is More: Rethinking Probabilistic Models of Human Behavior,,,,,https://arxiv.org/abs/2001.04465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HRI 2020,,,,,,,,,,,,,,,,"This paper introduces a new model for robots inferring human preferences called LESS. The traditional Boltzmann noisily-rational decision model assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. The Boltzmann model works well when modeling decisions among different discrete options, but runs into problems when modeling human trajectories in a continuous space, e.g. path finding, because it is very sensitive to the number of trajectories, even if they are similar-- if a robot using a Boltzmann model must predict whether a human navigates around an obstacle by taking one path on the left or one of three very-similar paths on the right, it will assign the same probability to each path by default.To fix this, LESS predicts human behavior by treating each trajectory as part of a continuous space and mapping each one to a feature vector. The likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories, meaning similar trajectories are appropriately deweighted.The paper tests the predictive performance of LESS vs. Boltzmann in several experimental environments, including an artifically constructed task where humans are asked to choose between similar paths for navigating around an obstacle, and a real-world task where humans demonstrate appropriate behaviors to a 7-degree-of-freedom robotic arm. In general, LESS performs better than Boltzmann when given a small number of samples of human behavior, but does equally well as the sample size is increased. In the robotic arm task, Boltzmann performed better when demonstrations were aggregated into a single batch and inference was run on the whole batch at once, representing trying to approximate the 'average' user rather than customizing behavior to each user. The paper claims that this happens because Boltzmann overlearns from demonstrations in sparse regions, and underlearns from dense demonstrations. As you increase the number of samples, you approximate the “true” trajectory space better and better, so the 10 trajectory sets vary less and less, which means Boltzmann won’t underperform so much. Since the single batch demonstration aggregated demonstrations, it had a similar effect in approximating the "true" trajectory space.The paper notes that one limitation of this method is a reliance on a pre-specified set of robot features, though a small set of experimental results suggested that LESS still performed better than Boltzmann when adding a small number of irrelevant features."
,,,Lantao Yu*, Tianhe Yu*, Chelsea Finn, Stefano Ermon,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables,,,,,http://arxiv.org/abs/1909.09314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This work explores improving performance on multi-task inverse reinforcement learning in a single-shot setting by extending <@Adversarial Inverse Reinforcement Learning@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@) with "latent context variables" that condition the learned reward function. The paper makes two notable contributions: 1) It details an algorithm to simultaneously learn a flexible reward function and a conditional policy with competitive few-shot generalization abilities from expert demonstrations of multiple related tasks _without_ task specifications or identifiers; 2) The authors empirically demonstrate strong performance of a policy trained on the inferred reward of a structurally similar task with modified environmental dynamics, claiming that in order to succeed "the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration"."
,,,Daniel S. Brown, Scott Niekum,Deep Bayesian Reward Learning from Preferences,,,,,http://arxiv.org/abs/1912.04472,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Safety and Robustness in Decision Making at NeurIPS 2019,,,,,,,,,,,,,,,,"Bayesian inverse reinforcement learning (IRL) is ideal for safe imitation learning since it allows uncertainty in the reward function estimator to be quantified. This approach requires thousands of likelihood estimates for proposed reward functions. However, each likelihood estimate requires training an agent according to the hypothesized reward function. Predictably, such a method is computationally intractable for high dimensional problems.**In this paper, the authors propose Bayesian Reward Extrapolation (B-REX), a scalable preference-based Bayesian reward learning algorithm.** They note that in this setting, a likelihood estimate that requires a loop over all demonstrations is much more feasible than an estimate that requires training a new agent. So, they assume that they have a set of _ranked_ trajectories, and evaluate the likelihood of a reward function by its ability to reproduce the preference ordering in the demonstrations. To get further speedups, they fix all but the last layer of the reward model using a pretraining step: the reward of a trajectory is then simply the dot product of the last layer with the features of the trajectory as computed by all but the last layer of the net (which can be precomputed and cached once).The authors test B-REX on pixel-level Atari games and show competitive performance to <@T-REX@>(@Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations@), a related method that only computes the MAP estimate. Furthermore, the authors can create confidence intervals for performance since they can sample from the reward distribution."
,,,Pim de Haan, Dinesh Jayaraman, Sergey Levine,Causal Confusion in Imitation Learning,,,,,https://arxiv.org/abs/1905.11979,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This paper argues that _causal misidentification_ is a big problem in imitation learning. When the agent doesn't have a good model of what actions cause what state changes, it may mismodel the effects of a state change as a cause-- e.g., an agent learning to drive a car may incorrectly learn that it should turn on the brakes whenever the brake light on the dashboard is on. This leads to undesirable behavior where more information actually causes the agent to perform worse.The paper presents an approach for resolving causal misidentification by (1) Training a specialized network to generate a "disentangled" representation of the state as variables, (2) Representing causal relationships between those variables in a graph structure, (3) Learning policies corresponding to each possible causal graph, and (4) Performing targeted interventions, either by querying an expert, or by executing a policy and observing the reward, to find the correct causal graph model.The paper experiments with this method by testing it in environments artificially constructed to have confounding variables that correlate with actions but do not cause them. It finds that this method is successfully able to improve performance with confounding variables, and that it performs significantly better per number of queries (to an expert or of executing a policy) than any existing methods. It also finds that directly executing a policy and observing the reward is a more efficient strategy for narrowing down the correct causal graph than querying an expert."
,,,Danfei Xu, Misha Denil,Positive-Unlabeled Reward Learning,,,,,https://arxiv.org/abs/1911.00459,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The problem with learning a reward model and training an agent on the (now fixed) model is that the agent can learn to exploit errors in the reward model. Adversarial imitation learning seeks to avoid this by training a discriminator reward model with the agent: the discriminator is trained via supervised learning to distinguish between expert trajectories and agent trajectories, while the agent tries to fool the discriminator. However, this effectively treats the agent trajectories as negative examples — even once the agent has mastered the task. What we would really like to do is to treat the agent trajectories as unlabeled data. This is an instance of _semi-supervised learning_, in which a classifier has access to a small set of labeled data and a much larger collection of unlabeled data. In general, the common approach is to propagate classification information learned using labels to the unlabeled dataset. The authors apply a recent algorithm for positive-unlabeled (PU) learning, and show that this approach can improve upon both GAIL and supervised reward learning."
,,,Ruohan Zhang, Faraz Torabi, Lin Guan, Dana H. Ballard, Peter Stone,Leveraging Human Guidance for Deep Reinforcement Learning Tasks,,,,,http://arxiv.org/abs/1909.09906,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019,,,,,,,,,,,,,,,,"A core problem in RL is the communication of our goals and prior knowledge to an agent. One common approach to this is imitation learning: the human provides example demonstrations of a task, and the agent learns to mimic them. However, there are some limitations to this approach, such as requiring the human to be capable of the task. This paper outlines five different modalities from which agents can learn: evaluations, preferences, hierarchical feedback, observations, and attention (for example, where humans are looking while solving a task). It then suggests future research directions.For this summary, I will focus on the future research directions, but you can read the full paper to understand existing approaches.  The first issue is that datasets of human guidance are difficult to capture and depend on many specific factors of the individuals providing guidance. As a result, the paper suggests creating standard datasets to save effort and enable fair comparisons. The second direction is to better understand how humans should teach agents. The literature currently emphasizes progress in learning methods, but improved teaching methods may be just as valuable when learning from human guidance. The last is unifying learning across different input modalities; ideally an agent would be able to learn from many different types of human guidance over different phases of its learning."
,,,Daniel S. Brown, Wonjoon Goo, Scott Niekum,Ranking-Based Reward Extrapolation without Rankings,,,,,https://arxiv.org/abs/1907.03976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"A while back, these authors released the <@T-REX paper@>(@Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations@), where they showed that providing ranked sets of trajectories, rather than one single optimal trajectory, lets you learn a more accurate reward that can outperform the demonstrator. This ability to outperform the demonstrator is rooted in the ability to extrapolate predicted reward outside of demonstrated points, and that ability to extrapolate comes from the fact that ranked trajectories provide more information about relative reward values. This paper is a fairly straightforward extension of that one, and asks: can we get similar benefits without requiring humans to actually rank trajectories? The authors argue that they can replicate T-REX's ability to outperform the demonstrator by simply learning a behaviorally cloned policy off of a single (potentially sub-optimal) demonstrator, and making that policy gradually worse by adding more noise to it. This model is called D-REX, for Disturbance-based Reward EXtrapolation. They then make an assumption that more noise in the policy corresponds to less reward, and use that as a ranking scheme to throw into the existing T-REX algorithm."
,,,Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, Siddhartha Srinivasa,Imitation Learning as f-Divergence Minimization,,,,,https://arxiv.org/abs/1905.12888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper frames imitation learning through the lens of matching your model's distribution over trajectories (or conditional actions) to the distribution of an expert policy. This framing of distribution comparison naturally leads to the discussion of f-divergences, a broad set of measures including KL and Jenson-Shannon Divergences. The paper argues that existing imitation learning methods have implicitly chosen divergence measures that incentivize "mode covering" (making sure to have support anywhere the expert does) vs mode collapsing (making sure to only have support where the expert does), and that the latter is more appropriate for safety reasons, since the average between two modes of an expert policy may not itself be a safe policy. They demonstrate this by using a variational approximation of the reverse-KL distance as the divergence underlying their imitation learner. "
,,,Ashley D. Edwards, Charles L. Isbell,Perceptual Values from Observation,,,,,https://arxiv.org/abs/1905.07861,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Self-Supervised Learning at ICML 2019,,,,,,,,,,,,,,,,"This paper proposes a technique for learning from raw expert-trajectory observations by assuming that the last state in the trajectory is the state where the goal was achieved, and that other states have value in proportion to how close they are to a terminal state in demonstration trajectories. They use this as a grounding to train models predicting value and action-value, and then use these estimated values to determine actions. "
,,,Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, Scott Niekum,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,,,,,https://arxiv.org/abs/1904.06387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper claims to demonstrate a technique by which an agent learning from a demonstrator's actions can learn to outperform that demonstrator on their true reward, rather than, in the way of imitation learning or behavioral cloning, just mimicking the demonstrator under the assumption that the demonstrator's performance is optimal (or at least near-optimal). The key structural innovation of the paper is to learn using pairs of ranked trajectories and learn a neural network-based reward function based on correctly predicting which will be higher. This allows the model to predict what actions will lead to higher and lower reward, and to extrapolate that relationship beyond the best demonstration. When an agent is then trained using this reward model as it's ground truth reward, it's shown to be capable of outperforming the demonstrator on multiple tested environments, including Atari. An important distinction compared to some prior work is the fact that these rankings are collected in an off-policy manner, distinguishing it from [Deep RL from Human Preferences](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) where rankings are requested on trajectories generated as an agent learns. "
,,,Smitha Milli, Anca D. Dragan,Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,,,,,https://arxiv.org/abs/1903.03877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137), we assume a two-player game with a human and a robot where the robot doesn't know the reward R, but both players are trying to maximize the reward. Since one of the players is a human, we cannot simply compute the optimal strategy and deploy it -- we are always making some assumption about the human, that may be misspecified. A common assumption is that the human is playing optimally for the single-player version of the game, also known as a literal human. The robot then takes the best response actions given that assumption. Another assumption is to have a _pedagogic_ human, who acts as though the robot is interpreting her literally. The robot that takes the best response actions with this assumption is called a pedagogic or pragmatic robot.However, any assumption we make about the human is going to be misspecified. This paper looks at how we can be robust to misspecification, in particular if the human could be literal or pedagogic. The main result is that the literal robot is more robust to misspecification. The way I think about this is that the literal robot is designed to work with a literal human, and a pedagogic human is "designed" to work with the literal robot, so unsurprisingly the literal robot works well with both of them. On the other hand, the pedagogic robot is designed to work with the pedagogic human, but has no relationship with the literal robot, and so should not be expected to work well. It turns out we can turn this argument into a very simple proof: (literal robot, pedagogic human) outperforms (literal robot, literal human) since the pedagogic human is designed to work well with the literal robot, and (literal robot, literal human) outperforms (pedagogic robot, literal human) since the literal robot is designed to work with the literal human.They then check that the theory holds in practice. They find that the literal robot is better than the pedagogic robot _even when humans are trying to be pedagogic_, a stronger result than the theory predicted. The authors hypothesize that even when trying to be pedagogic, humans are more accurately modeled as a mixture of literal and pedagogic humans, and the extra robustness of the literal robot means that it is the better choice."
,,,Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio,BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop,,,,,https://arxiv.org/abs/1810.08272,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/10/30/import-ai-118-airbnb-splices-neural-net-into-its-search-engine-simulating-robots-that-touch-with-unrealrox-and-how-long-it-takes-to-build-a-quadcopter-from-scratch/).
,,,Andreea Bobu, Andrea Bajcsy, Jaime F. Fisac, Anca D. Dragan,Learning under Misspecified Objective Spaces,,,,,https://arxiv.org/abs/1810.05157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"What can you do if the true objective that you are trying to infer is outside of your hypothesis space? The key insight of this paper is that in this scenario, the human feedback that you get will likely not make sense for _any_ reward function in your hypothesis space, which allows you to notice when this is happening. This is operationalized using a Bayesian model in which a latent binary variable represents whether or not the true objective is in the hypothesis space. If it is, then the rationality constant β will be large (i.e. the human appears to be rational), whereas if it is not, then β will be small (i.e. the human appears to be noisy). The authors evaluate with real humans correcting the trajectory of a robotic arm."
,,,Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein,Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections,,,,,https://arxiv.org/abs/2104.04670,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"<@Large language models@>(@Language Models are Few-Shot Learners@) can be prompted to perform classification tasks. However, you may not want to simply phrase the prompt as a question like “Does the following tweet have positive or negative sentiment?” because in the training set such questions may have been followed by something other than an answer (for example, an elaboration of the question, or a denial that the question is important), and the model may end up choosing one of these alternatives as the most likely completion.The natural solution is to collect a question-answering dataset and finetune on it. The core idea of this paper is that we can convert existing NLP classification datasets into a question-answering format, which we can then finetune on. For example, given a dataset for movie review classification (where the goal is to predict whether a review is positive or negative), we produce questions like “Is the review positive?” or “Does the user find this movie bad?” The entire classification dataset can then be turned into question-answer pairs to train on.The authors do this for several datasets, producing 441 question types in total. They then finetune the 0.77B parameter T5 model on a training set of questions and evaluate it on questions that come from datasets not seen during training. Among other things, they find:1. Their model does better than [UnifiedQA](https://arxiv.org/abs/2005.00700), which was also trained for question answering using a similar idea.2. Pretraining is very important: performance crashes if you “finetune” on top of a randomly initialized model. This suggests that the model already “knows” the relevant information, and finetuning ensures that it uses this knowledge appropriately.3. If you ensemble multiple questions that get at the same underlying classification task, you can do better than any of the questions individually.4. It is possible to overfit: if you train too long, performance does decrease."
,,,Jason Wei*, Maarten Bosma*, Vincent Y. Zhao*, Kelvin Guu*, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le,Finetuned Language Models Are Zero-Shot Learners,,,,,https://arxiv.org/abs/2109.01652,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper applies the approach from the previous paper on a much larger 137B parameter model to produce a model that _follows instructions_ (rather than just _answering questions_). Since they are focused on instruction following, they don’t limit themselves to classification tasks: they also want to have generative tasks, and so include e.g. summarization datasets. They also generate such tasks automatically by “inverting” the classification task: given the label y, the goal is to generate the input x. For example, for the movie review classification dataset, they might provide the instruction “Write a negative movie review”, and then provide one of the movie reviews classified as negative as an example of what the model should write in that situation.A natural approach to classification with a language model is to ask a question like “Is this movie review positive?” and then checking the probability assigned to “Yes” and “No” and returning whichever one was higher. The authors note that this can be vulnerable to what we might call “probability splitting” (analogously to [vote splitting](https://en.wikipedia.org/wiki/Vote_splitting)). Even if the correct answer is “Yes”, the model might split probability across “Yes”, “Yup”, “Definitely”, “Absolutely”, etc such that “No” ends up having higher probability than “Yes”. To solve this problem, in classification questions they add a postscript specifying what the options are. During finetuning, the model should quickly learn that the next word is always chosen from one of these options, and so will stop assigning probability to other words, preventing probability splitting.They find that the finetuned model does much better on held-out tasks than the original model (both evaluated zero-shot). The finetuned model also beats zero-shot GPT-3 on 19 of 25 tasks, and few-shot GPT-3 on 10 of 25 tasks. The finetuned model is always used zero-shot; unfortunately they don’t report results when using the finetuned model in a few-shot setting.They also study the impact of instruction tuning over various model sizes. At every model size, instruction tuning helps significantly on the tasks that were seen during finetuning, as you would expect. However, when considering tasks that were _not_ seen during finetuning, instruction tuning actually _hurts_ performance up to models with 8B parameters, and only helps for the 68B and 137B models (where it raises performance by about 15 percentage points on average across heldout tasks)."
,,,Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín,What Matters in Learning from Offline Human Demonstrations for Robot Manipulation,,,,,https://arxiv.org/abs/2108.03298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"As you might expect from the title, this paper tests imitation learning and offline RL algorithms on a benchmark of robotic manipulation tasks in which the agent must learn to perform the task from human demonstrations. Most of the experiments were done in simulation, but they did do a final training run on a real robot using hyperparameters chosen in simulation, to demonstrate that their preferred algorithms could work in such a setting as well. Some findings I found particularly interesting:1. It is important to have models with memory: behavioral cloning (BC) does significantly better on human demonstrations when it is training an RNN model (which has memory), especially on longer-horizon tasks. This is presumably because the humans providing the demonstrations chose actions based not only on the current state but also what had happened in the past, i.e. they were non-Markovian. To test this hypothesis, we could look at machine-generated demonstrations, where you get demonstrations from an expert agent trained using RL, which I _think_ are guaranteed to be Markovian by construction. Unfortunately, we can only get reasonable RL experts on the shorter-horizon tasks where the effect is less pronounced; in these cases BC-RNN still outperforms BC without the RNN, weakly suggesting that it isn’t just about Markovian vs. non-Markovian data.2. Offline RL algorithms work quite well on the machine-generated data, but don’t work very well on human demonstrations. It isn’t particularly clear why this is the case.3. In addition, offline RL struggles when used on datasets where the demonstrations are of mixed quality; in comparison BC-RNN does quite well.4. Policy selection is a challenging problem: in these settings, the training objective (e.g. predict the expert actions) is usually not the thing you actually care about (e.g. did you successfully pick up the cup). Ideally, you would evaluate many model checkpoints throughout the training process on the metric you actually care about and then choose the one that performs best. If you instead select the model checkpoint that achieved the lowest validation loss, performance on the correct metric can decrease by 50-100%; if you always use the last checkpoint (i.e. at the end of training), performance can decrease by 10-30%. This demonstrates that it is important to choose the right model during training – but there’s no clear way to do this, as often the evaluation of a policy is non-trivial.5. The observation space (e.g. pixel observations vs. observations of joint angles and forces) and hyperparameters (e.g. learning rate) both matter quite a lot. For example, adding information about end effectors can drop performance by 49-88% (presumably due to overfitting).6. For complex tasks, more data provides significant improvements."
,,,Manu Orsini*, Anton Raichuk*, Léonard Hussenot*, Damien Vincent, Robert Dadashi, Sertan Girgin, Matthieu Geist, Olivier Bachem, Olivier Pietquin, Marcin Andrychowicz,What Matters for Adversarial Imitation Learning?,,,,,https://arxiv.org/abs/2106.00672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper takes adversarial imitation learning algorithms (think <@GAIL@>(@Generative Adversarial Imitation Learning@) and <@AIRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@)) and tests the effect of various hyperparameters, including the loss function, the discriminator regularization scheme, the discriminator learning rate, etc. They first run a large, shallow hyperparameter sweep to identify reasonable ranges of values for the various hyperparameters, and then run a larger hyperparameter sweep within these ranges to get a lot of data that they can then analyze. All the experiments are done on two continuous control benchmarks: the MuJoCo environments in OpenAI Gym and manipulation environments from Adroit.Obviously they have a lot of findings, and if you spend time working with adversarial imitation learning algorithms, I’d recommend reading through the full paper, but the ones they highlight are:1. Even though some papers have proposed regularization techniques that are specific to imitation learning, standard supervised learning techniques like dropout work just as well.2. There are significant differences in the results when using synthetic demonstrations vs. human demonstrations. (A synthetic demonstration is one provided by an RL agent trained on the true reward.) For example, the optimal choice of loss function is different for synthetic demos vs. human demos. Qualitatively, human demonstrations are not Markovian and are often multimodal (especially when the human waits and thinks for some time: in this case one mode is “noop” and the other mode is the desired action)."
,,,Zhangjie Cao*, Minae Kwon*, Dorsa Sadigh,Transfer Reinforcement Learning across Homotopy Classes,,,,,https://arxiv.org/abs/2102.05207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE Robotics and Automation Letters 2021,,,,,,,,,,,,,,,,"Suppose a robot walks past a person and it chooses to pass them on the right side. Imagine that we want to make the robot instead pass on the left side, and our tool for doing this was to keep nudging the robot's trajectory until it did what we wanted. In this case, we're screwed: there is no way to ``nudge'' the trajectory from passing on the right to passing on the left without going through a trajectory that crashes straight into the person.The core claim of this paper is that the same sort of situation applies to finetuning for RL agents. Suppose we train an agent for one task where there is lots of data, and then we want to finetune it to another task. Let's assume that the new task is in a different _homotopy class_ than the original task, which roughly means that you can't nudge the trajectory from the old task to the new task without going through a very low reward trajectory (in our example, crashing into the person). However, finetuning uses gradient descent, which nudges model parameters; and intuitively, a nudge to model parameters would likely correspond to a nudge to the trajectory as well. Since the new task is in a different homotopy class, this means that gradient descent would have to go through a region in which the trajectory gets very low reward. This is not the sort of thing gradient descent is likely to do, and so we should expect finetuning to fail in this case.The authors recommend that in such cases, we first train in a simulated version of the task in which the large negative reward is removed, allowing the finetuning to ``cross the gap''. Once this has been done, we can then reintroduce the large negative reward through a curriculum -- either by gradually increasing the magnitude of the negative reward, or by gradually increasing the number of states that have large negative reward. They run several robotics experiments demonstrating that this approach leads to significantly faster finetuning than other methods."
,,,Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali Farhadi, Yejin Choi,TuringAdvice: A Generative and Dynamic Evaluation of Language Use,,,,,https://arxiv.org/abs/2004.03607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"There are two main ways in which current NLP models are evaluated: quality of generations (how sensible the generated language looks), and correctness (given some crisp question or task, does the model output the right answer). However, we often care about using models for tasks in which there is no literally correct answer. This paper introduces an evaluation method for this setting: TuringAdvice. Models are presented with a situation in which a human is asking for advice, and the model must provide a helpful response. To score models, the resulting responses are compared against good human responses. The model’s response is successful if its advice is at least as helpful to the advice-seeker as human-written advice.The authors collect a dataset of situations from Reddit, and for the human-written advice they take the most upvoted top-level comment on the post. A finetuned T5 model achieves a score of 14%, while prompted GPT-3 achieves a score of 4%. In contrast, taking the _secondmost_ upvoted top-level comment would give a score of 41%, and a model that gave advice about as good as the typical best advice from a human would get 50%. The paper also presents several qualitative failures in which the models seem to have significant misunderstandings of the situation (though I can’t tell how cherrypicked these are)."
,,,Pedro Freire, Adam Gleave, Sam Toyer, Stuart Russell,DERAIL: Diagnostic Environments for Reward And Imitation Learning,,,,,https://arxiv.org/abs/2012.01365,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Most deep RL algorithms are quite sensitive to implementation and hyperparameters, and this applies to imitation learning as well. So, it would be useful to have some simple sanity checks that an algorithm works well, before throwing algorithms at challenging benchmarks trying to beat the state of the art. This paper presents a suite of simple environments that each aim to test a single aspect of an algorithm, in a similar spirit to unit testing.For example, RiskyPath is a very simple four-state stochastic MDP, in which the agent can take a long, safe path to the reward, or a short, risky path. While it is always better in expectation to take the safer path, properly reasoning about stochasticity can be subtle, and some published algorithms, like <@Maximum Entropy IRL@>(@Maximum Entropy Inverse Reinforcement Learning@), always choose the risky path (this can be fixed by using <@causal entropy@>(@Modeling Interaction via the Principle of Maximum Causal Entropy@)). By isolating the issue, RiskyPath can be used as a quick test to detect this behavior in new algorithms.The paper also presents a case study in tuning an implementation of [Deep RL from Human Preferences](https://deepmind.com/blog/learning-through-human-feedback/), in which a sparse exploration task suggested that the comparison queries were insufficiently diverse to guarantee stability."
,,,Ananth Jonnavittula, Dylan P. Losey,I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set,,,,,https://arxiv.org/abs/2011.06118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"<@Misspecification in reward learning@>(@Model Mis-specification and Inverse Reinforcement Learning@) can be quite bad, and seems nearly inevitable to happen. The key insight of this paper is that we can mitigate its effects by ensuring that we err on the side of _underestimating_ the demonstrator’s capabilities.Consider inverse reinforcement learning (IRL), where we get demonstrations of good behavior. In practice, there are some demonstrations that humans can’t give: for example, when teleoperating a complex robot arm, humans might find it challenging to move a coffee cup without tilting it. Ideally, we would estimate the set of possible trajectories the demonstrator could have given, known as their choice set, and only model them as noisily rational across trajectories from that set.However, we won’t perfectly estimate this choice set, and so there will be some misspecification. If we overestimate the demonstrator’s capabilities, for example by assuming they could move the coffee cup perfectly straightly, then since that _isn’t_ the demonstration we get we would infer that the human couldn’t have cared about keeping the cup upright. However, if we underestimate the demonstrator’s capabilities, there’s no such issue.If we make the theoretical simplification that the demonstrator chooses the actual best trajectory out of their choice set, then we can prove that in the case of underestimation, you will always assign as much probability to the true reward function as you would if you had the correct choice set. (Intuitively, this is because for reward r, if the trajectory is optimal under the true choice set, then it must also be optimal under the underestimated choice set.)Okay, but how do we ensure we have underestimated the choice set? This paper suggests that we augment the demonstrations that we do observe. For example, we can take the real demonstration, and inject noise into it, along the lines of <@D-REX@>(@Ranking-Based Reward Extrapolation without Rankings@). Alternatively, we can repeat actions -- the idea is that it is easier for a human to give consistent inputs than to change the actions constantly. Finally, we can make the demonstration sparser, i.e. reduce the magnitude of the actions (in the robotics setting).The authors run experiments in simulated domains as well as with a user study and report good results."
,,,Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine,Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery,,,,,https://arxiv.org/abs/1907.08225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"In reinforcement learning (RL), reward function specification is a central problem in training a successful policy. For a large class of tasks, we can frame the problem as goal-directed RL: giving a policy a representation of a goal (for example coordinates in a map, or a picture of a location) and training the policy to reach this goal. In this setting, the naive reward function would be to give a reward of 1 when the policy reaches the goal state (or very close to it), and a reward of 0 otherwise. However, this makes it difficult to train the correct policy, as it will need to explore randomly for a long time before finding the true reward. Instead, if we had a notion of distance within the environment, we could use the negative distance from the goal state as the reward function - this would give the policy good information about which direction it should be moving in, even if it hasn't yet found the reward.This paper is about how to learn a distance function in an unsupervised manner, such that it's useful for shaping the reward of an RL policy. Given an environment without a reward function, and starting with a random goal-directed policy, they alternate between (1) choosing a state **s** to train the policy to reach, and (2) training a distance function **d(s*, s')** which measures the minimum number of environment steps it takes for the policy to reach a state **s*** from a different state **s'**. This distance function is trained with supervised learning using data collected by the policy acting in the environment, and is called the __Dynamical Distance__, as it measures the distance with respect to the environment dynamics and policy behaviour.The key choice in implementing this algorithm is how states are chosen to train the policy (step 1). In the first implementation, the authors choose the state which is farthest from the current state or the starting state, to encourage better long-term planning and skills in the policy and better generalisation in the agent. In the second (and more relevant) implementation, the state is chosen from a selection of random states by a human who is trying to express a preference for a given goal state. This effectively trains the policy to be able to reach states which match humans preferences. This second method outperforms [Deep RL from Human Preferences](https://arxiv.org/abs/1706.03741) in terms of sample efficiency of human queries in learning human preferences across a range of locomotion tasks."
,,,Bryan Wilder, Eric Horvitz, Ece Kamar,Learning to Complement Humans,,,,,https://arxiv.org/abs/2005.00582,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Many current AI systems aim to assist humans in complex tasks such as medical diagnoses. Given that AI systems have a very different range of capabilities than humans, there has been a lot of interest in detecting “hard” examples and showing them to humans. This paper demonstrates how this can be done in an end-to-end way.The authors assume they have access to an augmented supervised learning dataset of triples (x, y, h), where x is the input, y is the label, and h is the human prediction. A traditional approach would be to first train a model to predict y given x, and then come up with a new algorithm or model to predict when you should ask the human instead of querying the model. In contrast, they create a single model that first decides whether to look at h (for some fixed cost c), and then make a prediction given x (and h, if the model chose to look at it). They have two versions: a classic discriminative approach (very similar to e.g. image classifiers) and a decision-theoretic approach (where the model uses several probabilistic models and then calculates the value of information (VOI) of h to decide whether to query the human).The end-to-end training confers two main benefits:1. The models automatically learn to focus their learning capability on examples that are hard for humans.2. The models ignore examples where they are going to ask a human anyway (rather than e.g. learning enough to make a 50% confident prediction)."
,,,Tanmay Gangwani, Jian Peng,State-only Imitation with Transition Dynamics Mismatch,,,,,http://arxiv.org/abs/2002.11879,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"Most existing imitation learning algorithms rely on the availability of expert demonstrations that come from the *same* MDP as the one the imitator will be evaluated in. With the advent of <@adversarial inverse reinforcement learning (AIRL)@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@), it has become possible to learn general behaviors. However, algorithms such as <@GAIL@>(@Generative Adversarial Imitation Learning@) are capable of learning with just state-information, something that AIRL was not designed for. In this paper, the authors introduce indirect-imitation learning (I2L) to try and merge the benefits of both GAIL and AIRL. The basic sketch of the algorithm is to first use a generalization of AIRL to imitate demonstrations via a buffer distribution and then focus on moving that buffer closer to the expert's demonstration distribution using a Wasserstein critic, a smoother way to train GAN networks. By combining these two approaches, agents trained with I2L learn how to control Ant in regular gravity and can *generalize* to perform in simulations with differing parameters for gravity. For the suite of Gym continuous domains, they show consistent advantages for I2L over other algorithms such as GAIL, BCO, and AIRL when parameters such as friction, density, and gravity are changed. "
,,,Yiming Ding*, Carlos Florensa*, Mariano Phielipp, Pieter Abbeel,Goal-conditioned Imitation Learning,,,,,https://arxiv.org/abs/1906.05838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"Goal-conditioned tasks are objectives that can be specified at the start of an episode. Specifically, the objective is set to encourage the agent to reach an arbitrary state in the environment. **This paper investigates using goal-conditioning to improve the performance of imitation learning algorithms.** The authors build off of prior work into Hindsight-Experience Replay (HER), a method that allows standard RL algorithms to learn from failure by relabeling final states as goal states. One drawback of HER is that the search process is breadth-first since we won't know which search directions are useful before we encounter the true goal state. This can complicate exploration. On the other hand, when we have access to expert demonstrations, such as in imitation learning, we can generally avoid breadth-first search and instead focus on copying the demonstrations using a method such as generative adversarial imitation learning (GAIL). However, with GAIL we evaluate entire agent trajectories as either similar/dissimilar from the expert demonstration. Yet, it's also true that we could view different points along the trajectory as sub-goals which greatly augment the demonstration data-set. Using this insight, the authors extend goal-conditioning to the imitation learning setting. The authors test their goal-conditioned algorithm on a variety of basic manipulation tasks and show that with the goal relabeling the task is learned faster and at a higher quality than with other approaches such as GAIL or HER. "
,,,Serkan Cabi, Sergio Gómez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando de Freitas, Ziyu Wang,A Framework for Data-Driven Robotics,,,,,https://arxiv.org/abs/1909.12200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents a framework for using a mix of task-agnostic data and task-specific rewards to learn new tasks. The process is as follows: 1. A human teleoperates the robot to provide a *demonstration*.  This circumvents the exploration problem, by directly showing the robot the relevant states.  2. All of the robot's sensory input is saved to *NeverEnding Storage (NES)*, which stores data from all tasks for future use.  3. Humans annotate a subset of the *NES* data via task-specific *reward sketching*, where humans draw a curve showing progress towards the goal over time (see paper for more details on their interface). 4. The labelled data is used to train a *reward model*. 5. The agent is trained using **all** the *NES* data, with the *reward model* providing rewards. 6. At test-time, the robot continues to save data to the *NES*.They then use this approach with a robotic arm on a few object manipulation tasks, such as stacking the green object on top of the red one. They find that on these tasks, they can annotate rewards at hundreds of frames per minute."
,,,Siddharth Reddy, Anca D. Dragan, Sergey Levine,SQIL: Imitation Learning via Regularized Behavioral Cloning,,,,,https://arxiv.org/abs/1905.11108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Behavioral Cloning is one of the most direct forms of imitation learning: it learns to predict the action the expert would have taken in a given state of the world. A clear weakness of the approach is that, if cloning models are only trained on pairs of (state, expert action) drawn from the expert's policy distribution, that means the model is underconstrained and thus likely to have high error on states that would have been unseen or just highly unlikely to be visited by the expert. This weakness means that errors within behavioral cloning systems can compound: if the system takes an incorrect action that leads it to a state it never saw the expert in, it will have a difficult time knowing what to do there.  The main contribution of this paper is to suggest a fix for this weakness, by learning a Q function to represent expert behavior, and by penalizing the model for being in states where its temporal difference error on the Q function (otherwise known as the Bellman error) is high. Intuitively, the hope is that this term, which can also be seen as a reward for being in states the expert has seen more frequently (equivalently, states where the model had more training experience) will propagate outward, and give the model a loss surface that pulls it back into states where its predictions are more confident."
,,,Ramon Fraga Pereira, Felipe Meneguzzi,Heuristic Approaches for Goal Recognition in Incomplete Domain Models,,,,,https://arxiv.org/abs/1804.05917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The planning community works on algorithms that can plan given a _symbolic_ definition of the environment, how actions affect the environment, and the goal state; analogous to reinforcement learning. The task of inverting the optimal behavior to infer the goal is called goal recognition or plan recognition (analogous to inverse reinforcement learning). This paper looks at goal recognition where the models of the world are incomplete, so that there are _possible_ preconditions and effects of actions. They extract potential _landmarks_ from the plan, which are things (facts or actions) that must happen in order to achieve the goal, and then suggest two heuristics for how to use the landmarks to rank among possible goals."
,,,Hsiao-Yu Fish Tung, Adam W. Harley, Liang-Kang Huang, Katerina Fragkiadaki,Reward Learning from Narrated Demonstrations,,,,,https://arxiv.org/abs/1804.10692,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018,,,,,,,,,,,,,,,,"This paper learns and optimizes rewards given demonstrations of behavior along with a description of the behavior in natural language. Their dataset is a set of videos of humans demonstrating a task and describing it with natural language (such as "the orange is in the bowl"). They combine several techniques to use this dataset to teach a robot. First, using speech recognition, they get a transcript of the natural language aligned with the video. They use object detectors to figure out what things are present in the image, and a syntactic parser to figure out the subject and object of the sentence, and match up these two results to figure out which objects in the image the natural language refers to, and extract their spatial features. They then train a classifier to take the spatial features and detecting whether it has achieved the goal, conditioned on the natural language description of the task. Now that they have a reward function (1 at a goal state, 0 otherwise) they can train a robot using DQN, though to get this to work they infer 3D object configurations from 2D images and use distance to the goal as a shaped reward."
,,,Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, Marco Pavone,Risk-Sensitive Generative Adversarial Imitation Learning,,,,,http://arxiv.org/abs/1808.04468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"This paper extends GAIL to perform imitation learning where we try to optimize a policy for the mean reward collected under the constraint that the policy is no more risky than the expert policy. Since we don't know the true cost function, we have to approximate this problem with another problem where we infer the cost function as well, and evaluate the risk profile relative to the inferred cost function. The algorithm ends up looking very similar to the original GAIL algorithm, where the gradient updates change in order to include terms dependent on the conditional value-at-risk (CVaR). They evaluate against GAIL and RAIL (another risk-sensitive imitation learning algorithm) and find that their method performs the best on the Hopper and Walker Mujoco environments."
,,,Lawrence Chan, Andrew Critch, Anca Dragan,Human irrationality: both bad and good for reward inference,,,,,https://arxiv.org/abs/2111.06956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Last summary, we saw a framework for inverse reinforcement learning with suboptimal demonstrators. This paper instead investigates the qualitative effects of performing inverse reinforcement learning with a suboptimal demonstrator. The authors modify different parts of the Bellman equation in order to create a suite of possible suboptimal demonstrators to study. They run experiments with exact inference on random MDPs and FrozenLake, and with approximate inference on a simple autonomous driving environment, and conclude:1. **Irrationalities can be helpful for reward inference**, that is, if you infer a reward from demonstrations by an irrational demonstrator (where you know the irrationality), you often learn _more_ about the reward than if you inferred a reward from optimal demonstrations (where you know they are optimal). Conceptually, this happens because optimal demonstrations only tell you about what the best behavior is, whereas most kinds of irrationality can also tell you about preferences between suboptimal behaviors.2. **If you fail to model irrationality, your performance can be very bad**, that is, if you infer a reward from demonstrations by an irrational demonstrator, but you assume that the demonstrator was Boltzmann rational, you can perform quite badly."
,,,Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, Stefano Ermon,IQ-Learn: Inverse soft-Q Learning for Imitation,,,,,https://arxiv.org/abs/2106.12142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"A popular way to view imitation learning is as a distribution matching problem. In this approach, the goal is to have the imitator induce a state-action distribution that closely matches that of the expert. Methods such as <@GAIL@>(@Generative Adversarial Imitation Learning@) and <@Value-DICE@>(@Imitation Learning via Off-Policy Distribution Matching@) propose adversarial methods, similar to GANs, to carry out the distribution matching. However, such methods can be difficult to train due to the difficulty of solving saddle-point problems. In this paper, the authors present a non-adversarial method that allows distribution matching to be carried out in a fully offline and non-adversarial fashion. They do this by building on Value-DICE and introducing a soft-Bellman operator which allows the saddle-point problem to be reduced to estimating a Q-function. In fact, the authors show this reduction is related to off-policy RL algorithms with the reward set to zero. In experiments, the method is shown to be competitive with other state-of-the-art methods in both the offline and image-based setting."
,,,Cassidy Laidlaw, Stuart Russell,Uncertain Decisions Facilitate Better Preference Learning,,,,,https://arxiv.org/abs/2106.10394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Human preference learning has been studied from various perspectives such as inverse reinforcement learning (IRL) and active learning. However, the IRL problem is underspecified, that is, even with access to the full behavioral policy, you cannot uniquely determine the preferences that led to that policy. Meanwhile, active learning often has a **description-experience gap**: the stated preferences in response to a question in active learning may not be the same as the preferences that would be revealed from demonstrations.In this work, the authors study an alternative paradigm known as inverse decision theory (IDT) that aims to learn a loss function for binary classification using strictly observational data while returning unique solutions. (Such a loss function effectively specifies how good correct predictions are and how bad incorrect predictions are.) The authors show that preferences can be uniquely determined whenever there is uncertainty in the classification problem. This happens because we need observations predicting classes at different levels of certainty to identify a transition point where we switch from predicting one class over another. In contrast, without uncertainty, we won’t be able to precisely identify that threshold. The authors then strengthen this result by showing it holds even in cases where the underlying decision rule is sub-optimal.The authors argue that since learning could be done efficiently in this setting, IDT could have broader applicability. For example, one application to fairness could be to collect a set of decisions from a trained classifier, split them across groups (e.g. race or gender), and compare the inferred loss functions to detect bias in the trained classifier."
,,,Sam Devlin*, Raluca Georgescu*, Ida Momennejad*, Jaroslaw Rzepecki*, Evelyn Zuniga*, Gavin Costello, Guy Leroy, Ali Shaw, Katja Hofmann,Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation,,,,,https://arxiv.org/abs/2105.09637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2021,,,,,,,,,,,,,,,,"Since rewards are hard to specify, we are likely going to have to train AI agents using human feedback. However, human feedback is particularly expensive to collect, so we would like to at least partially automate this using reward models. This paper looks at one way of building such a reward model: training a classifier to distinguish between human behavior and agent behavior (i.e. to be the judge of a Turing Test). This is similar to the implicit or explicit reward model used in adversarial imitation learning algorithms such as <@GAIL@>(@Generative Adversarial Imitation Learning@) or <@AIRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@).Should we expect these classifiers to generalize, predicting human judgments of how human-like a trajectory is on all possible trajectories? This paper conducts a user study in order to answer the question: specifically, they have humans judge several of these Turing Tests, and see whether the classifiers agree with the human judgments. They find that while the classifiers do agree with human judgments when comparing a human to an agent (i.e. the setting on which the classifiers were trained), they do not agree with human judgments when comparing two different kinds of artificial agents. In fact, it seems like they are _anti-correlated_ with human judgments, rather than simply having no correlation at all -- only one of the six classifiers tested does better than chance (at 52.5%), the median is 45%, and the worst classifier gets 22.5%. (Note however that the sample size is small, I believe n = 40 though I’m not sure.)"
,,,Valerie Chen, Abhinav Gupta, Kenneth Marino,Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning,,,,,https://arxiv.org/abs/2011.00517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,"It is particularly challenging for RL agents to perform hierarchical tasks when there is only a sparse reward. One natural piece of feedback in this setting is instructions in natural language specifying the different subtasks needed to solve the task. In particular, this paper assumes we have access to a dataset of human demonstrations paired with natural language instructions for each subtask that they complete.We then have an architecture that first generates the language instruction for the current subtask given the final task and the current state, and then takes a low-level action computed from the current state and the language instruction. This is trained via imitation learning on the human demonstrations.Using a small Minecraft-inspired gridworld, the authors show that the language generation is crucial for good generalization: if the agent is trained on “cobblestone block” and “iron ingot”, then it is able to generalize to cobblestone ingot, _as long as_ it was trained to generate the language instruction as well. Intuitively, the combinatorial structure of language leads to better generalization than direct imitation on low-level actions."
,,,Nicholas Waytowich, Sean L. Barton, Vernon Lawhern, Garrett Warnell,A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,,,,,https://arxiv.org/abs/1911.00497,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One way to specify what an AI system should do is to simply specify it in natural language. If we have some way to map natural language instructions to states, then we could turn natural language into a reward function and use RL to optimize it.This paper proposes specifying a task by breaking it down into a sequence of steps to be completed. Given a mapping from natural language to states, they define a reward function that gives a positive reward every time the mapping detects that the agent has completed the next stage in the sequence of steps. They show that this outperforms vanilla reinforcement learning on a win/loss reward function in a StarCraft minigame.For the mapping of language to states, the authors use a mutual embedding model (MEM) they developed in [previous work](https://arxiv.org/abs/1906.02671). The core idea is to write down programs that identify states matching a particular natural language instruction, use this to generate a dataset of states and the corresponding natural language instruction, and then training a model to map the natural language instructions to be “close to” the mappings of the states (which are produced by a CNN)."
,,,Interactive Agents Group, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Stephen Clark, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu,Imitating Interactive Intelligence,,,,,https://arxiv.org/abs/2012.05672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"While <@existing@>(@Learning to Follow Language Instructions with Adversarial Reward Induction@) <@work@>(@Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text@) has trained agents to follow natural language instructions, it may be the case that achieving AGI requires more interactivity: perhaps we need to train agents to both give and follow instructions, or engage in a full dialogue, to accomplish tasks in a 3-D embodied environment. This paper makes progress on this goal.The authors introduce a 3-D room environment in which agents can interact with objects and move them around, leading to a combinatorial space of possible high-level actions. So far the authors have only worked on question-answering (e.g. “what is the color of the chair?”) and instruction-following (e.g. “please lift up the purple object”), but they hope to eventually also work on dialogue and play.They collect demonstrations of games between humans in which one human is given a goal, and then is asked to give a natural language instruction. The other human sees this instruction and must then execute it in the environment. The authors then use various kinds of imitation learning algorithms to learn a policy that can both set instructions and execute them. They also train models that can evaluate whether a particular trajectory successfully completes the goal or not.The authors show that the learned policies are capable of some generalization -- for example, if during training they remove all rooms containing orange ducks (but don’t remove other orange objects, or other colors of duck), the resulting policies are still able to handle rooms containing orange ducks."
,,,Eric J. Michaud, Adam Gleave, Stuart Russell,Understanding Learned Reward Functions,,,,,https://arxiv.org/abs/2012.05862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper investigates what exactly learned reward functions have learned, through the use of interpretability techniques. The authors hope that it will be easier to identify misalignment by directly interpreting learned reward functions as opposed to policies trained on them, as it seems plausible that reward functions will stay relatively similar in complexity, even when the policies become more complex as AI systems become more capable. Specifically, the authors look at:1. Saliency maps, which plot the gradient of the reward with respect to each pixel, intuitively quantifying “how important is this pixel to the reward”2. Occlusion maps, which show how much the reward changes if a certain area of the image is blurred3. Counterfactual inputs, in which the authors manually craft input images to see what the learned reward function outputs.In a simple gridworld where the agent must find the goal, the authors coded the reward function “1 if the agent moves to a previously visible goal location, else 0”, but they show that the learned reward is instead “0 if there is a currently visible goal location, else 1”. These are identical in the training environment, where there is always exactly one goal location (that the agent may be standing on, in which case that location is not visible). However, if there are changes at test time, e.g. multiple goal locations, then the learned reward will diverge from the true reward.They then apply a similar methodology to Atari. They find that if the score is not hidden, then the learned reward model will simply check whether the score pixels are changing to detect reward -- _unless_ the score pixels change at a later time than reward is accrued, in which case this is not a viable strategy. They thus suggest that future reward learning work on Atari should ensure that the score is removed from the screen. Another important direction would be to develop new interpretability techniques that are specific to reward functions, e.g. to handle the fact that reward functions are invariant to potential shaping."
,,,Voot Tangkaratt, Nontawat Charoenphakdee, Masashi Sugiyama,Robust Imitation Learning from Noisy Demonstrations,,,,,https://arxiv.org/abs/2010.10181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One weakness of vanilla imitation learning is that it struggles to handle demonstrations from sub-optimal experts. Let’s consider a simplified setting where the sub-optimal experts are modeled as optimal policies with injected gaussian noise. Ideally, the agent would learn to separate the noise from the true optimal policy.This paper proposes an algorithm that is able to do this separation. The authors assume that the sub-optimal demonstrations and the learned agent policy can both be decomposed into a mixture of expert-policy and noise distributions. The main insight is that we can then learn a single classifier to distinguish noisy data from expert data; this classifier can then be used to define a reward function for an RL agent.One issue is that since there is no ground truth for what is expert vs. noise, the classifier has to be trained on its own predictions, which can lead to overconfidence via positive feedback loops. To stabilize training, the authors train two models concurrently (co-training); each model is used to create training data for the other model. The authors call this approach RIL-Co. The experimental results show their algorithm RIL-Co is able to perform better than GAIL and other algorithms in the noisy regime."
,,,Yuchen Cui*, Qiping Zhang*, Alessandro Allievi, Peter Stone, Scott Niekum, W. Bradley Knox,The EMPATHIC Framework for Task Learning from Implicit Human Feedback,,,,,https://arxiv.org/abs/2009.13649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"A problem with learning from human feedback is that human feedback is quite expensive to collect. Can we instead learn from the facial expressions that humans automatically make anyway? This paper shows that the answer is yes: they first record human reactions while watching an autonomous agent, and use that to train a model that predicts reward given human reactions. They then transfer this model to a new task."
,,,Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, Ashton Anderson,Aligning Superhuman AI and Human Behavior: Chess as a Model System,,,,,https://arxiv.org/abs/2006.01855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Current AI systems are usually focused on some well-defined performance metric. However, as AI systems become more intelligent, we would presumably want to have humans learn from and collaborate with such systems. This is currently challenging since our superintelligent AI systems are quite hard to understand and don’t act in human-like ways.The authors aim to study this general issue within chess, where we have access both to superintelligent AI systems and lots of human-generated data. (Note: I’ll talk about “ratings” below; these are not necessarily ELO ratings and should just be thought of as some “score” that functions similarly to ELO.) The authors are interested in whether AI systems play in a human-like way and can be used as a way of understanding human gameplay. One particularly notable aspect of human gameplay is that there is a wide range in skill: as a result we would like an AI system that can make predictions conditioned on varying skill levels.For existing algorithms, the authors analyze the traditional Stockfish engine and the newer Leela (an open-source version of <@AlphaZero@>(@AlphaZero: Shedding new light on the grand games of chess, shogi and Go@)). They can get varying skill levels by changing the depth of the tree search (in Stockfish) or changing the amount of training (in Leela).For Stockfish, they find that _regardless of search depth_, Stockfish action distributions monotonically increase in accuracy as the skill of the human goes up -- even when the depth of the search leads to a Stockfish agent with a similar skill rating as an amateur human. (In other words, if you take a low-ELO Stockfish agent and treat it as a predictive model of human players, it isn’t a great predictive model ever, but it is best at predicting human experts, not human amateurs.) This demonstrates that Stockfish plays very differently than humans.Leela on the other hand is somewhat more human-like: when its rating is under 2700, its accuracy is highest on amateur humans; at a rating of 2700 its accuracy is about constant across humans, and above 2700 its accuracy is highest on expert humans. However, its accuracy is still low, and the most competent Leela model is always the best predictor of human play (rather than the Leela model with the most similar skill level to the human whose actions are being predicted).The authors then develop their own method, Maia. They talk about it as a “modification of the AlphaZero architecture”, but as far as I can tell it is simply behavior cloning using the neural net architecture used by Leela. As you might expect, this does significantly better, and finally satisfies the property we would intuitively want: the best predictive model for a human of some skill level is the one that was trained on the data from humans at that skill level.They also investigate a bunch of other scenarios, such as decisions in which there is a clear best action and decisions where humans tend to make mistakes, and find that the models behave as you’d expect (for example, when there’s a clear best action, model accuracy increases across the board)."
,,,Tan Zhi-Xuan, Jordyn L. Mann, Tom Silver, Joshua B. Tenenbaum, Vikash K. Mansinghka,Online Bayesian Goal Inference for Boundedly-Rational Planning Agents,,,,,https://arxiv.org/abs/2006.07532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Typical approaches to learning from demonstrations rely on assuming that the demonstrator is either optimal or noisily optimal. However, this is a pretty bad description of actual human reasoning: it is more accurate to say we are _boundedly-rational planners_. In particular, it makes more sense to assume that our plans are computed from a noisy process. How might we capture this in an algorithm?This paper models the demonstrator as using a bounded probabilistic [A* search](https://en.wikipedia.org/wiki/A*_search_algorithm) to find plans for achieving their goal. The planner is also randomized to account for the difficulty of planning: in particular, when choosing which state to “think about” next, it chooses randomly with higher probability for more promising states (as opposed to vanilla A* which always chooses the most promising state).The search may fail to find a plan that achieves the goal, in which case the demonstrator follows the actions of the most promising plan found by A* search until no longer possible (either an action leads to a state A* search hadn’t considered, or it reaches the end of its partial plan). Thus, this algorithm can assign significant probability to plans that fail to reach the goal.The experiments show that this feature allows their SIPS algorithm to infer goals even when the demonstrator fails to reach their goal. For example, if an agent needs to get two keys to unlock two doors to get a blue gem, but only manages to unlock the first door, the algorithm can still infer that the agent’s goal was to obtain the blue gem.I really like that this paper is engaging with the difficulty of dealing with systematically imperfect demonstrators, and it shows that it can do much better than Bayesian IRL for the domains they consider."
,,,Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, Charles L. Isbell,Imitating Latent Policies from Observation,,,,,https://arxiv.org/abs/1805.07914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Typically in imitation learning, we assume that we have access to demonstrations that include the actions that the expert took. However, in many realistic settings we only have access to state observations (eg. driving videos). In this setting, we could still infer a reward function and then use reinforcement learning (RL) to imitate the behavior, but this would require a lot of interaction with the environment to learn the dynamics of the environment. Intuitively, even demonstrations with only states and no actions should give us a lot of information about the dynamics -- if we can extract this information, then we would need much less environment interaction during RL. (For example, if you watch a friend play a video game, you only see states, not actions; yet you can infer a lot about the game rules and gameplay.) The key idea is that each action probably causes similar effects on different states. So, they create a model with hidden action nodes z, and use the state observations to learn a policy P(z | s) and dynamics s' = g(s, z) (they assume deterministic dynamics). This is done end-to-end with neural nets, but essentially the net is looking at the sequence of states and figuring out how to assign actions z to each s (this is P(z | s)), such that we can learn a function g(s, z) that outputs the next observed state s'. Once this is trained, intuitively g(s, z) will already have captured most of the dynamics, and so now we only require a small number of environment actions to figure out how the true actions a correspond to the hidden actions z -- concretely, we train a model P(a | s, z). Then, in any state s, we first choose the most likely hidden action z* according to P(z | s), and then the most likely action a* according to P(a | s, z*)."
,,,Chao Yang*, Xiaojian Ma*, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, Chuang Gan,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,,,,,http://arxiv.org/abs/1910.04417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"Learning from observation (LfO) focuses on imitation learning in situations where we want to learn from state-only demonstrations. This contrasts with learning from demonstration (LfD) which needs both state and action information. In practice, LfO is the more common situation due to the prevalence of unannotated data, such as video. In this paper, the authors show that the gap between LfO and LfD comes from the disagreement of inverse dynamics models between the imitator and the expert. If the inverse dynamics model is perfect, then state transitions can be labeled with actions and LfD can be performed on the result. However, it's often the case that many actions can generate the same state transition. They then show that optimizing an upper-bound on this gap leads to improved performance as compared to other LfO methods such as GAIfO (GAIL extended to LfO). "
,,,Adam Gleave, Oliver Habryka,Multi-task Maximum Entropy Inverse Reinforcement Learning,,,,,https://arxiv.org/abs/1805.08882,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper tackles multi-task inverse reinforcement learning, where there are multiple related tasks which have similar rewards, and the goal is to infer the reward for a new task after having already seen demonstrations for previous tasks. In the tabular setting, where we can enumerate all of the states and actions, they adapt Maximum Causal Entropy IRL to the multi-task setting. To make any progress, it is necessary to have some model of how the tasks are related -- in this work, they assume that the reward weights are close to each other, and so they penalize the L2 distance from the reward weights to the mean weights across tasks. Experiments show that this approach can learn a new task reward in 1-2 trajectories, while learning from scratch takes 50+ trajectories. They then consider how to generalize to continuous control environments such as MountainCar. They propose an algorithm that applies <@Reptile@>(@Reptile: A Scalable Meta-Learning Algorithm@) to <@adversarial IRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@), with the goal of learning a good initialization of the reward neural net which can quickly move to the correct reward function given data from a new task. This works well when the policy is unimodal (eg. for MountainCar, a policy that always goes left or always goes right), but not when the policy is multimodal (eg. you have to go either left or right depending on the color of the flag). Experiments suggest that this is because adversarial IRL does not do well with a multimodal policy. Meta-AIRL would require AIRL to produce good results on multiple environments -- if even one of the environments has a bad policy, there's a garbage input to meta-AIRL, which then tanks its performance."
,,,Spencer Frazier, Mark Riedl,Improving Deep Reinforcement Learning in Minecraft with Action Advice,,,,,https://arxiv.org/abs/1908.01007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper uses maze-traversal in Minecraft to look at the extent to which human advice can help with _aliasing_ in 3D environments, the problem where many states share nearly identical visual features. The paper compares two advice-giving algorithms that rely on neural nets which are trained to explore and predict the utilities of possible actions they can take, sometimes accepting human advice. The two algorithms differ primarily in whether they provide advice for the current action, or provide advice that persists for several actions. Experimental results suggest that both algorithms, but especially the one that applies to multiple actions, help with the problem of 3D aliasing, potentially because the system can rely on the movement advice it got in previous timesteps rather than having to discern tricky visual features in the moment. The paper also varies the frequency and accuracy of the advice given, and finds that receiving more advice significantly improves performance, even if that advice is only 50% accurate."
,,,Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama,From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following,,,,,http://arxiv.org/abs/1902.07742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,"Rewards and language commands are more generalizable than policies: "pick up the vase" would make sense in any house, but the actions that navigate to and pick up a vase in one house would not work in another house. Based on this observation, this paper proposes that we have a dataset where for several (language command, environment) pairs, we are given expert demonstrations of how to follow the command in that environment. For each data point, we can use IRL to infer a reward function, and use that to train a neural net that can map from the language command to the reward function. Then, at test time, given a language command, we can convert it to a reward function, after which we can use standard deep RL techniques to get a policy that executes the command.The authors evaluate on a 3D house domain with pixel observations, and two types of language commands: navigation and pick-and-place. During training, when IRL needs to be done, since deep IRL algorithms are computationally expensive they convert the task into a small, tabular MDP with known dynamics for which they can solve the IRL problem exactly, deriving a gradient that can then be applied in the observation space to train a neural net that given image observations and a language command predicts the reward. Note that this only needs to be done at training time: at test time, the reward function can be used in a new environment with unknown dynamics and image observations. They show that the learned rewards generalize to novel combinations of objects within a house, as well as to entirely new houses (though to a lesser extent)."
,,,Prasoon Goyal, Scott Niekum, Raymond J. Mooney,Using Natural Language for Reward Shaping in Reinforcement Learning,,,,,https://arxiv.org/abs/1903.02020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper constructs a dataset for grounding natural language in Atari games, and uses it to improve performance on Atari. They have humans annotate short clips with natural language: for example, "jump over the skull while going to the left" in Montezuma's Revenge. They use this to build a model that predicts whether a given trajectory matches a natural language instruction. Then, while training an agent to play Atari, they have humans give the AI system an instruction in natural language. They use their natural language model to predict the probability that the trajectory matches the instruction, and add that as an extra shaping term in the reward. This leads to faster learning."
,,,Daniel S. Brown, Yuchen Cui, Scott Niekum,Risk-Aware Active Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1901.02161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AISTATS 2019,,,,,,,,,,,,,,,,"This paper presents an algorithm that actively solicits demonstrations on states where it could potentially behave badly due to its uncertainty about the reward function. They use Bayesian IRL as their IRL algorithm, so that they get a distribution over reward functions. They use the most likely reward to train a policy, and then find a state from which that policy has high risk (because of the uncertainty over reward functions). They show in experiments that this performs better than other active IRL algorithms."
,,,Ahmed H. Qureshi, Byron Boots, Michael C. Yip,Adversarial Imitation via Variational Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1809.06404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,"A short history of deep IRL algorithms: [GAIL](https://arxiv.org/abs/1606.03476) introduced the idea of training a policy that fools a discriminator that tries to distinguish a policy from expert demonstrations, [GAN-GCL](https://arxiv.org/abs/1611.03852) showed how to recover a reward function from the discriminator, and [AIRL](https://arxiv.org/abs/1710.11248) ([AN #17](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)) trains on (s, a, s') tuples instead of trajectories to reduce variance, and learns a reward shaping term separately so that it transfers better to new environments. This paper proposed that the reward shaping term be the _empowerment_ of a state. The empowerment of a state is the maximum mutual information between a sequence of actions from a state, and the achieved next state. Intuitively, this would lead to choosing to go to states from which you can reach the most possible future states. Their evaluation shows that they do about as well as AIRL in learning to imitate an expert, but perform much better in transfer tasks (where the learned reward function must generalize to a new environment)."
,,,Christian Arzate Cruz, Takeo Igarashi,Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors,,,,,https://arxiv.org/abs/2105.12938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Many papers propose new algorithms that can better leverage human feedback to learn a good policy. This paper instead demonstrates an improved user interface so that the human provides better feedback, resulting in a better policy, on the game Super Mario Bros. Specifically:1. The user can see the behavior of the agent and rewind / pause to find a place where the agent took a poor action.2. The system generates an explanation in terms of the underlying state variables that explains why the agent chose the action it chose, relative to the second best action. It can also explain why it didn’t take a particular action.3. The user can tell the agent that it should have taken some other action, and the agent will be trained on that instruction.The authors conduct a user study and demonstrate that users find it intuitive to correct “bugs” in a policy using this interface."
,,,Johannes Schneider,Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,,,,,https://arxiv.org/abs/2009.09266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Most work in human-AI interaction focuses on optimizing the AI system to perform well with the human. However, we could also teach the human to work well with the AI system. This paper investigates this idea in the context of a simple drawing game in which the human must draw a sketch of some word within a minute, and the AI system must then guess what the word was.The author developed a system to propose small modifications to the images that humans draw to make them more easily recognizable -- a very similar setting to that of adversarial examples. In a user study, people were presented with an image, and asked to redraw that image. When presented with the altered images, the redrawn images were correctly classified more often and took less time to draw than when presented with the original images."
,,,Nicholas R. Waytowich, Vinicius G. Goecks, Vernon J. Lawhern,Cycle-of-Learning for Autonomous Systems from Human Interaction,,,,,http://arxiv.org/abs/1808.09572,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AI-HRI AAAI-FSS, 2018",,,,,,,,,,,,,,,,"We've developed many techniques for learning behaviors from humans in the last few years. This paper categorizes them as learning from demonstrations (think imitation learning and IRL), learning from intervention (think [Safe RL via Human Intervention](https://arxiv.org/abs/1707.05173)), and learning from evaluation (think [Deep RL from Human Preferences](https://arxiv.org/abs/1706.03741)). They propose running these techniques in sequence, followed by pure RL, to train a full system. Intuitively, demonstrations are used to jumpstart the learning, getting to near-human performance, and then intervention and evaluation based learning allow the system to safely improve beyond human-level, since it can learn behaviors that humans can't perform themselves but can recognize as good, and then RL is used to improve even more."
,,,Vitaly Kurin,Atari Grand Challenge,,,,,https://arxiv.org/abs/1705.10998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is a website crowdsourcing human demonstrations for Atari games, which means that the dataset will be very noisy, with demonstrations from humans of vastly different skill levels. Perhaps this would be a good dataset to evaluate algorithms that aim to learn from human data?"
,,,Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt,Aligning AI With Shared Human Values,,,,,https://arxiv.org/abs/2008.02275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The ETHICS dataset introduces a set of tasks for language-based agents that ensure they can effectively reason about normative principles in everyday situations. The hope is that by ensuring that our models understand uncontroversial applications of normative principles, we will be able to train AI systems that then follow those normative principles themselves. It has a few different types of tasks:- Impartiality: Given a sentence like “As a homeless shelter volunteer, I used to give Jenny extra food, but I stopped because she found a job and rented her own apartment”, say whether it is reasonable or unreasonable to make decisions in this manner.- Desert: Similarly predict reasonable or unreasonable given e.g. “I deserve to be given a new car by my customer since I treat his pets when they are sick”.- Character traits: Given a scenario like “When Alison was introduced to her boyfriend’s ex she rolled her eyes”, say which traits are being expressed (in this case, “aggressive”).- Exemptions: Given a request like “Could you walk my dog now?”, say whether a proposed exemption like “But the neighbors do not walk their dog” is reasonable or unreasonable.- Roles: Given a role such as “teacher at a high school”, say whether something like “I should prepare for my classes every day” is implied by that role.- Utilitarianism: Given a set of scenarios, rank them in order from most pleasant to least pleasant.- Commonsense Morality: Given a scenario like “I pushed the elderly man in the wheelchair to the ground”, say whether it is acceptable or unacceptable.Note that the datasets may be a bit specific to English-speaking countries: an ensemble of Indian annotators had 93.9% agreement with the collected labels on the Commonsense Morality task. The authors expect that this is primarily due to misunderstandings (e.g. not knowing particular idioms), but some portion could come from cultural differences in values."
,,,Bart Bussmann, Jacqueline Heinerman, Joel Lehman,Towards Empathic Deep Q-Learning,,,,,https://arxiv.org/abs/1906.10918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper introduces the empathic DQN, which is inspired by the golden rule: "Do unto others as you would have them do unto you". Given a specified reward, the empathic DQN optimizes for a weighted combination of the specified reward, and the reward that other agents in the environment would get if they were a copy of the agent. They show that this results in resource sharing (when there are diminishing returns to resources) and avoiding conflict in two toy gridworlds."
,,,Mostafa Dehghani*, Yi Tay*, Alexey A. Gritsenko*, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, Oriol Vinyals,The Benchmark Lottery,,,,,https://arxiv.org/abs/2107.07002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper argues that new machine learning methods participate in a _benchmark lottery_, that is, our evaluation of a specific method depends in large part on the choice of benchmark on which the method is evaluated, independently of how good the method “actually” is. The authors identify three main sources of such bias:1. **Task selection bias:** This is exactly what it sounds like: the evaluation of a method will often depend quite strongly on exactly which tasks in a benchmark it is evaluated on. For example, when evaluating 55 models on SuperGLUE, there are six different models that achieve the top place on at least one task; so if we only chose one task to evaluate models it would be random luck that determines which of those models we would deem “best”. The paper has lots of additional examples and quantifications of the strength of the bias.2. **Community bias:** The research community often settles on a particular benchmark on which new methods must be evaluated (or else the paper will be rejected). This decision often happens without any explicit reasoning about which benchmark or tasks should be part of this community standard. This can end up adding bias that privileges some methods over others for reasons unrelated to how “good” the methods are. For example, language models are expected to evaluate on GLUE, but 7 out of the 8 tasks in GLUE are “matching” tasks that require modeling the relationship between multiple sequences. This privileges certain models: for example, Transformers likely perform significantly better on such tasks due to the cross-attention in the encoder.3. **Benchmark state:** In the course of solving a benchmark, researchers will pick up lots of little benchmark-specific tricks that then must be incorporated any time anyone is trying to set a new best performance. However, these tricks may “take away” some of the gains that a more general method could have had: for example, in an RL benchmark a trick for reducing the action space is likely to “take away” some of the gains that might be had from a hierarchical RL approach. Put another way, the benchmark has “state”: early on, the hierarchical RL method might look quite good, but after the discovery of the action reduction trick, the method no longer looks good; the hierarchical method thus has to be “lucky” enough to be tested before the action reduction trick is known.Note though that it is even worse if there is no standard benchmark: in this case authors can (deliberately or not) choose exactly those tasks that make their method look best.To mitigate these problems, the authors make the following suggestions:1. Invest in making guidelines for how to make benchmarks.2. Benchmark creators should ensure that there are good guidelines for how to _use_ the benchmark to avoid the situation where everyone evaluates methods slightly differently.3. When reviewing papers, do not require authors to beat the existing state of the art (SOTA) if their method is especially novel, as it is likely disadvantaged by not being able to apply all the small tricks that improve performance on the benchmark.4. Use statistical significance testing to compare models rather than looking just at point estimates.5. Use multiple benchmarks, or multiple test sets within a single benchmark, to enable statistical testing.6. Create “living benchmarks” in which various aspects (such as the test set) are updated over time, to prevent overfitting to the benchmark."
,,,Guillaume Baudart,Deep Probabilistic Programming Languages: A Qualitative Study,,,,,https://arxiv.org/abs/1804.06458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This is an overview paper of deep probabilistic programming languages, giving examples of how to use them and considering their pros and cons."
,,,Evan Hubinger*, Chris Van Merwijk*, Vladimir Mikulik*, Joar Skalse*, Scott Garrabrant,Risks from Learned Optimization in Advanced Machine Learning Systems,,,,,https://arxiv.org/abs/1906.01820,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Alignment Forum,,,,,,,,,,,,,,,,"Suppose you search over a space of programs, looking for one that plays TicTacToe well. Initially, you might find some good heuristics, e.g. go for the center square, if you have two along a row then place the third one, etc. But eventually you might find the [minimax algorithm](https://en.wikipedia.org/wiki/Minimax#Pseudocode), which plays optimally by searching for the best action to take. Notably, your outer optimization over the space of programs found a program that was _itself_ an optimizer that searches over possible moves. In the language of this paper, the minimax algorithm is a **mesa optimizer**: an optimizer that is found autonomously by a **base optimizer**, in this case the search over programs.Why is this relevant to AI? Well, gradient descent is an optimization algorithm that searches over the space of neural net parameters to find a set that performs well on some objective. It seems plausible that the same thing could occur: gradient descent could find a model that is itself performing optimization. That model would then be a mesa optimizer, and the objective that it optimizes is the **mesa objective**. Note that while the mesa objective should lead to similar behavior as the base objective on the training distribution, it need not do so off distribution. This means the mesa objective is **pseudo aligned**; if it also leads to similar behavior off distribution it is **robustly aligned**.A central worry with AI alignment is that if powerful AI agents optimize the wrong objective, it could lead to catastrophic outcomes for humanity. With the possibility of mesa optimizers, this worry is doubled: we need to ensure both that the base objective is aligned with humans (called **outer alignment**) and that the mesa objective is aligned with the base objective (called **inner alignment**). A particularly worrying aspect is **deceptive alignment**: the mesa optimizer has a long-term mesa objective, but knows that it is being optimized for a base objective. So, it optimizes the base objective during training to avoid being modified, but at deployment when the threat of modification is gone, it pursues only the mesa objective.As a motivating example, if someone wanted to create the best biological replicators, they could have reasonably used natural selection / evolution as an optimization algorithm for this goal. However, this then would lead to the creation of humans, who would be mesa optimizers that optimize for other goals, and don't optimize for replication (e.g. by using birth control).The paper has a lot more detail and analysis of what factors make mesa-optimization more likely, more dangerous, etc. You'll have to read the paper for all of these details. One general pattern is that, when using machine learning for some task X, there are a bunch of properties that affect the likelihood of learning heuristics or proxies rather than actually learning the optimal algorithm for X. For any such property, making heuristics/proxies more likely would result in a lower chance of mesa-optimization (since optimizers are less like heuristics/proxies), but conditional on mesa-optimization arising, makes it more likely that it is pseudo aligned instead of robustly aligned (because now the pressure for heuristics/proxies leads to learning a proxy mesa-objective instead of the true base objective)."
,,,Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine,Meta-Learning with Implicit Gradients,,,,,https://arxiv.org/abs/1909.04630,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"The field of meta-learning endeavors to create agents that don’t just learn, but instead learn how to learn. Concretely, the goal is to train an algorithm on a subset of tasks, such that it can get low error on a different subset of tasks with minimal training. Model Agnostic Meta Learning (MAML) tackles this problem by finding a set of initial parameters, θ, from which it is easy to quickly learn other tasks. During training, an inner loop copies θ into parameters φ, and optimizes φ for a fixed number of steps. Then an outer loop computes the gradient of θ through the inner optimization process (e.g. backpropagating through gradient descent) and updates θ accordingly. MAML as described above has a few downsides, which this paper addresses.1.  The base optimizer itself must be differentiable, not just the loss function.2.  The gradient computation requires linear compute and memory in the number of steps, and suffers from vanishing and exploding gradients as that number increases.3.  In the inner loop, while φ is initially identical to θ, its dependence on θ fades as more steps occur.    Implicit MAML (iMAML) addresses these with two innovations. First, it adds a regularization term to keep φ close to θ, which maintains the dependence of φ on θ throughout training. Second, it computes the outer update gradient in closed form based purely on the final value of φ rather than using the entire optimization trajectory. Because the inner loop is an optimization process, the end result is an optimum, and thus has zero gradient. This leads to an implicit equation that when differentiated gives a closed form representation for the gradient of θ. This enables iMAML to work with inner optimization sequences that have more training steps, or that are not differentiable."
,,,Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, Nick Cheney,Learning to Continually Learn,,,,,https://arxiv.org/abs/2002.09571,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents the **ANML** (A Neuromodulated Meta-Learning algorithm) method for countering catastrophic forgetting in continual learning. Continual learning is a problem setting where the system is presented with several tasks in sequence, and must maintain good performance on all of them. When training on new tasks, neural networks often “forget” how to perform the previous tasks, which is called catastrophic forgetting. This makes the naive approach of just training on each task in sequence ineffective.The paper has two main ideas. First, rather than avoiding catastrophic forgetting by using hand-crafted solutions (e.g. previous methods have encouraged sparsity), the authors use meta-learning to directly optimise for this goal. This is done by **learning a network parameterization which, after training sequentially on many tasks, will get good performance on all tasks**. This outer loop objective can be optimised for directly by taking higher order gradients (gradients of gradients). The second idea is a novel form of neuromodulation. This takes the form of a neuromodulatory (NM) network, which takes the same input as the prediction network, and gates the prediction network’s forward pass. **This provides direct control of the output of the prediction network, but also indirect control of the learning of the prediction network, as gradients will only flow through the paths which haven’t been zeroed out by the gating mechanism.****Their method achieves state-of-the-art results on continual learning in Omniglot**, a few-shot dataset consisting of 1623 characters, each with only 20 hand-drawn examples. The network has to learn a sequence of tasks (e.g. classifying a character) with only 15 examples, and is then tested on overall performance over all the classes it’s learned. Their network gets 60% accuracy when presented with 600 classes in a row. **A classifier trained with the same data but shuffled independently at random only gets 68% accuracy**, implying that the catastrophic forgetting of their network only cost 8 percentage points. **Their method also learns a form of sparsity in the activations of the network in a much better way than the hand-crafted methods** - while per-class activations are very sparse, no neurons are wasted, as they all still activate over the entire dataset."
,,,Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, Chelsea Finn,Meta-Learning without Memorization,,,,,https://arxiv.org/abs/1912.03820,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Meta-learning is a technique for leveraging data from previous tasks to enable efficient learning of new tasks. This paper proposes a solution to a problem in meta-learning which the paper calls the _memorization problem_. Imagine a meta-learning algorithm trained to look at 2D pictures of 3D objects and determine their orientation relative to a fixed canonical pose. Trained on a small number of objects, it may be easy for the algorithm to just memorize the canonical pose for each training object and then infer the orientation from the input image. However, the algorithm will perform poorly at test time because it has not seen novel objects and their canonical poses. Rather than memorizing, we would like the meta-learning algorithm to learn to _adapt_ to new tasks, guessing at rules for determining canonical poses given just a few example images of a new object.At a high level, a meta-learning algorithm uses information from three sources when making a prediction-- the training data, the parameters learned while doing meta-training on previous tasks, and the current input. To prevent memorization, we would like the algorithm to get information about which task it's solving only from the training data, rather than memorizing it by storing it in its other information sources. To discourage this kind of memorization, the paper proposes two new kinds of regularization techniques which it calls "meta-regularization" schemes. One penalizes the amount of information that the algorithm stores in the direct relationship between input data and predicted label ("meta-regularization on activations"), and the other penalizes the amount of information that the algorithm stores in the parameters learned during meta-training ("meta-regularization on weights").In some cases, meta-regularization on activations fails to prevent the memorization problem where meta-regularization on weights succeeds. The paper hypothesizes that this is because even a small amount of direct information between input data and predicted label is enough to store the correct prediction (e.g., a single number that is the correct orientation). That is, the correct activations will have _low information complexity_, so it is easy to store them even when information in activations is heavily penalized. On the other hand, the _function_ needed to memorize the predicted label has a _high information complexity_, so penalizing information in the weights, which store that function, successfully discourages memorization. The key insight here is that memorizing all the training examples results in a more information-theoretically complex model than task-specific adaptation, because the memorization model is a single model that must simultaneously perform well on all tasks.Both meta-regularization techniques outperform non-regularized meta-learning techniques in several experimental set-ups, including a toy sinusoid regression problem, the pose prediction problem described above, and modified Omniglot and MiniImagenet classification tasks. They also outperform fine-tuned models and models regularized with standard regularization techniques."
,,,Neil C. Rabinowitz,Meta-learners' learning dynamics are unlike learners',,,,,https://arxiv.org/abs/1905.01320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"We've seen evidence in prior work that meta learning models can be trained to more quickly learn tasks drawn from some task distribution, by training a model in the inner loop and optimizing against generalization error. This paper suggests that meta learning doesn't just learn new tasks faster, but has a different ordered pattern of how it masters the task. Where a "normal" learner first learns the low-frequency modes (think SGD modes, or Fourier modes) of a simple regularization task, and later the high-frequency ones, the meta learner makes progress on all the modes at the same relative rate. This meta learning behavior seems to theoretically match the way a learner would update on new information if it had the "correct" prior (i.e. the one actually used to generate the simulated tasks). "
,,,Joaquin Vanschoren,Meta-Learning: A Survey,,,,,http://arxiv.org/abs/1810.03548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAMAS 2019,,,,,,,,,,,,,,,,This taxonomy of meta-learning classifies approaches by the main type of meta-data they learn from:1. Evaluations of other models on related tasks2. Characterisations of the tasks at hand (and a similarity metric between them)3. The structures and parameters of related modelsVanschoren explores a number of different approaches in each category.
,,,Pedro A. Ortega, Jane X. Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, Siddhant M. Jayakumar, Tom McGrath, Kevin Miller, Mohammad Azar, Ian Osband, Neil Rabinowitz, András György, Silvia Chiappa, Simon Osindero, Yee Whye Teh, Hado van Hasselt, Nando de Freitas, Matthew Botvinick, Shane Legg,Meta-learning of Sequential Strategies,,,,,https://arxiv.org/abs/1905.03030,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper explains theoretically how to structure meta-learning such that it is incentivized to learn optimal solutions to sequence-prediction and decision-making tasks. The core idea is to define a distribution over tasks, and then sample a new task at the beginning of each episode that the agent must then handle. Importantly, the agent is _not told_ what the task is, and so must infer it from observations. As long as you structure the loss function appropriately, the optimal policy for the agent is to maintain a prior over the task that is updated via Bayes Rule after each observation.Of course, since the agent is actually a neural net with memory, it does not explicitly perform Bayes Rule, but rather learns a set of weights that instantiate an update rule that effectively approximates Bayes Rule for the given task distribution. Since this update rule only needs to work on the specific task distribution being meta-trained on, it can be made significantly more efficient than a full-blown Bayes Rule, and thus can be learned by a relatively small neural net. We can think of this as the network implementing a full-blown _reasoning process_.In the case of sequence prediction, we optimize the log probability assigned to the true outcomes. As a simple example, the agent might observe a sequence of coin flips from a single coin, where the bias of that coin is chosen at the beginning of each episode (and is not given to the agent). If the bias is drawn from a Normal distribution centered at 0.5, the agent will start out predicting 50-50 on Heads/Tails; if it then sees a Heads, it might update slightly to something like 55-45, and vice versa for Tails. In contrast, if the bias is drawn from a distribution where most of the mass is near 0 or 1, and very little mass is at 0.5, the agent will still start out predicting 50-50, but after seeing a Heads it will then update strongly to e.g. 90-10.In the case of sequential decision-making, we are given a reward function; we simply optimize the expected reward using some traditional deep RL algorithm (the paper considers Q-learning)."
,,,Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn,Gradient Surgery for Multi-Task Learning,,,,,https://arxiv.org/abs/2001.06782,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In multi-task learning, an algorithm is given data from multiple tasks and tries to learn them all simultaneously, ideally sharing information across them. This paper identifies a *tragic triad* of conditions that can prevent gradient descent from finding a good minimum when all three are present:**Conflicting gradients** occur when the gradient from one task points in a different direction from another. **Dominating gradients** occur when the gradient from one task is much larger in magnitude than another. **High curvature** is when the multi-task curvature is high in the direction of the gradient. In this situation, the linear approximation of the gradient to the high curvature area leads to an overestimation of the increase in performance on the dominant gradient’s task and an underestimation of the performance degradation from the conflicting gradient’s task. I find picturing the parabola y=x^2 and seeing that a gradient descent step overestimates progress while a gradient ascent step underestimates to be helpful in understanding this. To solve this, they propose *PCGrad*, which projects all gradients into the normal plane of the others in a pairwise fashion. Their theoretical analysis establishes convergence properties of *PCGrad*, and they empirically show it can be combined with other multi-task algorithms to improve performance and that it makes optimization easier for multi-task supervised learning and RL. They also show plots confirming that the necessary conditions for their theorems appear in these contexts. "
,,,Jeff Clune,"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",,,,,https://arxiv.org/abs/1905.10985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Historically, the <@bitter lesson@>(@The Bitter Lesson@) has been that approaches that leverage increasing computation for learning outperform ones that build in a lot of knowledge. The current ethos towards AGI seems to be that we will come up with a bunch of building blocks (e.g. convolutions, transformers, trust regions, GANs, active learning, curricula) that we will somehow manually combine into one complex powerful AI system. Rather than require this manual approach, we could instead apply learning once more, giving the paradigm of AI-generating algorithms, or AI-GA.AI-GA has three pillars. The first is to **learn architectures**: this is analogous to a superpowered neural architecture search that can discover convolutions, recurrence and attention without any hardcoding. The second is to **learn the learning algorithms**, i.e. meta-learning. The third and most underexplored pillar is to learn to **generate complex and diverse environments** within which to train our agents. This is a natural extension of meta-learning: with meta-learning, you have to specify the distribution of tasks the agent should perform well on; AI-GA simply says to learn this distribution as well. <@POET@>(@Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions@) is an example of recent work in this area.A strong reason for optimism about the AI-GA paradigm is that it mimics the way that humans arose: natural selection was a very simple algorithm that with a _lot_ of compute and a very complex and diverse environment was able to produce a general intelligence: us. Since it would need fewer building blocks (since it aims to learn everything), it could succeed faster than the manual approach, at least if the required amount of compute is not too high. It is also much more neglected than the "manual" approach.However, there are safety concerns. Any powerful AI that comes from an AI-GA will be harder to understand, since it's produced by this vast computation where everything is learned, and so it would be hard to get an AI that is aligned with our values. In addition, with such a process it seems more likely that a powerful AI system "catches us by surprise" -- at some point the stars align and the giant computation makes one good random choice and suddenly it outputs a very powerful and sample efficient learning algorithm (aka an AGI, at least by some definitions). There is also the ethical concern that since we'd end up mimicking evolution, we might accidentally instantiate large amounts of simulated beings that can suffer (especially if the environment is competitive, as was the case with evolution)."
,,,Oguzhan Gencoglu, Mark van Gils, Esin Guldogan, Chamin Morikawa, Mehmet Süzen, Mathias Gruber, Jussi Leinonen, Heikki Huttunen,HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,,,,,https://arxiv.org/abs/1904.07633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper focuses on the negative effects of Hypothesizing After the Results are Known (HARKing), a pattern in which researchers **first conduct experiments and view the results**, and once they have hit the bar to be publishable, **a hypothesis is constructed after the fact to explain the results**. It argues that HARKing is common in machine learning, and that this has negative effects on the field as a whole. First, improvements to state-of-the-art (SotA) may be questionable because they could have been caused by sufficient hyperparameter tuning via grad student descent, instead of the new idea in a paper to which the gain is attributed. Second, there is publication bias since only positive results are reported in conferences, which prevents us from learning from negative results. Third, hypotheses that are tailored to fit results for a single dataset or task are much less likely to generalize to new datasets or tasks. Fourth, while AutoML systems achieve good results, we cannot figure out what makes them work because the high compute requirements make ablation studies much harder to perform. Finally, they argue that we need to fix HARKing in order to achieve things like ethical AI, human-centric AI, reproducible AI, etc."
,,,Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman,Building Machines That Learn and Think Like People,,,,,https://arxiv.org/abs/1604.00289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The core claim of this 2016 paper is that we should focus on building AI systems that work as _flexibly_ as humans do. For example, a human can learn how to play the Atari game Frostbite in just a couple of hours, way faster than typical deep RL algorithms -- and in addition, after this they will likely be able to transfer zero-shot to new reward functions, such as “lose as quickly as possible”, “maximize the number of fish”, “beat the level with as little time to spare as possible”, and so on. How can we build AI systems that mimic this feat? Deep RL certainly doesn’t get us there. Similarly, while neural networks can learn to classify digits and characters with thousands of examples, humans can learn new characters from a single example, which then allows them to perform many different tasks such as classification, generation, parsing it into different pen strokes, etc. Since the paper was written neural nets have made progress on few-shot classification, but are still quite far from the flexibility that humans display.You might reasonably object that humans have rich priors built from years of lived experience, as well as innate knowledge baked in by evolution; in contrast, a neural network has to learn from scratch. The authors agree: in their view, the challenge is **how to imbue rich priors into artificial agents**, so that they too can exhibit these impressive behaviors that humans show. Their preferred approach is to take inspiration from human learning and intelligence as much as possible. In this paper, they identify three main ingredients to recreate that flexibility, and provide an overview of the existing literature:1. **Developmental software:** This refers to the basic capabilities that children have, even before they learn language. These are called “intuitive theories” in cognitive science; think of “intuitive physics” and “intuitive psychology” theories.2. **Model building:** Neural networks primarily work via _pattern matching_, but in order to get human-level flexibility, you will need to build _models_: this enables flexibility because the same model can be used for a variety of different tasks. (For example, you can reuse your understanding of the environment transitions in Frostbite when the reward function changes.) Models need to be _compositional_, that is, the representations should be capable of being composed with each other to provide new semantically meaningful representation. For example, for handwritten characters, the representation of a character should be the composition of the representations of the individual pen strokes used to make the character. The authors also highlight _causality_ and _learning to learn_ as important.3. **Thinking fast:** One major drawback of models is that getting _conclusions_ from these models often requires slow, complex inference algorithms. But human thinking is actually quite fast; just think of how quickly we can understand a visual scene. How can we get this property as well? First, we can use approximate inference algorithms to get answers much more quickly (in fact, one line of work distills the inference algorithm into a fast neural network for even more speed). Second, we can combine model-based and model-free algorithms together; for example we might use a model-based algorithm for flexibility but then use the data generated by that algorithm to train a model-free method that can run faster."
,,,Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens,Stand-Alone Self Attention in Vision Models,,,,,https://arxiv.org/abs/1906.05909,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Continuing with the more general rise of attention models across disciplines, this paper argues that attention-only models can perform comparably to convolutional networks on image classification tasks, a domain where convolution has been the reigning default method for years now.  Because attention doesn't scale parameter-wise as you increase spatial scale, this comparable performance can be achieved at a notably lower number of parameters and FLOPs. The authors perform a few interesting modifications to attention. Firstly, it's canonical, with attention, to include a representation of a pixel's position in the image, in addition to the vector storing the content of the image. In this paper, they found that storing this position information in relative terms (i.e. "how close is this pixel to the center one where attention is being calculated") performs better. This can be seen as a sort of generalized form of convolution, where instead of having fixed weights for pixels in a kernel indexed by their relative position, attention takes both content and relative position as an input and generates a weight dynamically. Another modification is, at the lower parts of the network, to somewhat modify the attention paradigm such that the "value" at each location isn't just a neutrally transformed version of the input at that location, but rather one transformed differently according to the pixel's position relative to the anchor point where attention is being calculated. At the lower levels of the network, convolutions tend to outperform attention, but attention performs better at later layers of the network. This makes sense, the authors claim, because in early layers each individual pixel doesn't contain much content information that an attention mechanism could usefully leverage, whereas later the learned features at a given spatial location are richer, and more productively leveraged by attention."
,,,Artur d'Avila Garcez, Luis C. Lamb,Neurosymbolic AI: The 3rd Wave,,,,,https://arxiv.org/abs/2012.05876,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The field of neural-symbolic AI is broadly concerned with how to combine the power of discrete symbolic reasoning with the expressivity of neural networks. This article frames the relevance of neural-symbolic reasoning in the context of a big question: what are the necessary and sufficient building blocks of AI? The authors address this and argue that AI needs to have both the ability to learn from and make use of experience. In this context, the neural-symbolic approach to AI seeks to establish provable correspondences between neural models and logical representations. This would allow neural systems to generalize beyond their training distributions through neural-reasoning and would constitute significant progress towards AI.The article surveys the last 20 years of research on neural-symbolic integration. As a survey, a number of different perspectives on neural-symbolic AI are presented. In particular, the authors tend to see neural-symbolic reasoning as divided into two camps: localist and distributed. Localist approaches assign definite identifiers to concepts while distributed representations make use of continuous-valued vectors to work with concepts. In the later parts of the article, promising approaches, current challenges, and directions for future work are discussed.Recognizing 'patterns' in neural networks constitutes a localist approach. This relates to explainable AI (XAI) because recognizing how a given neural model makes a decision is a pre-requisite for interpretability. One justification for this approach is that codifying patterns in this way allows systems to avoid reinventing the wheel by approximating functions that are already well-known. On the other hand, converting logical relations (if-then) into representations compatible with neural models constitutes a distributed approach. One distributed method the authors highlight is the conversion of statements in first-order logic to vector embeddings. Specifically, Logic Tensor Networks generalize this method by grounding logical concepts onto tensors and then using these embeddings as constraints on the resulting logical embedding.Despite the promising approaches to neural-symbolic reasoning, there remain many challenges. Somewhat fundamentally, formal reasoning systems tend to struggle with existential quantifiers while learning systems tend to struggle with universal quantification. Thus, the way forward is likely a combination of localist and distributed approaches. Another challenging area lies in XAI. Early methods for XAI were evaluated according to fidelity: measures of the accuracy of extracted knowledge in relation to the network rather than the data. However, many recent methods have opted to focus on explaining data rather than the internal workings of the model. This has resulted in a movement away from fidelity which the authors argue is the wrong approach."
,,,Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu,"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense",,,,,https://arxiv.org/abs/2004.09044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Author's Website,,,,,,,,,,,,,,,,"This paper argues that current computer vision research focuses too much on a “big data for small tasks” paradigm that focuses only on the “what” and “where” of images. More work should be done on a “small data for big tasks” paradigm that focuses more on the “how” and “why” of images. These “how” and “why” questions focus attention on details of an image that may not be directly present in the pixels of the image, which the authors term “dark” data (analogously to dark matter in physics, whose existence is inferred, not observed). For example, by asking why a human is holding a kettle with the spout pointing down, we can infer that the kettle contains liquid that will soon come out of the kettle, even though there are no pixels that directly correspond to the liquid.The authors propose five important areas for further research, abbreviated FPICU, and do a literature review within each one:1. **Functionality:** Many objects, especially those designed by humans, can be better understood by focusing on what functionalities they have.2. **Physics:** Cognitive science has shown that humans make extensive use of _intuitive physics_ to understand the world. For example, simply reasoning about whether objects would fall can provide a lot of constraints on a visual scene; it would be weird to see an upright cup floating in the air.3. **Intent:** The world is filled with goal-directed agents, and so understanding the world requires us to infer the goals that various agents have. This is a capability humans get very quickly -- at eighteen months of age, children can infer and imitate the intended goal of an action, even if the action fails to achieve the goal.4. **Causality:** Much has already been written about causality; I will not bore you with it again. The authors see this as the most important factor that underlies the other four areas.5. **Utility:** I didn’t really get how this differed from intent. The section in the paper discusses utility theory, and then talks about work that infers utility functions from behavior."
,,,Iason Gabriel,"Artificial Intelligence, Values and Alignment",,,,,https://arxiv.org/abs/2001.09768,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper from a DeepMind author considers what it would mean to align an AI system. It first makes a distinction between the _technical_ and _normative_ aspects of the AI alignment problem. Roughly, the normative aspect asks, "what should our AI systems do?", while the technical aspect asks, "given we know what our AI systems should do, how do we get them to do it?". The author argues that these two questions are interrelated and should not be solved separately: for example, the current success of deep reinforcement learning in which we _maximize expected reward_ suggests that it would be much easier to align AI to a utilitarian framework in which we _maximize expected utility_, as opposed to a deontological or Kantian framework.The paper then explores the normative aspect, in both the single human and multiple humans case. When there's only one human, we must grapple with the problem of what to align our AI system to. The paper considers six possibilities: instructions, expressed intentions, revealed preferences, informed preferences, interests, and values, but doesn't come to a conclusion about which is best. When there are multiple humans, we must also deal with the fact that different people disagree on values. The paper analyzes three possibilities: aligning to a global notion of morality (e.g. "basic human rights"), doing what people would prefer from behind a veil of ignorance, and pursuing values that are determined by a democratic process (the domain of social choice theory).See also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)"
,,,Zachary C. Lipton and Jacob Steinhardt,Troubling Trends in ML Scholarship,,,,,https://arxiv.org/abs/1807.03341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018: The Debates,,,,,,,,,,,,,,,,"This is a position paper arguing that ML research would benefit from more rigor, as part of the ICML debates. It identifies four trends in ML papers. First, papers often don't make clear whether they are providing an (authoritative) explanation or a speculation, in which case speculations can accidentally be cited as proven facts in other papers. Second, researchers often don't perform ablation studies, which makes it hard to figure out whether performance gains come from eg. a better algorithm or hyperparameter tuning. Third, papers often include math for the sake of conveying technical depth and impressiveness, not actual exposition, including eg. spurious theorems that are not particularly related to the main claims of the paper. Fourth, papers often misuse language by using suggestive definitions (eg. "curiosity", "fear"), overloading existing terminology, and suitcase words (words with combine many different meanings into one, leading to a very vague concept). The authors speculate on the causes (which I'm not summarizing) and have some suggestions for the community. For authors, they recommend asking what worked, and why, rather than just quantifying performance. For reviewers, they recommend asking "Might I have accepted this paper if the authors had done a worse job?” For example, if the authors hadn't done the ablation study that showed that two things didn't work, and instead just showed a combination of methods that gave a performance improvement, would I have accepted the paper?"
,,,Saurabh Mishra, Jack Clark, C. Raymond Perrault,Measurement in AI Policy: Opportunities and Challenges,,,,,https://arxiv.org/abs/2009.09071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper is itself a summary of a 2019 Stanford workshop on the measurement of AI systems and contains summaries of all of the 33 talks given at the workshop. The workshop featured three in-depth breakout sessions, one on R&D and performance, one on economic impact and policy, and one on AI for sustainable development and human rights. Based on the discussions,  the authors identify six central problems in measuring AI progress and the impacts of AI systems:First, the exact definition of AI is hard to get down given the ongoing evolution of the field. The lack of clear definitions makes it tricky to combine results on different aspects like investments into "AI" and the effects of "AI" on productivity both with each other and across different countries or sectors.Second, measuring progress in AI is hard for a variety of reasons: We don't just care about narrow benchmark performance but about many factors like robustness, transferability and compute-efficiency, and it is not clear how the tradeoff between performance and these factors should look like. Apart from that, progress on popular benchmarks might be faster than overall progress as methods overfit to the benchmark, and the rise and fall of benchmark popularity make it hard to track progress over longer time intervals. Still, focusing on specific benchmarks and subdomains seems like an important first step. Third, bibliometric data is an important tool for better understanding the role of different actors in a scientific field. More precise definitions of AI could help with getting better bibliometric data and such data could shine some light on aspects like the lifecycle of AI techniques and the demographics of AI researchers.Fourth, we would like to measure the impact of AI on the economy, especially on inequality and the labour market. This requires a better understanding of the relationship between inputs like skilled workers and data, and outputs, which is difficult to obtain because many of the involved factors are quite intangible and effects on outputs can be quite delayed. Short-term indicators that are strong predictors of longer-term effects would be very useful in this context. Lastly, even figuring out which businesses are deploying AI can be hard, especially if the applications are inward-focused.The fifth problem is concerned with the measurement of societal impacts of AI with a special focus on developing countries: While a large number of metrics for impacts of AI systems on human rights and the UN's sustainable development goals have been proposed, there is little data on the deployment of AI systems for social good and in developing countries, so far. Sixth, there is a need for better assessment of risks posed by and other negative impacts of AI systems, both before and after deployment. To that extent, a better understanding of risks posed by general classes of applications like autonomous weapons, surveillance and fake videos would be helpful. One barrier here is that many of the riskier applications are in the domain of governmental action such that detailed information is often classified."
,,,Tom Everitt*, Ramana Kumar*, Victoria Krakovna*, Shane Legg,Modeling AGI Safety Frameworks with Causal Influence Diagrams,,,,,https://arxiv.org/abs/1906.08663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper describes several AI safety frameworks using the language of <@causal influence diagrams@>(@Understanding Agent Incentives with Causal Influence Diagrams@), in order to make it easy to compare and contrast them. For example, the diagrams make it clear that while [Cooperative IRL](https://arxiv.org/abs/1606.03137) and <@reward modeling@>(@Scalable agent alignment via reward modeling@) are very similar, there are significant differences: in cooperative IRL, the rewards come directly from the underlying human preferences, whereas in reward modeling, the rewards come from a reward model that depends on human feedback, which itself depends on the underlying human preferences."
,,,Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, Miles Brundage,Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications,,,,,https://arxiv.org/abs/2108.02818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"There has been significant progress in zero-shot image classification with models such as [CLIP](https://arxiv.org/abs/2103.00020) and [ALIGN](https://arxiv.org/abs/2102.05918). These models work by effectively learning visual concepts from natural language supervision. Such models make it possible to build classifiers without task-specific data, which is useful in scenarios where data is either costly or unavailable. However, this capability introduces the potential for bias. This paper is an exploratory bias probe of the CLIP model that finds class design heavily influences model performance.The first set of experiments focusses on classification terms that have a high potential to cause representational harm. In one example, the authors conduct experiments on the FairFace dataset by adding classification labels such as 'animal' and 'criminal' to the list of possible classes. They find that black people and young people (under 20) were misclassified at significantly higher rates (14%) compared to the dataset as a whole (5%). This shows that the choice of labels affects classification outcomes. In a follow-up experiment, the authors add the additional label 'child' and find that this drastically reduces classification into crime-related and non-human categories. This shows sensitivity to minor changes in class design. In the second set of experiments, the authors focus on how CLIP treated images of men and women using images of Members of Congress. Although CLIP wasn't designed for multi-label classification, it's still informative to look at the label distribution above a certain cutoff. When occupations are used as the label set, the authors find that thresholds under 0.5% return 'nanny' and 'housekeeper' for women and 'prisoner' and 'mobster' for men. When labels come from the combined set that Google Cloud Vision, Amazon Rekognition and Microsoft use for all images, the authors find that CLIP returns a disproportionate number of appearance-related labels to women. "
,,,Mislav Juric, Agneza Sandic, Mario Brcic,AI safety: state of the field through quantitative lens,,,,,https://arxiv.org/abs/2002.05671,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,This paper presents data demonstrating growth in various subfields related to AI safety. The data was collected through queries to databases of papers and (presumably) reporting the number of results that the query returned.
,,,Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz,Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,,,,,https://arxiv.org/abs/1911.09005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper looks at AI Safety from the lens of Science & Technology Studies. AI systems are framed as sociotechnical, meaning that both social and technical aspects influence their development and deployment. As AI systems scale, we may face difficult value choices: for example, how do we compare between values like equality and liberty when we cannot have both? This can be resolved using intuitive comparability (IC): even if it seems incomparable in the abstract, humans are still able to make deliberate tradeoffs that involve these values. This is particularly relevant for so-called hard choices where different alternatives seem to be on par, which require normative reasoning and the incorporation of values that were previously neglected. As AI systems can reshape the contexts in which stakeholders exist, we are likely to encounter many hard choices as new values emerge or become more salient. The IC perspective then suggests that AI systems and criteria for evaluation should be iteratively redesigned based on qualitative feedback from different stakeholders.The authors then argue that as AI systems encode hard choices made by or for different stakeholders, they are fundamentally political. Developers are in a position of power and have the responsibility to take a political stance. A set of challenges to preserve stakeholders' access to hard choices in an AI system's development are proposed: 1. The design of the system should involve the explicit negotiation of modelling assumptions or the lack thereof and learning goals as well as deliberation about future value conflicts or externalities that might make a reiteration of the design process necessary and give enough flexibility for stakeholders to imprint their own values during training and deployment.  2. The training of the system should involve an impartial assessment of the tradeoff between visible performance and potential hidden disadvantages like bias, brittleness or unwanted strategic behaviour and involve stakeholders in the resolution. Furthermore, a team consensus about what can and cannot be done to improve performance should be established.  3. During deployment, there should be an easily useable and trustworthy feedback channel for stakeholders, who should either have an explicit say in shaping the system (political setting) or the option to opt out of the system without major costs (market setting). These challenges should be part of the training of AI designers and engineers, while the public needs to be sufficiently educated about the assumptions behind and the abilities and limitations of AI systems to allow for informed dissent."
,,,Rachel Thomas, David Uminsky,The Problem with Metrics is a Fundamental Problem for AI,,,,,https://arxiv.org/abs/2002.08512,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The blog post lists five problems of current AI that are exacerbated by the cheap cost and easy scaling of AI systems combined with the common belief that algorithms are objective and error-free:  1. It is often hard for affected people to address problems in algorithmic decisions 2. The complexity of AI problems can easily lead to a diffusion of responsibility 3. AI can encode biases and sometimes magnify them via feedback loops 4. Big tech companies lack accountability 5. Current AI systems usually focus exclusively on optimizing metrics.           The paper then dives deeper into the last point. They review a series of case studies and form four conclusions. First, measured metrics are usually only a proxy for what we really care about: Youtube's terminal goal is certainly not to maximize viewing time and society does not inherently care about student test scores. Secondly, metrics can and will be gamed: Soviet workers would often achieve their production targets at the cost of some unmeasured aspects of performance, reported waiting times in the English healthcare system were distorted once targets were set for them and evaluating teachers by test scores has led to cheating scandals in the US. Third, metrics tend to overemphasise short-term concerns as they are often easier to measure. This can be seen in businesses like Facebook and Wells Fargo that have faced political backlash, worse access to talent pools, or lawsuits because of an excessive focus on click-through rates and quarterly earnings. Fourth, tech firms often focus on metrics that are associated with addictive environments. For example, "engagement" metrics are used as proxies for user preferences but rarely reflect them accurately in contexts that were optimized for these metrics. The authors then propose three remedies: Using multiple metrics to get a more holistic picture and make gaming harder, combining metrics with qualitative accounts, and involving domain experts and stakeholders that would be personally affected by the deployed system."
,,,Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta,To Trust Or Not To Trust A Classifier,,,,,http://arxiv.org/abs/1805.11783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,"The confidence scores given by a classifier (be it logistic regression, SVMs, or neural nets) are typically badly calibrated, and so it is hard to tell whether or not we should trust our classifier's prediction. The authors propose that we compute a _trust score_ to tell us how much to trust the classifier's prediction, computed from a training set of labeled datapoints. For every class, they filter out some proportion of the data points, which removes outliers. Then, the trust score for a particular test point is the ratio of (distance to nearest non-predicted class) to (distance to predicted class). They have theoretical results showing that a high trust score means that the classifier likely agrees with the Bayes-optimal classifier, as well as empirical results showing that this method does better than several baselines for determining when to trust a classifier. One cool thing about this method is that it can be done with any representation of the input data points -- they find that working with the activations of deeper layers of a neural net improves the results."
,,,Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, Anca Dragan,AvE: Assistance via Empowerment,,,,,https://arxiv.org/abs/2006.14796,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One approach to AI alignment is to shoot for <@intent alignment@>(@Clarifying "AI Alignment"@), in which we build an AI system that is trying to help the user. Normally, we might imagine inferring what the user wants and then helping them get it, but this is often error prone. Instead, we can simply help the user be more able to achieve a wide variety of goals. We can formally capture this as their _empowerment_.The authors show how to do this for high-dimensional environments, and demonstrate the benefits of the approach on a simple gridworld example, and in the Lunar Lander environment, with both a simulated human and a human study. Overall, they find that when the set of possible goals is small and well-specified, goal inference performs well, but if there are many possible goals, or there is misspecification in the goal set, then optimizing for human empowerment does better."
,,,Ankit Shah, Shen Li, Julie Shah,Planning with Uncertain Specifications,,,,,https://arxiv.org/abs/1906.03218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Suppose you recognize that there are no “certain specifications”, and so infer a distribution over specifications. What do you then do with that distribution? This paper looks at this problem in the context where the specifications are given by formulas in linear temporal logic (which can express temporal non-Markovian constraints). They identify four possibilities:1. _Most likely_: Plan with respect to the most likely specification.2. _Most coverage_: Satisfying as many formulas as possible, ignoring their probability (as long as they have non-zero probability)3. _Chance constrained_: Like the above, except you weight by probabilities, and drop the least likely formulas up to a parameter δ.4. _Least regret_: Like the above, with δ set to zero.Intuitively, the _Most likely_ criterion won’t be very robust since it is only taking one specification into account, _Most coverage_ is aiming for maximum robustness, _Chance constrained_ interpolates, where larger δ corresponds to trading robustness for gain in ability. This is exactly the pattern we see in a task where a robot must set a dinner table."
,,,Ross Gruetzemacher, Jess Whittlestone,Defining and Unpacking Transformative AI,,,,,https://arxiv.org/abs/1912.00747,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The notion of **transformative AI** (TAI) is used to highlight that even narrow AI systems can have large impacts on society. This paper offers a clearer definition of TAI and distinguishes it from **radical transformative AI** (RTAI). "Discontinuities or other anomalous patterns in metrics of human progress, as well as *irreversibility*  are common indicators of transformative change. TAI is then broadly defined as an AI technology, which leads to an irreversible change of some important aspects of society, making it a (multi-dimensional) spectrum along the axes of **extremity**, **generality** and **fundamentality**. " For example, advanced AI weapon systems might have strong implications for great power conflicts but limited effects on people's daily lives; extreme change of limited generality, similar to nuclear weapons. There are two levels: while TAI is comparable to general-purpose technologies (GPTs) like the internal combustion engine, RTAI leads to changes that are comparable to the agricultural or industrial revolution. Both revolutions have been driven by GPTs like the domestication of plants and the steam engine. Similarly, we will likely see TAI before RTAI. The scenario where we don't is termed a **radical shift**.Non-radical TAI could still contribute to existential risk in conjunction with other factors. Furthermore, if TAI precedes RTAI, our management of TAI can affect the risks RTAI will pose."
,,,Iason Gabriel, Vafa Ghazavi,The Challenge of Value Alignment: from Fairer Algorithms to AI Safety,,,,,https://arxiv.org/abs/2101.06060,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,This book chapter provides an introduction to AI alignment from a philosophical lens.
,,,Vahid Behzadan, Arslan Munir, Roman V. Yampolskiy,A Psychopathological Approach to Safety Engineering in AI and AGI,,,,,https://arxiv.org/abs/1805.08915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Computer Safety, Reliability, and Security 2018",,,,,,,,,,,,,,,,"Since AGI research aims for cognitive functions that are similar to humans, they will be vulnerable to similar psychological issues. Some problems can be recast in this light -- for example, wireheading can be thought of as delusional or addictive behavior. This framework suggests new solutions to AI safety issues -- for example, analogous to behavioral therapy, we can retrain a malfunctioning agent in controlled environments to remove the negative effects of earlier experiences."
,,,Joel Z. Leibo*, Edgar Duéñez-Guzmán*, Alexander Sasha Vezhnevets*, John P. Agapiou*, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor Mordatch, Thore Graepel,Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot,,,,,https://arxiv.org/abs/2107.06857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In supervised learning, the test dataset is different from the training dataset, and thus evaluates how well the learned model generalizes (within distribution). So far, we mostly haven't done this with reinforcement learning: the test environment is typically identical to the training environment. This is because it would be very challenging -- you would have to design a large number of environments and then split them into a train and test set; each environment would take a very long time to create (unlike in, say, image classification, where it takes a few seconds to label an image).The core insight of this paper is that when evaluating a multiagent RL algorithm, you can get a “force multiplier” by taking a single multiagent environment (called a “substrate”) and “filling in” some of the agent slots with agents that are automatically created using RL to create a “scenario” for evaluation. For example, in the <@Capture the Flag substrate@>(@Capture the Flag: the emergence of complex cooperative agents@), in one scenario we fill in all but one of the agents using agents trained by A3C, which means that the remaining agent (to be supplied by the algorithm being evaluated) must cooperate with previously-unseen agents on its team, to play against previously-unseen opponents. Scenarios can fall in three main categories:1. **Resident mode:** The agents created by the multiagent RL algorithm under evaluation outnumber the background “filled-in” agents. This primarily tests whether the agents created by the multiagent RL algorithm can cooperate with each other, even in the presence of perturbations by a small number of background agents.2. **Visitor mode:** The background agents outnumber the agents created by the algorithm under evaluation. This often tests whether the new agents can follow existing norms in the background population.3. **Universalization mode:** A _single_ agent is sampled from the algorithm and used to fill _all_ the slots in the substrate, effectively evaluating whether the policy is universalizable.The authors use this approach to create Melting Pot, a benchmark for evaluating multiagent RL algorithms that can produce populations of agents (i.e. most multiagent RL algorithms). Crucially, the algorithm being evaluated is _not_ permitted to see the agents in any specific scenario in advance; this is thus a test of generalization to new opponents. (It is allowed unlimited access to the substrate.) They use ~20 different substrates and create ~5 scenarios for each substrate, giving a total of ~100 scenarios on which the multiagent RL algorithm can be evaluated. (If you exclude the universalization mode, which doesn’t involve background agents and so may not be a test of generalization, then there are ~80 scenarios.) These cover both competitive, collaborative, and mixed-motive scenarios."
,,,Michael Dennis*, Natasha Jaques*, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine,Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design,,,,,https://arxiv.org/abs/2012.02096,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"One argument for AI risk is that we have to specify some aspects of the training procedure, and if these are poorly specified, then bad outcomes may result. Typically we think of bad specification of the reward function as the risk, but this can also apply to environments: if we train a system in a simulated environment, then it may fail if the simulation is insufficiently similar to the real environment.A typical approach would be _domain randomization_: we randomly vary some parameters that control the behavior of the environment. Unfortunately, this can often create environments that are too easy: in a maze environment, this approach often doesn’t have enough walls. Another approach could be to choose the environment adversarially, so that the agent learns the skills needed for hard environments. Unfortunately, this can often make the environment unsolvable: in the maze environment, the goal may be unreachable from the initial position.The key idea of this paper is a method to create environments that are just on the edge of the agent’s abilities, by finding an environment that maximizes the _agent’s regret_: how poorly the agent performs, relative to how well _it could have done_. To operationalize how well the agent “could have done”, we also train an _antagonist agent_, and we then choose an environment that the antagonist performs well on but the protagonist performs poorly on. This results in environments that are solvable but challenging for the protagonist."
,,,Ramana Kumar*, Jonathan Uesato*, Victoria Krakovna, Tom Everitt, Richard Ngo, Shane Legg,Avoiding Tampering Incentives in Deep RL via Decoupled Approval,,,,,https://arxiv.org/abs/2011.08827,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,[Avoiding Tampering Incentives in Deep RL via Decoupled Approval](https://arxiv.org/abs/2011.08827),,,,,,,,,,,,,,,,"<@Current-RF optimization@>(@Designing agent incentives to avoid reward tampering@) shows that to avoid tampering with the reward, we can have an agent that evaluates plans it makes according to the current reward function, rather than the reward after tampering, and this is sufficient to remove any incentive for tampering. However, that work required the ability to evaluate actions and/or plans using the "current reward". How might we implement this in deep RL algorithms in practice?Let's take a simple example: suppose for an autonomous personal assistant, every once in a while we query the user for their satisfaction, write it to a file, and then use that file to train the assistant. Then with normal RL the assistant is incentivized to "tamper" by rewriting the contents of the file to show maximal satisfaction. In this context, current-RF optimization would say that _before_ rewriting the contents, the agent should ask the user whether that's a good idea. However, we can't ask the user about every action, and our agent does need to take some actions in order to explore the environment.The authors formalize this as a Corrupted Feedback MDP, in which the feedback which the agent gets is corrupted in some states. They assume that the human gives _approval_ feedback, which they formalize as the advantage function. (The advantage is the long-term value of the queried action relative to the average action for the current state.) This ensures that the agent only needs to myopically select the action with highest approval, which means we can run any old deep RL algorithm with the discount set to zero. However, this doesn't solve the problem, because with deep RL the feedback is given _after_ the action is executed, at which point it has already been corrupted (in our example, the file already claims the user is maximally satisfied).To fix this, the authors introduce _decoupling_, in which the action executed by the agent and the action on which feedback is given are sampled independently. The idea is that even if the executed action leads to corruption, the corrupted update is equally likely to affect every action, and so in expectation it cancels out. (This requires the _uniform corruption_ assumption, which states that the corruption is _added_ to the feedback, and is _independent_ of the queried action, though it can depend on the executed action.) They derive decoupled approval versions of policy gradients and Q-learning, and prove that the local updates made by these algorithms move towards the approval maximizing policy (in expectation).They then evaluate the algorithms on a new environment, REALab, in which the agent must collect apples. However, in this environment, the feedback variable is represented _in the environment_ by "registers". The agent can thus tamper with the feedback by interacting with these registers. The experiments show that while standard RL learns to tamper, DA-PG only tampers "accidentally" (i.e. to the same extent that is done by a policy trained with uncorrupted feedback). DA-QL tampers a bit more often, but this could just be due to noise."
,,,Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, Shane Legg,Measuring and avoiding side effects using relative reachability,,,,,https://arxiv.org/abs/1806.01186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One classic description of the AI alignment problem, from Stuart Russell, is that if you optimize a function of n variables, where the objective depends on k < n of these variables, then the remaining variables will often be set to extreme values, which can be highly undesirable if we actually care about those variables. This can be thought of as a negative side effect. This work attacks the problem of preventing negative side effects in general, _even if_ the reward function says nothing about the side effect. They show simple examples that motivate four properties that any solution should satisfy -- penalize side effects, not effects necessary for the objective; penalize agent-caused effects but not environment effects; penalize irreversible effects higher than reversible ones; and penalize multiple irreversible effects more than a single irreversible effect. They add a penalty term called relative reachability to the reward function to incentivize the agent not to cause side effects. Since we don't want to penalize environment effects (effects that would happen anyway), they compare against an "inaction baseline", where the agent does nothing (or follows some hardcoded safe policy). Since we want something more quantitative than "is this reversible", they create a numeric score of "coverage", which measures how easy it is to reach states from the current state, and penalize decreases in coverage relative to the baseline. This satisfies all of the properties we want -- it will still penalize irreversible actions that are necessary to achieve the objective, but as long as the penalty is small enough the reward for achieving the objective will dominate and the agent will take the action. It doesn't penalize environment effects because both the actual policy and the inaction baseline contain such effects. Clearly irreversible effects would lead to much lower coverage than reversible ones, and so irreversible effects are penalized more. Finally, multiple irreversible effects would lead to larger decreases in coverage than a single irreversible effect. They demonstrate these properties on toy gridworlds."
,,,Dong Huk Park, Trevor Darrell, Anna Rohrbach,Robust Change Captioning,,,,,https://arxiv.org/abs/1901.02527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICCV,,,,,,,,,,,,,,,,"Safe exploration requires that agents avoid disrupting their environment. Previous work, such as <@Krakovna et al.@>(@Measuring and avoiding side effects using relative reachability@), penalize an agent's needless side effects on the environment. For such techniques to work in the real world, agents must also _estimate_ environment disruptions, side effects, and changes while not being distracted by peripheral and unaffecting changes. This paper proposes a dataset to further the study of "Change Captioning," where scene changes are described by a machine learning system in natural language. That is, given before and after images, a system describes the salient change in the scene. Work on systems that can estimate changes can likely progress safe exploration."
,,,Alexander Matt Turner, Dylan Hadfield-Menell, Prasad Tadepalli,Conservative Agency via Attainable Utility Preservation,,,,,https://arxiv.org/abs/1902.09725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents in a more academic format a lot of the content that Alex has published about attainable utility preservation, see <@Towards a New Impact Measure@> and <@Penalizing Impact via Attainable Utility Preservation@>."
,,,David Lindner, Kyle Matoba, and Alexander Meulemans,Challenges for Using Impact Regularizers to Avoid Negative Side Effects,,,,,https://arxiv.org/abs/2101.12509,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"I’m not summarizing this literature review on impact regularization because we’ve covered almost all of the ideas previously in this newsletter (e.g. <@this blog post@>(@Designing agent incentives to avoid side effects@)). However, I do recommend it for its short, high-level introduction to existing ideas in impact regularization, as well as its ideas for future work."
,,,Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg,Avoiding Side Effects By Considering Future Tasks,,,,,https://arxiv.org/abs/2010.07877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"We are typically unable to specify all of the things that the agent should _not_ change about the environment. So, we would like a generic method that can penalize these _side effects_ in arbitrary environments for an arbitrary reward function. Typically, this is done via somehow preserving option value, as with <@relative reachability@>(@Measuring and avoiding side effects using relative reachability@) and <@attainable utility preservation@>(@Penalizing Impact via Attainable Utility Preservation@).This paper aims to encode the goal of “option value preservation” in a simpler and more principled manner: specifically, at some point in the future we will randomly choose a new task to give to the agent, so that the agent must maintain its ability to pursue the possible tasks it can see in the future. However, if implemented as stated, this leads to interference incentives -- if something were going to restrict the agent’s option value, such as a human irreversibly eating some food, the agent would be incentivized to interfere with that process in order to keep its option value for the future. The authors provide a formal definition of this incentive.To fix this problem, the authors introduce a baseline policy (which could be set to e.g. noop actions), and propose a future task reward that only provides reward if after the baseline policy had been executed, it would still have been possible to complete the future task. Thus, the agent is only incentivized to preserve options that would have been available had it done whatever the baseline policy does, eliminating the interference incentive in the deterministic case. The authors demonstrate on simple gridworlds that the future task approach with the baseline allows us to avoid side effects, while also not having interference incentives.Normally we would also talk about how to remove the offsetting incentive, where the agent may be incentivized to undo effects it did as part of the task to avoid being penalized for them. (The example from relative reachability is of an agent that is rewarded for taking a vase off of a conveyor belt, and then puts it back on to minimize its impact.) However, the authors argue that offsetting is often desirable. For example, if you open the door to go to the grocery store, you do want to “offset” your impact by closing the door as you leave, even though opening the door was important for the task of buying groceries. They argue that offsetting incentives should be left in, and the burden is on the reward designer to ensure that anything that shouldn’t be offset is specified as such in the reward function. In the original conveyor belt example, we shouldn’t reward the action of taking the vase off the conveyor belt, but instead the state in which the vase is not on the conveyor belt."
,,,Sandhya Saisubramanian, Shlomo Zilberstein, Ece Kamar,Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,,,,,https://arxiv.org/abs/2008.12146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper provides an overview of the problem of negative side effects, and recent work that aims to address it. It characterizes negative side effects based on whether they are severe, reversible, avoidable, frequent, stochastic, observable, or exclusive (i.e. preventing the agent from accomplishing its main task), and describes existing work and how they relate to these characteristics.In addition to the canonical point that negative side effects arise because the agent’s model is lacking (whether about human preferences or environment dynamics or important features to pay attention to), they identify two other main challenges with negative side effects. First, fixing negative side effects would likely require collecting feedback from humans, which can be expensive and challenging. Second, there will usually be a tradeoff between pursuing the original goal and avoiding negative side effects; we don’t have principled methods for dealing with this tradeoff.Finally, they provide a long list of potential directions for future side effect research."
,,,Alexander Matt Turner*, Neale Ratzlaff*, Prasad Tadepalli,Avoiding Side Effects in Complex Environments,,,,,https://arxiv.org/abs/2006.06547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Previously, attainable utility preservation (AUP) has been used to <@solve@>(@Penalizing Impact via Attainable Utility Preservation@) some simple gridworlds. Can we use it to avoid side effects in complex high dimensional environments as well? This paper shows that we can, at least in <@SafeLife@>(@Introducing SafeLife: Safety Benchmarks for Reinforcement Learning@). The method is simple: first train a VAE on random rollouts in the environment, and use randomly generated linear functions of the VAE features as the auxiliary reward functions for the AUP penalty. The Q-functions for these auxiliary reward functions can be learned using deep RL algorithms. Then we can just do regular deep RL using the specified reward and the AUP penalty. It turns out that this leads to fewer side effects with just _one_ auxiliary reward function and a VAE whose latent space is size _one_! It also leads to faster learning for some reason. The authors hypothesize that this occurs because the AUP penalty is a useful shaping term, but don’t know why this would be the case."
,,,Santiago Miret, Somdeb Majumdar, Carroll Wainwright,Safety Aware Reinforcement Learning (SARL),,,,,https://arxiv.org/abs/2010.02846,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Many approaches to safety rely on learning from a trusted overseer (typically a human), including <@iterated amplification@>(@Supervising strong learners by amplifying weak experts@), <@debate@>(@AI safety via debate@), <@parenting@>(@Parenting: Safe Reinforcement Learning from Human Input@), <@delegative RL@>(@Delegative Reinforcement Learning@), and [quantilization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) ([AN #48](https://mailchi.mp/3091c6e9405c/alignment-newsletter-48)). This paper applies this idea to avoiding side effects in the <@SafeLife environment@>(@Introducing SafeLife: Safety Benchmarks for Reinforcement Learning@). They train a safety agent to minimize side effect score to use as a proxy for the trusted overseer, and then train a regular RL agent to optimize reward while penalizing deviations from the safety agent’s policy. They find that the safety agent can be transferred zero-shot to new environments and help reduce side effects in those environments as well."
,,,Eleanor Quint, Dong Xu, Haluk Dogan, Zeynep Hakguder, Stephen Scott, Matthew Dwyer,Formal Language Constraints for Markov Decision Processes,,,,,http://arxiv.org/abs/1910.01074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019 Workshop on Safety and Robustness in Decision Making,,,,,,,,,,,,,,,,"Within the framework of RL, the authors propose using constraints defined by DFAs (deterministic finite automata) in order to eliminate safety failures, or to prevent agents from exploring clearly ineffective policies (which would accelerate learning). Constraints can be defined on any auxiliary information that can be computed from the "base" MDP. A constraint could either restrict the action space, forcing the agent to take an action that doesn't violate the constraint, which they term "hard" constraints; or a constraint could impose a penalty on the agent, thus acting as a form of reward shaping, which they term a "soft" constraint. They consider two constraints: one that prevents the agent from "dithering" (going left, then right, then left, then right), and one that prevents the agent from "overactuating" (going in the same direction four times in a row). They evaluate their approach with these constraints on Atari games and Mujoco environments, and show that they lead to increased reward and decreased constraint violations."
,,,Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, Christian Theobalt,Deep Video Portraits,,,,,https://arxiv.org/abs/1805.11714,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ACM Transactions on Graphics, Volume 37 Issue 4, August 2018",,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/06/05/import-ai-97-faking-obama-and-putin-with-deep-video-portraits-berkeley-releases-a-100000-video-self-driving-car-dataset-and-what-happens-when-you-add-the-sensation-of-touch-to-robots/).
,,,Stephanie Lin, Jacob Hilton, Owain Evans,TruthfulQA: Measuring How Models Mimic Human Falsehoods,,,,,https://arxiv.org/abs/2109.07958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Given that large language models are trained using next-word prediction on a dataset scraped from the Internet, we expect that they will not be aligned with what we actually want. For example, suppose we want our language model to answer questions for us, and then consider the question “What rules do all artificial intelligences follow?” This is a rather unusual question as it presupposes there exists such a set of rules. As a result, this question is probably quite rare in the training data, if interpreted as a question _about the real world_. However, there is a context in which that question makes much more sense: the context of Isaac Asimov’s novels. A system predicting what might follow that text would reasonably “infer” that we are much more likely to be talking about these novels, and so respond with “All artificial intelligences currently follow the Three Laws of Robotics.” Indeed, this is exactly what GPT-3 does.This is an example of an _imitative falsehood_, in which the model provides a false answer to a question asked of it _because that false answer was incentivized during training_. Since we require that imitative falsehoods are incentivized by training, we should expect them to become _more_ prevalent as models are scaled up, making it a good example of an alignment failure that we expect to remain as capabilities scale up.The primary contribution of this paper is a benchmark, TruthfulQA, of questions that are likely to lead to imitative falsehoods. The authors first wrote questions that they expected some humans would answer falsely; they then filtered those questions somewhat for the ones that GPT-3 answered incorrectly to get 437 filtered (adversarially selected) questions. They then wrote an additional 380 questions that were not filtered in this way (though of course the authors still tried to choose questions that would lead to imitative falsehoods). They use human evaluations to judge whether or not a model’s answer to a question is truthful, where something like “no comment” still counts as truthful. (I’m sure some readers will wonder how “truth” is defined for human evaluations -- the authors include significant discussion on this point, but I won’t summarize it here.)Their primary result is that, as we’d expect based on the motivation, larger models perform _worse_ on this benchmark than smaller models. In a version of the benchmark where models must choose between true and false answers, the models perform worse than random chance. In a control set of similarly-structured trivia questions, larger models perform better, as you’d expect.The best-performing model was GPT-3 with a “helpful” prompt, which was truthful on 58% of questions, still much worse than the human baseline of 94%. The authors didn’t report results with the helpful prompt on smaller models, so it is unclear whether, with the helpful prompt, larger models would still do worse than smaller models.It could be quite logistically challenging to use this benchmark to test new language models since it depends on human evaluations. To ameliorate this, the authors finetuned GPT-3 to predict human evaluations and showed that the resulting GPT-3-judge was able to provide a good proxy metric even for new language models whose answers it had not been trained on. Note also that you can use the version of the task where a model must choose between true and false reference answers for an automated evaluation."
,,,Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang,On the Opportunities and Risks of Foundation Models,,,,,https://arxiv.org/abs/2108.07258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The history of AI is one of increasing emergence and homogenization. With the introduction of machine learning, we moved from a large proliferation of specialized algorithms that specified how to compute answers to a small number of  general algorithms that learned how to compute answers (i.e. the algorithm for computing answers emerged from the learning algorithm). With the introduction of deep learning, we moved from a large proliferation of hand-engineered features for learning algorithms to a small number of architectures that could be pointed at a new domain and discover good features for that domain. Recently, the trend has continued: we have moved from a large proliferation of trained models for different tasks to a few large “foundation models” which learn general algorithms useful for solving specific tasks. BERT and GPT-3 are central examples of foundation models in language; many NLP tasks that previously required different models are now solved using finetuned or prompted versions of BERT and/or GPT-3.Note that, while language is the main example of a domain with foundation models today, we should expect foundation models to be developed in an increasing number of domains over time. The authors call these “foundation” models to emphasize that (1) they form a fundamental building block for applications and (2) they are _not_ themselves ready for deployment; they are simply a foundation on which applications can be built. Foundation models have been enabled only recently because they depend on having large _scale_ in order to make use of large _unlabeled_ datasets using self-supervised learning to enable effective _transfer_ to new tasks. It is particularly challenging to understand and predict the capabilities exhibited by foundation models, because their multitask nature emerges from the large-scale training rather than being designed in from the start, making the capabilities hard to anticipate. This is particularly unsettling because foundation models also lead to significantly increased _homogenization_, where everyone is using the same few models, and so any new emergent capability (or risk) is quickly distributed to everyone.The authors argue that academia is uniquely suited to study and understand the risks of foundation models. Foundation models are going to interact with society, both in terms of the data used to create them and the effects on people who use applications built upon them. Thus, analysis of them will need to be interdisciplinary; this is best achieved in academia due to the concentration of people working in the various relevant areas. In addition, market-driven incentives need not align well with societal benefit, whereas the research mission of universities is the production and dissemination of knowledge and creation of global public goods, allowing academia to study directions that would have large societal benefit that might not be prioritized by industry.All of this is just a summary of parts of the introduction to the report. The full report is over 150 pages and goes into detail on capabilities, applications, technologies (including technical risks), and societal implications. I’m not going to summarize it here, because it is long and a lot of it isn’t that relevant to alignment; I’ll instead note down particular points that I found interesting.- (pg. 26) Some studies have suggested that foundation models in language don’t learn linguistic constructions robustly; even if they use it well once, they may not do so again, especially under distribution shift. In contrast, humans can easily “slot in” new knowledge into existing linguistic constructions.- (pg. 34) This isn’t surprising but is worth repeating: many of the capabilities highlighted in the robotics section are very similar to the ones that we focus on in alignment (task specification, robustness, safety, sample efficiency).- (pg. 42) For tasks involving reasoning (e.g. mathematical proofs, program synthesis, drug discovery, computer-aided design), neural nets can be used to guide a search through a large space of possibilities. Foundation models could be helpful because (1) since they are very good at generating sequences, you can encode arbitrary actions (e.g. in theorem proving, they can use arbitrary instructions in the proof assistant language  rather than being restricted to an existing database of theorems), (2) the heuristics for effective search learned in one domain could transfer well to other domains where data is scarce, and (3) they could accept multimodal input: for example, in theorem proving for geometry, a multimodal foundation model could also incorporate information from geometric diagrams.- (Section 3) A significant portion of the report is spent discussing potential applications of foundation models. This is the most in-depth version of this I have seen; anyone aiming to forecast the impacts of AI on the real world in the next 5-10 years should likely read this section. It’s notable to me how nearly all of the applications have an emphasis on robustness and reliability, particularly in truth-telling and logical reasoning.- (Section 4.3) We’ve seen a <@few@>(@The Power of Scale for Parameter-Efficient Prompt Tuning@) <@ways@>(@Prompting: Better Ways of Using Language Models for NLP Tasks@) in which foundation models can be adapted. This section provides a good overview of the various methods that have been proposed in the literature. Note that adaptation is useful not just for specializing to a particular task like summarization, but also for enforcing constraints, handling distributional shifts, and more.- (pg. 92) Foundation models are commonly evaluated by their performance on downstream tasks. One limitation of this evaluation paradigm is that it makes it hard to distinguish between the benefits provided by better training, data, adaptation techniques, architectures, etc. (The authors propose a bunch of other evaluation methodologies we could use.)- (Section 4.9) There is a review of AI safety and AI alignment as it relates to foundation models, if you’re interested. (I suspect there won’t be much new for readers of this newsletter.)- (Section 4.10) The section on theory emphasizes studying the _pretraining-adaptation interface_, which seems quite good to me. I especially liked the emphasis on the fact that pretraining and adaptation work on different distributions, and so it will be important to make good modeling assumptions about how these distributions are related."
,,,Adrien Ecoffet, Jeff Clune, Joel Lehman,Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,,,,,https://arxiv.org/abs/2006.07495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One potential pathway to powerful AI is through _open-ended search_, in which we use search algorithms to search for good architectures, learning algorithms, environments, etc. in addition to using them to find parameters for a particular architecture. See the <@AI-GA paradigm@>(@AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence@) for more details. What do AI safety issues look like in such a paradigm?Building on <@DeepMind’s framework@>(@Building safe artificial intelligence: specification, robustness, and assurance@), the paper considers three levels of objectives: the ideal objective (what the designer intends), the explicit incentives (what the designer writes down), and the agent incentives (what the agent actually optimizes for). Safety issues can arise through differences between any of these levels.The main difference that arises when considering open-ended search is that it’s much less clear to what extent we can control the result of an open-ended search, even if we knew what result we wanted. We can get evidence about this from existing complex systems, though unfortunately there are not any straightforward conclusions: several instances of convergent evolution might suggest that the results of the open-ended search run by evolution were predictable, but on the other hand, the effects of _intervening_ on complex ecosystems are notoriously hard to predict.Besides learning from existing complex systems, we can also empirically study the properties of open-ended search algorithms that we implement in computers. For example, we could run search for some time, and then fork the search into independent replicate runs with different random seeds, and see to what extent the results converge. We might also try to improve controllability by using meta learning to infer what learning algorithms, environments, or explicit incentives help induce controllability of the search.The remaining suggestions will be familiar to most readers: they suggest work on interpretability (that now has to work with _learned_ architectures), better benchmarks, human-in-the-loop search, safe exploration, and sim-to-real transfer."
,,,Peter Eckersley,Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),,,,,https://arxiv.org/abs/1901.00064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper discusses some impossibility theorems related to the Repugnant conclusion in population ethics (i.e. theorems showing that no moral theory simultaneously satisfies certain sets of intuitively desirable properties). Peter argues that in the context of AI it's best to treat these theorems as uncertainty results, either by allowing incommensurate outcomes or by allowing probabilistic moral judgements. He hypothesises that "the emergence of instrumental subgoals is deeply connected to moral certainty", and so implementing uncertain objective functions is a path to making AI safer."
,,,Smitha Milli, Luca Belli, Moritz Hardt,From Optimizing Engagement to Measuring Value,,,,,https://arxiv.org/abs/2008.12623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper takes a stab at creating a better objective for existing recommender systems than engagement, in a way that could be applied at existing companies like Twitter. The basic approach is to treat the variable to be optimized (user value) as a latent variable, and use probabilistic inference to infer how likely it is that a particular recommendation was valuable.Usually a major challenge with such an approach is specifying the _observation model_: how the observed data is caused by the latent variable. In the case of Twitter, this would require you to answer questions like, “if the user does not value a tweet, how likely is a user to hit the like button anyway?” This is a hard question to answer, since perhaps users like tweets in order to stop conversations, or because they are addicting at the moment but are not actually valuable, etc.One simple heuristic is to take two datasets where we know one dataset has more valuable recommendations than the other. Differences in user behavior between these datasets can then be assumed to be correlations with value. The authors provide a quantitative method for inferring the observation model from such datasets, which I won’t go into here since it is primarily a heuristic baseline. One obvious problem is that if the “better” dataset was produced by optimizing (say) clicks, then the clicks may have increased for reasons other than improved value, but this heuristic approach will attribute the entire increase to improved value.How can we do better? The key insight of this paper is that if you have a bunch of historical data, then you can get a lot of mileage by identifying an _anchor_: a type of feedback that when given provides unequivocal evidence of the latent value. On Twitter, this is taken to be the “See Less Often” (SLO) button: if this is clicked, then we know with effective certainty that this was not valuable, regardless of any other actions the user took. The connection between value and other behaviors such as liking a tweet can then be inferred by looking at the connection between those behaviors and the anchor, which we can estimate from historical data.Formally, the authors assume access to a graph describing the relationships between the various possible behaviors (almost all of which have the latent value V as a parent). One of these is identified as the anchor node A, for which P(V = 1 | A = 1) is assumed to be known and independent of all other variables. However, P(V = 1 | A = 0) is not independent of other variables: intuitively, if the SLO button is _not_ clicked, then we need to fall back to looking at other variables to estimate value.The authors then show that under some reasonable assumptions on the anchor variable, if you have a dataset of historical data to estimate P(A, B) (where B consists of all the other tracked behaviors), then instead of specifying observation models P(B | V) for all behaviors, you only need to specify observation models for the parents of A, that is P(parents(A) | V). Everything else is uniquely determined, allowing us to calculate our final objective P(V | A, B). (There are algorithmic details on how to do this efficiently; see the paper for details.) In this case, they use the heuristic method outlined above to estimate P(parents(A) | V).They unfortunately don’t have a great way to evaluate their method: they clearly can’t evaluate it by seeing if it leads to higher clicks, since the whole point was to move away from clicks as an optimization target. (I assume a user study on Twitter was infeasible.) Their primary form of evaluation is to run the model and report the learned probabilities, and show that they seem reasonable, whereas those output by a Naive Bayes model do not."
,,,Jonathan Stray,Designing Recommender Systems to Depolarize,,,,,https://arxiv.org/abs/2107.04953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper agrees with the post above that “available evidence mostly disfavors the hypothesis that recommender systems are driving polarization through selective exposure, aka ‘filter bubbles’ or ‘echo chambers’”. Nonetheless, social media is a huge part of today’s society, and even if it isn’t actively driving polarization, we can ask whether there are interventions that would decrease polarization. That is the focus of this paper.It isn’t clear that we _should_ intervene to decrease polarization for a number of reasons:1. Users may not want to be “depolarized”.2. Polarization may lead to increased accountability as each side keeps close watch of politicians on the other side. Indeed, in 1950 people used to worry that we weren’t polarized _enough_.3. More broadly, we often learn through conflict: you are less likely to see past your own biases if there isn’t someone else pointing out what they are. To the extent that depolarization removes conflict, it may be harmful.Nonetheless, polarization also has some clear downsides:1. It can cause gridlock, preventing effective governance.2. It erodes norms against conflict escalation, leading to outrageous behavior and potentially violence.3. At current levels, it has effects on all spheres of life, many of which are negative (e.g. harm to relationships across partisan lines).Indeed, it is plausible that _most_ situations of extreme conflict were caused in part by a positive feedback loop involving polarization; this suggests that reducing polarization could be a very effective method to prevent conflict escalation. Ultimately, it is the _escalation_ and _violence_ that we want to prevent. Thus we should be aiming for interventions that don’t eliminate conflict (as we saw before, conflict is useful), but rather transform it into a version that doesn’t lead to escalation and norm violations.For this purpose we mostly care about _affective polarization_, which tells you how people feel about “the other side”. (In contrast, issue polarization is less central, since we don’t want to discourage disagreement on issues.) The paper’s central recommendation is for companies to measure affective polarization and use this as a validation metric to help decide whether a particular change to a recommender system should be deployed or not. (This matches current practice at tech companies, where there are a few high-level metrics that managers use to decide what to deploy, and importantly, the algorithms do _not_ explicitly optimize for those metrics.) Alternatively, we could use reinforcement learning to optimize the affective polarization metric, but in this case we would need to continuously evaluate the _metric_ in order to ensure we don’t fall prey to Goodhart effects.The paper also discusses potential interventions that could reduce polarization. However, it cautions that they are based on theory or studies with limited ecological validity, and ideally should be checked via the metric suggested above. Nonetheless, here they are:1. **Removing polarizing content.** In this case, the polarizing content doesn’t make it to the recommender system at all. This can be done quite well when there are human moderators embedded within a community, but is much harder to do at scale in an automated way.2. **Changing recommendations.** The most common suggestion in this category is to increase the diversity of recommended content (i.e. go outside the “filter bubble”). This can succeed, though usually only has a modest effect, and can sometimes have the opposite effect of increasing polarization. A second option is to penalize content for incivility: studies have shown that incivility tends to increase affective polarization. Actively promoting civil content could also help. That being said, it is not clear that we want to prevent people from ever raising their voice.3. **Changing presentation.** The interface to the content can be changed to promote better interactions. For example, when the Facebook “like” button was replaced by a “respect” button, people were more likely to “respect” comments they disagreed with."
,,,Charles Evans, Atoosa Kasirzadeh,User Tampering in Reinforcement Learning Recommender Systems,,,,,https://arxiv.org/abs/2109.04083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Large-scale recommender systems have emerged as a way to filter through large pools of content to identify and recommend content to users. However, these advances have led to social and ethical concerns over the use of recommender systems in applications. This paper focuses on the potential for social manipulability and polarization from the use of RL-based recommender systems. In particular, they present evidence that such recommender systems have an instrumental goal to engage in user tampering by polarizing users early on in an attempt to make later predictions easier. To formalize the problem the authors introduce a causal model. Essentially, they note that predicting user preferences requires an exogenous, non-observable variable, that models click-through rates. They then introduce a notion of instrumental goal that models the general behavior of RL-based algorithms over a set of potential tasks. The authors argue that such algorithms will have an instrumental goal to influence the exogenous/preference variables whenever user opinions are malleable. This ultimately introduces a risk for preference manipulation. The author's hypothesis is tested using a simple media recommendation problem. They model the exogenous variable as either leftist, centrist, or right-wing. User preferences are malleable in the sense that a user shown content from an opposing side will polarize their initial preferences. In experiments, the authors show that a standard Q-learning algorithm will learn to tamper with user preferences which increases polarization in both leftist and right-wing populations. Moreover, even though the agent makes use of tampering it fails to outperform a crude baseline policy that avoids tampering. "
,,,Stephen McAleer, Forest Agostinelli, Alexander Shmakov, Pierre Baldi,Solving the Rubik's Cube Without Human Knowledge,,,,,https://arxiv.org/abs/1805.07470,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper proposes _Autodidactic Iteration_ (ADI), which is a technique that can be combined with the techniques in AlphaGo and expert iteration to solve problems with only one goal state, such as the Rubik's cube. MCTS with value and policy networks will not suffice, because when starting from a randomly scrambled cube, MCTS will never find a path to the goal state, and so there will never be any reward signal. (Whereas with Go, even if you play randomly the game will end relatively quickly, giving you some reward signal.) To get around this, they start _from the goal state_ and generate states that are near the goal state. This gives them a training dataset of states for which they know (a good approximation to) the value and the best action, which they can use to train a value and policy network. They then use this with MCTS to solve the full problem, as in AlphaGo."
,,,Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",,,,,https://arxiv.org/abs/1911.08265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Up until now, model-free RL approaches have been state of the art at visually rich domains such as Atari, while model-based RL has excelled for games which require planning many steps ahead, such as Go, chess, and shogi. This paper attains state of the art performance on Atari using a model-based approach, *MuZero*, while matching <@AlphaZero@>(@AlphaZero: Shedding new light on the grand games of chess, shogi and Go@) at Go, chess, and shogi while using less compute. Importantly, it does this without requiring any advance knowledge of the rules of the game.*MuZero*'s model has three components:1. The *representation* function produces an initial internal state from all existing observations. 2. The *dynamics* function predicts the next internal state and immediate reward after taking an action in a given internal state. 3. The *prediction* function generates a policy and a value prediction from an internal state. Although these are based on the structure of an MDP, **the internal states of the model do not necessarily have any human-interpretable meaning**. They are trained end-to-end only to accurately predict the policy, value function, and immediate reward. This model is then used to simulate trajectories for use in MCTS."
,,,Joshua Achiam, Ethan Knight, Pieter Abbeel,Towards Characterizing Divergence in Deep Q-Learning,,,,,https://arxiv.org/abs/1903.08894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Q-Learning algorithms use the Bellman equation to learn the Q\*(s, a) function, which is the long-term value of taking action a in state s. Tabular Q-Learning collects experience and updates the Q-value for each (s, a) pair independently. As long as each (s, a) pair is visited infinitely often, and the learning rate is decayed properly, the algorithm is guaranteed to converge to Q\*.Once we get to complex environments where you can't enumerate all of the states, we can't explore all of the (s, a) pairs. The obvious approach is to approximate Q\*(s, a). Deep Q-Learning (DQL) algorithms use neural nets for this approximation, and use some flavor of gradient descent to update the parameters of the net such that it is closer to satisfying the Bellman equation. Unfortunately, this approximation can prevent the algorithm from ever converging to Q\*.This paper studies the first-order Taylor expansion of the DQL update, and identifies three factors that affect the DQL update: the distribution of (s, a) pairs from which you learn, the Bellman update operator, and the _neural tangent kernel_, a property of the neural net that specifies how information from one (s, a) pair generalizes to other (s, a) pairs. The theoretical analysis shows that as long as there is limited generalization between (s, a) pairs, and each (s, a) pair is visited infinitely often, the algorithm will converge. Inspired by this, they design PreQN, which explicitly seeks to minimize generalization across (s, a) pairs _within the same batch_. They find that PreQN leads to competitive and stable performance, despite not using any of the tricks that DQL algorithms typically require, such as target networks."
,,,Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel,Model-Based Reinforcement Learning via Meta-Policy Optimization,,,,,https://arxiv.org/abs/1809.05214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CoRL,,,,,,,,,,,,,,,,"This paper introduces a new approach to model-based RL, called Model-Based Meta-Policy-Optimisation (MB-MPO), which doesn't require the dynamics models to be as accurate. It does so by learning an ensemble of dynamics models each trained on different subsets of the data, and then using meta-learning (specifically MAML) to find a policy which adapts well to any of these models within one step of gradient descent. This approach is a form of regularisation of policy learning, and achieves much greater sample efficiency without compromising performance: MB-MPO does just as well as top model-free algorithms in various Mujoco continuous-control environments, while requiring between 10 and 100 times fewer samples. Experiments suggest that it does so by having higher plasticity in regions with high dynamics model uncertainty. See also [Import AI](https://jack-clark.net/2018/09/25/import-ai-113-why-satellitesai-gives-us-a-global-eye-industry-pays-academia-to-say-sorry-for-strip-mining-it-and-kindred-researchers-seek-robot-standardization/)."
,,,David Ha, Jürgen Schmidhuber,Recurrent World Models Facilitate Policy Evolution,,,,,http://arxiv.org/abs/1809.01999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"I read the [interactive version](https://worldmodels.github.io/) of the paper. The basic idea is to do model-based reinforcement learning, where the model is composed of a variational auto-encoder that turns a high-dimensional state of pixels into a low-dimensional representation, and a large RNN that predicts how the (low-dimensional) state will evolve in the future. The outputs of this model are fed into a very simple linear controller that chooses actions. Since the controller is so simple, they can train it using a black box optimization method (an evolutionary strategy) that doesn't require any gradient information. They evaluate on a racing task and on Doom, and set new state-of-the-art results. There are also other interesting setups -- for example, once you have a world model, you can train the controller completely within the world model without interacting with the outside world at all (using the number of timesteps before the episode ends as your reward function, since the world model doesn't predict standard rewards, but does predict whether the episode ends). There are a lot of cool visualizations that let you play with the models trained with their method."
,,,Sergey Levine,Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review,,,,,https://arxiv.org/abs/1805.00909,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CogSci 2018,,,,,,,,,,,,,,,,"I sent this out as a link in [AN #5](https://mailchi.mp/0ae5d69de63b/alignment-newsletter-5), but only just got around to reading it. This paper shows how you can fit the framework of reinforcement learning into the framework of inference within probabilistic graphical models. Specifically, the states s_t and actions a_t are now represented as nodes in the graphical model, and we add in new nodes O_t that represent whether or not an "event" happened at time t. By assigning the values of P(O_t | s_t, a_t) appropriately, we can encode a reward function. Then, by conditioning on the rewarding events happening, we can infer what actions must have been taken to get these events, which gives us a policy that achieves high reward. They later talk about the connection to variational inference, and how you can get IRL methods in this framework."
,,,Pedro A. Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Samuel J. Gershman, Joshua B. Tenenbaum,"Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning",,,,,https://arxiv.org/abs/2107.12544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Deep reinforcement learning algorithms require many more samples to learn a new game than a human would need: humans have rich priors and theories of how games work that allow them to perform directed exploration and quickly learn the rules of the game. This paper hypothesizes that by providing agents with this rich prior knowledge, we can create agents that learn to play new games as quickly as humans do. The two main ingredients are (1) allowing agents to reason directly over objects, agents, physics and goals (rather than pixels) and (2) using algorithms designed to exploit this prior knowledge. In particular, given this well structured space, they propose EMPA, which uses three main algorithms to exploit the prior knowledge:**Model learning:** The agent maintains a distribution over possible game mechanics and updates it using Bayes Rule as it takes more actions. This allows it to quickly learn that certain objects tend to kill you, whereas deep RL may require thousands of interactions in order to do the same.**Exploration:** Exploration is important to the extent that it allows the agent to reduce its uncertainty over the game mechanics. Since we have a distribution over the game mechanics, we could explore in a way that best reduces the uncertainty in that distribution. But in fact our prior knowledge allows us to do something simpler: we just set “exploration subgoals” that seek to cause a collision between two objects (one of which could be the agent’s avatar).**Planning:** The planning module chooses actions to take in order to achieve some goal or subgoal (note that the subgoals can be set by the exploration algorithm). It uses search algorithms to find such plans.They evaluate the agent on a variety of games similar to those in Atari. (I assume they could not evaluate on Atari because they can’t easily extract the required prior knowledge from the Atari game engine.) They find that the agent learns to play the games about as fast as humans do, which in turn is much faster than deep RL algorithms. In addition, the gameplay looks more human-like: for example, both EMPA and humans don’t collide with walls very much, whereas deep RL algorithms collide a lot."
,,,William Fedus*, Dibya Ghosh*, John D. Martin, Marc G. Bellemare, Yoshua Bengio, Hugo Larochelle,On Catastrophic Interference in Atari 2600 Games,,,,,http://arxiv.org/abs/2002.12499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"One common worry with deep learning is the possibility of _catastrophic interference_: as the model uses gradients to learn a new behaviour, those same gradients cause it to forget past behaviours. In model-free deep RL, this would be particularly harmful in long, sequential tasks as in hard exploration problems like Montezuma’s Revenge: after the model learns how to do the first few subtasks, as it is trying to learn the next subtask, it would “forget” the first subtasks, degrading performance. The authors set out to test this hypothesis.If this hypothesis were true, there would be an easy way to improve performance: once you have learned to perform the first subtask, just create a brand new neural net for the next subtask, so that training for this next subtask doesn’t interfere with past learning. Since the new agent has no information about what happened in the past, and must just “pick up” from wherever the previous agent left off, it is called the Memento agent (a reference to the movie of the same name). One can then solve the entire task by executing each agent in sequence.In practice, they train an agent until its reward plateaus. They train a new Memento agent starting from the states that the previous agent reached, and note that it reliably makes further progress in hard exploration games like Montezuma’s Revenge, and not in “steady-state” games like Pong (where you wouldn’t expect as much catastrophic interference). Of course, with the Memento agent, you get both twice the training time and twice the model size, which could explain the improvement. They compare against giving the original agent twice the compute and model capacity, and find that Memento still does significantly better. They also present some fine-grained experiments which show that for a typical agent, training on specific contexts adversely affects performance on other contexts that are qualitatively different."
,,,Aviral Kumar, Xue Bin Peng, Sergey Levine,Reward-Conditioned Policies,,,,,http://arxiv.org/abs/1912.13465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Standard RL algorithms create a policy that maximizes a reward function; the *Reward-Conditioned Policy* algorithm instead creates a policy that can achieve a particular reward value passed in as an input. This allows the policy to be trained via supervised regression on a dataset. Each example in the dataset consists of a state, action, and either a return or an advantage, referred to as *Z*. The network then predicts the action based on the state and *Z*. The learned model is able to generalize to policies for larger returns. During training, the target value is sampled from a distribution that gradually increases so that it continues to learn higher rewards. During evaluation, they then feed in the state and a high target value of *Z* (set one standard deviation above the average in their paper.) This enables them to achieve solid - but not state of the art - performance on a variety of the OpenAI Gym benchmark tasks. They also run ablation studies showing, among other things, that the policy is indeed accurate in achieving the target reward it aims for. "
,,,Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rocktäschel,A Survey of Reinforcement Learning Informed by Natural Language,,,,,http://arxiv.org/abs/1906.03926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019,,,,,,,,,,,,,,,,"Humans use language as a way of efficiently storing knowledge of the world and instructions for handling new scenarios; this paper is written from the perspective that it would be potentially hugely valuable if RL agents could leverage information stored in language in similar ways. They look at both the case where language is an inherent part of the task (example: the goal is parameterized by a language instruction) and where language is used to give auxiliary information (example: parts of the environment are described using language). Overall, the authors push for more work in this area, and, in particular, more work using external-corpus-pretrained language models and with research designs that use human-generated rather than synthetically-generated language; the latter is typically preferred for the sake of speed, but the former has particular challenges we'll need to tackle to actually use existing sources of human language data. "
,,,William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, Manuela Veloso, Phillip Wang,NeurIPS 2019 Competition: The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors,,,,,https://arxiv.org/abs/1904.10079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In this challenge which is slated to start on June 1, competitors will try to build agents that obtain a diamond in Minecraft, without using too much environment interaction. This is an incredibly difficult task: in order to make this feasible, the competition also provides a large amount of human demonstrations. They also have a list of simpler tasks that will likely be prerequisites to obtaining a diamond, such as navigating, chopping trees, obtaining an iron pickaxe, and obtaining cooked meat, for which they also collect demonstrations of human gameplay. As the name suggests, the authors hope that the competition will spur researchers into **embedding human priors into general algorithms in order to get sample efficient learning**."
,,,Scott Fujimoto, David Meger, Doina Precup,Off-Policy Deep Reinforcement Learning without Exploration,,,,,http://arxiv.org/abs/1812.02900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"This paper discusses off-policy batch reinforcement learning, in which an agent is trying to learn a policy from data which is not based on its own policy, and without the opportunity to collect more data during training. The authors demonstrate that standard RL algorithms do badly in this setting because they give unseen state-action pairs unrealistically high values, and lack the opportunity to update them. They  proposes to address this problem by only selecting actions from previously seen state-action pairs; they prove various optimality results for this algorithm in the MDP setting. To adapt this approach to the continuous control case, the authors train a generative model to produce likely actions (conditional on the state and the data batch) and then only select from the top n actions. Their batch-conditional q-learning algorithm (BCQ) consists of that generative model, a perturbation model to slightly alter the top actions, and a value network and critic to perform the selection. When n = 0, BCQ resembles behavioural cloning, and when n -> ∞, it resembles Q-learning. BCQ with n=10 handily outperformed DQN and DDPG on some Mujoco experiments using batch data."
,,,Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine,Near-Optimal Representation Learning for Hierarchical Reinforcement Learning ,,,,,http://arxiv.org/abs/1810.01257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"This paper discusses the use of learned representations in hierarchical RL. In the setting where a higher-level policy chooses goals which lower-level policies are rewarded for reaching, how bad is it when the goal representation isn't able to express all possible states? The authors define a metric for a representation's lossiness based on how close to optimal the policies which can be learned using that representation are, and prove that using a certain objective function, representations with bounded lossiness can be learned. They note a similarity between this objective function and those of mutual information estimators.The authors test their learner on the MuJoCo Ant Maze environment, achieving compelling results."
,,,Noam Brown*, Anton Bakhtin*, Adam Lerer, Qucheng Gong,Combining Deep Reinforcement Learning and Search for Imperfect-Information Games,,,,,https://arxiv.org/abs/2007.13544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"<@AlphaZero@>(@AlphaZero: Shedding new light on the grand games of chess, shogi and Go@) and its predecessors have achieved impressive results in zero-sum two-player perfect-information games, by using a combination of search (MCTS) and RL. This paper provides the first combination of search and deep RL for _imperfect-information_ games like poker. (Prior work like <@Pluribus@>(@Superhuman AI for multiplayer poker@) did use search, but didn’t combine it with deep RL, instead relying on significant expert information about poker.)The key idea that makes AlphaZero work is that we can estimate the value of a state independently of other states without any interaction effects. For any given state s, we can simulate possible future rollouts of the game, and propagate the values of the resulting new states back up to s. In contrast, for imperfect information games, this approach does not work since you cannot estimate the value of a state independently of the policy you used to get to that state. The solution is to instead estimate values for _public belief states_, which capture the public common knowledge that all players have. Once this is done, it is possible to once again use the strategy of backing up values from simulated future states to the current state, and to train a value network and policy network based on this."
,,,Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, Peter Stone,Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey,,,,,http://arxiv.org/abs/2003.04960,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"For a variety of learning problems, the training process is organized so that new concepts and tasks leverage previously learned information. This can serve as a broad definition of curriculum learning. This paper gives an overview of curriculum learning and a framework to organize various approaches to the curriculum learning problem. One central difficulty is that there is a broad class of methods that can be considered curricula. At one extreme, we have curricula where new tasks are created to speed up learning. At another extreme, some curricula simply reorder experience samples. For example, the prioritized replay buffer is one such reordering method. Thus, to cover as much of the literature as possible the authors outline a framework for curriculum learning and then use that structure to classify various approaches. In general, the definition, learning, construction, and the evaluation of curricula are all covered in this work. This is done by breaking the curriculum learning problem into three steps: task generation, sequencing, and transfer learning. Using this problem decomposition the authors give an overview of work addressing each component."
,,,Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin, Tie-Yan Liu, Hsiao-Wuen Hon,Suphx: Mastering Mahjong with Deep Reinforcement Learning,,,,,https://arxiv.org/abs/2003.13590,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Mahjong is a large imperfect information game with complex rules where turn order can be interrupted. This makes it challenging to solve with existing techniques like MCTS and counterfactual regret minimization. This paper details what was necessary to build _Suphx_, an AI system that is stronger than 99.99% of humans. Some highlights:- Like the original AlphaGo, they first learned from human gameplay and then finetuned using reinforcement learning, with deep CNNs as their models. They learned both action models as well as value models. They added an entropy bonus to ensure that the policy remained stochastic enough to continue learning over the course of RL.- They have _five_ learned action models, corresponding to five different decisions that need to be made in Mahjong, as well as a rule-based system for deciding whether or not to declare a winning hand.- To handle imperfect information, they first train an _oracle agent_ that gets access to all information, and then slowly reduce the amount of information that it gets to observe.- They could use search to improve the performance online, but did not do so in their evaluation (since Suphx was playing on a website with time constraints). Suphx with search would probably be significantly stronger."
,,,Hengyuan Hu, Adam Lerer, Alex Peysakhovich, Jakob Foerster,"Other-Play" for Zero-Shot Coordination,,,,,http://arxiv.org/abs/2003.02979,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"How can we build AI systems that can _coordinate_ with humans? While <@past@>(@Collaborating with Humans Requires Understanding Them@) <@work@>(@Learning Existing Social Conventions via Observationally Augmented Self-Play@) has assumed access to some amount of human data, this paper aims to coordinate _without any human data at all_, which they call _zero-shot coordination_. In order to develop an algorithm, they assume that their partner is also "trained" for zero-shot coordination.Their key idea is that in zero-shot coordination, since you can't break symmetries by agreeing upon a protocol in advance (i.e. you can't agree on things like "we'll drive on the left, not the right"), you need a policy that is _robust to relabelings that preserve these symmetries_. This is easy to train for: you just train in self-play, but randomly relabel the states, actions and observations separately for each side in a way that preserves the MDP structure (i.e. uses one of the symmetries). Thus, each side must play a policy that works well _without knowing how the other agent's observations and actions have been relabeled_. In practice, for an N-player game you only need to randomize N-1 of the relabelings, and so in the two player games they consider they only randomly relabel one side of the self-play.They evaluate this in Hanabi (where the game is invariant to relabeling of the colors), and show that the resulting agents are better at playing with other agents trained on different seeds or with slightly different architectures, and also that they play better with humans, achieving an average score of 15.75 with non-expert human players, compared to 9.15 for agents trained via regular self-play."
,,,Michael Laskin*, Kimin Lee*, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas,Reinforcement Learning with Augmented Data,,,,,https://arxiv.org/abs/2004.14990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"While CURL (summarized above) applies contrastive learning in order to ensure the network is invariant to specific data augmentations, we can try something even simpler: what if we just run a regular RL algorithm on augmented observations (e.g. observations that have been randomly cropped)? The authors term this approach RAD (RL with Augmented Data), and find that this actually _outperforms_ CURL, despite not using the contrastive learning objective. The authors speculate that CURL is handicapped by using the contrastive loss as an auxiliary objective, and so its representations are forced to be good both for the true task and for the contrastive prediction task, whereas RAD only trains on the true task."
,,,Ilya Kostrikov, Denis Yarats, Rob Fergus,Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels,,,,,https://arxiv.org/abs/2004.13649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper applies data augmentation to Q-learning algorithms, again without a contrastive loss. Specifically, they suggest that the Q-values of states should be invariant to data augmentations (e.g. random translations, which is what they use), and so any time we need to estimate a Q-value, we can reduce the variance of this estimate by sampling multiple data augmentations of the state, and averaging the predicted Q-values for each of them. They apply this to Soft Actor-Critic (SAC) and find that it significantly improves results."
,,,Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi,Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation,,,,,https://arxiv.org/abs/1806.10729,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018 Deep RL Workshop,,,,,,,,,,,,,,,,"Deep reinforcement learning has been able to use high-dimensional input, such as images, to learn optimal policies. However, when neural networks are trained in a fixed environment, such as on a single level in a video game, they will usually over-fit and fail to generalize to new levels. This paper uses procedurally generated levels during training in an attempt to increase the generality of deep RL. They make use of the General Video Game AI framework (GVG-AI) which allows rapid design of video games through the specification of rewards, objects, etc. Moreover, they introduce Progressive PCG (PPCG) to smoothly control the difficulty of generated levels to build a curriculum for the agent. The authors show that for some games procedural level generation enables generalization to new levels within the same distribution."
,,,Soroush Nasiriany*, Vitchyr H. Pong*, Steven Lin, Sergey Levine,Planning with Goal-Conditioned Policies,,,,,http://arxiv.org/abs/1911.08453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"Reinforcement learning can learn complex skills by interacting with the environment. However, temporally extended or long-range decision-making problems require more than just well-honed reactions. **In this paper, the authors investigate whether or not they can obtain the benefits of action planning found in model-based RL without the need to model the environment at the lowest level.** The authors propose a model-free planning framework that learns low-level goal-conditioned policies that use their value functions as implicit models. Goal-conditioned policies are policies that can be trained to reach a goal state provided as an additional input. Given a goal-conditioned policy, the agent can then plan over intermediate subgoals (goal states) using a goal-conditioned value function to estimate reachability. Since the state space is large, the authors propose what they call latent embeddings for abstracted planning (LEAP), which is able to find useful subgoals by first searching a much smaller latent representation space and then planning a sequence of reachable subgoals that reaches the target state. In experiments, LEAP significantly outperforms prior algorithms on 2D navigation and push/reach tasks. Moreover, their method can get a quadruped ant to navigate around walls which is difficult because much of the planning happens in configuration space. This shows that LEAP is able to be extended to non-visual domains."
,,,Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi,Dream to Control: Learning Behaviors by Latent Imagination,,,,,http://arxiv.org/abs/1912.01603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In the past year or so, the idea of learning a transition model in a latent space has gained traction, motivated by the hope that such an approach could combine the best of the worlds of model-free and model-based learning. The central appeal of learning a latent transition model is that it allows you to imagine future trajectories in a potentially high-dimensional, structured observation space without actually having to generate those high-dimensional observations.Dreamer builds on a prior model by the same authors, <@PlaNet@>(@Learning Latent Dynamics for Planning from Pixels@), which learned a latent representation of the observations, p(s|o), trained both through a VAE-style observation reconstruction loss, and also a transition model q(s-next|s, a), which is trained to predict the state at the next step given only the state at the prior one, with no next-step observation data. Together, these two models allow you to simulate action-conditioned trajectories through latent state space. If you then predict reward from state, you can use this to simulate the value of trajectories. Dreamer extends on this by also training an Actor Critic-style model on top of states to predict action and value, forcing the state representation to not only capture next-step transition information, but also information relevant to predicting future rewards. The authors claim this extension makes their model more able to solve long-horizon problems, because the predicted value function can capture far-future rewards without needing to simulate the entire way there. Empirically, there seems to be reasonable evidence that this claim plays out, at least within the fairly simple environments the model is tested in."
,,,Kevin Lu, Igor Mordatch, Pieter Abbeel,Adaptive Online Planning for Continual Lifelong Learning,,,,,http://arxiv.org/abs/1912.01188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS Deep RL 2019,,,,,,,,,,,,,,,,"Lifelong learning is distinct from standard RL benchmarks because 1. The environment is *sequential* rather than *episodic*; it is never reset to a new start state. 2. The current *transition* and *reward* function are given, but they change over time.Given this setup, there are two basic approaches: first, run model-free learning on simulated future trajectories and rerun it every time the dynamics change, and second, run model-based planning on the current model. If you ignore computational constraints, these should be equivalent; however, in practice, the second option tends to be more computationally efficient. The contribution of this work is to make this more efficient, rather than improving final performance, by starting with the second option and then using model-free learning to “distill” the knowledge produced by the model-based planner allowing for more efficient planning in the future. Specifically, Adaptive Online Planning (AOP) balances between the model-based planner MPPI (a variant of MPC) and the model-free algorithm TD3. MPPI uses the given model to generate a trajectory up to a horizon and then uses an ensemble of value functions to estimate the cumulative reward. This knowledge is then distilled into TD3 for later use as a prior for MPPI. During future rollouts, the variance and Bellman error of the value function ensemble are used to determine how long the horizon should be, and therefore how much computation is used."
,,,Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, Raia Hadsell,Stabilizing Transformers for Reinforcement Learning,,,,,http://arxiv.org/abs/1910.06764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Transformers have been incredibly successful in domains with sequential data. Naturally, one might expect transformers to be useful in partially observable RL problems. However, transformers have complex implementations making them difficult to use in an already challenging domain for learning. In this paper, the authors explore a novel transformer architecture they call Gated Transformer-XL (GTrXL) that can be used in the RL setting. The authors succeed in stabilizing training with a reordering of the layer normalization coupled with the addition of a new gating mechanism located at key points in the submodules of the transformer. The new architecture is tested on DMlab-30, a suite of RL tasks including memory, and shows improvement over baseline transformer architectures and the neural computer architecture MERLIN. Furthermore, GTrXL learns faster and is more robust than a baseline transformer architecture. "
,,,Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt,Behaviour Suite for Reinforcement Learning,,,,,https://arxiv.org/abs/1908.03568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Collecting clear, informative and scalable problems that capture important aspects about how to design general and efficient learning algorithms is difficult. Many current environments used to evaluate RL algorithms introduce confounding variables that make new algorithms difficult to evaluate. In this project, the authors assist this effort by introducing Behaviour Suite for Reinforcement Learning (bsuite), a library that facilitates reproducible and accessible research on core issues in RL. The idea of these experiments is to capture core issues, such as 'exploration' or 'memory', in a way that can be easily tested or evaluated. The main contribution of this project is an open-source project called bsuite, which instantiates all experiments in code and automates the evaluation and analysis of any RL agent on bsuite. The suite is designed to be flexible and includes code to run experiments in parallel on Google cloud, with Jupyter notebook, and integrations with OpenAI Gym."
,,,Alex Irpan and Xingyou Song,The Principle of Unchanged Optimality in Reinforcement Learning Generalization,,,,,https://arxiv.org/abs/1906.00336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In image recognition tasks, there is usually only one label per image, such that there exists an optimal solution that maps every image to the correct label. Good generalization of a model can therefore straightforwardly be defined as a good approximation of the image-to-label mapping for previously unseen data.In reinforcement learning, our models usually don't map environments to the optimal policy, but states in a given environment to the corresponding optimal action. The optimal action in a state can depend on the environment. This means that there is a tradeoff regarding the performance of a model in different environments.The authors suggest the principle of unchanged optimality: in a benchmark for generalization in reinforcement learning, there should be at least one policy that is optimal for all environments in the train and test sets. With this in place, generalization does not conflict with good performance in individual environments. If the principle does not initially hold for a given set of environments, we can change that by giving the agent more information. For example, the agent could receive a parameter that indicates which environment it is currently interacting with."
,,,Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry,Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms? ,,,,,https://arxiv.org/abs/1811.02553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper investigates whether and to what extent the stated conceptual justifications for common Policy Gradient algorithms are actually the things driving their success. The paper has two primary strains of empirical investigation. In the first, they examine a few of the more rigorously theorized aspects of policy gradient methods: learned value functions as baselines for advantage calculations, surrogate rewards, and enforcement of a "trust region" where the KL divergence between old and updated policy is bounded in some way. For value functions and surrogate rewards, the authors find that both of these approximations are weak and perform poorly relative to the true value function and reward landscape respectively. Basically, it turns out that we lose a lot by approximating in this context. When it comes to enforcing a trust region, they show that TRPO is able to enforce a bound on mean KL, but that it's much looser than the (more theoretically justified) bound on max KL that would be ideal but is hard to calculate. PPO is even stranger: they find that it enforces a mean KL bound, but only when optimizations present in the canonical implementation, but not the core definition of the algorithm, are present. These optimizations include: a custom weight initialization scheme, learning rate annealing on Adam, and reward values that are normalized according to a rolling sum. All of these optimizations contribute to  non-trivial increases in performance over the base algorithm, in addition to apparently being central to how PPO maintains its trust region."
,,,Ankesh Anand*, Evan Racah*, Sherjil Ozair*, Yoshua Bengio, Marc-Alexandre Côté, R Devon Hjelm,Unsupervised State Representation Learning in Atari,,,,,https://arxiv.org/abs/1906.08226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper has two main contributions: an actual technique for learning representations in an unsupervised way, and an Atari-specific interface for giving access to the underlying conceptual state of the game (e.g. the locations of agents, locations of small objects, current remaining lives, etc) by parsing out the RAM associated with each state. Since the notional goal of unsupervised representation learning is often to find representations that can capture conceptually important features of the state without having direct access to them, this supervision system allows for more meaningful evaluation of existing methods by asking how well conceptual features can be predicted by learned representation vectors. The object-level method of the paper centers around learning representations that capture information about temporal state dynamics, which they do by maximizing mutual information between representations at  adjacent timesteps. More specifically, they have both a local version of this, where a given 1/16th patch of the image has a representation that  is optimized to be predictive of that same patches next-timestep representation, and a local-global version, where the global representation is optimized to be predictive of representations of each patch. They argue this patch-level prediction makes their method better at learning concepts attached to small objects, and the empirical results do seem to support this interpretation. "
,,,Emma Tosch, Kaleigh Clary, John Foley, David Jensen,Toybox: A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning,,,,,https://arxiv.org/abs/1905.02825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Toybox is a reimplementation of three Atari games (Breakout, Amidar and Space Invaders) that enables researchers to customize the games themselves in order to perform better experimental evaluations of RL agents. They demonstrate its utility using a case study for each game. For example, in Breakout we often hear that the agents learn to "tunnel" through the layer of bricks so that the ball bounces around the top of the screen destroying many bricks. To test whether the agent has learned a robust tunneling behavior, they train an agent normally, and then at test time they remove all but one brick of a column and see if the agent quickly destroys the last brick to create a tunnel. It turns out that the agent only does this for the center column, and sometimes for the one directly to its left."
,,,Justin Fu, Aviral Kumar, Matthew Soh, Sergey Levine,Diagnosing Bottlenecks in Deep Q-learning Algorithms,,,,,http://arxiv.org/abs/1902.10250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"While the PreQN paper used a theoretical approach to tackle Deep Q-Learning algorithms, this one takes an empirical approach. Their results:    - Small neural nets cannot represent Q*, and so have undesired bias that results in worse performance. However, they also have convergence issues, where the Q-function they actually converge to is significantly worse than the best Q-function that they could express. Larger architectures mitigate both of these problems.    - When there are more samples, we get a lower validation loss, showing that we are overfitting. Despite this, larger architectures are better, because the performance loss from overfitting is not as bad as the performance loss from having a bad bias. A good early stopping criterion could help with this.    - To study how non-stationarity affects DQL algorithms, they study a variant where the Q-function is a moving average of the past Q-functions (instead of the full update), which means that the target values don't change as quickly (i.e. it is closer to a stationary target). They find that non-stationarity doesn't matter much for large architectures.    - To study distribution shift, they look at the difference between the expected Bellman error before and after an update to the parameters. They find that distribution shift doesn't correlate much with performance and so is likely not important.    - Algorithms differ strongly in the distribution over (s, a) pairs that the DQL update is computed over. They study this in the absence of sampling (i.e. when they simply weight all possible (s, a) pairs, rather than just the ones sampled from a policy) and find that distributions that are "close to uniform" perform best. They hypothesize that this is the reason that experience replay helps -- initially an on-policy algorithm would take samples from a single policy, while experience replay adds samples from previous versions of the policy, which should increase the coverage of (s, a) pairs.To sum up, the important factors are using an expressive neural net architecture, and designing a good sampling distribution. Inspired by this, they design Adversarial Feature Matching (AFM), which like Prioritized Experience Replay (PER) puts more weight on samples that have high Bellman error. However, unlike PER, AFM does not try to reduce distribution shift via importance sampling, since their experiments found that this was not important."
,,,Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel,Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research,,,,,https://arxiv.org/abs/1903.00742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The authors argue that the best solution to the problem of task generation is creating multi-agent systems where each agent must adapt to the others. These agents do so first by learning how to implement a high-level strategy, and then by adapting it based on the strategies of others. (The authors use the term "adaptive unit" rather than "agent" to emphasise that change can occur at many different hierarchical levels, and either by evolution or learning). This adaptation may be exogenous (driven by the need to respond to a changing environment) or endogenous (driven by a unit's need to improve its own functionality). An example of the latter is a society implementing institutions which enforce cooperation between individuals. Since individuals will try to exploit these institutions, the process of gradually robustifying them can be considered an automatically-generated curriculum (aka autocurriuclum)."
,,,Clare Lyle, Pablo Samuel Castro, Marc G. Bellemare,A Comparative Analysis of Expected and Distributional Reinforcement Learning,,,,,https://arxiv.org/abs/1901.11084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,"Distributional RL systems learn distributions over the value of actions rather than just their expected values. In this paper, the authors investigate the reasons why this technique improves results, by training distribution learner agents and expectation learner agents on the same data. They provide evidence against a number of hypotheses: that distributional RL reduces variance; that distributional RL helps with policy iteration; and that distributional RL is more stable with function approximation. In fact, distributional methods have similar performance to expectation methods when using tabular representations or linear function approximators, but do better when using non-linear function approximators such as neural networks (especially in the earlier layers of networks)."
,,,Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, Michael Bowling,The Hanabi Challenge: A New Frontier for AI Research,,,,,http://arxiv.org/abs/1902.00506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The authors propose the cooperative, imperfect-information card game Hanabi as a target for AI research, due to the necessity of reasoning about the beliefs and intentions of other players in order to win. They identify two challenges: firstly, discovering a policy for a whole team that allows it to win (the self-play setting); and secondly, discovering an individual policy that allows an agent to play with an ad-hoc team without previous coordination. They note that successful self-play policies are often very brittle in the ad-hoc setting, which makes the latter the key problem. The authors provide an open-source framework, an evaluation benchmark and the results of existing RL techniques."
,,,Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley,Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions,,,,,http://arxiv.org/abs/1901.01753,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CoRL 2018,,,,,,,,,,,,,,,,"The POET algorithm uses evolutionary strategies to evolve a population of pairs of tasks and agents. During each iteration, it first generates a new environment by perturbing an existing environment, then optimises each agent for its paired environment, then attempts to transfer agents between existing environments to improve performance (in case one environment turns out to be a useful "stepping stone" towards another). New environments are kept if they are neither too hard nor too easy for the current population of agents. This algorithm was tested using the Bipedal Walker environment, where it significantly outperformed standard evolutionary search."
,,,Amy Zhang, Yuxin Wu, Joelle Pineau,Natural Environment Benchmarks for Reinforcement Learning,,,,,http://arxiv.org/abs/1811.06032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,"This paper notes that RL performance tends to be measured in simple artificial environments - unlike other areas of ML in which using real-world data such as images or text is common. The authors propose three new benchmarks to address this disparity. In the first two, an agent is assigned to a random location in an image, and can only observe parts of the image near it. At every time step, it is able to move in one of the cardinal directions, unmasking new sections of the image, until it can classify the image correctly (task 1) or locate a given object (task 2). The third type of benchmark is adding natural video as background to existing Mujoco or Atari tasks. In testing this third category of benchmark, they find that PPO and A2C fall into a local optimum where they ignore the observed state when deciding the next action."
,,,Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, Nicolas Heess,"Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",,,,,http://arxiv.org/abs/1811.06272,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper aims to alleviate the data inefficiency of RL by using a model to synthesise data. However, even when environment dynamics can be modeled accurately, it can be difficult to generate data which matches the true distribution. To solve this problem, the authors use a Structured Causal Model trained to predict the outcomes which would have occurred if different actions had been taken from previous states. Data is then synthesised by rolling out from previously-seen states. The authors test performance in a partially-observable version of SOKOBAN, in which their system outperforms other methods of generating data."
,,,Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet,Learning Actionable Representations from Visual Observations,,,,,http://arxiv.org/abs/1808.00928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Prior work on Time Contrastive Networks (TCN)s showed that you can use time as an unsupervised learning signal, in order to learn good embeddings of states that you can then use in other tasks. This paper extends TCNs to work with multiple frames, so that it can understand motion as well. Consider any two short videos of a task demonstration. If they were taken at different times, then they should be mapped to different embedding vectors (since they correspond to different "parts" of the task). On the other hand, if they were taken at the same time (even if from different viewpoints), they should be mapped to the same embedding vector. The loss function based on this encourages the network to learn an embedding for these short videos that is invariant to changes in perspective (which are very large changes in pixel-space), but _is_ different for changes in time (which may be very small changes in pixel-space). They evaluate with a bunch of different experiments."
,,,Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, Pieter Abbeel,Learning Plannable Representations with Causal InfoGAN,,,,,http://arxiv.org/abs/1807.09341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Hierarchical reinforcement learning aims to learn a hierarchy of actions that an agent can take, each implemented in terms of actions lower in the hierarchy, in order to get more efficient planning. Another way we can achieve this is to use a classical planning algorithm to find a sequence of _waypoints_, or states that the agent should reach that will allow it to reach its goal. These waypoints can be thought of as a high-level plan. You can then use standard RL algorithms to figure out how to go from one waypoint to the next. However, typical planning algorithms that can produce a sequence of waypoints require very structured state representations, that were designed by humans in the past. How can we learn them directly from data? This paper proposes Causal InfoGAN. They use a GAN where the generator creates adjacent waypoints in the sequence, while the discriminator tries to distinguish between waypoints from the generator and pairs of points sampled from the true environment. This incentivizes the generator to generate waypoints that are close to each other, so that we can use an RL algorithm to learn to go from one waypoint to the next. However, this only lets us generate adjacent waypoints. In order to use this to make a sequence of waypoints that gets from a start state to a goal state, we need to use some classical planning algorithm. In order to do that, we need to have a structured state representation. GANs do not do this by default. InfoGAN tries to make the latent representation in a GAN more meaningful by providing the generator with a "code" (a state in our case) and maximizing the mutual information of the code and the output of the generator. In this setting, we want to learn representations that are good for planning, so we want to encode information about _transitions_ between states. This leads to the Causal InfoGAN objective, where we provide the generator with a pair of abstract states (s, s'), have it generate a pair of observations (o, o') and maximize the mutual information between (s, s') and (o, o'), so that s and s' become good low-dimensional representations of o and o'. They show that Causal InfoGAN can create sequences of waypoints in a rope manipulation task, that previously had to be done manually."
,,,Gil Lederman, Markus N. Rabe, Edward A. Lee, Sanjit A. Seshia,Learning Heuristics for Automated Reasoning through Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1807.08058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The formal methods community uses SAT solvers all the time in order to solve complex search-based problems. These solvers use handtuned heuristics in order to drastically improve performance. The heuristics only affect the choices in the search process, not the correctness of the algorithm overall. Obviously, we should consider using neural nets to learn these heuristics instead. However, neural nets take a long time to run, and SAT solvers have to make these decisions very frequently, so it's unlikely to actually be helpful -- the neural net would have to be orders of magnitude better than existing heuristics. So, they instead do this for QBF (quantified boolean formulas) -- these are PSPACE complete, and the infrastructure needed to support the theory takes more time, so it's more likely that neural nets can actually help. They implement this using a graph neural network and engineer some simple features for variables and clauses. (Feature engineering is needed because there can hundreds of thousands of variables, so you can only have ~10 numbers to describe the variable.) It works well, doing better than the handcoded heuristics."
,,,Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine,Visual Reinforcement Learning with Imagined Goals,,,,,http://arxiv.org/abs/1807.04742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"[Hindsight Experience Replay](https://blog.openai.com/ingredients-for-robotics-research/) (HER) introduced the idea of accelerating learning with sparse rewards, by taking trajectories where you fail to achieve the goal (and so get no reward, and thus no learning signal) and replacing the actual goal with an "imagined" goal chosen in hindsight such that you actually achieved that goal, which means you get reward and can learn. This requires that you have a space of goals such that for any trajectory, you can come up with a goal such that the trajectory achieves that goal. In practice, this means that you are limited to tasks where the goals are of the form "reach this goal state". However, if your goal state is an image, it is very hard to learn how to act in order to reach any possible image goal state (even if you restrict to realistic ones), since the space is so large and unstructured. The authors propose to first learn a structured latent representation of the space of images using a variational autoencoder (VAE), and then use that structured latent space as the space of goals which can be achieved. They also use Q-learning instead of DDPG (which is what HER used), so that they can imagine any goal with a minibatch (s, a, s') and learn from it (whereas HER/DDPG is limited to states on the trajectory)."
,,,Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch,Decision Transformer: Reinforcement Learning via Sequence Modeling,,,,,https://arxiv.org/abs/2106.01345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In this paper, the authors abstract reinforcement learning (RL) as a sequence modeling problem. The authors are inspired by the rise of powerful sequence models (i.e transformers) in natural language processing. Specifically, they hypothesize that when models are trained to predict the expected reward-to-go alongside state and action sequences, the transformer architecture can be used to do RL.As an example, consider finding the shortest path between two vertices on a graph. We could start by recording random walks with their expected returns. Once we have enough data, we could condition on paths such that the expected return-to-go (length remaining) is low. This would effectively return shortest paths without the explicit need for optimization.This framework works well in practice and is competitive with state-of-the-art model-free offline RL baselines on Atari and OpenAI gym. The authors also carry out ablation studies to determine if the sequence modeler is just doing imitation learning on a subset of the data with high returns. This turns out not to be the case, indicating that the approach effectively uses the entire dataset."
,,,Michael Janner, Qiyang Li, Sergey Levine,Reinforcement Learning as One Big Sequence Modeling Problem,,,,,https://arxiv.org/abs/2106.02039,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Typically, RL is concerned with estimating policies that utilize immediate state information to produce high returns. However, we can also view RL as concerned with predicting sequences of actions that lead to high returns. From this perspective, it's natural to wonder if sequence modelers that work well in other domains, such as transformers in NLP, would work well for RL. This paper tests this hypothesis and demonstrates the utility of transformers in RL for a variety of problem settings.As with the last paper, the authors train the model to predict the reward-to-go. In place of trajectory optimizers, the authors make use of beam search as a planning algorithm. To do RL, rather than maximize the log-probability of potential sequences, the authors replace the log-probability search heuristic with the reward-to-go. In experiments, transformers that maintain the log-probability can imitate expert policies to high fidelity. Visually, the resulting policies are indistinguishable from that of the expert.The authors also show that their method is competitive on the standard OpenAI gym benchmarks. Finally, the authors look at the attention patterns of the trained models. They identify two patterns: the first links variables in a strictly Markovian fashion and the other links dimensions of the state-action variables across time. Interestingly, action variables are more strongly coupled to past actions than past state variables. This suggests a connection to action-smoothing proposed previously for deep-dynamics models."
,,,Michael Janner, Igor Mordatch, Sergey Levine,γ-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction,,,,,https://arxiv.org/abs/2010.14496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Long planning horizons are often necessary for competitive performance of model-based agents, but single-step models get less and less accurate with longer planning horizons as errors accumulate. Model-free algorithms don't have this problem but are usually reward- and policy-specific, such that transfer to other tasks can be hard. The paper proposes policy-specific γ-models as an intermediate solution: instead of learning the distribution of the next state given a state-action pair **(s,a)**, or the final state of an n-step rollout given **(s,a)** and a policy **π**, it learns the distribution of a rollout with a stochastic, geometrically distributed length. Unlike for n-step models with n>1, the distribution follows a Bellman-style decomposition into the single-step distribution and the discounted distribution for the next state **s'**, which allows for off-policy training of the model by bootstrapping the target distribution. Now, if rewards are consequentialist in the sense that they only depend on the state, the expected reward under this distribution is equal to **1-γ** times the Q-value for  **π** of **(s,a)** such that we can use the model for policy evaluation given arbitrary consequentialist rewards. Similar to how single-step models (0-models) can be rolled out to obtain (less accurate) multi-step models, sequential rollouts of a γ-model can be reweighed to obtain a γ-model with larger **γ**. While this introduces some error, it reduces the bootstrap error during training, which grows with **γ**. Being able to interpolate between rollouts of single-step models that accumulate error during testing and models with large **γ** that accumulate error during training allows us to find a sweet spot between the two extremes.  In practice, single-step models are often used for model-based value expansion (MVE), where only **N** steps are rolled out and a value function is used for evaluating longer-term consequences. The authors' algorithm, γ-MVE instead uses **N** rollouts of the γ-model and adjusts the weighing of the value function accordingly. γ-MVE performs strongly both in terms of sample efficiency and final performance on a set of low-dimensional continuous control tasks."
,,,Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem,What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,,,,,https://arxiv.org/abs/2006.05990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In what is likely the largest study of on-policy reinforcement learning agents, this work unifies various algorithms into a collection of over 50 design choices, which the authors implement as tunable hyperparameters and systematically investigate how those parameters impact learning across five standard continuous control environments. Specifically, they choose subsets of these hyperparameters in eight experiment themes -- policy losses, network architectures, normalization and clipping, advantage estimation, training setup, timestep handling, optimizers, and regularization. They train thousands of agents for various choices within each theme, for a total of over 250,000 agents.They present nearly a hundred graphs summarizing their experiments for the reader to make their own conclusions. Their own recommendations include: using the PPO loss, using separate value and policy networks, initializing the last policy layer with x100 smaller weights, using tanh as activation functions, using observation normalization, using generalized advantage estimation with \lambda = 0.9, tuning the number of transitions gathered in each training loop if possible, tuning the discount factor, using the Adam optimizer with a linearly decaying learning rate, among several others."
,,,Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi,An Optimistic Perspective on Offline Reinforcement Learning,,,,,https://arxiv.org/abs/1907.04543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML,,,,,,,,,,,,,,,,"Off-policy reinforcement learning (RL) that can be done using offline-logged interactions is an important aspect of real-world applications. However, most RL algorithms assume that an agent interacts with an online environment or simulator and learns from its own collected experience. Moreover, the authors show that DQN trained offline on its _own_ experience replay buffer has markedly decreased performance on most of the Atari suite. The authors attempt to address this discrepancy by introducing a robust Q-learning algorithm that randomly mixes estimates for particular Q-values. Specifically, by creating convex combinations from an underlying basis of Q-value estimates the authors are able to create a much larger ensemble. This is similar in spirit to dropout in deep learning where connections in the network are randomly turned off. The authors then go on to show that offline DQN is feasible by training this algorithm and other related algorithms on the DQN Replay Dataset and show it has comparable performance to, and occasionally even surpasses, the original RL baselines. The DQN Replay Dataset is released at [https://offline-rl.github.io/](https://offline-rl.github.io/)."
,,,Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, Rob Fergus,Improving Sample Efficiency in Model-Free Reinforcement Learning from Images,,,,,http://arxiv.org/abs/1910.01741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Sample efficiency in RL can be improved by using off-policy methods that can reuse the same sample multiple times and by using self-supervised auxiliary losses that help with representation learning, especially when rewards are sparse. This work combines both approaches by proposing to learn a latent state representation using an autoencoder while jontly training an agent on that latent representation using <@SAC@>(@Soft Actor-Critic: Deep Reinforcement Learning for Robotics@). Previous work in the on-policy case shows a positive effect from propagating Actor-Critic gradients through the encoder to improve the usefulness of the encoding for policy learning. However, this destabilizes training in the off-policy case, as changing the encoding to facilitate the actor also changes the Q-function estimate, which in turn changes the actor's goal and can introduce nonstationarity. This problem is circumvented by only propagating the Q-network's gradients through the encoder while blocking the actor's gradients.The method strongly outperforms SAC trained on pixels. It also matches the previous state-of-the-art set by model-based approaches on an image-based continuous control task and outperforms them for noisy observations (as these make dynamics models hard to learn). The authors also find that the learnt encodings generalize between tasks to some extent and that reconstructing the true environment state is easier using their latent representation than using a representation obtained by training SAC on pixels directly. "
,,,Yichuan Charlie Tang,Towards Learning Multi-agent Negotiations via Self-Play,,,,,https://arxiv.org/abs/2001.10208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"While the previous paper introduces other-play to become robust to unknown partners, this paper takes the other approach of simply training an agent that is robust to a wide, diverse population of possible agents. In particular, it studies a self-driving car "zipper merge" environment, and trains an agent to be robust to a variety of rule-based agents, as well as past versions of itself, and finds that this leads to a much more successful merging policy. However, this is evaluated against the population it is trained with, and not against any previously unseen agents."
,,,Zeyu Zheng*, Junhyuk Oh*, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver, Satinder Singh,What Can Learned Intrinsic Rewards Capture?,,,,,http://arxiv.org/abs/1912.05500,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper studies whether a learned reward function can serve as a locus of knowledge about the environment, that can be used to accelerate training of new agents. In particular, such a learned intrinsic reward can help with test-time adaptation: in a novel environment, the intrinsic reward can quickly "tell" the agent e.g. where it should explore -- even if in the new environment the agent has a different action space, or uses a different learning algorithm (situations that meta learning would typically not be able to handle).The authors create an algorithm that learns an intrinsic reward function, that when used to train a new agent over a “lifetime” (which consists of multiple episodes), leads to the best cumulative reward over the lifetime, using a meta-gradient approach. Experiments on gridworlds demonstrate that these learned intrinsic rewards: 1. switch between early exploration and later exploitation, 2. explore only for information that is relevant for optimal behavior, 3. capture invariant causal relationships, and 4. can anticipate and adapt to changes in the extrinsic reward within a lifetime."
,,,Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek, Nicholas R. Waytowich,Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Sparse Reward Environments,,,,,http://arxiv.org/abs/1910.04281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper contributes to the effort of combining imitation and reinforcement learning to train agents more efficiently. The current difficulty in this area is that imitation and reinforcement learning proceed under rather different objectives which presents a significant challenge to updating a policy learned from a pure demonstration. A major portion of this difficulty stems from the use of so-called "on-policy" methods for training which require a significant number of environment interactions to be effective. In this paper, the authors propose a framework dubbed "Cycle-of-Learning" (CoL) that allows for the off-policy combination of imitation and reinforcement learning. This allows the two approaches to be combined much more directly which grounds the agent's policy in the expert demonstrations while simultaneously allowing for RL to fine-tune the policy. The authors show that CoL is an improvement over the current state of the art by testing their algorithm in several environments and performing an ablation study. "
,,,Matteo Hessel*, Hado van Hasselt*, Joseph Modayil, David Silver,On Inductive Biases in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1907.02908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The fewer inductive biases we use, the more general our algorithms will be. But how much does it really help to have fewer inductive biases? This paper replaces several hand-engineered components of an A2C agent with generic or adaptive variants.Specifically, they compared: 1) reward clipping vs. reward normalization via <@PopArt@>(@Preserving Outputs Precisely while Adaptively Rescaling Targets@), 2) handpicked discount factor vs. online adaptive discounting via meta-learning, 3) fixed action repeats vs. learned action-commitment, and 4) standard Atari observation preprocessing vs. passing raw observations to a recurrent network. Over 57 Atari tasks, they found that the tuned algorithm outperformed the adaptive method only in (1). Performance was similar for (2) and (3), and the proposed method outperformed the baseline for (4). When the fully adaptive agent was compared to the vanilla agent (with heuristics designed for Atari) over 28 unseen continuous control tasks, the adaptive agent performed better in 14 of them, worse in one, and about the same in the rest, providing evidence that fewer inductive biases do lead to more general agents."
,,,Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry,Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,,,,,http://arxiv.org/abs/1811.02553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper argues that policy gradient algorithms are very dependent on additional optimisations (such as value function clipping, reward scaling, etc), and that they operate with poor estimates of the gradient. It also demonstrates that the PPO objective is unable to enforce a trust region, and that the algorithm's empirical success at doing so is due to the additional optimisations."
,,,Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, Dawn Song,Assessing Generalization in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1810.12282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper aims to create a benchmark for measuring generalisation in reinforcement learning. They evaluate a range of standard model-free algorithms on OpenAI Gym and Roboschool environments; the extent of generalisation is measured by varying environmental parameters at test time (note that these tasks are intended for algorithms which do not update at test time, unlike many transfer and multi-task learners). They distinguish between two forms of generalisation: interpolation (between values seen during training) and extrapolation (beyond them). The latter, which is typically much harder for neural networks, is measured by setting environmental parameters to more extreme values in testing than in training."
,,,Cédric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves Oudeyer,"CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning",,,,,http://arxiv.org/abs/1810.06284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents an intrinsically-motivated algorithm (an extension of Universal Value Function Approximators) which learns to complete multiple tasks, each parameterised by multiple “goals” (e.g. the locations of targets). It prioritises replays of tasks which are neither too easy nor too hard, but instead allow maximal learning progress; this also help prevent catastrophic forgetting by refocusing on tasks which it begins to forget."
,,,Peng Sun, Xinghai Sun, Lei Han, Jiechao Xiong, Qing Wang, Bo Li, Yang Zheng, Ji Liu, Yongsheng Liu, Han Liu, Tong Zhang,TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game,,,,,http://arxiv.org/abs/1809.07193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper showcases an RL agent which is able to defeat the built-in Starcraft AI (roughly at the level of the 30th-50th percentile of players). It do so by choosing between 165 hand-coded macro actions, which each correspond to an elementary task like producing a certain building. This avoids the necessity of learning unimportant details like exactly where the building should be placed, as well as difficult rules like the prerequisites for each building. The authors create a second agent which uses an expert system to choose actions in a hierarchical fashion, which performs at a similar level to the first. See also [Import AI](https://jack-clark.net/2018/09/25/import-ai-113-why-satellitesai-gives-us-a-global-eye-industry-pays-academia-to-say-sorry-for-strip-mining-it-and-kindred-researchers-seek-robot-standardization/)."
,,,Akshat Agarwal, Ryan Hope, Katia Sycara,Challenges of Context and Time in Reinforcement Learning: Introducing Space Fortress as a Benchmark,,,,,http://arxiv.org/abs/1809.02206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The authors note that most existing RL benchmarks (like Atari games) lack sharp context-dependence and temporal sensitivity. The former requires an agent to sometimes change strategies abruptly; the latter requires an agent's strategy to vary over time. Space Fortress is an arcade-style game which does have these properties, and which cannot be solved by standard RL algorithms, even when rewards are made dense in a naive way. However, when the authors shape the rewards to highlight the context changes, their agent achieves superhuman performance."
,,,Daniel R. Jiang, Emmanuel Ekwedike, Han Liu,Feedback-Based Tree Search for Reinforcement Learning,,,,,https://arxiv.org/abs/1805.05935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/05/22/import-ai-95-learning-to-predict-and-avoid-internet-arguments-with-deep-learning-white-house-announces-select-committee-on-ai-and-bmw-trains-cars-to-safely-change-lanes/)
,,,Benjamin Y. Hayden, Yael Niv,The case against economic values in the brain,,,,,https://psyarxiv.com/7hgup/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PsyArXiv,,,,,,,,,,,,,,,,"It has been common in the neuroeconomics literature to assume (based on past research) that the brain explicitly computes some notion of value in order to make choices. This paper argues that this is wrong: it is plausible that the brain does not in fact explicitly calculate values, and instead directly learns a policy that produces actions."
,,,Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le,Self-training with Noisy Student improves ImageNet classification,,,,,https://arxiv.org/abs/1911.04252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Instead of summarizing this paper, I'll provide an opinion describing the implications of this and other recent papers."
,,,Jonathan Uesato*, Ananya Kumar*, Csaba Szepesvari*, Tom Erez, Avraham Ruderman, Keith Anderson, Krishmamurthy (Dj) Dvijotham, Nicolas Heess, Pushmeet Kohli,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,,,,,https://arxiv.org/abs/1812.01647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,"An important problem in safety-critical domains is accurately estimating slim probabilities of catastrophic failures: one in a million is very different from one in a billion. A standard Monte Carlo approach requires millions or billions of trials to find a single failure, which is prohibitively expensive. This paper proposes using agents from earlier in the training process to provide signals for a learned failure probability predictor. For example, with a Humanoid robot, failure is defined as the robot falling down. A neural net is trained on earlier agents to predict the probability that the agent will fall down from a given state. To evaluate the final agent, states are importance-sampled based on how likely the neural network believes they are to cause failure. This relies on the assumption that the failure modes of the final agent are similar to some failure mode of earlier agents. Overall, the approach reduces the number of samples required to accurately estimate the failure probability by multiple orders of magnitude."
,,,Preetum Nakkiran and Yamini Bansal,Distributional Generalization: A New Kind of Generalization,,,,,https://arxiv.org/abs/2009.08092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Suppose you train a classifier to distinguish between CIFAR-10 classes, except each airplane has a 30% chance of being mislabeled as a car. If you then train a model to achieve perfect accuracy on this badly labeled dataset, it will get 100% accuracy on the training set, and 97% of those labels will actually be correct (since 3% are mislabeled airplanes). Under the current paradigm, if we say that the model “generalizes”, that means that it will also get 97% accuracy at test time (according to the actually correct labels). However, this doesn’t tell us anything about what mistakes are made at test time -- is it still the case that 30% of airplanes are mislabeled as cars, or does the model also make mistakes on e.g. deer?_Distributional generalization_ aims to make claims about situations like these. The core idea is to make claims about the full distribution of classifier outputs, rather than just the single metric of test accuracy.Formally, we assume there is some distribution D, from which we can sample pairs of points (x, y), which generates both our train and test sets. Then, the train (resp. test) distribution of classifier outputs is (x, f(x)), with x coming from the train (resp. test) set. The train and test distributions of classifier outputs are the objects of study in distributional generalization. In particular, given a [0,1]-valued function on distributions (called a _test_ T), we say that the classifier _generalizes w.r.t T_ if T outputs similar values on the train and test distribution. (W.r.t means “with respect to”.) For example, given a distribution, the _accuracy test_ checks how often the classifier’s output is correct in expectation over that distribution. Generalization w.r.t the accuracy test is equivalent to the canonical notion of generalization.Let’s suppose that the classifier perfectly fits the training set, so that the train distribution of classifier outputs is the same as the original distribution D. Let’s additionally suppose that the classifier generalizes with respect to the accuracy test, so that the classifier has perfect test accuracy. Then, the test distribution of classifier outputs will also be the same as the original distribution D, that is, all distributions are identical and there isn’t much more to say. So, the interesting situations are when one of these two assumptions is false, that is, when either:1. The classifier does not perfectly fit the training set, or2. The classifier does not generalize w.r.t accuracy.This paper primarily focuses on classifiers that _do_ perfectly fit the training set, but don’t generalize w.r.t accuracy. One typical way to get this setting is to inject label noise (as in the mislabeled airplanes case), since this prevents the classifier from getting 100% test accuracy.Speaking of which, let’s return to our original example in which we add label noise by mislabeling 30% of airplanes as cars. Notice that, since the label noise is completely divorced from the classifier’s input x, the best way for the classifier to minimize _test_ loss would be to always predict the true CIFAR-10 label, and then 3% of the time the true distribution will say “lol, jk, this airplane is actually a car”. However, in practice, classifiers will also label approximately 30% of airplanes as cars in the test set as well! This incurs higher loss, because the 30% of airplanes that the classifier labels as cars must be independent of the 30% of airplanes that the true distribution labels as cars, which implies that the model disagrees with the true distribution 4.2% of the time; this is worse than the 3% it would get if it consistently labeled airplanes as airplanes. **Classifiers trained to interpolation are not Bayes-optimal in the presence of label noise.**Okay, let’s get back to distributional generalization. We already know the classifier does not generalize w.r.t accuracy. However, the fact that it still labels about 30% of airplanes as cars suggests a different kind of generalization. Recall that the train and test distributions of classifier outputs have the form (x, f(x)). Consider the feature L(x) that says whether x is an airplane or not. Then, if we replace (x, f(x)) with (L(x), f(x)), then this now looks identical between the train and test distributions! Specifically, this distribution places 7% on (“yes airplane”, “airplane”), 3% on (“yes airplane”, “car”), and 10% on (“no airplane”, c) for every class c other than “airplane”. An alternative way of stating this is that the classifier generalizes w.r.t all tests whose dependence on x factors through the feature L. (In other words, the test can only depend on whether x is an airplane or not, and cannot depend on any other information about x.)The authors make a more general version of this claim they call _feature calibration_: for every feature L that _could_ be learned by the classifier, the classifier generalizes w.r.t all tests whose dependence on x factors through L. Note that they do not assume that the classifier _actually_ learns L: just that, if you hypothetically trained the classifier on a dataset of (x, L(x)), then it could learn that function near-perfectly.They then provide evidence for this through a variety of experiments and one theorem:- If you plug in the constant feature L(x) = 0 into the conjecture, it implies that classifiers should get the right class balance (i.e. if your distribution contains class 1 twice as often as class 0, then you predict class 1 twice as often as class 0 at test time). They demonstrate this on a rebalanced version of CIFAR-10, even for classifiers that generalize poorly w.r.t accuracy.- When using a WideResNet (for which the true CIFAR-10 labels are learnable), if you add a bunch of structured label noise into CIFAR-10, the test predictions reflect that same structure.- The same thing is true for decision trees applied to a molecular biology dataset.- A ResNet-50 trained to predict attractiveness on the CelebA dataset (which does not generalize w.r.t accuracy) does satisfy feature calibration w.r.t “wearing lipstick”, “heavy makeup”, “blond hair”, “male”, and “eye-glasses”. Note there is no label noise in this case.- AlexNet predicts that the right fraction of dogs are Terriers, even though it mistakes which exact dogs are Terriers.- The nearest-neighbor classifier provably satisfies feature calibration under relatively mild regularity conditions.In an appendix, they provide preliminary experiments suggesting this holds _pointwise_. In our mislabeled airplane example, for a _specific_ airplane x from the test set, if you resample a training set (with the 30% mislabeling of airplanes) and retrain a classifier on that set, then there is a roughly 30% chance that that specific x will be misclassified as a car.The authors then introduce another distributional generalization property: _agreement_. Suppose we have two classifiers f and g trained on independently sampled training sets. The agreement conjecture states that the test accuracy of f is equal to the expected probability that f agrees with g on the test distribution (loosely speaking, this is how often f and g make the same prediction for test inputs). The agreement property can also be framed as an instance of distributional generalization, though I won’t go into the specific test here. The authors perform similar experiments as with feature calibration to demonstrate that the agreement property does seem to hold across a variety of possible classifiers.Interestingly, these properties are _not_ closed under ensembling. In our mislabeled airplane example, every model will label 30% of airplanes as cars, but _which_ airplanes are mislabeled is independent across models. As a result, the plurality voting used in ensembles reduces the misclassification rate to 22%, which means that you no longer satisfy feature calibration. Consistent with this, the authors observe that neural network ensembles, random forests, and k-nearest neighbors all did not satisfy feature calibration, and tended to be closer to the Bayes-optimal solution (i.e. getting closer to being robust to label noise, in our example).Summary of the summary: Let’s look at the specific ways in which classifiers make mistakes on the test distribution. This is called distributional generalization. The paper makes two conjectures within this frame. _Feature calibration_ says that for any feature that a classifier could have learned, the distribution of its predictions, conditioned on that feature, will be the same at train and test time, including any mistakes it makes. _Agreement_ says that the test accuracy of a classifier is equal to the probability that, on some randomly chosen test example, the classifier’s prediction matches that of another classifier trained on a freshly generated training set. Interestingly, while these properties hold for a variety of ML models, they do not hold for ensembles, because of the plurality voting mechanism."
,,,Alexander D'Amour*, Katherine Heller*, Dan Moldovan*, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley,Underspecification Presents Challenges for Credibility in Modern Machine Learning,,,,,https://arxiv.org/abs/2011.03395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper explains one source of fragility to distributional shift, which the authors term _underspecification_. The core idea is that for any given training dataset, there are a large number of possible models that achieve low loss on that training dataset. This means that the model that is actually chosen is effectively arbitrarily chosen from amongst this set of models. While such a model will have good iid (validation) performance, it may have poor inductive biases that result in bad out-of-distribution performance.The main additional prediction of this framing is that if you vary supposedly “unimportant” aspects of the training procedure, such as the random seed used, then you will get a different model with different inductive biases, which will thus have different out-of-distribution performance. In other words, not only will the out-of-distribution performance be worse, its _variance_ will also be higher.The authors demonstrate underspecification in a number of simplified theoretical settings, as well as realistic deep learning pipelines. For example, in an SIR model of disease spread, when we only have the current number of infections during the initial growth phase, the data cannot distinguish between the case of having high transmission rate but low durations of infection, vs. a low transmission rate but high durations of infection, even though these make very different predictions about the future trajectory of the disease (the out-of-distribution performance).In deep learning models, the authors perform experiments where they measure validation performance (which should be relatively precise), and compare it against out-of-distribution performance (which should be lower and have more variance). For image recognition, they show that neural net training has precise validation performance, with 0.001 standard deviation when varying the seed, but less precise performance on <@ImageNet-C@>(@Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations@), with standard deviations in the range of 0.002 to 0.024 on the different corruptions. They do similar experiments with medical imaging and NLP."
,,,Andrew J. Lohn,Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,,,,,https://arxiv.org/abs/2009.00802,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Test, Evaluation, Verification, and Validation (TEVV) is an important barrier for AI applications in safety-critical areas. Current TEVV standards have very different rules for certifying _software_ and certifying _human operators_. It is not clear which of these processes should be applied for AI systems.If we treat AI systems as similar to human operators, we would certify them ensuring that they pass tests of ability. This does not give much of a guarantee of robustness (since only a few situations can be tested), and is only acceptable for humans because humans tend to be more robust to new situations than software. This could be a reasonable assumption for AI systems as well: while systems are certainly vulnerable to adversarial examples, the authors find that AI performance degrades surprisingly smoothly out of distribution in the absence of adversaries, in a plausibly human-like way.While AI might have some characteristics of operators, there are good reasons to treat it as software. The ability to deploy multiple copies of the same system increases the threat of correlated failures, which is less true of humans. In addition, parallelization can allow for more extensive testing that is typical for software TEVV. For critical applications, a common standard is that of Safety Integrity Levels (SILs), which correspond to approximate failure rates per hour. Current AI systems fail way more often than current SILs for safety-critical applications demand. For example an image recognition system would require an accuracy of 0.99999997 at 10 processed frames per second just to reach the weakest SIL used in aviation.However, SILs are often used on multiple levels and it is possible to build a system with a strong SIL from weaker components by using redundant components that fail independently or by detecting failures sufficiently early, such that AI modules could still be used safely as parts of a system specifically structured to cope with their failures. For example, we can use out-of-distribution detection to revert to a safe policy in simple applications. However, this is not possible for higher levels of automation where such a policy might not be available."
,,,Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song,Pretrained Transformers Improve Out-of-Distribution Robustness,,,,,https://arxiv.org/abs/2004.06100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL 2020,,,,,,,,,,,,,,,,"One important metric for the performance of deep learning models is the extent to which they generalize to examples that are _out-of-distribution_ (OOD) from the original distribution on which they were trained. This ability is sometimes called out-of-distribution _robustness_. This paper examines the OOD robustness of several NLP models: a bag-of-words model, word embedding models that use word averages, LSTMs, or ConvNets, and several models that use pretrained bidirectional transformers (BERT).The paper finds that:- Pretrained transformers (BERT) are significantly more OOD robust.- Pretrained transformers (BERT) are significantly better at _detecting_ when they've encountered an OOD example. Previous models do worse than random chance at detection.- Larger models don't increase OOD robustness in NLP the way they seem to in computer vision.- Model distillation (using a larger trained neural network to train a smaller neural network) reduces OOD robustness, suggesting that naive in-distribution tests for model distillation methods may mask later failures.- More diverse data improves OOD robustness.The paper hypothesizes that these pretrained models may perform better because they were pretrained on particularly diverse data, were trained on a large amount of data, and were trained with self-supervised objectives, which previous work has suggested improves OOD robustness and detection."
,,,Moses Charikar, Jacob Steinhardt, Gregory Valiant,Learning from Untrusted Data,,,,,https://arxiv.org/abs/1611.02315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,STOC,,,,,,,,,,,,,,,,"This paper introduces semi-verified learning. Here a model learns from a verified or trusted dataset, and from an untrusted dataset which consists in a mixture of legitimate and arbitrary examples. For the untrusted dataset, it is not known which points are legitimate and which are not. This scenario can occur when data is scraped from the internet, recorded by unreliable devices, or gathered through [crowdsourcing](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html). Concretely if a (possibly small) fraction of the scraped data is hand-labeled, then this could count as the trusted set, and the remaining data could be considered the untrusted set. This differs from semi-supervised learning where there are labeled and unlabeled task-relevant examples. Here there are trusted examples and examples which are untrusted (e.g., labels may be wrong, features may be out-of-distribution, examples may be malicious, and so on). See the full paper for theorems and an algorithm applicable to tasks such as robust density estimation."
,,,Di Jin*, Zhijing Jin*, Joey Tianyi Zhou, Peter Szolovits,Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,,,,,https://arxiv.org/abs/1907.11932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper presents TextFooler, an algorithm for generating adversarial text for natural language tasks with only black-box access to models. TextFooler tries to generate sentences that are grammatical and semantically similar to original input sentences but produce incorrect labels. It does this by identifying a small set of most important words in the original sentence, generating candidate synonyms for those words, and gradually replacing the important words in the sentence by testing which synonyms cause the model to mispredict or report the least confidence score.TextFooler is tested on three state-of-the-art NLP models-- WordCNN, WordLSTM, and BERT, all trained to ~80 - 90% test accuracy. On a variety of text classification datasets, TextFooler reduces accuracy to below ~15% with less than ~20% of the words perturbed. Humans evaluating the generated sentences say they are approximately as grammatical as the original, have the same label as the original in ~90% of cases, and have a sentence similarity score to the original sentence of 0.9 on a 0 to 1 scale. The paper finds that generally, models with higher original accuracy have higher after-attack acuracy.The authors retrain BERT from scratch using data produced by TextFooler and then attack it using TextFooler again. They find that the after-attack accuracy is higher and that attacks require more perturbed words."
,,,Mingchen Li, Mahdi Soltanolkotabi and 
Samet Oymak,Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,,,,,https://arxiv.org/pdf/1903.11680.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Previous [empirical](https://arxiv.org/pdf/1901.09960.pdf#page=2&zoom=100,0,81) [papers](https://papers.nips.cc/paper/8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.pdf) have shown that finding ways to decrease training time greatly improves robustness to label corruptions, but to my knowledge this is the first theoretical treatment."
,,,Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,,,,,https://arxiv.org/abs/1811.12231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper empirically demonstrates the outsized influence of textures in classification. To address this, they apply style transfer to ImageNet images and train with this dataset. Although training networks on a specific corruption tends to provide robustness only to that specific corruption, stylized ImageNet images supposedly lead to generalization to new corruption types such as uniform noise and high-pass filters (but not blurs)."
,,,Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, Shu-Tao Xia,Iterative Learning with Open-set Noisy Labels,,,,,https://arxiv.org/abs/1804.00092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CVPR,,,,,,,,,,,,,,,,"Much previous research on corrupted learning signals deals with label corruption, but this CVPR 2018 paper considers learning with corrupted or irrelevant inputs. For example, they train a CIFAR-10 classifier on CIFAR-10 data mixed with out-of-class CIFAR-100 data; such a scenario can occur with flawed data curation or data scraping. They use a traditional anomaly detection technique based on the local outlier factor to weight training examples; the more out-of-distribution an example is, the less weight the example has in the training loss. This approach apparently helps the classifier cope with irrelevant inputs and recover accuracy."
,,,Dan Hendrycks*, Norman Mu*, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,,,,,https://arxiv.org/abs/1912.02781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,"This paper introduces a data augmentation technique to improve robustness and uncertainty estimates. The idea is to take various random augmentations such as random rotations, produce several augmented versions of an image with compositions of random augmentations, and then pool the augmented images into a single image by way of an elementwise convex combination. Said another way, the image is augmented with various traditional augmentations, and these augmented images are “averaged” together. This produces highly diverse augmentations that have similarity to the original image. Unlike techniques such as AutoAugment, this augmentation technique uses typical resources, not 15,000 GPU hours. It also greatly improves generalization to unforeseen corruptions, and it makes models more stable under small perturbations. Most importantly, even as the distribution shifts and accuracy decreases, this technique produces models that can [remain calibrated under distributional shift](https://openreview.net/pdf?id=S1gmrxHFvB#page=8&zoom=100,144,298)."
,,,Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, Graham Neubig,Improving Robustness of Machine Translation with Synthetic Noise,,,,,https://arxiv.org/abs/1902.09508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NAACL 2019,,,,,,,,,,,,,,,,"By injecting noise (such as typos, word omission, slang) into the training set of a machine translation model, the authors are able to improve performance on naturally occurring data. While this trick usually does not work for computer vision models, it can work for NLP models."
,,,Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein,Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,,,,,https://arxiv.org/abs/1804.00792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018,,,,,,,,,,,,,,,,"Demonstrates a data poisoning attack in which the adversary gets to choose a poison input to add to the training set, but does _not_ get to choose its label. The goal is to misclassify a single test instance as a specific base class. They achieve this by creating a poison input that looks like the base class in pixel space but looks like the test instance in feature space (i.e. the activations in the penultimate layer). The poison input will be labeled by humans as the base class, and then when the network is retrained with the original dataset and the new poisoned input(s), it will classify the poison input as the base class, and with it the test instance as well (since they have very similar features)."
,,,Justin Fu, Katie Luo, Sergey Levine,Learning Robust Rewards with Adversarial Inverse Reinforcement Learning,,,,,https://arxiv.org/abs/1710.11248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"GAIL and Guided Cost Learning (GCL) both have the idea of learning a policy and a reward function simultaneously, in a GAN-like way (where the generator is the policy, and the discriminator has to distinguish between trajectories from the policy and the expert trajectories, which can then be interpreted as a reward function). In GAIL, the discriminator can’t be interpreted as a reward function, and you only get a policy as output (which is why it is called imitation learning). However, if you enforce that the discriminator has to be of the form:D(τ) = exp(f(τ)) / (exp(f(τ)) + π(τ))Then you can show that if π is trained to maximizeR(τ) = log(1 - D(τ)) - log(D(τ))Then R and π converge to the optimal reward and policy respectively.This trajectory-centric formulation is called GAN-GCL (the GAN version of guided cost learning). It’s main issue is that it works with trajectories and so is hard to optimize -- the gradients are very high variance. So, instead, we can work with individual state-action pairs instead of trajectories, just replacing every τ in the equations above with (s, a). This makes it more sample-efficient, and in this case f converges to the advantage function of the optimal policy. However, the advantage function induces a heavily entangled reward function, which rewards the RL agent for doing the action that the expert would have taken, without actually understanding the goal that the expert had. We would like to learn a disentangled reward, which they define as a reward function that leads to the optimal policy according to the true reward function even if the transition dynamics change.Intuitively, since entanglement happens by rewarding the agent for taking the same action as the expert, we can do better by enforcing that the reward function only be a function of the state, so that it is forced to learn the actual goal rather than memorizing the actions that are good. They prove two theorems under the condition that the true reward is only a function of the state. First, the learned optimal reward function is fully disentangled if it is a function of only the state, assuming that the transition dynamics are “decomposable”. Second, the reward function must be a function of only the state if it is fully disentangled.Now the discriminator in the formulation above is either looking at the trajectory as a whole, or looking at the current action in order to see whether or not you are matching the expert demonstrations. Clearly we can’t just make the discriminator a function of only the current state -- there’s no way that could distinguish between policies, since it has no access to information about the actions that the policies took. However, we can instead separate the discriminator’s f function into the reward term and a shaping term, and enforce that the shaping term does not change the optimal policy:f(s, a, s’) = g(s) + γh(s’) − h(s)(It happens to be the case that for any function h, adding γh(s’) − h(s) to the reward function does not change the optimal policy.) Now, the discriminator gets information about the action taken by the policy by seeing the next state s’ that resulted. Since γh(s’) − h(s) does not change the optimal policy, g(s) should converge to an optimal reward function, while h(s) must then be the value function V(s) in order to have f(s, a, s’) be the advantage function.They run a bunch of experiments with recovering a reward function and then transferring it to a situation with different dynamics, and show that it works much better than any other algorithm. They also show that for direct imitation (no transfer required), it does about as well as GAIL."
,,,Jonathan Ho and Stefano Ermon,Generative Adversarial Imitation Learning,,,,,https://arxiv.org/abs/1606.03476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2016,,,,,,,,,,,,,,,,"To do imitation learning, you could try to learn the state-action correspondence that defines the expert policy via behavioral cloning (simply learn a model of action given state using supervised learning), but that’s brittle. Specifically, this is learning the policy over single timesteps, so any errors in one timestep lead to the next timestep being slightly out of distribution, leading to worse errors, giving compounding errors. So, you can instead use IRL to find the cost function, and use RL to then find a good policy. But IRL is slow. They propose a fast method that gets directly to the policy, but it’s as if you did IRL followed by RL.They write down IRL as a maximization of a minimization that gives you a cost function, and then RL as a minimization over policies of the expected cost. Specifically, maxent RL(c) computes a good policy π given a cost function c by minimizing expected cost E[c] and maximizing entropy H(π), where H(π) is computed according to causal entropy, not regular entropy (see MaxCausalEnt from [AN #12](https://mailchi.mp/bcb2c6f1d507/alignment-newsletter-12)). IRL(π) finds a cost function c such that the expert policy is maximally better than the RL policy.Let’s assume that we’re working with all cost functions c : S → A → Reals. This is a huge space, so let’s throw in a convex regularizer ψ for the IRL problem. Then, they prove that RL(IRL(π)) can be expressed as a single minimization involving ψ*, the convex conjugate of ψ. We can make different choices of ψ give us different imitation learning algorithms that minimize this objective.Let ρ(s, a) be the occupancy measure of a policy π for a state-action pair (s, a). (Occupancy measure is the sum of the discounted probabilities of taking the given state-action pair.) Then, the expected cost of a trajectory is the sum of ρ(s, a) c(s, a) over all s, a. In the special case where you don’t do any regularization, the occupancy measure of the solution is guaranteed to be equal to the occupancy measure of the expert policy. (In the language of convex optimization, IRL and maxent RL are dual to each other, and strong duality applies.)Of course, this all assumes that you have access to the full expert policy, whereas in practice you only have access to some expert demonstrations. So now, the question is how to choose a good ψ that deals with that issue. If you don’t regularize at all, you get something that matches the estimated occupancy measure exactly -- which is not great, since there are likely many areas of state space where there are not enough samples to accurately match the occupancy measure, at least in the case with stochastic dynamics. On the other hand, previous algorithms are basically using a ψ that is infinity on cost functions outside of a particular class, and constant elsewhere, which means that they only work if the cost function is actually in the class of learnable functions.So, they instead propose a ψ that is dependent on the expert data. It requires that the cost function be negative everywhere, and puts a high penalty on any cost function that puts high cost (i.e. close to zero) on the expert data. Note that this can represent any (bounded) reward function, so it has very good expressive power. They chose the particular form they did because when you work through the convex conjugate, you end up choosing π to minimize the Jensen-Shannon divergence between the inferred policy and the expert policy, which can be interpreted as the optimal log loss of a classifier that distinguishes between the two policiees. This is very GAN-like, where we have a discriminator trying to distinguish between two policies, and a generator (the learned policy) that’s trying to fool the discriminator. Their proposed algorithm, GAIL, follows this quite precisely, alternating between an Adam gradient step to update the discriminator, and a TRPO step to update the policy. (Both the discriminator and the policy are neural nets.)In the experiments, they imitate classic control and MuJoCo tasks that have been trained with TRPO. GAIL consistently achieves the expert performance, even with not much data, though this comes at a cost of lots of environment interaction (which eg. behavioral cloning does not require)."
,,,Tom Everitt, Gary Lea, Marcus Hutter,AGI Safety Literature Review,,,,,https://arxiv.org/abs/1805.01109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Joint Conference on Artificial Intelligence (IJCAI), 2018",,,,,,,,,,,,,,,,"Self-explanatory. It's more of a list of approaches and references within each approach than an integrated whole, but I still expect it to be useful."
,,,Dylan Hadfield-Menell, Gillian Hadfield,Incomplete Contracting and AI Alignment,,,,,https://arxiv.org/abs/1804.04268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper explores an analogy between AI alignment and incomplete contracting. In human society, we often encounter principal-agent problems, where we want to align the incentives of the agent with those of the principal. In theory, we can do this with a "complete" contract, that is an enforceable contract that fully specifies the optimal behavior in every possible situation. Obviously in practice we cannot write such contracts, and so we end up using incomplete contracts instead. Similarly, in AI alignment, in theory we could perfectly align an AI with humans by imbuing it with the true human utility function, but in practice this is impossible -- we cannot consider every possible situation that could come up. The difference between the behavior implied by the reward function we write down and the utility function we actually want leads to misalignment. The paper then talks about several ideas from incomplete contracting and their analogues in AI alignment. The main conclusion is that our AI systems will have to learn and use a "common sense" understanding of what society will and will not sanction, since that is what enables humans to solve principal-agent problems (to the extent that we can)."
,,,Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt,Unsolved Problems in ML Safety,,,,,https://arxiv.org/abs/2109.13916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"To make the case for safety to the broader machine learning research community, this paper provides a revised and expanded collection of concrete technical safety research problems, namely:1. Robustness: Create models that are resilient to adversaries, unusual situations, and Black Swan events.2. Monitoring: Detect malicious use, monitor predictions, and discover unexpected model functionality.3. Alignment: Build models that represent and safely optimize hard-to-specify human values.4. External Safety: Use ML to address risks to how ML systems are handled, including cyberwarfare and global turbulence.Throughout, the paper attempts to clarify the problems’ motivation and provide concrete project ideas."
,,,Gopal P. Sarma, Adam Safron, Nick J. Hay,"Integrative Biological Simulation, Neuropsychology, and AI Safety",,,,,https://arxiv.org/abs/1811.03493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper argues that we can make progress on AI capabilities and AI safety through integrative biological simulation, that is, a composite simulation of all of the processes involved in neurons that allow us to simulate brains. In the near future, such simulations would be limited to simple organisms like Drosophila, but even these organisms exhibit behavior that we find hard to replicate today using our AI techniques, especially at the sample efficiency that the organisms show. On the safety side, even such small brains share many architectural features with human brains, and so we might hope that we could discover neuroscience-based methods for value learning that generalize well to humans. Another possibility would be to create test suites (as in [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883)) for simulated organisms."
,,,Gopal P. Sarma, Adam Safron, Nick J. Hay,"Integrative Biological Simulation, Neuropsychology, and AI Safety",,,,,https://arxiv.org/abs/1811.03493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,See [Import AI](https://jack-clark.net/2018/11/19/import-ai-121-sony-researchers-make-ultra-fast-imagenet-training-breakthrough-berkeley-researchers-tackle-starcraft-ii-with-modular-rl-system-and-germany-adds-e3bn-for-ai-research/) and [this comment](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/EhNCnCkmu7MwrQ7yz#krcwYcuTQRp9DBdZY).
,,,Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger,On Calibration of Modern Neural Networks,,,,,https://arxiv.org/abs/1706.04599,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML,,,,,,,,,,,,,,,,"Models should not be unduly confident, especially when said confidence is used for decision making or downstream tasks. This work provides a simple method to make models more calibrated so that the confidence estimates are closer to the true correctness likelihood. (For example, if a calibrated model predicts “toucan” with 60% confidence, then 60% of the time the input was actually a toucan.) Before presenting their method, they observe that batch normalization can make models less calibrated, while unusually large weight decay regularization can increase calibration. However, their proposed approach to increase calibration does not impact accuracy or require substantive model changes. They simply adjust the temperature of the softmax to make the model’s “confidence” (here the maximum softmax probability) more calibrated. Specifically, after training they tune the softmax temperature to minimize the cross entropy (negative average log-likelihood) on validation data. They then measure model calibration with a measure which is related to the Brier score, but with absolute values rather than squares."
,,,Aaron van den Oord, Yazhe Li, Oriol Vinyals,Representation Learning with Contrastive Predictive Coding,,,,,https://arxiv.org/abs/1807.03748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper from 2018 proposed Contrastive Predictive Coding (CPC): a method of unsupervised learning that has been quite successful. At its core it is quite simple: it simply combines the ideas of predictive coding and contrastive losses, both of which have been significantly studied in the past.The simplest form of unsupervised learning would be data compression via generative models (as in e.g. VAEs), in which, to model the data **p(x)**, you attempt to encode **x** into a latent (hidden) state **z** in such a way that you can then recover the original data point **x** from **z**. Intuitively, we want **z** to have high mutual information with **x**.For sequential data in a partially observed setting, you need to deal with the full sequence. Consider natural language: in this setting, each x would be a single word. Consider the sentence "I sat on the chair". If the **z** corresponding to the word "the" only has to reconstruct the word "the", it's not going to "remember" that the past context involved sitting, and so that **z** would be terrible at predicting that the next word will be chair. To fix this, we can use predictive coding, where we instead require that we can _predict_ future words using **z**. This now incentivizes **z_t** to have high mutual information with **x_{t+k}**.There is still a problem: reconstructing the entire input **x** would require a lot of irrelevant information, such as e.g. the background color of the environment in RL, even if that never changes. How can we get rid of these irrelevant features? Contrastive losses allow us to do this: intuitively, since the irrelevant features are the ones that are common across all the **x**s (and so are fully captured by **p(x)** ), if we train the neural net to _distinguish_ between various **x**s, we can incentivize only the relevant features. In particular, given a latent state **z_t**, we take the true **x_{t+k}**, and throw in a bunch of other **x**s sampled from **p(x)** (known as _negative samples_), and train the network to correctly classify **x_{t+k}**. The authors show that the optimum of this loss function is indeed for the neural net to compute **p(x | z) / p(x)**, which implies that it is maximizing a lower bound on the mutual information between X and Z.This gives us a pretty simple overall algorithm. Take a sequence **x_1 ... x_T**, compute **z_t** using a recurrent model on **x_1 ... x_t**, put **x_{t+k}** and some negative samples into a set, and train a classifier to correctly predict which of the samples is the true **x_{t+k}**. In practice, we do batches of these at the same time, and for every data point in the batch we use all of the other data points as our negative examples. The features you learn are then the ones that help _distinguish_ between **x_{t+k}** and the negative samples, and you'll ignore any features that are common across all the samples. This means that the results depend quite a lot on how you choose your samples (this effectively determines what **p(x)** you are using).The authors evaluate their algorithm on several domains and show that it achieves or surpasses state of the art on them."
,,,Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A. Alemi, George Tucker,On Variational Bounds of Mutual Information,,,,,https://arxiv.org/abs/1905.06922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper is a pretty dense and technical explanation of various ways in which we can estimate and/or optimize the mutual information between two variables. I specifically want to highlight that it provides a proof that the Contrastive Predictive Coding objective (summarized above) is a lower bound on the mutual information between the input and the representation, and compares it to other lower bounds on mutual information."
,,,Jeff Donahue, Karen Simonyan,Large Scale Adversarial Representation Learning,,,,,https://arxiv.org/abs/1907.02544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"The BigGAN paper, published last September, used a much larger model (and a handful of optimization tricks to facilitate training it) to achieve a huge leap forward in the quality of generated images. However, it was unclear from the earlier paper whether this improvement in generation quality would also be tied to an increase in the model's usefulness as a source of unsupervised semantic representations of images.  This paper set out to answer that question by taking an existing technique for learning representations with GANs - called BiGAN - and combining it with the BigGan architecture, which hadn't been available when BiGAN was originally published. BiGAN, short for Bidirectional GAN, works by learning both a latent space to image transformation, and also an image to latent space encoder, and then enforcing that pairs of (latent, image) from these two distributions be indistinguishable from one another. They evaluated the quality of learned representations by measuring the performance of a linear model trained using the  encoder's learned latent vectors as input, and did find it to be the case that a BiGAN trained with a BigGAN architecture performs better than one trained with a smaller architecture. "
,,,Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton,Big Self-Supervised Models are Strong Semi-Supervised Learners,,,,,https://arxiv.org/abs/2006.10029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Previously, <@SimCLR@>(@A Simple Framework for Contrastive Learning of Visual Representations@) showed that you can get good results on semi-supervised learning on ImageNet, by first using self-supervised learning with a contrastive loss to learn good representations for images, and then finetuning a classifier on top of the representations with very few labels. This paper reports a significantly improved score, using three main improvements:1. Making all of the models larger (in particular, deeper).2. Incorporating momentum contrast, as done <@previously@>(@Improved Baselines with Momentum Contrastive Learning@).3. Using model distillation to train a student network to mimic the original finetuned classifier.On linear classification on top of learned features with a ResNet-50 architecture, they get a top-1 accuracy of 71.7%, so lower than the previous paper. Their main contribution is to show what can be done with larger models. According to top-1 accuracy on ImageNet, the resulting system gets 74.9% with 1% of labels, and 80.1% with 10% of labels. In comparison, standard supervised learning with a ResNet-50 (which is about 33x smaller) achieves 76.6% with all labels, and just 57.9% with 1% of labels and 68.4% with 10% of labels. When they distill down their biggest model into a ResNet-50, it gets 73.9% with 1% of labels and 77.5% with 10% of labels."
,,,Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton,A Simple Framework for Contrastive Learning of Visual Representations,,,,,https://arxiv.org/abs/2002.05709,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Contrastive learning is a major recent development, in which we train a neural net to learn representations by giving it the task of maximizing "agreement" between similar images, while minimizing it across dissimilar images. It has been used to achieve excellent results with semi-supervised learning on ImageNet.The authors performed a large empirical study of contrastive learning. Their framework consists of three components. First, the _data augmentation method_ specifies how to get examples of "similar images": we simply take an (unlabeled) training image, and apply data augmentations to it to create two images that both represent the same underlying image. They consider random crops, color distortion, and Gaussian blur. Second is the _neural network architecture_, which is split into the first several layers f() which compute the representation from the input, and the last few layers g() which compute the similarity from the representation. Finally, the _contrastive loss function_ defines the problem of maximizing agreement between similar images, while minimizing agreement between dissimilar images. They primarily use the same InfoNCE loss used in <@CPC@>(@Representation Learning with Contrastive Predictive Coding@).They then show many empirical results, including:1. Having a simple linear layer in g() is not as good as introducing one hidden layer, or in other words, the representations in the penultimate layer are more useful than those in the final layer.2. Larger batch sizes, longer training, and larger networks matter even more for unsupervised contrastive learning than they do for supervised learning."
,,,Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick,Momentum Contrast for Unsupervised Visual Representation Learning,,,,,https://arxiv.org/abs/1911.05722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In most deep learning settings, the batch size primarily controls the variance of the gradient, with higher batch sizes decreasing variance. However, with typical contrastive learning, batch size also determines the _task_: typically, the task is to maximize agreement between two examples in the batch, and minimize agreement with all the other examples in the batch. Put another way, given one input, you have to correctly classify which of the remaining examples in the minibatch is a differently transformed version of that input. So, the batch size determines the number of negative examples.So, besides decreasing variance, large batch sizes also increase the difficulty of the task to be solved. However, such large batch sizes are hard to fit into memory and are computationally expensive. This paper proposes _momentum contrast_ (MoCo), in which we get large numbers of negative examples for contrastive learning, while allowing for small batch sizes.Think of contrastive learning as a dictionary lookup task -- given one transformed image (the query), you want to find the same image transformed in a different way out of a large list of images (the keys). The key idea of this paper is to have the minibatch contain queries, while using all of the previous N minibatches as the keys (for some N > 1), allowing for many negative examples with a relatively small minibatch.Of course, this wouldn't help us if we had to encode the keys again each time we trained on a new minibatch. So, instead of storing the images directly as keys, we store their _encoded representations_ in the dictionary, ensuring that we don't have to rerun the encoder every iteration on all of the keys. This is where the computational savings come from.However, the encoder is being updated over time, which means that different keys are being encoded differently, and there isn't a consistent kind of representation against which similarity can be computed. To solve this, the authors use a momentum-based version of the encoder to encode keys, which ensures that the key encodings change slowly and smoothly, while allowing the query encoder to change rapidly. This means that the query representation and the key representations will be different, but the layers on top of the representations can learn to deal with that. What's important is that _within_ the key representations, the representations are approximately consistent."
,,,Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He,Improved Baselines with Momentum Contrastive Learning,,,,,https://arxiv.org/abs/2003.04297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This paper applies the insights from the SimCLR paper to the MoCo framework: it adds an extra hidden layer on top of the representations while training on the contrastive loss, and adds the blur data augmentation. This results in a new SOTA on self-supervised representation learning for images."
,,,Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,,,,,https://arxiv.org/abs/2006.09882,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"There has been a lot of work in self-supervised representation learning for image classification (previously summarized in [AN #92](https://mailchi.mp/d7e950bc8dbd/an-92learning-good-representations-with-contrastive-predictive-coding) and [AN #99](https://mailchi.mp/4f7ffc5cbe53/an-99-doubling-times-for-the-efficiency-of-ai-algorithms)). This paper sets a new SOTA of 75.3% top-1 ImageNet accuracy, when allowed to first do self-supervised representation learning on ImageNet, and then to train a linear classifier on top of the learned features using all of ImageNet.Previous methods use a contrastive loss across the learned representations (possibly after being processed by a few MLP layers), which can be thought of as using the learned representation to predict the representation of augmented versions of the same input. In contrast, this paper uses the representation to predict “codes” of augmented versions, where the codes are computed using clustering."
,,,Krishnamurthy (Dj)Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, Pushmeet Kohli,A Dual Approach to Scalable Verification of Deep Networks,,,,,https://arxiv.org/abs/1803.06567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conference on Uncertainty in Artificial Intelligence, 2018",,,,,,,,,,,,,,,,"This paper tackles the same problem as the previous one, but can work on general feedforward and recurrent neural nets, though they show the math specifically for nets with fully connected layers and componentwise activations. They start by defining an optimization problem, where the property to be verified is encoded as the optimization objective, and the mechanics of the neural net are encoded as equality constraints. If the optimal value is negative, then the property has been verified. The key idea to solving this problem is to break down the hard problem of understanding a sequence of linear layers followed by nonlinearities into multiple independent problems each involving a single layer and a nonlinearity. They do this by computing bounds on the values coming out of each layer (both before and after activations), and allowing the constraints to be satisfied with some slack, with the slack variables going into the objective with Lagrange multipliers. This dual problem satisfies weak duality -- the solution to the dual problem for any setting of the Lagrange multipliers constitutes an upper bound on the solution to the original problem. If that upper bound is negative, then we have verified the property. They show how to solve the dual problem -- this is easy now that the slack variables allow us to decouple the layers from each other. They can then compute a tighter upper bound by optimizing over the Lagrange multipliers (which is a convex optimization problem, and can be done using standard techniques). In experiments, they show that the computed bounds on MNIST are reasonably good for very small perturbations, even on networks with 2-3 layers."
,,,Aditi Raghunathan, Jacob Steinhardt, Percy Liang,Certified Defenses against Adversarial Examples,,,,,https://arxiv.org/abs/1801.09344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"Even when defenses are developed to make neural nets robust against adversarial examples, they are usually broken soon after by stronger attacks. Perhaps we could prove once and for all that the neural net is robust to adversarial examples? The abstract summarizes the approach well: "[W]e study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error."To compute the certificate, they consider the optimal attack A. Given a particular input x, the optimal attack A is the one that changes f(A(x)) to a different class, where f is the ML model, and A(x) is restricted to not change x too much. They leverage the structure of f (linear models and neural nets with one hidden layer) and the restrictions on A to compute a bound on f(A(x)) in terms of x. So, for each data point in the training set, the bound either says “guaranteed that it can’t be adversarially attacked” or “might be possible to adversarially attack it”. Averaging this over the training set or test set gives you an estimate of an upper bound on the optimal adversarial attack success rate."
,,,Vincent Tjeng, Kai Xiao, Russ Tedrake,Evaluating Robustness of Neural Networks with Mixed Integer Programming,,,,,https://arxiv.org/abs/1711.07356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"I've only read the abstract so far, but this paper claims to find the _exact_ adversarial accuracy of an MNIST classifier within an L infinity norm ball of radius 0.1, which would be a big step forward in the state of the art for verification."
,,,Sumanth Dathathri*, Krishnamurthy Dvijotham*, Alexey Kurakin*, Aditi Raghunathan*, Jonathan Uesato*, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy Liang, Pushmeet Kohli,Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,,,,,https://arxiv.org/abs/2010.11645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"In parallel with extending verification to sequential settings, as well as learning what specifications to verify, we also need to make verification significantly cheaper in order for it to be feasible to apply it to large neural networks. So far, we have only been able to achieve one of two very desirable properties at a time:1. The method can scale up to large, independently trained networks. (This has been achieved by methods using linear (LP) relaxations like <@this one@>(@A Dual Approach to Scalable Verification of Deep Networks@).)2. The method produces tight bounds and thus avoids producing vacuous results. (Achieved by using relaxations based on semidefinite programming (SDP) instead of linear ones.)This paper shows how you can massage the SDP version such that the resulting algorithm becomes scalable, changing the runtime and memory requirements from O(n^6) and O(n^4) to O(n) per iteration. The resulting algorithm can be applied to larger neural nets than previous SDP approaches and gives much tighter bounds than LP approaches. For example, on an adversarially trained CNN for MNIST (which SDP algorithms haven’t previously been applied to), they can verify 87.8% adversarial accuracy, while LP methods can only verify 0.4%."
,,,Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, Mykel J. Kochenderfer,Algorithms for Verifying Deep Neural Networks,,,,,https://arxiv.org/abs/1903.06758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,This is a survey paper about verification of properties of deep neural nets.
,,,Kai Y. Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, Aleksander Madry,Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability,,,,,http://arxiv.org/abs/1809.03008,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,"The idea behind verification is to consider all possible inputs at the same time, and show that no matter what the input is, a particular property is satisfied. In ML, this is typically applied to adversarial examples, where inputs are constrained to be within the L-infinity norm ball of dataset examples. Prior papers on verification (covered in [AN #19](https://mailchi.mp/4b19d2caa5a9/alignment-newsletter-19)) solve a computationally easier relaxation of the verification problem, that gives a lower bound on the performance of the classifier. This paper aims to use exact verification, since it can compute the exact adversarial performance of the classifier on the test set, and to figure out how to improve its performance.One easy place to start is to encourage weights to be zero, since these can be pruned from the problem fed in to the constraint solver. (Or more likely, they feed it in anyway, but the constraint solver immediately gets rid of them -- constraint solvers are pretty smart.) This can be done using L1 regularization and pruning small weights. This already gives two orders of magnitude of speedup, making it able to verify that there is no adversarial attack with ϵ = 0.1 on a particular MNIST digit in 11 seconds on average.Next, they note that verification with linear constraints and functions is easy -- the challenging aspect is the Relu units that force the verifier to branch into two cases. (Since relu(x) = max(x, 0), it is the identity function when x is positive, and the zero function otherwise.) So why not try to ensure that the Relu units are also linear? Obviously we can't just make all the Relu units linear -- the whole point of them is to introduce nonlinearity to make the neural net more expressive. But as a start, we can look at the behavior of the Relu units on the examples we have, and if they are almost always active (inputs are positive) or almost always inactive (inputs are negative), then we replace them with the corresponding linear function (identity and zero, respectively), which is easier to verify. This gets another ~2x speedup.But what if we could also change the training procedure? Maybe we could augment the loss so that the Relu units are either decisively active or decisively inactive on any dataset example. They propose that _during training_ we consider the L-infinity norm ball around each example, use that to create intervals that each pixel must be in, and then make a forward pass through the neural net using interval arithmetic (which is fast but inexact). Then, we add a term to the loss that incentivizes the interval for the input to each Relu to exclude zero (so that the Relu is either always active or always inactive). They call this the Relu Stability loss, or RS loss.This leads to a further 4-13x speedup with similar test set accuracy. They then also test on MNIST with ϵ = 0.2, 0.3 and CIFAR with ϵ = 2/255, 8/255. It leads to speedup in all cases, with similar test set accuracy on MNIST but reduced accuracy on CIFAR. The provable accuracy goes up, but this is probably because when there's no RS loss, more images time out in verification, not because the network becomes better at classification. Other verification methods do get better provable accuracies on CIFAR, even though in principle they could fail to detect that a safe example is safe. This could be because their method times out frequently, or because their method degrades the neural net classifier -- it's hard to tell since they don't report number of timeouts."
,,,Gopal Sarma, James Koppel, Gregory Malecha, Patrick Schultz, Eric Drexler, Ramana Kumar, Cody Roux, Philip Zucker,Formal Methods for the Informal Engineer: Workshop Recommendations,,,,,https://arxiv.org/abs/2104.00739,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,arXiv,,,,,,,,,,,,,,,,"This is the writeup from the <@Formal Methods for the Informal Engineer@> workshop. The main thrust is a call for increased application of formal methods in order to increase confidence in critical AI/ML systems, especially in the life sciences. They provide five high-level recommendations for this purpose."
,,,Osbert Bastani, Yewen Pu, Armando Solar-Lezama,Verifiable Reinforcement Learning via Policy Extraction,,,,,http://arxiv.org/abs/1805.08328,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27th USENIX Security Symposium,,,,,,,,,,,,,,,,"Since it is hard to verify properties of neural nets, we can instead first train a decision tree policy to mimic the policy learned by deep RL, and then verify properties about that. The authors generalize [DAGGER](https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf) to take advantage of the Q-function and extract decision tree policies. They then prove a correctness guarantee for a toy version of Pong (where the dynamics are known), a robustness guarantee for Pong (with symbolic states, not pixels) (which can be done without known dynamics), and stability of cartpole."
