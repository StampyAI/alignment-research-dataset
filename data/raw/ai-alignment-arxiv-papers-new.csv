Url,title,authors,date_published,source,text,id,source_type,converted_with,data_last_modified,abstract,author_comment,journal_ref,doi,primary_category,categories
https://arxiv.org/abs/1802.07228,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.11328,Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,"['Zitong Yang', 'Yaodong Yu', 'Chong You', 'Jacob Steinhardt', 'Yi Ma']",2020-02-26 07:21:54+00:00,arxiv,...,84cdb2427f8ca93ef1b4885f3d74f565,html,markdownify,2020-12-08 03:10:44+00:00,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2010.11645,Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,"['Sumanth Dathathri', 'Krishnamurthy Dvijotham', 'Alexey Kurakin', 'Aditi Raghunathan', 'Jonathan Uesato', 'Rudy Bunel', 'Shreya Shankar', 'Jacob Steinhardt', 'Ian Goodfellow', 'Percy Liang', 'Pushmeet Kohli']",2020-10-22 12:32:29+00:00,arxiv,...,8a2b39e63b8a3c8a7785f59d8a79a654,html,markdownify,2020-11-03 17:48:49+00:00,"Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2002.11708,Generalized Hindsight for Reinforcement Learning,"['Alexander C. Li', 'Lerrel Pinto', 'Pieter Abbeel']",2020-02-26 18:57:05+00:00,arxiv,...,9356d793e200b609d547ff082c40aba2,html,markdownify,2020-02-26 18:57:05+00:00,"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2011.08512,Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,['Sean McGregor'],2020-11-17 08:55:14+00:00,arxiv,...,79a72e8d661f7b15894b394a3ae86af1,html,markdownify,2020-11-17 08:55:14+00:00,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.","6 pages, 7 figures, Pre-print accepted to Innovative Applications of
  Artificial Intelligence (IAAI-21)",,,cs.CY,"['cs.CY', 'cs.SE', 'K.4.0; K.4.3; I.2.0']"
https://arxiv.org/abs/2002.11089,Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"['Benjamin Eysenbach', 'Xinyang Geng', 'Sergey Levine', 'Ruslan Salakhutdinov']",2020-02-25 18:36:31+00:00,arxiv,...,611ee256868bc153b62b0e3e35d0f57b,html,markdownify,2020-02-25 18:36:31+00:00,"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2005.01643,"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","['Sergey Levine', 'Aviral Kumar', 'George Tucker', 'Justin Fu']",2020-05-04 17:00:15+00:00,arxiv,...,8eb67228cca162a8f266e1ffa04a8c48,html,markdownify,2020-11-01 23:50:25+00:00,"In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2009.12612,Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"['Greg Anderson', 'Abhinav Verma', 'Isil Dillig', 'Swarat Chaudhuri']",2020-09-26 14:51:04+00:00,arxiv,...,142894a2ca6587707657160c840c22af,html,markdownify,2020-10-26 14:02:51+00:00,"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2007.08124,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,"['Jian Liu', 'Leyang Cui', 'Hanmeng Liu', 'Dandan Huang', 'Yile Wang', 'Yue Zhang']",2020-07-16 05:52:16+00:00,arxiv,...,52bf518d902e096bc7d6e317a379c755,html,markdownify,2020-07-16 05:52:16+00:00,"Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset",Accepted by IJCAI2020,,,cs.CL,['cs.CL']
https://arxiv.org/abs/2003.07305,DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,"['Aviral Kumar', 'Abhishek Gupta', 'Sergey Levine']",2020-03-16 16:18:52+00:00,arxiv,...,a04144cf23f62921d71c240b07698865,html,markdownify,2020-03-16 16:18:52+00:00,"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",Pre-print,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2007.02382,Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions,"['Michael Chang', 'Sidhant Kaushik', 'S. Matthew Weinberg', 'Thomas L. Griffiths', 'Sergey Levine']",2020-07-05 16:41:09+00:00,arxiv,...,bd0f847947d713296e9ee0d78c7e257f,html,markdownify,2020-08-14 05:20:29+00:00,"This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.","18 pages, 13 figures, accepted to the International Conference on
  Machine Learning (ICML) 2020",,,cs.LG,"['cs.LG', 'cs.GT', 'cs.MA', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1907.07273,An Inductive Synthesis Framework for Verifiable Reinforcement Learning,"['He Zhu', 'Zikang Xiong', 'Stephen Magill', 'Suresh Jagannathan']",2019-07-16 21:57:17+00:00,arxiv,...,ce65ac5ccc4e4b03c4f4132358966525,html,markdownify,2019-07-16 21:57:17+00:00,"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",Published on PLDI 2019,,10.1145/3314221.3314638,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML', '00-02']"
https://arxiv.org/abs/1911.00497,A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,"['Nicholas Waytowich', 'Sean L. Barton', 'Vernon Lawhern', 'Garrett Warnell']",2019-10-31 22:37:54+00:00,arxiv,...,e777a6f66b25f17932ae3f5216bca1bb,html,markdownify,2019-10-31 22:37:54+00:00,"While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.","Presented at the Imitation, Intent and Interaction (I3) workshop,
  ICML 2019. arXiv admin note: substantial text overlap with arXiv:1906.02671",,,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/2011.04483,A Theory of Universal Learning,"['Olivier Bousquet', 'Steve Hanneke', 'Shay Moran', 'Ramon van Handel', 'Amir Yehudayoff']",2020-11-09 15:10:32+00:00,arxiv,...,8e5b3d4f9722ef8776675ce6db77969f,html,markdownify,2020-11-09 15:10:32+00:00,"How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its ""learning curve"", that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.   In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.   For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.",,,,cs.LG,"['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']"
https://arxiv.org/abs/2012.08630,Open Problems in Cooperative AI,"['Allan Dafoe', 'Edward Hughes', 'Yoram Bachrach', 'Tantum Collins', 'Kevin R. McKee', 'Joel Z. Leibo', 'Kate Larson', 'Thore Graepel']",2020-12-15 21:39:50+00:00,arxiv,...,aebd663b551c8a9ce2eab56700100f4e,html,markdownify,2020-12-15 21:39:50+00:00,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation.   We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",,,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/2107.10939,What are you optimizing for? Aligning Recommender Systems with Human Values,"['Jonathan Stray', 'Ivan Vendrov', 'Jeremy Nixon', 'Steven Adler', 'Dylan Hadfield-Menell']",2021-07-22 21:52:43+00:00,arxiv,...,af5b6653fdcc185ef2a412b62833c755,html,markdownify,2021-07-22 21:52:43+00:00,"We describe cases where real recommender systems were modified in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classifiers from human-created data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.","Originally presented at the ICML 2020 Participatory Approaches to
  Machine Learning workshop",,,cs.IR,"['cs.IR', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/1808.08460,The Social Cost of Strategic Classification,"['Smitha Milli', 'John Miller', 'Anca D. Dragan', 'Moritz Hardt']",2018-08-25 18:31:52+00:00,arxiv,...,e8212d13202134ee9d9193a0b7752742,html,markdownify,2018-11-22 13:51:18+00:00,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2107.01969,The MineRL BASALT Competition on Learning from Human Feedback,"['Rohin Shah', 'Cody Wild', 'Steven H. Wang', 'Neel Alex', 'Brandon Houghton', 'William Guss', 'Sharada Mohanty', 'Anssi Kanervisto', 'Stephanie Milani', 'Nicholay Topin', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']",2021-07-05 12:18:17+00:00,arxiv,...,0731465f4e87f6922f0580c4bca85299,html,markdownify,2021-07-05 12:18:17+00:00,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve.   The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations.   Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",NeurIPS 2021 Competition Track,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1901.08654,The Assistive Multi-Armed Bandit,"['Lawrence Chan', 'Dylan Hadfield-Menell', 'Siddhartha Srinivasa', 'Anca Dragan']",2019-01-24 21:52:01+00:00,arxiv,...,1704eb962b34839b638a8bb3ab5c2057,html,markdownify,2019-01-24 21:52:01+00:00,"Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.",Accepted to HRI 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1705.09990v1,Should Robots be Obedient?,"['Smitha Milli', 'Dylan Hadfield-Menell', 'Anca Dragan', 'Stuart Russell']",2017-05-28 20:51:19+00:00,arxiv,...,c69174ffa080b9f7c6bea067aac714b2,html,markdownify,2017-05-28 20:51:19+00:00,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",Accepted to IJCAI 2017,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1910.02910,Scaled Autonomy: Enabling Human Operators to Control Robot Fleets,"['Gokul Swamy', 'Siddharth Reddy', 'Sergey Levine', 'Anca D. Dragan']",2019-09-22 01:00:49+00:00,arxiv,...,8009104b300e6e46b7ae5872b17457de,html,markdownify,2020-03-08 22:36:53+00:00,"Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a simulated navigation task. We also run a hardware demonstration that illustrates how our method can be applied to a real-world mobile robot navigation task.","Accepted to International Conference on Robotics and Automation
  (ICRA) 2020",,,cs.RO,"['cs.RO', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1806.02501,Simplifying Reward Design through Divide-and-Conquer,"['Ellis Ratner', 'Dylan Hadfield-Menell', 'Anca D. Dragan']",2018-06-07 03:49:05+00:00,arxiv,...,37a16748d312498ef9b8e086327771ee,html,markdownify,2018-06-07 03:49:05+00:00,"Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-and-conquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We find that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.",Robotics: Science and Systems (RSS) 2018,,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2001.09318,Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors,"['Raphael KÃ¶ster', 'Dylan Hadfield-Menell', 'Gillian K. Hadfield', 'Joel Z. Leibo']",2020-01-25 14:00:33+00:00,arxiv,...,97ece8afa29781c0e7c3d4e1ba1e9a56,html,markdownify,2020-01-25 14:00:33+00:00,"How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a ""silly rule"") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.",,,,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/1802.01744,Shared Autonomy via Deep Reinforcement Learning,"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']",2018-02-06 00:45:12+00:00,arxiv,...,293c66b526f71b92473c1a1f43a7a074,html,markdownify,2018-05-23 03:12:34+00:00,"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user's policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user's actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user's input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) flying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user's private information through observations, but receives a reward signal and user input that both depend on the user's intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user's input. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",Accepted to the Robotics: Science and Systems (RSS) 2018 conference,,,cs.LG,"['cs.LG', 'cs.HC', 'cs.RO']"
https://arxiv.org/abs/2005.10141,Rational Consensus,"['Joseph Y. Halpern', 'Xavier Vilaca']",2020-05-20 15:39:55+00:00,arxiv,...,0528fb1f7347e629a9844159ea176d74,html,markdownify,2020-05-20 15:39:55+00:00,"We provide a game-theoretic analysis of consensus, assuming that processes are controlled by rational agents and may fail by crashing. We consider agents that \emph{care only about consensus}: that is, (a) an agent's utility depends only on the consensus value achieved (and not, for example, on the number of messages the agent sends) and (b) agents strictly prefer reaching consensus to not reaching consensus. We show that, under these assumptions, there is no \emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking, this means that there must always exist a \emph{failure pattern} (a description of who fails, when they fail, and which agents they do not send messages to in the round that they fail) and initial preferences for which an agent can gain by deviating. On the other hand, if we assume that there is a distribution $\pi$ on the failure patterns and initial preferences, then under minimal assumptions on $\pi$, there is a Nash equilibrium that tolerates $f$ failures (i.e., $\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 < n$ (where $n$ is the total number of agents). Moreover, we show that a slight extension of the Nash equilibrium strategy is also a \emph{sequential} equilibrium (under the same assumptions about the distribution $\pi$).","Appears in Proceedings of the 35th Annual ACM Symposium on Principles
  of Distributed Computing, 2016",,,cs.DC,"['cs.DC', 'cs.GT']"
https://arxiv.org/abs/1806.00109v1,Probabilistically Safe Robot Planning with Confidence-Based Human Predictions,"['Jaime F. Fisac', 'Andrea Bajcsy', 'Sylvia L. Herbert', 'David Fridovich-Keil', 'Steven Wang', 'Claire J. Tomlin', 'Anca D. Dragan']",2018-05-31 21:47:34+00:00,arxiv,...,96467ace2e7f1f1af1af9745738d073f,html,markdownify,2018-05-31 21:47:34+00:00,"In order to safely operate around humans, robots can employ predictive models of human motion. Unfortunately, these models cannot capture the full complexity of human behavior and necessarily introduce simplifying assumptions. As a result, predictions may degrade whenever the observed human behavior departs from the assumed structure, which can have negative implications for safety. In this paper, we observe that how ""rational"" human actions appear under a particular model can be viewed as an indicator of that model's ability to describe the human's current motion. By reasoning about this model confidence in a real-time Bayesian framework, we show that the robot can very quickly modulate its predictions to become more uncertain when the model performs poorly. Building on recent work in provably-safe trajectory planning, we leverage these confidence-aware human motion predictions to generate assured autonomous robot motion. Our new analysis combines worst-case tracking error guarantees for the physical robot with probabilistic time-varying human predictions, yielding a quantitative, probabilistic safety certificate. We demonstrate our approach with a quadcopter navigating around a human.",Robotics Science and Systems (RSS) 2018,,,cs.RO,"['cs.RO', 'cs.LG']"
https://arxiv.org/abs/1902.04198,Preferences Implicit in the State of the World,"['Rohin Shah', 'Dmitrii Krasheninnikov', 'Jordan Alexander', 'Pieter Abbeel', 'Anca Dragan']",2019-02-12 00:50:56+00:00,arxiv,...,cfffd6b954d12b508c6c4c18a1e213c6,html,markdownify,2019-04-18 22:15:37+00:00,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",Published at ICLR 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1906.09136,Categorizing Wireheading in Partially Embedded Agents,"['Arushi Majha', 'Sayan Sarkar', 'Davide Zagami']",2019-06-21 13:38:35+00:00,arxiv,...,5953c9a2aded8a5ce615b729318b7524,html,markdownify,2019-06-21 13:38:35+00:00,"$\textit{Embedded agents}$ are not explicitly separated from their environment, lacking clear I/O channels. Such agents can reason about and modify their internal parts, which they are incentivized to shortcut or $\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we provide a taxonomy of ways by which wireheading can occur, followed by a definition of wirehead-vulnerable agents. Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially embedded agents and identify wireheading opportunities that such agents can exploit, experimentally demonstrating the results with the GRL simulation platform AIXIjs. We contextualize wireheading in the broader class of all misalignment problems - where the goals of the agent conflict with the goals of the human designer - and conjecture that the only other possible type of misalignment is specification gaming. Motivated by this taxonomy, we define wirehead-vulnerable agents as embedded agents that choose to behave differently from fully dualistic agents lacking access to their internal parts.",Accepted at the AI Safety Workshop in IJCAI 2019,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2007.01223,Verifiably Safe Exploration for End-to-End Reinforcement Learning,"['Nathan Hunt', 'Nathan Fulton', 'Sara Magliacane', 'Nghia Hoang', 'Subhro Das', 'Armando Solar-Lezama']",2020-07-02 16:12:20+00:00,arxiv,...,88ed3a4aaf67199cb889656c8560c9f7,html,markdownify,2020-07-02 16:12:20+00:00,"Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.",,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO', 'F.3.1; I.2.8']"
https://arxiv.org/abs/1910.13369,A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning,"['Somil Bansal', 'Andrea Bajcsy', 'Ellis Ratner', 'Anca D. Dragan', 'Claire J. Tomlin']",2019-10-29 16:34:00+00:00,arxiv,...,496d0ad5ed15c481fb21fba26578bb9f,html,markdownify,2020-04-05 21:10:25+00:00,"Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. Although this enables data and priors to improve the human model, observation models are difficult to specify and priors may be incorrect, leading to erroneous state predictions that can degrade the safety of the robot motion plan. In this work, we seek to design a predictor which is more robust to misspecified models and priors, but can still leverage human behavioral data online to reduce conservatism in a safe way. To do this, we cast human motion prediction as a Hamilton-Jacobi reachability problem in the joint state space of the human and the belief over the model parameters. We construct a new continuous-time dynamical system, where the inputs are the observations of human behavior, and the dynamics include how the belief over the model parameters change. The results of this reachability computation enable us to both analyze the effect of incorrect priors on future predictions in continuous state and time, as well as to make predictions of the human state in the future. We compare our approach to the worst-case forward reachable set and a stochastic predictor which uses Bayesian inference and produces full future state distributions. Our comparisons in simulation and in hardware demonstrate how our framework can enable robust planning while not being overly conservative, even when the human model is inaccurate.",,,,cs.RO,"['cs.RO', 'cs.LG', 'cs.SY', 'eess.SY']"
https://arxiv.org/abs/2007.05408v1,Machine Learning Explainability for External Stakeholders,"['Umang Bhatt', 'McKane Andrus', 'Adrian Weller', 'Alice Xiang']",2020-07-10 14:27:06+00:00,arxiv,...,4fe3bd52d6c0ecf739e16d6faf2e9015,html,markdownify,2020-07-10 14:27:06+00:00,"As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2001.04335,Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society,"['Carina Prunkl', 'Jess Whittlestone']",2020-01-13 15:22:42+00:00,arxiv,...,101380a862b5557570a7e68731c4d29e,html,markdownify,2020-01-21 12:18:20+00:00,"One way of carving up the broad ""AI ethics and society"" research space that has emerged in recent years is to distinguish between ""near-term"" and ""long-term"" research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1902.09725v3,Conservative Agency via Attainable Utility Preservation,"['Alexander Matt Turner', 'Dylan Hadfield-Menell', 'Prasad Tadepalli']",2019-02-26 04:42:54+00:00,arxiv,...,a04e6386adba1b1579be6b97e3f2fb1e,html,markdownify,2020-06-10 15:10:04+00:00,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","Published in AI, Ethics, and Society 2020",,10.1145/3375627.3375851,cs.AI,['cs.AI']
https://arxiv.org/abs/1905.10615,Adversarial Policies: Attacking Deep Reinforcement Learning,"['Adam Gleave', 'Michael Dennis', 'Cody Wild', 'Neel Kant', 'Sergey Levine', 'Stuart Russell']",2019-05-25 15:23:19+00:00,arxiv,...,1bb099aec22672233335a3f8111e765d,html,markdownify,2021-01-17 19:25:56+00:00,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",Presented at ICLR 2020,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML', 'I.2.6']"
https://arxiv.org/abs/1907.01475,Generalizing from a few environments in safety-critical reinforcement learning,"['Zachary Kenton', 'Angelos Filos', 'Owain Evans', 'Yarin Gal']",2019-07-02 16:12:34+00:00,arxiv,...,675fa80370fc7c5d5e9c4cffda3a3de2,html,markdownify,2019-07-02 16:12:34+00:00,"Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2102.05008,Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,"['Lewis Hammond', 'James Fox', 'Tom Everitt', 'Alessandro Abate', 'Michael Wooldridge']",2021-02-09 18:20:50+00:00,arxiv,...,c6561d74c1546d1d9e21992ac67b0e1d,html,markdownify,2021-02-09 18:20:50+00:00,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.","Accepted to the 20th International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS-21)",,,cs.MA,"['cs.MA', 'cs.AI', 'cs.GT']"
https://arxiv.org/abs/2102.01685v2,Agent Incentives: A Causal Perspective,"['Tom Everitt', 'Ryan Carey', 'Eric Langlois', 'Pedro A Ortega', 'Shane Legg']",2021-02-02 18:52:41+00:00,arxiv,...,e1510538208bd91d32bd354babdc1015,html,markdownify,2021-03-15 20:08:39+00:00,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.","In Proceedings of the AAAI 2021 Conference. Supersedes
  arXiv:1902.09980, arXiv:2001.07118",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1905.12186,Asymptotically Unambitious Artificial General Intelligence,"['Michael K Cohen', 'Badri Vellambi', 'Marcus Hutter']",2019-05-29 02:48:15+00:00,arxiv,...,10423cba6b722a62ae90f65f4fbce90b,html,markdownify,2020-07-21 13:27:38+00:00,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",9 pages with 5 figures; 10 page Appendix with 2 figures,Proc.AAAI. 34 (2020) 2467-2476,,cs.AI,"['cs.AI', 'I.2.0, I.2.6', 'I.2.0; I.2.6']"
https://arxiv.org/abs/1903.01021,A Strongly Asymptotically Optimal Agent in General Environments,"['Michael K. Cohen', 'Elliot Catt', 'Marcus Hutter']",2019-03-04 00:02:58+00:00,arxiv,...,c8c04dcf829fca4639fb3262051a116e,html,markdownify,2019-05-27 04:30:13+00:00,"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.","7 pages, 3 figures",Proc.IJCAI (2019) 2179-2186,,cs.LG,"['cs.LG', 'cs.AI', 'I.2.6; I.2.8']"
https://arxiv.org/abs/1811.09716,"Robustness via curvature regularization, and vice versa","['Seyed-Mohsen Moosavi-Dezfooli', 'Alhussein Fawzi', 'Jonathan Uesato', 'Pascal Frossard']",2018-11-23 22:03:40+00:00,arxiv,...,0d693a1b7fed0f2954d4a5aded284055,html,markdownify,2018-11-23 22:03:40+00:00,"State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.",,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1612.01474,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,"['Balaji Lakshminarayanan', 'Alexander Pritzel', 'Charles Blundell']",2016-12-05 18:54:43+00:00,arxiv,...,08d5c6c25555c380df5d34c3718e5daa,html,markdownify,2017-11-04 01:33:43+00:00,"Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.",NIPS 2017,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1806.01186v2,Penalizing side effects using stepwise relative reachability,"['Victoria Krakovna', 'Laurent Orseau', 'Ramana Kumar', 'Miljan Martic', 'Shane Legg']",2018-06-04 16:30:17+00:00,arxiv,...,92bcc8e74c0a68fcf6fb7560e55dfbcb,html,markdownify,2019-03-08 09:17:21+00:00,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1811.06521,Reward learning from human preferences and demonstrations in Atari,"['Borja Ibarz', 'Jan Leike', 'Tobias Pohlen', 'Geoffrey Irving', 'Shane Legg', 'Dario Amodei']",2018-11-15 18:33:43+00:00,arxiv,...,a696e80d5935552b460d6fa5ab70a009,html,markdownify,2018-11-15 18:33:43+00:00,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",NIPS 2018,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/2004.13654,Pitfalls of learning a reward function online,"['Stuart Armstrong', 'Jan Leike', 'Laurent Orseau', 'Shane Legg']",2020-04-28 16:58:58+00:00,arxiv,...,76c1f43fafbf26d73ef9471c258d7f30,html,markdownify,2020-04-28 16:58:58+00:00,"In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1810.12715,On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models,"['Sven Gowal', 'Krishnamurthy Dvijotham', 'Robert Stanforth', 'Rudy Bunel', 'Chongli Qin', 'Jonathan Uesato', 'Relja Arandjelovic', 'Timothy Mann', 'Pushmeet Kohli']",2018-10-30 13:12:47+00:00,arxiv,...,c4f6472d7c4326bc670d00729d2d49cc,html,markdownify,2019-08-29 12:23:52+00:00,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.","[v2] Best paper at NeurIPS SECML 2018 Workshop [v4] Accepted at ICCV
  2019 under the title ""Scalable Verified Training for Provably Robust Image
  Classification""",,,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1802.03493,More Robust Doubly Robust Off-policy Evaluation,"['Mehrdad Farajtabar', 'Yinlam Chow', 'Mohammad Ghavamzadeh']",2018-02-10 01:32:03+00:00,arxiv,...,3e02ed09c6181a40a224626fd4893712,html,markdownify,2018-05-23 18:13:43+00:00,"We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t.~the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1802.07740,Machine Theory of Mind,"['Neil C. Rabinowitz', 'Frank Perbet', 'H. Francis Song', 'Chiyuan Zhang', 'S. M. Ali Eslami', 'Matthew Botvinick']",2018-02-21 19:00:10+00:00,arxiv,...,a7bfb2465ba3b9ebff224769eaa68840,html,markdownify,2018-03-12 21:37:03+00:00,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.","21 pages, 15 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1906.02845,Likelihood Ratios for Out-of-Distribution Detection,"['Jie Ren', 'Peter J. Liu', 'Emily Fertig', 'Jasper Snoek', 'Ryan Poplin', 'Mark A. DePristo', 'Joshua V. Dillon', 'Balaji Lakshminarayanan']",2019-06-07 00:01:42+00:00,arxiv,...,64f41b014d4768921f3f51367c564125,html,markdownify,2019-12-05 19:59:35+00:00,"Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.",Accepted to NeurIPS 2019,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1911.04266,(When) Is Truth-telling Favored in AI Debate?,"['VojtÄch KovaÅÃ­k', 'Ryan Carey']",2019-11-11 13:49:43+00:00,arxiv,...,8328dec929faebd22d9b447ecaf42b87,html,markdownify,2021-03-16 15:42:42+00:00,"For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.","In SafeAI Workshop at AAAI, 2019",,,cs.AI,"['cs.AI', 'cs.GT']"
https://arxiv.org/abs/1707.05173v1,Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,"['William Saunders', 'Girish Sastry', 'Andreas Stuhlmueller', 'Owain Evans']",2017-07-17 14:13:40+00:00,arxiv,...,bb7d16889f95be24e4bcc1c423c830a2,html,markdownify,2017-07-17 14:13:40+00:00,"AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.",,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.NE']"
https://arxiv.org/abs/2001.00463,The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,"['Toby Shevlane', 'Allan Dafoe']",2019-12-27 10:20:44+00:00,arxiv,...,7771d6d0ea14f3c9cc44e833cd8072ac,html,markdownify,2020-01-09 23:24:21+00:00,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2001.05068,Social and Governance Implications of Improved Data Efficiency,"['Aaron D. Tucker', 'Markus Anderljung', 'Allan Dafoe']",2020-01-14 22:26:12+00:00,arxiv,...,297627e683c48363478a26d1d826c8fe,html,markdownify,2020-01-14 22:26:12+00:00,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the ""AI production function"", will be key to understanding the development of the AI industry and its societal impacts.","7 pages, 2 figures, accepted to Artificial Intelligence Ethics and
  Society 2020",,10.1145/3375627.3375863,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1912.01683v9,Optimal Policies Tend to Seek Power,"['Alexander Matt Turner', 'Logan Smith', 'Rohin Shah', 'Andrew Critch', 'Prasad Tadepalli']",2019-12-03 20:45:49+00:00,arxiv,...,cb222d5e429dea249f7c4ee98a656e2d,html,markdownify,2021-12-03 17:27:16+00:00,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.","Accepted to NeurIPS 2021 as spotlight paper. 12 pages, 44 pages with
  appendices",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1910.05789v2,On the Utility of Learning about Humans for Human-AI Coordination,"['Micah Carroll', 'Rohin Shah', 'Mark K. Ho', 'Thomas L. Griffiths', 'Sanjit A. Seshia', 'Pieter Abbeel', 'Anca Dragan']",2019-10-13 17:17:52+00:00,arxiv,...,f95ab49b05715d6df44d5345001cb109,html,markdownify,2020-01-09 00:51:44+00:00,"While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.","Published at NeurIPS 2019
  (http://papers.nips.cc/paper/8760-on-the-utility-of-learning-about-humans-for-human-ai-coordination)",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', 'stat.ML']"
https://arxiv.org/abs/1911.02320,Nonverbal Robot Feedback for Human Teachers,"['Sandy H. Huang', 'Isabella Huang', 'Ravi Pandya', 'Anca D. Dragan']",2019-11-06 11:26:31+00:00,arxiv,...,dfd3286d73c36e8f5aa332217a8bece6,html,markdownify,2019-11-06 11:26:31+00:00,"Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.",CoRL 2019,,,cs.RO,"['cs.RO', 'cs.HC', 'cs.LG']"
https://arxiv.org/abs/1805.08882,Multi-task Maximum Entropy Inverse Reinforcement Learning,"['Adam Gleave', 'Oliver Habryka']",2018-05-22 21:57:34+00:00,arxiv,...,29cd82248b3205d5c68a82d1c9292094,html,markdownify,2018-07-15 13:58:18+00:00,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.","Presented at 1st Workshop on Goal Specifications for Reinforcement
  Learning (ICML/IJCAI/AAMAS 2018)",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6']"
https://arxiv.org/abs/1807.05185,Model Reconstruction from Model Explanations,"['Smitha Milli', 'Ludwig Schmidt', 'Anca D. Dragan', 'Moritz Hardt']",2018-07-13 17:15:00+00:00,arxiv,...,28d8f5d6ba273eaa8614d2223c212e3a,html,markdownify,2018-07-13 17:15:00+00:00,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1811.01267,Legible Normativity for AI Alignment: The Value of Silly Rules,"['Dylan Hadfield-Menell', 'McKane Andrus', 'Gillian K. Hadfield']",2018-11-03 19:09:18+00:00,arxiv,...,f925759c268c889de3b98b14b6781ec6,html,markdownify,2018-11-03 19:09:18+00:00,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems, in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules--rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.HC']"
https://arxiv.org/abs/1901.01291,On the Utility of Model Learning in HRI,"['Gokul Swamy', 'Jens Schulz', 'Rohan Choudhury', 'Dylan Hadfield-Menell', 'Anca Dragan']",2019-01-04 19:55:49+00:00,arxiv,...,77011e7c8b1f081b92875fbd9784a5b9,html,markdownify,2020-05-22 02:34:37+00:00,"Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a ""theory of mind"" to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods for an autonomous driving task under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot's theory of mind for the human are wrong, as they inevitably will be in practice.",,,,cs.RO,"['cs.RO', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1906.09624v1,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","['Rohin Shah', 'Noah Gundotra', 'Pieter Abbeel', 'Anca D. Dragan']",2019-06-23 18:41:31+00:00,arxiv,...,e32efd2fba15b6b63005aedabb9c9355,html,markdownify,2019-06-23 18:41:31+00:00,"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",Published at ICML 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1810.05157,Learning under Misspecified Objective Spaces,"['Andreea Bobu', 'Andrea Bajcsy', 'Jaime F. Fisac', 'Anca D. Dragan']",2018-10-11 17:58:27+00:00,arxiv,...,dcd64eda4e9b3114bc0bc809a56f375b,html,markdownify,2018-10-26 05:21:19+00:00,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator.",Conference on Robot Learning (CoRL) 2018,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1912.02624,Learning Efficient Representation for Intrinsic Motivation,"['Ruihan Zhao', 'Stas Tiomkin', 'Pieter Abbeel']",2019-12-04 07:48:40+00:00,arxiv,...,b9f051678b28ac68e3ed105c019b0fe0,html,markdownify,2020-08-02 23:07:25+00:00,"Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the ""Water-Filling"" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.",,,,cs.LG,"['cs.LG', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1711.02827v2,Inverse Reward Design,"['Dylan Hadfield-Menell', 'Smitha Milli', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']",2017-11-08 04:44:32+00:00,arxiv,...,44c1c54d551686cc36d57842aeacf703,html,markdownify,2020-10-07 15:41:58+00:00,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.","Advances in Neural Information Processing Systems 30 (NIPS 2017)
  Revised Oct 2020 to fix a typo in Eq. 3",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1805.12573,Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,"['Kelvin Xu', 'Ellis Ratner', 'Anca Dragan', 'Sergey Levine', 'Chelsea Finn']",2018-05-31 17:29:25+00:00,arxiv,...,1de433f2fb00dd8fffd11601f7878224,html,markdownify,2019-10-14 19:55:15+00:00,"A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1812.09376,Human-AI Learning Performance in Multi-Armed Bandits,"['Ravi Pandya', 'Sandy H. Huang', 'Dylan Hadfield-Menell', 'Anca D. Dragan']",2018-12-21 21:28:11+00:00,arxiv,...,a87703ae42626b3074c192e3eab8fcef,html,markdownify,2018-12-21 21:28:11+00:00,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.","Artificial Intelligence, Ethics and Society (AIES) 2019",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1810.05766,Hierarchical Game-Theoretic Planning for Autonomous Vehicles,"['Jaime F. Fisac', 'Eli Bronstein', 'Elis Stefansson', 'Dorsa Sadigh', 'S. Shankar Sastry', 'Anca D. Dragan']",2018-10-13 00:02:54+00:00,arxiv,...,94c1fe81a73f41c406d88c9e898d4317,html,markdownify,2018-10-13 00:02:54+00:00,"The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle's planning and its predictions of other drivers' behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon ""strategic"" game with simplified dynamics and full information structure, and a short-horizon ""tactical"" game with full dynamics and a simplified information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver's ability and incentives to influence each other. In addition, our approach admits non-deterministic models of human decision-making, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.",Submitted to ICRA 2019,,,cs.RO,"['cs.RO', 'cs.AI', 'cs.MA', 'math.OC', '68T40, 93C85, 91A25', 'I.2.9']"
https://arxiv.org/abs/1911.09005v1,Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']",2019-11-20 16:21:12+00:00,arxiv,...,6d3c0d9bf73617e1d05f6ee19efcaaf6,html,markdownify,2019-11-20 16:21:12+00:00,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",To be presented at the AI for Social Good workshop at NeurIPS 2019,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.SY', 'eess.SY']"
https://arxiv.org/abs/1804.04268,Incomplete Contracting and AI Alignment,"['Dylan Hadfield-Menell', 'Gillian Hadfield']",2018-04-12 01:22:50+00:00,arxiv,...,259c872109320317ed1f30fcc5a692ec,html,markdownify,2018-04-12 01:22:50+00:00,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1802.01780,Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration,"['Chang Liu', 'Jessica B. Hamrick', 'Jaime F. Fisac', 'Anca D. Dragan', 'J. Karl Hedrick', 'S. Shankar Sastry', 'Thomas L. Griffiths']",2018-02-06 03:31:23+00:00,arxiv,...,7295d6454ea6c29bfcf68f2c375a60b0,html,markdownify,2018-02-06 03:31:23+00:00,"The study of human-robot interaction is fundamental to the design and use of robotics in real-world applications. Robots will need to predict and adapt to the actions of human collaborators in order to achieve good performance and improve safety and end-user adoption. This paper evaluates a human-robot collaboration scheme that combines the task allocation and motion levels of reasoning: the robotic agent uses Bayesian inference to predict the next goal of its human partner from his or her ongoing motion, and re-plans its own actions in real time. This anticipative adaptation is desirable in many practical scenarios, where humans are unable or unwilling to take on the cognitive overhead required to explicitly communicate their intent to the robot. A behavioral experiment indicates that the combination of goal inference and dynamic task planning significantly improves both objective and perceived performance of the human-robot team. Participants were highly sensitive to the differences between robot behaviors, preferring to work with a robot that adapted to their actions over one that did not.","Published at the International Conference on Autonomous Agents and
  Multiagent Systems (AAMAS 2016)","C. Liu, J. Hamrick, J. Fisac, A. Dragan, J. K. Hedrick, S. Sastry,
  T. Griffiths. ""Goal Inference Improves Objective and Perceived Performance in
  Human-Robot Collaboration"". Autonomous Agents and Multiagent Systems (AAMAS),
  2016",,cs.RO,"['cs.RO', 'cs.AI', 'cs.HC', '68T05', 'I.2.0; I.2.6; I.2.8; I.2.9']"
https://arxiv.org/abs/1810.08174,Establishing Appropriate Trust via Critical States,"['Sandy H. Huang', 'Kush Bhatia', 'Pieter Abbeel', 'Anca D. Dragan']",2018-10-18 17:29:47+00:00,arxiv,...,074acca7c8748b06ef8332593811a548,html,markdownify,2018-10-18 17:29:47+00:00,"In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",IROS 2018,,,cs.RO,['cs.RO']
https://arxiv.org/abs/1810.08167,Expressing Robot Incapability,"['Minae Kwon', 'Sandy H. Huang', 'Anca D. Dragan']",2018-10-18 17:14:04+00:00,arxiv,...,6e98f049d121ff4d19542ea037568bd6,html,markdownify,2020-06-12 14:11:28+00:00,"Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.",HRI 2018,,10.1145/3171221.3171276,cs.RO,['cs.RO']
https://arxiv.org/abs/2101.05507,Evaluating the Robustness of Collaborative Agents,"['Paul Knott', 'Micah Carroll', 'Sam Devlin', 'Kamil Ciosek', 'Katja Hofmann', 'A. D. Dragan', 'Rohin Shah']",2021-01-14 09:02:45+00:00,arxiv,...,a9d67de2a40ffec8eccb70b017b29219,html,markdownify,2021-01-14 09:02:45+00:00,"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', 'cs.MA']"
https://arxiv.org/abs/2007.02823,Dynamic Awareness,"['Joseph Y. Halpern', 'Evan Piermont']",2020-07-06 15:28:22+00:00,arxiv,...,1dcfa5748f4b95cf933c43c81eea456b,html,markdownify,2020-07-06 15:28:22+00:00,"We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $\phi$ in state $s$ of a model $M$, she transitions to state $s^*$ in a model $M^*$. We then discuss how such a model can be applied to information disclosure.","To appear in the 17th International Conference on Principles of
  Knowledge Representation and Reasoning",,,cs.AI,"['cs.AI', 'cs.LO', 'econ.TH']"
https://arxiv.org/abs/2102.03896,Consequences of Misaligned AI,"['Simon Zhuang', 'Dylan Hadfield-Menell']",2021-02-07 19:34:04+00:00,arxiv,...,dd8005c63777c23643ce8c44eed28d6b,html,markdownify,2021-02-07 19:34:04+00:00,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,,NeurIPS 2020,,cs.AI,['cs.AI']
https://arxiv.org/abs/2012.02096,Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design,"['Michael Dennis', 'Natasha Jaques', 'Eugene Vinitsky', 'Alexandre Bayen', 'Stuart Russell', 'Andrew Critch', 'Sergey Levine']",2020-12-03 17:37:01+00:00,arxiv,...,ee46dc16078be20574ff89ed9e180827,html,markdownify,2021-02-04 03:01:31+00:00,"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA']"
https://arxiv.org/abs/1905.09397,Cognitive Model Priors for Predicting Human Decisions,"['David D. Bourgin', 'Joshua C. Peterson', 'Daniel Reichman', 'Thomas L. Griffiths', 'Stuart J. Russell']",2019-05-22 23:05:53+00:00,arxiv,...,bf9b95c9a24aa6ffedc5cafd81b77c32,html,markdownify,2019-05-22 23:05:53+00:00,"Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ""cognitive model priors"" by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.",ICML 2019,"Proceedings of the 36th International Conference on Machine
  Learning, PMLR 97:5133-5141, 2019",,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1912.12613,Asking the Right Questions: Learning Interpretable Action Models Through Query Answering,"['Pulkit Verma', 'Shashank Rao Marpally', 'Siddharth Srivastava']",2019-12-29 09:05:06+00:00,arxiv,...,194af82c6c5c9125295455bbaa66c9b1,html,markdownify,2021-04-09 16:17:14+00:00,"This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent's internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.",AAAI 2021,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1906.02641,An Extensible Interactive Interface for Agent Design,"['Matthew Rahtz', 'James Fang', 'Anca D. Dragan', 'Dylan Hadfield-Menell']",2019-06-06 15:18:40+00:00,arxiv,...,7785dfb46d1906f3a4b9013245fde8d2,html,markdownify,2019-08-08 11:58:45+00:00,"In artificial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach defines a set of increasingly complex policies. The interface allows the user to switch between these policies at fixed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.","Presented at 2019 ICML Workshop on Human in the Loop Learning (HILL
  2019), Long Beach, USA",,,cs.LG,"['cs.LG', 'cs.HC', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1806.03820,"An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning","['Dhruv Malik', 'Malayandi Palaniappan', 'Jaime F. Fisac', 'Dylan Hadfield-Menell', 'Stuart Russell', 'Anca D. Dragan']",2018-06-11 06:06:43+00:00,arxiv,...,9f5d0f680d98169fba69e25eb833430b,html,markdownify,2018-06-11 06:06:43+00:00,"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2006.06547v2,Avoiding Side Effects in Complex Environments,"['Alexander Matt Turner', 'Neale Ratzlaff', 'Prasad Tadepalli']",2020-06-11 16:02:30+00:00,arxiv,...,d74b721fc0ea2227711579c66bd4c478,html,markdownify,2020-10-22 15:15:46+00:00,"Reward function specification can be difficult. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at https://avoiding-side-effects.github.io/.","Accepted as spotlight paper at NeurIPS 2020. 10 pages main paper; 19
  pages with appendices",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2008.02275,Aligning AI With Shared Human Values,"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'Jerry Li', 'Dawn Song', 'Jacob Steinhardt']",2020-08-05 17:59:16+00:00,arxiv,...,3c13721b70be9ef0165e300f9fac0ac7,html,markdownify,2021-07-24 04:40:33+00:00,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.","ICLR 2021; the ETHICS dataset is available at
  https://github.com/hendrycks/ethics/",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/1806.01946,Learning to Understand Goal Specifications by Modelling Reward,"['Dzmitry Bahdanau', 'Felix Hill', 'Jan Leike', 'Edward Hughes', 'Arian Hosseini', 'Pushmeet Kohli', 'Edward Grefenstette']",2018-06-05 22:01:51+00:00,arxiv,...,f0774e7221a566537a654e8865b8e554,html,markdownify,2019-12-23 16:41:02+00:00,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.","19 pages, 9 figures",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1906.08663,Modeling AGI Safety Frameworks with Causal Influence Diagrams,"['Tom Everitt', 'Ramana Kumar', 'Victoria Krakovna', 'Shane Legg']",2019-06-20 14:35:03+00:00,arxiv,...,e6106cce1d699a3ee48dedb797736490,html,markdownify,2019-06-20 14:35:03+00:00,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.",IJCAI 2019 AI Safety Workshop,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1902.02767,Hybrid Models with Deep and Invertible Features,"['Eric Nalisnick', 'Akihiro Matsukawa', 'Yee Whye Teh', 'Dilan Gorur', 'Balaji Lakshminarayanan']",2019-02-07 18:49:47+00:00,arxiv,...,5dd598e8b238139788c09bea76a1a2c1,html,markdownify,2019-05-29 13:52:04+00:00,"We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features), the density of the features, and p(targets | features), the predictive distribution, can be computed exactly in a single feed-forward pass. We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models. Moreover the generative component remains a good model of the input features despite the hybrid optimization objective. This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning. The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.",ICML 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.09136,Do Deep Generative Models Know What They Don't Know?,"['Eric Nalisnick', 'Akihiro Matsukawa', 'Yee Whye Teh', 'Dilan Gorur', 'Balaji Lakshminarayanan']",2018-10-22 08:32:02+00:00,arxiv,...,ad5ee698abe7bafe425e1e9a9306eda4,html,markdownify,2019-02-24 11:57:32+00:00,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",ICLR 2019,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1906.02530,Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift,"['Yaniv Ovadia', 'Emily Fertig', 'Jie Ren', 'Zachary Nado', 'D Sculley', 'Sebastian Nowozin', 'Joshua V. Dillon', 'Balaji Lakshminarayanan', 'Jasper Snoek']",2019-06-06 11:42:53+00:00,arxiv,...,c4e2bc0948810994eb5ac7202a71905a,html,markdownify,2019-12-17 21:30:28+00:00,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.","Advances in Neural Information Processing Systems, 2019",,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1605.03143v1,Avoiding Wireheading with Value Reinforcement Learning,"['Tom Everitt', 'Marcus Hutter']",2016-05-10 18:28:57+00:00,arxiv,...,4651c7a5aadbf88915e2cbabcaae6b97,html,markdownify,2016-05-10 18:28:57+00:00,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",Artificial General Intelligence (AGI) 2016,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1912.02781,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,"['Dan Hendrycks', 'Norman Mu', 'Ekin D. Cubuk', 'Barret Zoph', 'Justin Gilmer', 'Balaji Lakshminarayanan']",2019-12-05 18:18:10+00:00,arxiv,...,b037bc29f4e3c8c3bbbb0bdac0d5429c,html,markdownify,2020-02-17 06:16:13+00:00,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",Code available at https://github.com/google-research/augmix,,,stat.ML,"['stat.ML', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/1909.01492,Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation,"['Po-Sen Huang', 'Robert Stanforth', 'Johannes Welbl', 'Chris Dyer', 'Dani Yogatama', 'Sven Gowal', 'Krishnamurthy Dvijotham', 'Pushmeet Kohli']",2019-09-03 23:03:10+00:00,arxiv,...,a95e8318dfa873e90a4e252125e004cb,html,markdownify,2019-12-20 18:21:49+00:00,"Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation -- a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.",EMNLP 2019,,,cs.CL,"['cs.CL', 'cs.CR', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1811.04017,A generic framework for privacy preserving deep learning,"['Theo Ryffel', 'Andrew Trask', 'Morten Dahl', 'Bobby Wagner', 'Jason Mancuso', 'Daniel Rueckert', 'Jonathan Passerat-Palmbach']",2018-11-09 17:10:47+00:00,arxiv,...,19686dc6dd622fa8570ffeb7bf2d7abc,html,markdownify,2018-11-13 18:11:15+00:00,"We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.","PPML 2018, 5 pages",,,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1510.03370,Asymptotic Logical Uncertainty and The Benford Test,"['Scott Garrabrant', 'Siddharth Bhaskar', 'Abram Demski', 'Joanna Garrabrant', 'George Koleszarik', 'Evan Lloyd']",2015-10-12 17:14:44+00:00,arxiv,...,66ad31d15bb9d4c889e0a7066a4c4f18,html,markdownify,2015-10-12 17:14:44+00:00,"We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs ""true"" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.",,,,cs.LG,"['cs.LG', 'cs.AI', 'F.4.1']"
https://arxiv.org/abs/1805.07805,Constrained Policy Improvement for Safe and Efficient Reinforcement Learning,"['Elad Sarafian', 'Aviv Tamar', 'Sarit Kraus']",2018-05-20 17:47:03+00:00,arxiv,...,53a6861243d82990040099eacf86a389,html,markdownify,2019-07-10 20:12:07+00:00,"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2007.13544,Combining Deep Reinforcement Learning and Search for Imperfect-Information Games,"['Noam Brown', 'Anton Bakhtin', 'Adam Lerer', 'Qucheng Gong']",2020-07-27 15:21:22+00:00,arxiv,...,9606b8e90ad4065cc5f95b1ab39089e7,html,markdownify,2020-11-29 03:18:13+00:00,"The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.",,,,cs.GT,"['cs.GT', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1801.09344,Certified Defenses against Adversarial Examples,"['Aditi Raghunathan', 'Jacob Steinhardt', 'Percy Liang']",2018-01-29 02:08:21+00:00,arxiv,...,6eebfb7962761c9a90e5fdc185e5bb16,html,markdownify,2020-10-31 23:38:30+00:00,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.","Published at the International Conference on Learning Representations
  (ICLR) 2018",,,cs.LG,['cs.LG']
https://arxiv.org/abs/2101.12509,Challenges for Using Impact Regularizers to Avoid Negative Side Effects,"['David Lindner', 'Kyle Matoba', 'Alexander Meulemans']",2021-01-29 10:32:51+00:00,arxiv,...,014ac404d08c9cf0d35ad092de212543,html,markdownify,2021-02-23 13:49:47+00:00,"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",Presented at the SafeAI workshop at AAAI 2021,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2004.06496,Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning,"['Michael Everett', 'Bjorn Lutjens', 'Jonathan P. How']",2020-04-11 21:36:13+00:00,arxiv,...,c0f0d395ae21896724682f80885ba420,html,markdownify,2022-02-02 18:48:37+00:00,"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",arXiv admin note: text overlap with arXiv:1910.12908,,10.1109/TNNLS.2021.3056046,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1812.02953,Building Ethics into Artificial Intelligence,"['Han Yu', 'Zhiqi Shen', 'Chunyan Miao', 'Cyril Leung', 'Victor R. Lesser', 'Qiang Yang']",2018-12-07 09:18:01+00:00,arxiv,...,31d013a788762cb5508dbc6f61d75753,html,markdownify,2018-12-07 09:18:01+00:00,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",,"H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser & Q. Yang,
  ""Building Ethics into Artificial Intelligence,"" in Proceedings of the 27th
  International Joint Conference on Artificial Intelligence (IJCAI'18), pp.
  5527-5533, 2018",,cs.AI,['cs.AI']
https://arxiv.org/abs/1812.03980v1,Building Ethically Bounded AI,"['Francesca Rossi', 'Nicholas Mattei']",2018-12-10 18:58:05+00:00,arxiv,...,0a09a5275e036577d6cd8d1910804e57,html,markdownify,2018-12-10 18:58:05+00:00,"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI's freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven example-based approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.","Published at AAAI Blue Sky Track, winner of Blue Sky Award",,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/2006.10029,Big Self-Supervised Models are Strong Semi-Supervised Learners,"['Ting Chen', 'Simon Kornblith', 'Kevin Swersky', 'Mohammad Norouzi', 'Geoffrey Hinton']",2020-06-17 17:48:22+00:00,arxiv,...,a82fc11d4b5d5d8affb98ab97f307cb0,html,markdownify,2020-10-26 03:09:28+00:00,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.","NeurIPS'2020. Code and pretrained models at
  https://github.com/google-research/simclr",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1903.12261,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,"['Dan Hendrycks', 'Thomas Dietterich']",2019-03-28 20:56:37+00:00,arxiv,...,d211d775715723129877df4e4888ba1a,html,markdownify,2019-03-28 20:56:37+00:00,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.","ICLR 2019 camera-ready; datasets available at
  https://github.com/hendrycks/robustness ; this article supersedes
  arXiv:1807.01697",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1907.04543,An Optimistic Perspective on Offline Reinforcement Learning,"['Rishabh Agarwal', 'Dale Schuurmans', 'Mohammad Norouzi']",2019-07-10 07:23:27+00:00,arxiv,...,9bbbe80ded95628cd78265328ab984c2,html,markdownify,2020-06-22 04:32:50+00:00,"Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.","ICML 2020. An earlier version was titled ""Striving for Simplicity in
  Off-Policy Deep Reinforcement Learning"". Project Website:
  https://offline-rl.github.io","Proceedings of the 37th International Conference on Machine
  Learning, PMLR 119:104-114, 2020",,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1805.01109,AGI Safety Literature Review,"['Tom Everitt', 'Gary Lea', 'Marcus Hutter']",2018-05-03 04:26:48+00:00,arxiv,...,71765d736fb69fc66c8b33bef52445f0,html,markdownify,2018-05-21 16:30:20+00:00,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.","Published in International Joint Conference on Artificial
  Intelligence (IJCAI), 2018",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2001.09773,Algorithmic Fairness from a Non-ideal Perspective,"['Sina Fazelpour', 'Zachary C. Lipton']",2020-01-08 18:44:41+00:00,arxiv,...,75b821eae175e39b64e775b0295363fc,html,markdownify,2020-01-08 18:44:41+00:00,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.","Accepted for publication at the AAAI/ACM Conference on Artificial
  Intelligence, Ethics, and Society (AIES) 2020",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1712.04307,AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values,"['Gopal P. Sarma', 'Nick J. Hay', 'Adam Safron']",2017-12-08 19:40:15+00:00,arxiv,...,f6e363bcc0e92463b74293b5fe655cc8,html,markdownify,2018-09-08 20:44:43+00:00,We propose the creation of a systematic effort to identify and replicate key findings in neuropsychology and allied fields related to understanding human values. Our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of AI systems.,5 pages,"In: Gallina B., Skavhaug A., Schoitsch E., Bitsch F. (eds)
  Computer Safety, Reliability, and Security. SAFECOMP 2018. Lecture Notes in
  Computer Science, vol 11094. Springer, Cham",10.1007/978-3-319-99229-7_45,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/1806.04067,Adaptive Mechanism Design: Learning to Promote Cooperation,"['Tobias Baumann', 'Thore Graepel', 'John Shawe-Taylor']",2018-06-11 15:48:37+00:00,arxiv,...,b9bce4c1df060ee21b16e3467b4549a0,html,markdownify,2019-11-20 11:14:13+00:00,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.",,,,cs.GT,"['cs.GT', 'cs.AI']"
https://arxiv.org/abs/1805.08915v1,A Psychopathological Approach to Safety Engineering in AI and AGI,"['Vahid Behzadan', 'Arslan Munir', 'Roman V. Yampolskiy']",2018-05-23 00:19:07+00:00,arxiv,...,3312c3309d5e355342a3e974a037b530,html,markdownify,2018-05-23 00:19:07+00:00,"The complexity of dynamics in AI techniques is already approaching that of complex adaptive systems, thus curtailing the feasibility of formal controllability and reachability analysis in the context of AI safety. It follows that the envisioned instances of Artificial General Intelligence (AGI) will also suffer from challenges of complexity. To tackle such issues, we propose the modeling of deleterious behaviors in AI and AGI as psychological disorders, thereby enabling the employment of psychopathological approaches to analysis and control of misbehaviors. Accordingly, we present a discussion on the feasibility of the psychopathological approaches to AI safety, and propose general directions for research on modeling, diagnosis, and treatment of psychological disorders in AGI.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1712.04172v2,A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,"['Yueh-Hua Wu', 'Shou-De Lin']",2017-12-12 08:35:52+00:00,arxiv,...,8d38dd5f63123e80b210e46bae0fb5e1,html,markdownify,2018-09-10 04:59:19+00:00,"This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.",AAAI 2018 Oral Presentation,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1507.01986v1,Toward Idealized Decision Theory,"['Nate Soares', 'Benja Fallenstein']",2015-07-07 23:06:59+00:00,arxiv,...,675712158a5982defee9c8fc5ddb6200,html,markdownify,2015-07-07 23:06:59+00:00,"This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.",This is an extended version of a paper accepted to AGI-2015,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2001.00078,Regulatory Markets for AI Safety,"['Jack Clark', 'Gillian K. Hadfield']",2019-12-11 19:21:54+00:00,arxiv,...,7d0c6aa899929810141b017564ed01a4,html,markdownify,2019-12-11 19:21:54+00:00,We propose a new model for regulation to achieve AI safety: global regulatory markets. We first sketch the model in general terms and provide an overview of the costs and benefits of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones.,,,,cs.CY,"['cs.CY', 'econ.GN', 'q-fin.EC']"
https://arxiv.org/abs/1605.03142,Self-Modification of Policy and Utility Function in Rational Agents,"['Tom Everitt', 'Daniel Filan', 'Mayank Daswani', 'Marcus Hutter']",2016-05-10 18:25:49+00:00,arxiv,...,cb0a36eadb47b6492cf40f3e96f1a2cb,html,markdownify,2016-05-10 18:25:49+00:00,"Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.",Artificial General Intelligence (AGI) 2016,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1805.08313,Learning Safe Policies with Expert Guidance,"['Jessie Huang', 'Fa Wu', 'Doina Precup', 'Yang Cai']",2018-05-21 22:40:07+00:00,arxiv,...,1725c2f0ca5bcc5c29f02a30ccea25a6,html,markdownify,2018-11-21 17:17:23+00:00,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",Appears in NeurIPS 2018,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.10071,Learning Existing Social Conventions via Observationally Augmented Self-Play,"['Adam Lerer', 'Alexander Peysakhovich']",2018-06-26 15:46:44+00:00,arxiv,...,fdabad1fe19dc7e87af3fce75812ce71,html,markdownify,2019-03-13 17:48:23+00:00,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",Published in AAAI-AIES2019 - Best Paper,,,cs.AI,"['cs.AI', 'cs.GT']"
https://arxiv.org/abs/1803.08287,Learning-based Model Predictive Control for Safe Exploration,"['Torsten Koller', 'Felix Berkenkamp', 'Matteo Turchetta', 'Andreas Krause']",2018-03-22 09:41:45+00:00,arxiv,...,42cc70133d90e4765a472f417e6766c6,html,markdownify,2018-11-07 11:08:25+00:00,"Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.","Proc. of the Conference on Decision and Control, 2018",,,cs.SY,"['cs.SY', 'cs.AI', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/1811.03493,"Integrative Biological Simulation, Neuropsychology, and AI Safety","['Gopal P. Sarma', 'Adam Safron', 'Nick J. Hay']",2018-11-07 01:38:24+00:00,arxiv,...,471994392e4da2d677179414fcf395b8,html,markdownify,2019-01-21 19:04:47+00:00,"We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.",5 pages,"Proceedings of the AAAI Workshop on Artificial Intelligence Safety
  2019 co-located with the Thirty-Third AAAI Conference on Artificial
  Intelligence 2019 (AAAI 2019)",,cs.AI,"['cs.AI', 'cs.LG', 'cs.NE', 'q-bio.NC']"
https://arxiv.org/abs/1901.00064v3,Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),['Peter Eckersley'],2018-12-31 23:51:27+00:00,arxiv,...,251d546b08e57ade147353dcc7947f33,html,markdownify,2019-03-05 03:12:49+00:00,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense.   We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.","Published in SafeAI 2019: Proceedings of the AAAI Workshop on
  Artificial Intelligence Safety 2019",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1106.2657,I Don't Want to Think About it Now:Decision Theory With Costly Computation,"['Joseph Y. Halpern', 'Rafael Pass']",2011-06-14 09:52:38+00:00,arxiv,...,b5a841cab9e01fb890a03e3c11316034,html,markdownify,2011-06-14 09:52:38+00:00,"Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and and computational value of conversation.",In Conference on Knowledge Representation and Reasoning (KR '10),,,cs.GT,['cs.GT']
https://arxiv.org/abs/2005.11295,From ImageNet to Image Classification: Contextualizing Progress on Benchmarks,"['Dimitris Tsipras', 'Shibani Santurkar', 'Logan Engstrom', 'Andrew Ilyas', 'Aleksander Madry']",2020-05-22 17:39:16+00:00,arxiv,...,4905dfbb0113d9067e68dd125a882a3d,html,markdownify,2020-05-22 17:39:16+00:00,"Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.",,,,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1409.0813,Friendly Artificial Intelligence: the Physics Challenge,['Max Tegmark'],2014-09-02 18:20:28+00:00,arxiv,...,558bf2638243f42f157e70f50892fd30,html,markdownify,2014-09-03 15:05:07+00:00,"Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent ""Friendly AI"", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is ""meaning"" in a particle arrangement? What is ""life""? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.",3 pages,"In proceedings of the AAAI 2015 Workshop On AI and Ethics, p87,
  Toby Walsh, Ed. (2015)",,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1912.05743,Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,"['Akanksha Atrey', 'Kaleigh Clary', 'David Jensen']",2019-12-09 12:42:07+00:00,arxiv,...,c7994e4236edd3b0d2c53099562d6d55,html,markdownify,2020-02-20 21:40:15+00:00,"Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",Published at ICLR 2020,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2005.01831v1,Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,"['Peter Hase', 'Mohit Bansal']",2020-05-04 20:35:17+00:00,arxiv,...,ac545d878f5aae2fee71eafe6ff3e881,html,markdownify,2020-05-04 20:35:17+00:00,"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020",ACL 2020 (13 pages),,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2006.14804,Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation,"['Lin Guan', 'Mudit Verma', 'Sihang Guo', 'Ruohan Zhang', 'Subbarao Kambhampati']",2020-06-26 05:40:05+00:00,arxiv,...,98da18ebc1741cc5eb528bdbf122617a,html,markdownify,2021-10-26 19:16:10+00:00,"Human explanation (e.g., in terms of feature importance) has been recently used to extend the communication channel between human and agent in interactive machine learning. Under this setting, human trainers provide not only the ground truth but also some form of explanation. However, this kind of human guidance was only investigated in supervised learning tasks, and it remains unclear how to best incorporate this type of human knowledge into deep reinforcement learning. In this paper, we present the first study of using human visual explanations in human-in-the-loop reinforcement learning (HRL). We focus on the task of learning from feedback, in which the human trainer not only gives binary evaluative ""good"" or ""bad"" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the model to encode task-relevant features through a context-aware data augmentation that only perturbs irrelevant features in human salient information. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate the performance and sample efficiency of this approach. We show that our method significantly outperforms methods leveraging human explanation that are adapted from supervised learning, and Human-in-the-loop RL baselines that only utilize evaluative feedback.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2002.08484,Estimating Training Data Influence by Tracing Gradient Descent,"['Garima Pruthi', 'Frederick Liu', 'Mukund Sundararajan', 'Satyen Kale']",2020-02-19 22:40:32+00:00,arxiv,...,c68c9c7532443a54a3d5e2b393c11cb8,html,markdownify,2020-11-14 18:47:35+00:00,"We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.",NeurIPS 2020,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1811.05590,Emergence of Addictive Behaviors in Reinforcement Learning Agents,"['Vahid Behzadan', 'Roman V. Yampolskiy', 'Arslan Munir']",2018-11-14 01:30:00+00:00,arxiv,...,4da2ec96da5101f12368eeed94241ea6,html,markdownify,2018-11-14 01:30:00+00:00,"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1704.02882,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"['El Mahdi El Mhamdi', 'Rachid Guerraoui', 'Hadrien Hendrikx', 'Alexandre Maurer']",2017-04-10 14:38:37+00:00,arxiv,...,1af18e7b03e265c1234912cbbbd83d7d,html,markdownify,2017-05-22 11:01:28+00:00,"In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.MA', 'stat.ML']"
https://arxiv.org/abs/1808.04096,Directed Policy Gradient for Safe Reinforcement Learning with Human Advice,"['HÃ©lÃ¨ne Plisnier', 'Denis Steckelmacher', 'Tim Brys', 'Diederik M. Roijers', 'Ann NowÃ©']",2018-08-13 08:12:22+00:00,arxiv,...,3800c0b0ca249c139e5edce0ecea573a,html,markdownify,2018-08-13 08:12:22+00:00,"Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.","Accepted at the European Workshop on Reinforcement Learning 2018
  (EWRL14)",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2001.10208,Towards Learning Multi-agent Negotiations via Self-Play,['Yichuan Charlie Tang'],2020-01-28 08:37:33+00:00,arxiv,...,b9a1c9b7e121363abab7c34d984ba429,html,markdownify,2020-01-28 08:37:33+00:00,"Making sophisticated, robust, and safe sequential decisions is at the heart of intelligent systems. This is especially critical for planning in complex multi-agent environments, where agents need to anticipate other agents' intentions and possible future actions. Traditional methods formulate the problem as a Markov Decision Process, but the solutions often rely on various assumptions and become brittle when presented with corner cases. In contrast, deep reinforcement learning (Deep RL) has been very effective at finding policies by simultaneously exploring, interacting, and learning from environments. Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. We demonstrate this in a challenging multi-agent simulation of merging traffic, where agents must interact and negotiate with others in order to successfully merge on or off the road. While the environment starts off simple, we increase its complexity by iteratively adding an increasingly diverse set of agents to the agent ""zoo"" as training progresses. Qualitatively, we find that through self-play, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents. In addition, quantitatively, we show a dramatic improvement of the success rate of merging maneuvers from 63% to over 98%.","Autonomous Driving Workshop, IEEE International Conference on
  Computer Vision (ICCV 2019)",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.MA']"
https://arxiv.org/abs/2002.04833,Reward-rational (implicit) choice: A unifying formalism for reward learning,"['Hong Jun Jeon', 'Smitha Milli', 'Anca D. Dragan']",2020-02-12 08:07:49+00:00,arxiv,...,1d7c3b0c1b121d82a590c647f82f022f,html,markdownify,2020-12-11 17:56:03+00:00,"It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",Published at NeurIPS 2020,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', 'cs.RO']"
https://arxiv.org/abs/1805.07914,Imitating Latent Policies from Observation,"['Ashley D. Edwards', 'Himanshu Sahni', 'Yannick Schroecker', 'Charles L. Isbell']",2018-05-21 06:49:57+00:00,arxiv,...,6922a053ee9c67005d950c932e0cb1a6,html,markdownify,2019-05-13 07:13:07+00:00,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",Accepted to ICML 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1909.09314v2,Meta-Inverse Reinforcement Learning with Probabilistic Context Variables,"['Lantao Yu', 'Tianhe Yu', 'Chelsea Finn', 'Stefano Ermon']",2019-09-20 04:22:13+00:00,arxiv,...,00e091563bc71f76868c345fb2f56ef9,html,markdownify,2019-10-26 21:15:42+00:00,"Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",NeurIPS 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2006.13258v6,Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,"['Paul Barde', 'Julien Roy', 'Wonseok Jeon', 'Joelle Pineau', 'Christopher Pal', 'Derek Nowrouzezahrai']",2020-06-23 18:29:13+00:00,arxiv,...,f8e48ea358b02bd13942274f8d289e5c,html,markdownify,2021-04-16 10:09:13+00:00,"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.",,Advances in Neural Information Processing Systems 33 (2020),,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1908.01007,Improving Deep Reinforcement Learning in Minecraft with Action Advice,"['Spencer Frazier', 'Mark Riedl']",2019-08-02 18:36:44+00:00,arxiv,...,f10a18e795cff540ed455903bc6293fa,html,markdownify,2019-08-02 18:36:44+00:00,"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1909.05863,Finding Generalizable Evidence by Learning to Convince Q&A Models,"['Ethan Perez', 'Siddharth Karamcheti', 'Rob Fergus', 'Jason Weston', 'Douwe Kiela', 'Kyunghyun Cho']",2019-09-12 18:00:00+00:00,arxiv,...,9046260d0c9cb546ce05b1fb7fa5ec5b,html,markdownify,2019-09-12 18:00:00+00:00,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",EMNLP 2019. Code available at https://github.com/ethanjperez/convince,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.MA']"
https://arxiv.org/abs/1804.09160,No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,"['Xin Wang', 'Wenhu Chen', 'Yuan-Fang Wang', 'William Yang Wang']",2018-04-24 17:41:24+00:00,arxiv,...,1599d99c0ac7d4586cbcfdea77453550,html,markdownify,2018-07-09 00:15:14+00:00,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.","ACL 2018. 15 pages, 10 figures, 4 tables, with supplementary material",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/2002.09571,Learning to Continually Learn,"['Shawn Beaulieu', 'Lapo Frati', 'Thomas Miconi', 'Joel Lehman', 'Kenneth O. Stanley', 'Jeff Clune', 'Nick Cheney']",2020-02-21 22:52:00+00:00,arxiv,...,284e419c20b9f6b6fc7ab3ecc3adeb9d,html,markdownify,2020-03-04 03:22:48+00:00,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).",,,,cs.LG,"['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1907.11932,Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,"['Di Jin', 'Zhijing Jin', 'Joey Tianyi Zhou', 'Peter Szolovits']",2019-07-27 15:07:04+00:00,arxiv,...,16e5df3d9c1c145e7325b69d73d07653,html,markdownify,2020-04-08 23:10:10+00:00,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",AAAI 2020 (Oral),,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1912.09729,Mastering Complex Control in MOBA Games with Deep Reinforcement Learning,"['Deheng Ye', 'Zhao Liu', 'Mingfei Sun', 'Bei Shi', 'Peilin Zhao', 'Hao Wu', 'Hongsheng Yu', 'Shaojie Yang', 'Xipeng Wu', 'Qingwei Guo', 'Qiaobo Chen', 'Yinyuting Yin', 'Hao Zhang', 'Tengfei Shi', 'Liang Wang', 'Qiang Fu', 'Wei Yang', 'Lanxiao Huang']",2019-12-20 09:56:50+00:00,arxiv,...,82a7686478a55e78b651fd5c113ad136,html,markdownify,2020-12-15 14:21:40+00:00,"We study the reinforcement learning problem of complex action control in the Multi-player Online Battle Arena (MOBA) 1v1 games. This problem involves far more complicated state and action spaces than those of traditional 1v1 games, such as Go and Atari series, which makes it very difficult to search any policies with human-level performance. In this paper, we present a deep reinforcement learning framework to tackle this problem from the perspectives of both system and algorithm. Our system is of low coupling and high scalability, which enables efficient explorations at large scale. Our algorithm includes several novel strategies, including control dependency decoupling, action mask, target attention, and dual-clip PPO, with which our proposed actor-critic network can be effectively trained in our system. Tested on the MOBA game Honor of Kings, our AI agent, called Tencent Solo, can defeat top professional human players in full 1v1 games.",AAAI 2020,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/2002.09815,Neuron Shapley: Discovering the Responsible Neurons,"['Amirata Ghorbani', 'James Zou']",2020-02-23 03:29:58+00:00,arxiv,...,034753706a7cbc30a87248733d6d9b3f,html,markdownify,2020-11-13 22:06:48+00:00,"We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.",,,,stat.ML,"['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']"
https://arxiv.org/abs/1905.01296,PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,"['Nicholas Rhinehart', 'Rowan McAllister', 'Kris Kitani', 'Sergey Levine']",2019-05-03 17:54:09+00:00,arxiv,...,55f4e9360af8cfa2d65e2402557cfa86,html,markdownify,2019-09-30 16:45:21+00:00,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.","To appear at the IEEE International Conference on Computer Vision
  (ICCV 2019). Website: https://sites.google.com/view/precog",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1811.03516,Learning from Demonstration in the Wild,"['Feryal Behbahani', 'Kyriacos Shiarlis', 'Xi Chen', 'Vitaly Kurin', 'Sudhanshu Kasewa', 'Ciprian Stirbu', 'JoÃ£o Gomes', 'Supratik Paul', 'Frans A. Oliehoek', 'JoÃ£o Messias', 'Shimon Whiteson']",2018-11-08 16:03:23+00:00,arxiv,...,d3ed5e7548be8fb6c3643c634819f612,html,markdownify,2019-03-26 00:11:48+00:00,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.","Accepted to the IEEE International Conference on Robotics and
  Automation (ICRA) 2019; extended version with appendix",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1804.10692,Reward Learning from Narrated Demonstrations,"['Hsiao-Yu Fish Tung', 'Adam W. Harley', 'Liang-Kang Huang', 'Katerina Fragkiadaki']",2018-04-27 21:26:08+00:00,arxiv,...,004b1388cfc80ab31c425ac0725ec468,html,markdownify,2018-04-27 21:26:08+00:00,"Humans effortlessly ""program"" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.","The work has been accepted to Conference on Computer Vision and
  Pattern Recognition (CVPR) 2018",,,cs.CV,"['cs.CV', 'cs.RO']"
https://arxiv.org/abs/1906.10918,Towards Empathic Deep Q-Learning,"['Bart Bussmann', 'Jacqueline Heinerman', 'Joel Lehman']",2019-06-26 08:59:02+00:00,arxiv,...,20ee0cd0daa8c433610ef0ee991ba795,html,markdownify,2019-06-26 08:59:02+00:00,"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",To be presented as a poster at the IJCAI-19 AI Safety Workshop,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']"
https://arxiv.org/abs/1901.06085,Theory of Minds: Understanding Behavior in Groups Through Inverse Planning,"['Michael Shum', 'Max Kleiman-Weiner', 'Michael L. Littman', 'Joshua B. Tenenbaum']",2019-01-18 04:50:08+00:00,arxiv,...,7ab8cef1ded541fab4e85aa18cac1d02,html,markdownify,2019-01-18 04:50:08+00:00,"Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multi-agent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do.","published in AAAI 2019; Michael Shum and Max Kleiman-Weiner
  contributed equally",,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/2001.04465,LESS is More: Rethinking Probabilistic Models of Human Behavior,"['Andreea Bobu', 'Dexter R. R. Scobee', 'Jaime F. Fisac', 'S. Shankar Sastry', 'Anca D. Dragan']",2020-01-13 18:59:01+00:00,arxiv,...,429c0162dd722fe8015e43328cb7951d,html,markdownify,2020-01-13 18:59:01+00:00,"Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.","9 pages, 7 figures",,10.1145/3319502.3374811,cs.RO,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1807.09936,Multi-Agent Generative Adversarial Imitation Learning,"['Jiaming Song', 'Hongyu Ren', 'Dorsa Sadigh', 'Stefano Ermon']",2018-07-26 03:21:49+00:00,arxiv,...,e5dd7a1675e81b4aa4bf45fa5aa40505,html,markdownify,2018-07-26 03:21:49+00:00,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']"
https://arxiv.org/abs/1904.07854,End-to-End Robotic Reinforcement Learning without Reward Engineering,"['Avi Singh', 'Larry Yang', 'Kristian Hartikainen', 'Chelsea Finn', 'Sergey Levine']",2019-04-16 17:59:23+00:00,arxiv,...,89ac3c296ec6240f53062e117d87a37a,html,markdownify,2019-05-16 00:00:22+00:00,"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.","Accepted to RSS 2019. 14 pages and 13 figures including references
  and appendix. Website: https://sites.google.com/view/reward-learning-rl/home",,,cs.LG,"['cs.LG', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1903.02020,Using Natural Language for Reward Shaping in Reinforcement Learning,"['Prasoon Goyal', 'Scott Niekum', 'Raymond J. Mooney']",2019-03-05 19:20:35+00:00,arxiv,...,5da73c9755d566468b937d9b93db905a,html,markdownify,2019-05-31 04:58:07+00:00,"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",IJCAI 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1503.07619,Shared Autonomy via Hindsight Optimization,"['Shervin Javdani', 'Siddhartha S. Srinivasa', 'J. Andrew Bagnell']",2015-03-26 04:50:49+00:00,arxiv,...,eaca18172a8b69af0b38e4892415dc5e,html,markdownify,2015-04-17 20:20:50+00:00,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",,,,cs.RO,['cs.RO']
https://arxiv.org/abs/1904.06387,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,"['Daniel S. Brown', 'Wonjoon Goo', 'Prabhat Nagarajan', 'Scott Niekum']",2019-04-12 19:34:43+00:00,arxiv,...,2bde27603e24e6245a14de413e650982,html,markdownify,2019-07-09 03:51:47+00:00,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.","In proceedings of Thirty-sixth International Conference on Machine
  Learning (ICML 2019)",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.04303,Batch Active Preference-Based Learning of Reward Functions,"['Erdem BÄ±yÄ±k', 'Dorsa Sadigh']",2018-10-10 00:02:55+00:00,arxiv,...,b2bd7671a2be8c71c4958b22617bd674,html,markdownify,2018-10-10 00:02:55+00:00,"Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.","Proceedings of the 2nd Conference on Robot Learning (CoRL), October
  2018",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1905.11979v2,Causal Confusion in Imitation Learning,"['Pim de Haan', 'Dinesh Jayaraman', 'Sergey Levine']",2019-05-28 17:56:19+00:00,arxiv,...,851172699acfd6e4756761f53ad9dc13,html,markdownify,2019-11-04 12:59:24+00:00,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive ""causal misidentification"" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.","Published at NeurIPS 2019 9 pages, plus references and appendices",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2004.04136,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,"['Aravind Srinivas', 'Michael Laskin', 'Pieter Abbeel']",2020-04-08 17:40:43+00:00,arxiv,...,b9801d7b1064a3e2cade957c6dcfeba8,html,markdownify,2020-09-21 15:34:30+00:00,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://github.com/MishaLaskin/curl.","First two authors contributed equally, website:
  https://mishalaskin.github.io/curl code: https://github.com/MishaLaskin/curl",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1811.00525,On the Geometry of Adversarial Examples,"['Marc Khoury', 'Dylan Hadfield-Menell']",2018-11-01 17:47:10+00:00,arxiv,...,5a0909c04d90a14241cac8f2a3d06d13,html,markdownify,2018-12-11 21:43:30+00:00,"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.",Improvements to clarity and presentation over initial submission,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1804.00097,Adversarial Attacks and Defences Competition,"['Alexey Kurakin', 'Ian Goodfellow', 'Samy Bengio', 'Yinpeng Dong', 'Fangzhou Liao', 'Ming Liang', 'Tianyu Pang', 'Jun Zhu', 'Xiaolin Hu', 'Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Zhou Ren', 'Alan Yuille', 'Sangxia Huang', 'Yao Zhao', 'Yuzhe Zhao', 'Zhonglin Han', 'Junjiajia Long', 'Yerkebulan Berdibekov', 'Takuya Akiba', 'Seiya Tokui', 'Motoki Abe']",2018-03-31 00:52:20+00:00,arxiv,...,f3a21a4aeb18ec7077b3d352d29cf4b9,html,markdownify,2018-03-31 00:52:20+00:00,"To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.","36 pages, 10 figures",,,cs.CV,"['cs.CV', 'cs.CR', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1907.03046,Learning a Behavioral Repertoire from Demonstrations,"['Niels Justesen', 'Miguel Gonzalez Duque', 'Daniel Cabarcas Jaramillo', 'Jean-Baptiste Mouret', 'Sebastian Risi']",2019-07-05 23:08:08+00:00,arxiv,...,1c79f3d4ba4b00e8cfddc8259516ed96,html,markdownify,2019-07-05 23:08:08+00:00,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1911.00459,Positive-Unlabeled Reward Learning,"['Danfei Xu', 'Misha Denil']",2019-11-01 16:47:44+00:00,arxiv,...,6fc1f2bad339982c8872b6632f533600,html,markdownify,2019-11-01 16:47:44+00:00,"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1907.03843v2,Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,"['Pedro Fernandes', 'Francisco C. Santos', 'Manuel Lopes']",2019-06-26 10:18:19+00:00,arxiv,...,75fb9002ede977c133d747507036cd7d,html,markdownify,2020-12-22 18:11:35+00:00,"The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.",,"AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020",10.3233/AIC-201502,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2006.15191,"Is SGD a Bayesian sampler? Well, almost","['Chris Mingard', 'Guillermo Valle-PÃ©rez', 'Joar Skalse', 'Ard A. Louis']",2020-06-26 19:45:36+00:00,arxiv,...,1f03e309b1fe9944ba52cbf6925f9841,html,markdownify,2020-10-24 13:28:11+00:00,"Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$.   Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime.   While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",,"Journal of Machine Learning Research, 22 79 (2021), 1-64",,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2004.09044,"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","['Yixin Zhu', 'Tao Gao', 'Lifeng Fan', 'Siyuan Huang', 'Mark Edmonds', 'Hangxin Liu', 'Feng Gao', 'Chi Zhang', 'Siyuan Qi', 'Ying Nian Wu', 'Joshua B. Tenenbaum', 'Song-Chun Zhu']",2020-04-20 04:07:28+00:00,arxiv,...,ceba2ca531e2f5c651cc5aac926577c5,html,markdownify,2020-04-20 04:07:28+00:00,"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.","For high quality figures, please refer to
  http://wellyzhang.github.io/attach/dark.pdf","Engineering, Feb, 2020",10.1016/j.eng.2020.01.011,cs.AI,"['cs.AI', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/1604.00289,Building Machines That Learn and Think Like People,"['Brenden M. Lake', 'Tomer D. Ullman', 'Joshua B. Tenenbaum', 'Samuel J. Gershman']",2016-04-01 15:37:57+00:00,arxiv,...,d9cd028f256b4be19a9be114293b8a05,html,markdownify,2016-11-02 17:26:50+00:00,"Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","In press at Behavioral and Brain Sciences. Open call for commentary
  proposals (until Nov. 22, 2016).
  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary",,,cs.AI,"['cs.AI', 'cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1705.08807,When Will AI Exceed Human Performance? Evidence from AI Experts,"['Katja Grace', 'John Salvatier', 'Allan Dafoe', 'Baobao Zhang', 'Owain Evans']",2017-05-24 15:00:20+00:00,arxiv,...,136bc3f177a2e89d6c21c6b4faa095f6,html,markdownify,2018-05-03 20:14:21+00:00,"Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.","Accepted by Journal of Artificial Intelligence Research (AI and
  Society Track). Minor update to refer to related work (page 5)",,,cs.AI,"['cs.AI', 'cs.CY']"
https://arxiv.org/abs/1707.08747,A Formal Approach to the Problem of Logical Non-Omniscience,"['Scott Garrabrant', 'Tsvi Benson-Tilsen', 'Andrew Critch', 'Nate Soares', 'Jessica Taylor']",2017-07-27 07:49:01+00:00,arxiv,...,c350d8e2f6dec97d5b1bb8d8c101a396,html,markdownify,2017-07-27 07:49:01+00:00,"We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.","In Proceedings TARK 2017, arXiv:1707.08250","EPTCS 251, 2017, pp. 221-235",10.4204/EPTCS.251.16,cs.LO,"['cs.LO', 'F.4.0; G.3']"
https://arxiv.org/abs/2112.00861,A General Language Assistant as a Laboratory for Alignment,"['Amanda Askell', 'Yuntao Bai', 'Anna Chen', 'Dawn Drain', 'Deep Ganguli', 'Tom Henighan', 'Andy Jones', 'Nicholas Joseph', 'Ben Mann', 'Nova DasSarma', 'Nelson Elhage', 'Zac Hatfield-Dodds', 'Danny Hernandez', 'Jackson Kernion', 'Kamal Ndousse', 'Catherine Olsson', 'Dario Amodei', 'Tom Brown', 'Jack Clark', 'Sam McCandlish', 'Chris Olah', 'Jared Kaplan']",2021-12-01 22:24:34+00:00,arxiv,...,d6d7e67218e8bb8c1a79834c6fed6708,html,markdownify,2021-12-09 21:40:22+00:00,"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.","26+19 pages; v2 typos fixed, refs added, figure scale / colors fixed;
  v3 correct very non-standard TruthfulQA formatting and metric, alignment
  implications slightly improved",,,cs.CL,"['cs.CL', 'cs.LG']"
https://arxiv.org/abs/1308.3778,Game Theory with Translucent Players,"['Joseph Y. Halpern', 'Rafael Pass']",2013-08-17 12:29:53+00:00,arxiv,...,5c65f4e609accf8aba9f6e6bccb2922d,html,markdownify,2013-08-17 12:29:53+00:00,"A traditional assumption in game theory is that players are opaque to one another---if a player changes strategies, then this change in strategies does not affect the choice of other players' strategies. In many situations this is an unrealistic assumption. We develop a framework for reasoning about games where the players may be translucent to one another; in particular, a player may believe that if she were to change strategies, then the other player would also change strategies. Translucent players may achieve significantly more efficient outcomes than opaque ones.   Our main result is a characterization of strategies consistent with appropriate analogues of common belief of rationality. Common Counterfactual Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone counterfactually believes that everyone else is rational (i.e., all players i believe that everyone else would still be rational even if $i$ were to switch strategies), (3) everyone counterfactually believes that everyone else is rational, and counterfactually believes that everyone else is rational, and so on. CCBR characterizes the set of strategies surviving iterated removal of minimax dominated strategies, where a strategy s for player i is minimax dominated by s' if the worst-case payoff for i using s' is better than the best possible payoff using s.","Extended version of a paper that appear in the Conference on
  Theoretical Aspects of Rationality and Knowledge, 2013",,,cs.GT,['cs.GT']
https://arxiv.org/abs/1702.03465v2,Enabling Robots to Communicate their Objectives,"['Sandy H. Huang', 'David Held', 'Pieter Abbeel', 'Anca D. Dragan']",2017-02-11 22:39:39+00:00,arxiv,...,f56b3eff91ea2eb51ad07c3cf01c60bf,html,markdownify,2018-10-18 17:43:04+00:00,"The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.",RSS 2017,,10.15607/RSS.2017.XIII.059,cs.RO,"['cs.RO', 'cs.LG']"
https://arxiv.org/abs/2007.04068,Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,"['Shakir Mohamed', 'Marie-Therese Png', 'William Isaac']",2020-07-08 12:36:21+00:00,arxiv,...,4900ab6f299115e915b16463342afe70,html,markdownify,2020-07-08 12:36:21+00:00,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.","28 Pages. Accepted, to appear in: Philosophy and Technology (405),
  Springer. Submitted 16 January, Accepted 26 May 2020",,10.1007/s13347-020-00405-8,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2006.03357v2,Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent,"['Michael K. Cohen', 'Elliot Catt', 'Marcus Hutter']",2020-06-05 10:42:29+00:00,arxiv,...,b75c7c62928c866b2952389d11617b38,html,markdownify,2021-05-26 15:55:28+00:00,"Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either ""destroyed"" or ""incapacitated"" with probability 1. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent, Mentee, with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration. Critically, Mentee's exploration probability depends on the expected information gain from exploring. In a simple non-ergodic environment with a weak mentor, we find Mentee outperforms existing asymptotically optimal agents and its mentor.","13 pages, with 5 page appendix; 3 figures",Journal of Selected Areas in Information Theory 2 (2021),,cs.LG,"['cs.LG', 'cs.AI', 'I.2.0; I.2.6']"
https://arxiv.org/abs/2001.09768,"Artificial Intelligence, Values and Alignment",['Iason Gabriel'],2020-01-13 10:32:16+00:00,arxiv,...,b1f1cf2bffab1214159cfc17c2ea580d,html,markdownify,2020-10-05 12:03:19+00:00,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",,Minds and Machines 2020,10.1007/s11023-020-09539-2,cs.CY,['cs.CY']
https://arxiv.org/abs/1902.02918,Certified Adversarial Robustness via Randomized Smoothing,"['Jeremy M Cohen', 'Elan Rosenfeld', 'J. Zico Kolter']",2019-02-08 02:08:19+00:00,arxiv,...,48b45c5353cb57cd6f1c3b2877612d74,html,markdownify,2019-06-15 07:40:33+00:00,"We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\ell_2$ norm. This ""randomized smoothing"" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.",ICML 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1202.6177,Can Intelligence Explode?,['Marcus Hutter'],2012-02-28 10:46:29+00:00,arxiv,...,8acc1d17c0b945ffcbe4af2a91cb322d,html,markdownify,2012-02-28 10:46:29+00:00,"The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.",20 LaTeX pages,"Journal of Consciousness Studies, 19:1-2 (2012) 143-166",,cs.AI,"['cs.AI', 'physics.soc-ph']"
https://arxiv.org/abs/2006.01855,Aligning Superhuman AI with Human Behavior: Chess as a Model System,"['Reid McIlroy-Young', 'Siddhartha Sen', 'Jon Kleinberg', 'Ashton Anderson']",2020-06-02 18:12:52+00:00,arxiv,...,94cb8ba50e034bd4d252d8cfa71c0369,html,markdownify,2020-07-14 17:57:37+00:00,"As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance.   We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well.   We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.","11 pages, 11 figure, Proceedings of the 25th ACM SIGKDD international
  conference on Knowledge discovery and data mining, Virtual 2020",,10.1145/3394486.3403219,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/1607.08289v4,Mammalian Value Systems,"['Gopal P. Sarma', 'Nick J. Hay']",2016-07-28 01:22:26+00:00,arxiv,...,fdec8f4ccedb8c9f1c8e41d95ffff130,html,markdownify,2019-01-21 19:29:30+00:00,"Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical ""intelligence explosion,"" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The ""value alignment problem"" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of ""mammalian value systems"" points to a potential avenue for fundamental research in AI safety and AI ethics.",12 pages,Informatica Vol. 41 No. 3 (2017),,cs.AI,"['cs.AI', 'cs.CY', 'cs.HC', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/1901.11184,Human-Centered Artificial Intelligence and Machine Learning,['Mark O. Riedl'],2019-01-31 02:47:16+00:00,arxiv,...,01a4bdb2c9a90cfa0984fcc61e7fd158,html,markdownify,2019-01-31 02:47:16+00:00,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","Human Behavior and Emerging Technologies, volume 1",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2101.10305,Accumulating Risk Capital Through Investing in Cooperation,"['Charlotte Roman', 'Michael Dennis', 'Andrew Critch', 'Stuart Russell']",2021-01-25 18:41:45+00:00,arxiv,...,515699d236e0d175af4f6d3472fc5119,html,markdownify,2021-04-21 00:37:42+00:00,"Recent work on promoting cooperation in multi-agent learning has resulted in many methods which successfully promote cooperation at the cost of becoming more vulnerable to exploitation by malicious actors. We show that this is an unavoidable trade-off and propose an objective which balances these concerns, promoting both safety and long-term cooperation. Moreover, the trade-off between safety and cooperation is not severe, and you can receive exponentially large returns through cooperation from a small amount of risk. We study both an exact solution method and propose a method for training policies that targets this objective, Accumulating Risk Capital Through Investing in Cooperation (ARCTIC), and evaluate them in iterated Prisoner's Dilemma and Stag Hunt.",,,,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/2103.03386v1,Clusterability in Neural Networks,"['Daniel Filan', 'Stephen Casper', 'Shlomi Hod', 'Cody Wild', 'Andrew Critch', 'Stuart Russell']",2021-03-04 23:53:53+00:00,arxiv,...,cb97c3dddd707ff04f61b06976c041c5,html,markdownify,2021-03-04 23:53:53+00:00,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.","20 pages, 22 figures. arXiv admin note: text overlap with
  arXiv:2003.04881",,,cs.NE,['cs.NE']
https://arxiv.org/abs/2110.08058,Quantifying Local Specialization in Deep Neural Networks,"['Shlomi Hod', 'Daniel Filan', 'Stephen Casper', 'Andrew Critch', 'Stuart Russell']",2021-10-13 20:33:30+00:00,arxiv,...,ea254dd9448794548f703985acf40074,html,markdownify,2022-02-07 20:46:48+00:00,"A neural network is locally specialized to the extent that parts of its computational graph (i.e. structure) can be abstractly represented as performing some comprehensible sub-task relevant to the overall task (i.e. functionality). Are modern deep neural networks locally specialized? How can this be quantified? In this paper, we consider the problem of taking a neural network whose neurons are partitioned into clusters, and quantifying how functionally specialized the clusters are. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal local specialization and that statistical methods can be used to automatedly screen for sets of neurons that can be understood abstractly.","21 pages, 6 figures. Code is available at
  https://github.com/thestephencasper/detecting_nn_modularity",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']"
https://arxiv.org/abs/2104.03946v2,Learning What To Do by Simulating the Past,"['David Lindner', 'Rohin Shah', 'Pieter Abbeel', 'Anca Dragan']",2021-04-08 17:43:29+00:00,arxiv,...,3fec6df69a0bcaff46af6f54f58e087d,html,markdownify,2021-05-03 10:51:40+00:00,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",Presented at ICLR 2021,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2111.01705,AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements,"['Carolyn Ashurst', 'Emmie Hine', 'Paul Sedille', 'Alexis Carlier']",2021-11-02 16:17:12+00:00,arxiv,...,3bdf2f6d2da63c706cb5c4bdec175f9a,html,markdownify,2021-11-02 16:17:12+00:00,"Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",,,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2102.08686,Fully General Online Imitation Learning,"['Michael K. Cohen', 'Marcus Hutter', 'Neel Nanda']",2021-02-17 10:57:37+00:00,arxiv,...,09611c30781fbb84436e38659271bcee,html,markdownify,2022-10-04 15:57:04+00:00,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. In general, one mistake during learning can lead to completely different events. In the special setting of environments that restart, existing work provides formal guidance in how to imitate so that events unfold similarly, but outside that setting, no formal guidance exists. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes, and we allow our imitator to learn online from the demonstrator. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency. If any such event qualifies as ""dangerous"", our imitator would have the notable distinction of being relatively ""safe"".",13 pages with 8-page appendix,,,cs.LG,"['cs.LG', 'cs.AI', 'I.2.0; I.2.6']"
https://arxiv.org/abs/2001.07118,The Incentives that Shape Behaviour,"['Ryan Carey', 'Eric Langlois', 'Tom Everitt', 'Shane Legg']",2020-01-20 14:32:07+00:00,arxiv,...,eba56b5f8b8379d62a7d934cd084e2f6,html,markdownify,2021-03-15 20:02:54+00:00,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",In SafeAI workshop at AAAI. Superseded by arXiv:2102.01685,,,cs.AI,"['cs.AI', 'cs.LG', 'I.2.6; I.2.8']"
https://arxiv.org/abs/2004.07213,Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,"['Miles Brundage', 'Shahar Avin', 'Jasmine Wang', 'Haydn Belfield', 'Gretchen Krueger', 'Gillian Hadfield', 'Heidy Khlaaf', 'Jingying Yang', 'Helen Toner', 'Ruth Fong', 'Tegan Maharaj', 'Pang Wei Koh', 'Sara Hooker', 'Jade Leung', 'Andrew Trask', 'Emma Bluemke', 'Jonathan Lebensold', ""Cullen O'Keefe"", 'Mark Koren', 'ThÃ©o Ryffel', 'JB Rubinovitz', 'Tamay Besiroglu', 'Federica Carugati', 'Jack Clark', 'Peter Eckersley', 'Sarah de Haas', 'Maritza Johnson', 'Ben Laurie', 'Alex Ingerman', 'Igor Krawczuk', 'Amanda Askell', 'Rosario Cammarota', 'Andrew Lohn', 'David Krueger', 'Charlotte Stix', 'Peter Henderson', 'Logan Graham', 'Carina Prunkl', 'Bianca Martin', 'Elizabeth Seger', 'Noa Zilberman', 'SeÃ¡n Ã hÃigeartaigh', 'Frens Kroeger', 'Girish Sastry', 'Rebecca Kagan', 'Adrian Weller', 'Brian Tse', 'Elizabeth Barnes', 'Allan Dafoe', 'Paul Scharre', 'Ariel Herbert-Voss', 'Martijn Rasser', 'Shagun Sodhani', 'Carrick Flynn', 'Thomas Krendl Gilbert', 'Lisa Dyer', 'Saif Khan', 'Yoshua Bengio', 'Markus Anderljung']",2020-04-15 17:15:35+00:00,arxiv,...,d41072472c5bfb0fec576dffd390f812,html,markdownify,2020-04-20 19:10:58+00:00,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",,,,cs.CY,['cs.CY']
https://arxiv.org/abs/1712.05812v6,Occam's razor is insufficient to infer the preferences of irrational agents,"['Stuart Armstrong', 'SÃ¶ren Mindermann']",2017-12-15 19:05:01+00:00,arxiv,...,44642ae9abcc6de95ed2b2644b4a3c1e,html,markdownify,2019-01-11 14:36:40+00:00,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1803.04926,Active Reinforcement Learning with Monte-Carlo Tree Search,"['Sebastian Schulze', 'Owain Evans']",2018-03-13 16:35:25+00:00,arxiv,...,009261bf8debe5c75dc977f8a6922fea,html,markdownify,2018-03-26 16:11:56+00:00,"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.","11 pages, 10 figures",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1806.00610v2,Between Progress and Potential Impact of AI: the Neglected Dimensions,"['Fernando MartÃ­nez-Plumed', 'Shahar Avin', 'Miles Brundage', 'Allan Dafoe', 'Sean Ã hÃigeartaigh', 'JosÃ© HernÃ¡ndez-Orallo']",2018-06-02 09:21:12+00:00,arxiv,...,366f61b6e0bb5dbf6ef82971c52640f6,html,markdownify,2022-07-02 09:54:55+00:00,"We reframe the analysis of progress in AI by incorporating into an overall framework both the task performance of a system, and the time and resource costs incurred in the development and deployment of the system. These costs include: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, and (what kind of) time. These costs are distributed over the life cycle of the system, and may place differing demands on different developers and users. The multidimensional performance and cost space we present can be collapsed to a single utility metric that measures the value of the system for different stakeholders. Even without a single utility function, AI advances can be generically assessed by whether they expand the Pareto surface. We label these types of costs as neglected dimensions of AI progress, and explore them using four case studies: Alpha* (Go, Chess, and other board games), ALE (Atari games), ImageNet (Image classification) and Virtual Personal Assistants (Siri, Alexa, Cortana, and Google Assistant). This broader model of progress in AI will lead to novel ways of estimating the potential societal use and impact of an AI system, and the establishment of milestones for future progress.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1705.10557,Universal Reinforcement Learning Algorithms: Survey and Experiments,"['John Aslanides', 'Jan Leike', 'Marcus Hutter']",2017-05-30 11:41:00+00:00,arxiv,...,6b09a5a47f2ad43e95d6d7be3596eaa9,html,markdownify,2017-05-30 11:41:00+00:00,"Many state-of-the-art reinforcement learning (RL) algorithms typically assume that the environment is an ergodic Markov Decision Process (MDP). In contrast, the field of universal reinforcement learning (URL) is concerned with algorithms that make as few assumptions as possible about the environment. The universal Bayesian agent AIXI and a family of related URL algorithms have been developed in this setting. While numerous theoretical optimality results have been proven for these agents, there has been no empirical investigation of their behavior to date. We present a short and accessible survey of these URL algorithms under a unified notation and framework, along with results of some experiments that qualitatively illustrate some properties of the resulting policies, and their relative performance on partially-observable gridworld environments. We also present an open-source reference implementation of the algorithms which we hope will facilitate further understanding of, and experimentation with, these ideas.","8 pages, 6 figures, Twenty-sixth International Joint Conference on
  Artificial Intelligence (IJCAI-17)",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1906.01820,Risks from Learned Optimization in Advanced Machine Learning Systems,"['Evan Hubinger', 'Chris van Merwijk', 'Vladimir Mikulik', 'Joar Skalse', 'Scott Garrabrant']",2019-06-05 04:43:25+00:00,arxiv,...,1d1eb0566ea8d5f0940af7403e6dc039,html,markdownify,2021-12-01 11:22:52+00:00,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2105.06551v1,Axes for Sociotechnical Inquiry in AI Research,"['Sarah Dean', 'Thomas Krendl Gilbert', 'Nathan Lambert', 'Tom Zick']",2021-04-26 16:49:04+00:00,arxiv,...,c8b051da7ba803b2bdbb6509c30a3e9e,html,markdownify,2021-04-26 16:49:04+00:00,"The development of artificial intelligence (AI) technologies has far exceeded the investigation of their relationship with society. Sociotechnical inquiry is needed to mitigate the harms of new technologies whose potential impacts remain poorly understood. To date, subfields of AI research develop primarily individual views on their relationship with sociotechnics, while tools for external investigation, comparison, and cross-pollination are lacking. In this paper, we propose four directions for inquiry into new and evolving areas of technological development: value--what progress and direction does a field promote, optimization--how the defined system within a problem formulation relates to broader dynamics, consensus--how agreement is achieved and who is included in building it, and failure--what methods are pursued when the problem specification is found wanting. The paper provides a lexicon for sociotechnical inquiry and illustrates it through the example of consumer drone technology.","9 pages, 1 figure",,10.1109/TTS.2021.3074097,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2102.12092,Zero-Shot Text-to-Image Generation,"['Aditya Ramesh', 'Mikhail Pavlov', 'Gabriel Goh', 'Scott Gray', 'Chelsea Voss', 'Alec Radford', 'Mark Chen', 'Ilya Sutskever']",2021-02-24 06:42:31+00:00,arxiv,...,4ce6c5a5dd176d4afda3ce1cb0a2649e,html,markdownify,2021-02-26 23:26:05+00:00,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",,,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/2107.03374,Evaluating Large Language Models Trained on Code,"['Mark Chen', 'Jerry Tworek', 'Heewoo Jun', 'Qiming Yuan', 'Henrique Ponde de Oliveira Pinto', 'Jared Kaplan', 'Harri Edwards', 'Yuri Burda', 'Nicholas Joseph', 'Greg Brockman', 'Alex Ray', 'Raul Puri', 'Gretchen Krueger', 'Michael Petrov', 'Heidy Khlaaf', 'Girish Sastry', 'Pamela Mishkin', 'Brooke Chan', 'Scott Gray', 'Nick Ryder', 'Mikhail Pavlov', 'Alethea Power', 'Lukasz Kaiser', 'Mohammad Bavarian', 'Clemens Winter', 'Philippe Tillet', 'Felipe Petroski Such', 'Dave Cummings', 'Matthias Plappert', 'Fotios Chantzis', 'Elizabeth Barnes', 'Ariel Herbert-Voss', 'William Hebgen Guss', 'Alex Nichol', 'Alex Paino', 'Nikolas Tezak', 'Jie Tang', 'Igor Babuschkin', 'Suchir Balaji', 'Shantanu Jain', 'William Saunders', 'Christopher Hesse', 'Andrew N. Carr', 'Jan Leike', 'Josh Achiam', 'Vedant Misra', 'Evan Morikawa', 'Alec Radford', 'Matthew Knight', 'Miles Brundage', 'Mira Murati', 'Katie Mayer', 'Peter Welinder', 'Bob McGrew', 'Dario Amodei', 'Sam McCandlish', 'Ilya Sutskever', 'Wojciech Zaremba']",2021-07-07 17:41:24+00:00,arxiv,...,d6c35b597c3e2bd3af3a0eb938b123f7,html,markdownify,2021-07-14 17:16:02+00:00,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","corrected typos, added references, added authors, added
  acknowledgements",,,cs.LG,['cs.LG']
https://arxiv.org/abs/1606.01540,OpenAI Gym,"['Greg Brockman', 'Vicki Cheung', 'Ludwig Pettersson', 'Jonas Schneider', 'John Schulman', 'Jie Tang', 'Wojciech Zaremba']",2016-06-05 17:54:48+00:00,arxiv,...,7a4828fe30751ed5f31d116e20dc3b90,html,markdownify,2016-06-05 17:54:48+00:00,"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2005.14165,Language Models are Few-Shot Learners,"['Tom B. Brown', 'Benjamin Mann', 'Nick Ryder', 'Melanie Subbiah', 'Jared Kaplan', 'Prafulla Dhariwal', 'Arvind Neelakantan', 'Pranav Shyam', 'Girish Sastry', 'Amanda Askell', 'Sandhini Agarwal', 'Ariel Herbert-Voss', 'Gretchen Krueger', 'Tom Henighan', 'Rewon Child', 'Aditya Ramesh', 'Daniel M. Ziegler', 'Jeffrey Wu', 'Clemens Winter', 'Christopher Hesse', 'Mark Chen', 'Eric Sigler', 'Mateusz Litwin', 'Scott Gray', 'Benjamin Chess', 'Jack Clark', 'Christopher Berner', 'Sam McCandlish', 'Alec Radford', 'Ilya Sutskever', 'Dario Amodei']",2020-05-28 17:29:03+00:00,arxiv,...,4d2c578f1ba2e8e3c9199e70c5b0e85b,html,markdownify,2020-07-22 19:47:17+00:00,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",40+32 pages,,,cs.CL,['cs.CL']
https://arxiv.org/abs/2107.12808,Open-Ended Learning Leads to Generally Capable Agents,"['Open Ended Learning Team', 'Adam Stooke', 'Anuj Mahajan', 'Catarina Barros', 'Charlie Deck', 'Jakob Bauer', 'Jakub Sygnowski', 'Maja Trebacz', 'Max Jaderberg', 'Michael Mathieu', 'Nat McAleese', 'Nathalie Bradley-Schmieg', 'Nathaniel Wong', 'Nicolas Porcel', 'Roberta Raileanu', 'Steph Hughes-Fitt', 'Valentin Dalibard', 'Wojciech Marian Czarnecki']",2021-07-27 13:30:07+00:00,arxiv,...,31ba39d537999b18d972e649e26312dc,html,markdownify,2021-07-31 16:55:19+00:00,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA']"
https://arxiv.org/abs/2102.01293,Scaling Laws for Transfer,"['Danny Hernandez', 'Jared Kaplan', 'Tom Henighan', 'Sam McCandlish']",2021-02-02 04:07:38+00:00,arxiv,...,704a033311d3d4cdbd1d2bbcea6fc580,html,markdownify,2021-02-02 04:07:38+00:00,"We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.","19 pages, 15 figures",,,cs.LG,['cs.LG']
https://arxiv.org/abs/2006.14032,Compositional Explanations of Neurons,"['Jesse Mu', 'Jacob Andreas']",2020-06-24 20:37:05+00:00,arxiv,...,9a5c5237b9fe16c2fbb5a6fd4f14d36c,html,markdownify,2021-02-02 23:46:51+00:00,"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",NeurIPS 2020,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1811.10597,GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,"['David Bau', 'Jun-Yan Zhu', 'Hendrik Strobelt', 'Bolei Zhou', 'Joshua B. Tenenbaum', 'William T. Freeman', 'Antonio Torralba']",2018-11-26 18:59:07+00:00,arxiv,...,a6f481b92bf76ff8b4a6e4af125e0fef,html,markdownify,2018-12-08 22:56:10+00:00,"Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.   In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.","18 pages, 19 figures",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']"
https://arxiv.org/abs/1811.07807,Deeper Interpretability of Deep Networks,"['Tian Xu', 'Jiayu Zhan', 'Oliver G. B. Garrod', 'Philip H. S. Torr', 'Song-Chun Zhu', 'Robin A. A. Ince', 'Philippe G. Schyns']",2018-11-19 17:10:44+00:00,arxiv,...,1c374d827c3125057964b341917dc798,html,markdownify,2018-11-20 09:43:21+00:00,"Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. There is an increasing demand for explainable AI as these systems are deployed in the real world. However, understanding the information represented and processed in CNNs remains in most cases challenging. Within this paper, we explore the use of new information theoretic techniques developed in the field of neuroscience to enable novel understanding of how a CNN represents information. We trained a 10-layer ResNet architecture to identify 2,000 face identities from 26M images generated using a rigorously controlled 3D face rendering model that produced variations of intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors (i.e. 3D pose, illumination, scale and 2D translation). With our methodology, we demonstrate that unlike human's network overgeneralizes face identities even with extreme changes of face shape, but it is more sensitive to changes of texture. To understand the processing of information underlying these counterintuitive properties, we visualize the features of shape and texture that the network processes to identify faces. Then, we shed a light into the inner workings of the black box and reveal how hidden layers represent these features and whether the representations are invariant to pose. We hope that our methodology will provide an additional valuable tool for interpretability of CNNs.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1911.01547,On the Measure of Intelligence,['FranÃ§ois Chollet'],2019-11-05 00:31:38+00:00,arxiv,...,1bef6b4d36dee952307f19e9f46627b8,html,markdownify,2019-11-25 13:02:04+00:00,"To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ""buy"" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1705.08439,Thinking Fast and Slow with Deep Learning and Tree Search,"['Thomas Anthony', 'Zheng Tian', 'David Barber']",2017-05-23 17:48:51+00:00,arxiv,...,95b73a5b81faddd2448e98721d864f06,html,markdownify,2017-12-03 10:56:00+00:00,"Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.","v1 to v2: - Add a value function in MCTS - Some MCTS hyper-parameters
  changed - Repetition of experiments: improved accuracy and errors shown.
  (note the reduction in effect size for the tpt/cat experiment) - Results from
  a longer training run, including changes in expert strength in training -
  Comparison to MoHex. v3: clarify independence of ExIt and AG0. v4: see
  appendix E",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1702.08608,Towards A Rigorous Science of Interpretable Machine Learning,"['Finale Doshi-Velez', 'Been Kim']",2017-02-28 02:19:20+00:00,arxiv,...,3f522fe1c042185e0d418b788fec9a27,html,markdownify,2017-03-02 19:32:10+00:00,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1710.05060,Functional Decision Theory: A New Theory of Instrumental Rationality,"['Eliezer Yudkowsky', 'Nate Soares']",2017-10-13 19:51:38+00:00,arxiv,...,dc766717acdd4600ebcea3cddf8817ef,html,markdownify,2018-05-22 21:07:53+00:00,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2105.06791,Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations,"['Matthew Watson', 'Bashar Awwad Shiekh Hasan', 'Noura Al Moubayed']",2021-05-14 12:16:47+00:00,arxiv,...,0d86352868d031c8ffade0ff144370f5,html,markdownify,2021-10-31 00:08:57+00:00,"Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: $\approx$ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.","9 pages, 5 figures, 3 tables",,,cs.LG,"['cs.LG', 'I.2']"
https://arxiv.org/abs/2009.09153,Hidden Incentives for Auto-Induced Distributional Shift,"['David Krueger', 'Tegan Maharaj', 'Jan Leike']",2020-09-19 03:31:27+00:00,arxiv,...,86c1e71a7d5beb20a480ede2273e23b6,html,markdownify,2020-09-19 03:31:27+00:00,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1912.01217,SafeLife 1.0: Exploring Side Effects in Complex Environments,"['Carroll L. Wainwright', 'Peter Eckersley']",2019-12-03 06:44:48+00:00,arxiv,...,762888e45ac57548cb772c46e762fb05,html,markdownify,2021-02-26 05:49:51+00:00,"We present SafeLife, a publicly available reinforcement learning environment that tests the safety of reinforcement learning agents. It contains complex, dynamic, tunable, procedurally generated levels with many opportunities for unsafe behavior. Agents are graded both on their ability to maximize their explicit reward and on their ability to operate safely without unnecessary side effects. We train agents to maximize rewards using proximal policy optimization and score them on a suite of benchmark levels. The resulting agents are performant but not safe -- they tend to cause large side effects in their environments -- but they form a baseline against which future safety research can be measured.","Updated version was presented at the AAAI SafeAI 2020 Workshop, but
  now with updated contact info. Previously presented at the 2019 NeurIPS
  Safety and Robustness in Decision Making Workshop","CEUR Workshop Proceedings, 2560 (2020) 117-127",,cs.AI,['cs.AI']
https://arxiv.org/abs/2003.13350,Agent57: Outperforming the Atari Human Benchmark,"['AdriÃ  PuigdomÃ¨nech Badia', 'Bilal Piot', 'Steven Kapturowski', 'Pablo Sprechmann', 'Alex Vitvitskyi', 'Daniel Guo', 'Charles Blundell']",2020-03-30 11:33:16+00:00,arxiv,...,68823201efde77cdbadfab55e77c0692,html,markdownify,2020-03-30 11:33:16+00:00,"Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1807.04723,The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach,"['Iulian Vlad Serban', 'Chinnadhurai Sankar', 'Michael Pieper', 'Joelle Pineau', 'Yoshua Bengio']",2018-07-12 16:59:28+00:00,arxiv,...,92f1f3b974d4108647747042e880c387,html,markdownify,2018-07-12 16:59:28+00:00,"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.","26 pages, 2 figures, 4 tables",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.NE', 'stat.ML', 'I.5.1; I.2.7']"
https://arxiv.org/abs/1811.04551,Learning Latent Dynamics for Planning from Pixels,"['Danijar Hafner', 'Timothy Lillicrap', 'Ian Fischer', 'Ruben Villegas', 'David Ha', 'Honglak Lee', 'James Davidson']",2018-11-12 04:30:10+00:00,arxiv,...,19df673dd245b1e21bfcae65c1019a30,html,markdownify,2019-06-04 18:13:09+00:00,"Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.","20 pages, 12 figures, 1 table",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.05635,Self-Imitation Learning,"['Junhyuk Oh', 'Yijie Guo', 'Satinder Singh', 'Honglak Lee']",2018-06-14 16:25:55+00:00,arxiv,...,2a0d5fd39a78045e424e7e852794ffdb,html,markdownify,2018-06-14 16:25:55+00:00,"This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.10019,Adversarial Active Exploration for Inverse Dynamics Model Learning,"['Zhang-Wei Hong', 'Tsu-Jui Fu', 'Tzu-Yun Shann', 'Yi-Hsiang Chang', 'Chun-Yi Lee']",2018-06-26 14:33:22+00:00,arxiv,...,7cf390201178dbe2275c128b78aff365,html,markdownify,2020-03-17 03:48:55+00:00,"We present an adversarial active exploration for inverse dynamics model learning, a simple yet effective learning scheme that incentivizes exploration in an environment without any human intervention. Our framework consists of a deep reinforcement learning (DRL) agent and an inverse dynamics model contesting with each other. The former collects training samples for the latter, with an objective to maximize the error of the latter. The latter is trained with samples collected by the former, and generates rewards for the former when it fails to predict the actual action taken by the former. In such a competitive setting, the DRL agent learns to generate samples that the inverse dynamics model fails to predict correctly, while the inverse dynamics model learns to adapt to the challenging samples. We further propose a reward structure that ensures the DRL agent to collect only moderately hard samples but not overly hard ones that prevent the inverse model from predicting effectively. We evaluate the effectiveness of our method on several robotic arm and hand manipulation tasks against multiple baseline models. Experimental results show that our method is comparable to those directly trained with expert demonstrations, and superior to the other baselines even without any human priors.",Published as a conference paper at CoRL 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.07857,RUDDER: Return Decomposition for Delayed Rewards,"['Jose A. Arjona-Medina', 'Michael Gillhofer', 'Michael Widrich', 'Thomas Unterthiner', 'Johannes Brandstetter', 'Sepp Hochreiter']",2018-06-20 17:34:07+00:00,arxiv,...,dcc1d1135472b646469cc72faba75ba2,html,markdownify,2019-09-10 16:27:52+00:00,"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({\lambda}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.",9 Pages plus appendix. For videos https://goo.gl/EQerZV,,,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']"
https://arxiv.org/abs/2006.05990,What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,"['Marcin Andrychowicz', 'Anton Raichuk', 'Piotr StaÅczyk', 'Manu Orsini', 'Sertan Girgin', 'Raphael Marinier', 'LÃ©onard Hussenot', 'Matthieu Geist', 'Olivier Pietquin', 'Marcin Michalski', 'Sylvain Gelly', 'Olivier Bachem']",2020-06-10 17:59:03+00:00,arxiv,...,2a886ab2e9ed71d4679b3f432afc6983,html,markdownify,2020-06-10 17:59:03+00:00,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2010.14496,Generative Temporal Difference Learning for Infinite-Horizon Prediction,"['Michael Janner', 'Igor Mordatch', 'Sergey Levine']",2020-10-27 17:54:12+00:00,arxiv,...,cda251c9a66ebb789921bab509820977,html,markdownify,2021-11-29 00:51:39+00:00,"We introduce the $\gamma$-model, a predictive model of environment dynamics with an infinite probabilistic horizon. Replacing standard single-step models with $\gamma$-models leads to generalizations of the procedures central to model-based control, including the model rollout and model-based value estimation. The $\gamma$-model, trained with a generative reinterpretation of temporal difference learning, is a natural continuous analogue of the successor representation and a hybrid between model-free and model-based mechanisms. Like a value function, it contains information about the long-term future; like a standard predictive model, it is independent of task reward. We instantiate the $\gamma$-model as both a generative adversarial network and normalizing flow, discuss how its training reflects an inescapable tradeoff between training-time and testing-time compounding errors, and empirically investigate its utility for prediction and control.",NeurIPS 2020. Project page at: https://gammamodels.github.io/,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2106.02039,Offline Reinforcement Learning as One Big Sequence Modeling Problem,"['Michael Janner', 'Qiyang Li', 'Sergey Levine']",2021-06-03 17:58:51+00:00,arxiv,...,6699c2ef7581c5251f1ac8deaed0880d,html,markdownify,2021-11-29 00:56:52+00:00,"Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.","NeurIPS 2021 (spotlight). Project page and code at:
  https://trajectory-transformer.github.io/",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1806.05695,Evolving simple programs for playing Atari games,"['Dennis G Wilson', 'Sylvain Cussat-Blanc', 'HervÃ© Luga', 'Julian F Miller']",2018-06-14 18:10:46+00:00,arxiv,...,36396d9ca620ef7625126ff4e68add34,html,markdownify,2018-06-14 18:10:46+00:00,"Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but effective strategies can be found.",,,,cs.NE,"['cs.NE', 'cs.AI']"
https://arxiv.org/abs/1807.01672,Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization,"['Alexandre Laterre', 'Yunguan Fu', 'Mohamed Khalil Jabri', 'Alain-Sam Cohen', 'David Kas', 'Karl Hajjar', 'Torbjorn S. Dahl', 'Amine Kerkeni', 'Karim Beguir']",2018-07-04 16:40:53+00:00,arxiv,...,08bf3f79fd7a63d390ba5c06591908bb,html,markdownify,2018-12-06 23:32:05+00:00,"Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",,"Presented at the Thirty-second Conference on Neural Information
  Processing Systems (NeurIPS 2018), Deep Reinforcement Learning Workshop,
  Montreal, Canada, December 3-8, 2018",,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.10729,Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation,"['Niels Justesen', 'Ruben Rodriguez Torrado', 'Philip Bontrager', 'Ahmed Khalifa', 'Julian Togelius', 'Sebastian Risi']",2018-06-28 01:16:11+00:00,arxiv,...,a8bc8f125005f8278501ea2e13a546ac,html,markdownify,2018-11-29 18:10:13+00:00,"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.",Accepted to NeurIPS Deep RL Workshop 2018,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2106.01345v2,Decision Transformer: Reinforcement Learning via Sequence Modeling,"['Lili Chen', 'Kevin Lu', 'Aravind Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'Michael Laskin', 'Pieter Abbeel', 'Aravind Srinivas', 'Igor Mordatch']",2021-06-02 17:53:39+00:00,arxiv,...,63175412e0f927d9a603a25810902fe4,html,markdownify,2021-06-24 17:09:59+00:00,"We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.","First two authors contributed equally. Last two authors advised
  equally",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1809.02591,Learning Invariances for Policy Generalization,"['Remi Tachet', 'Philip Bachman', 'Harm van Seijen']",2018-09-07 17:32:19+00:00,arxiv,...,6e4eb456aa6f0a2beead6030dde2347d,html,markdownify,2020-12-12 12:57:19+00:00,"While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.","7 pages, 1 figure",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1807.03571,A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees,"['Min Wu', 'Matthew Wicker', 'Wenjie Ruan', 'Xiaowei Huang', 'Marta Kwiatkowska']",2018-07-10 11:28:46+00:00,arxiv,...,b38fa7de0d3d630ae5bb7533488f4de2,html,markdownify,2019-03-06 22:21:11+00:00,"Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",,Theoretical Computer Science 807 (2020) 298-329,10.1016/j.tcs.2019.05.046,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2104.00739,Formal Methods for the Informal Engineer: Workshop Recommendations,"['Gopal Sarma', 'James Koppel', 'Gregory Malecha', 'Patrick Schultz', 'Eric Drexler', 'Ramana Kumar', 'Cody Roux', 'Philip Zucker']",2021-04-01 19:22:42+00:00,arxiv,...,081880a74746ee041dbce00bff50d88d,html,markdownify,2021-04-01 19:22:42+00:00,"Formal Methods for the Informal Engineer (FMIE) was a workshop held at the Broad Institute of MIT and Harvard in 2021 to explore the potential role of verified software in the biomedical software ecosystem. The motivation for organizing FMIE was the recognition that the life sciences and medicine are undergoing a transition from being passive consumers of software and AI/ML technologies to fundamental drivers of new platforms, including those which will need to be mission and safety-critical. Drawing on conversations leading up to and during the workshop, we make five concrete recommendations to help software leaders organically incorporate tools, techniques, and perspectives from formal methods into their project planning and development trajectories.",6 pages,,10.31219/osf.io/t4qs8,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG', 'cs.PL', 'q-bio.OT']"
https://arxiv.org/abs/1811.02625,MixTrain: Scalable Training of Verifiably Robust Neural Networks,"['Shiqi Wang', 'Yizheng Chen', 'Ahmed Abdou', 'Suman Jana']",2018-11-06 20:47:28+00:00,arxiv,...,4427b23a8d22a020f8ff6ac7e44a7418,html,markdownify,2018-12-01 23:52:52+00:00,"Making neural networks robust against adversarial inputs has resulted in an arms race between new defenses and attacks. The most promising defenses, adversarially robust training and verifiably robust training, have limitations that restrict their practical applications. The adversarially robust training only makes the networks robust against a subclass of attackers and we reveal such weaknesses by developing a new attack based on interval gradients. By contrast, verifiably robust training provides protection against any L-p norm-bounded attacker but incurs orders of magnitude more computational and memory overhead than adversarially robust training.   We propose two novel techniques, stochastic robust approximation and dynamic mixed training, to drastically improve the efficiency of verifiably robust training without sacrificing verified robustness. We leverage two critical insights: (1) instead of over the entire training set, sound over-approximations over randomly subsampled training data points are sufficient for efficiently guiding the robust training process; and (2) We observe that the test accuracy and verifiable robustness often conflict after certain training epochs. Therefore, we use a dynamic loss function to adaptively balance them for each epoch.   We designed and implemented our techniques as part of MixTrain and evaluated it on six networks trained on three popular datasets including MNIST, CIFAR, and ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\%$ verified robust accuracy against $L_\infty$ norm-bounded attackers while taking $15$ and $3$ times less training time than state-of-the-art verifiably robust training and adversarially robust training schemes, respectively. Furthermore, MixTrain easily scales to larger networks like the one trained on ImageNet-200, significantly outperforming the existing verifiably robust training methods.",,,,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1807.00403,Towards Mixed Optimization for Reinforcement Learning with Program Synthesis,"['Surya Bhupatiraju', 'Kumar Krishna Agrawal', 'Rishabh Singh']",2018-07-01 21:52:07+00:00,arxiv,...,0d37551c2b465b76bb3324843534a1cb,html,markdownify,2018-07-03 22:08:06+00:00,"Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.","Updated publication details, format. Accepted at NAMPI workshop, ICML
  '18",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1904.09959,Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness,"['Greg Anderson', 'Shankara Pailoor', 'Isil Dillig', 'Swarat Chaudhuri']",2019-04-22 17:21:52+00:00,arxiv,...,474e4faaaa6ca9bcc26cd73f33ca86ae,html,markdownify,2019-05-01 15:25:46+00:00,"In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and ({\delta}-)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2 , Reluplex, and Reluval.",,,10.1145/3314221.3314614,cs.PL,"['cs.PL', 'cs.LG']"
https://arxiv.org/abs/1805.10265,Training verified learners with learned verifiers,"['Krishnamurthy Dvijotham', 'Sven Gowal', 'Robert Stanforth', 'Relja Arandjelovic', ""Brendan O'Donoghue"", 'Jonathan Uesato', 'Pushmeet Kohli']",2018-05-25 17:35:39+00:00,arxiv,...,a1c2e7419f576345411220ebae5dabea,html,markdownify,2018-05-29 09:48:05+00:00,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1711.07356,Evaluating Robustness of Neural Networks with Mixed Integer Programming,"['Vincent Tjeng', 'Kai Xiao', 'Russ Tedrake']",2017-11-20 15:05:33+00:00,arxiv,...,c08f238e0ec32c4d283bbf4dad7b574d,html,markdownify,2019-02-18 04:39:10+00:00,"Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples - slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional networks with an order of magnitude more ReLUs than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_\infty$ norm $\epsilon=0.1$: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness (to perturbations with bounded norm) for the remainder. Across all robust training procedures and network architectures considered, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.",Accepted as a conference paper at ICLR 2019,,,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']"
https://arxiv.org/abs/1810.10525,Toward an AI Physicist for Unsupervised Learning,"['Tailin Wu', 'Max Tegmark']",2018-10-24 17:59:57+00:00,arxiv,...,884477a107690db578ade5384d4229e1,html,markdownify,2019-09-02 01:18:25+00:00,"We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's razor, unification and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and ""snap"" learned theories into simple symbolic formulas. Theories are stored in a ""theory hub"", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the toy ""AI Physicist"" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.","Replaced to match accepted PRE version. Added references, improved
  discussion. 22 pages, 7 figs","Phys. Rev. E 100, 033311 (2019)",10.1103/PhysRevE.100.033311,physics.comp-ph,"['physics.comp-ph', 'cond-mat.dis-nn', 'cs.LG']"
https://arxiv.org/abs/1808.06508,Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"['Alessandro Achille', 'Tom Eccles', 'Loic Matthey', 'Christopher P. Burgess', 'Nick Watters', 'Alexander Lerchner', 'Irina Higgins']",2018-08-20 15:15:32+00:00,arxiv,...,c448289134de1523539da6279a5d094c,html,markdownify,2018-08-20 15:15:32+00:00,"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2003.04297,Improved Baselines with Momentum Contrastive Learning,"['Xinlei Chen', 'Haoqi Fan', 'Ross Girshick', 'Kaiming He']",2020-03-09 17:56:49+00:00,arxiv,...,4c18c1a36b08200e40b6f69d77ae5827,html,markdownify,2020-03-09 17:56:49+00:00,"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.","Tech report, 2 pages + references",,,cs.CV,['cs.CV']
https://arxiv.org/abs/1911.05722,Momentum Contrast for Unsupervised Visual Representation Learning,"['Kaiming He', 'Haoqi Fan', 'Yuxin Wu', 'Saining Xie', 'Ross Girshick']",2019-11-13 18:53:26+00:00,arxiv,...,3c563595dfca6df4111b65227179ded5,html,markdownify,2020-03-23 18:36:55+00:00,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.","CVPR 2020 camera-ready. Code:
  https://github.com/facebookresearch/moco",,,cs.CV,['cs.CV']
https://arxiv.org/abs/2002.05709,A Simple Framework for Contrastive Learning of Visual Representations,"['Ting Chen', 'Simon Kornblith', 'Mohammad Norouzi', 'Geoffrey Hinton']",2020-02-13 18:50:45+00:00,arxiv,...,70a49d1a53ac42a379b67a456ef85195,html,markdownify,2020-07-01 00:09:08+00:00,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","ICML'2020. Code and pretrained models at
  https://github.com/google-research/simclr",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/2005.10243,What Makes for Good Views for Contrastive Learning?,"['Yonglong Tian', 'Chen Sun', 'Ben Poole', 'Dilip Krishnan', 'Cordelia Schmid', 'Phillip Isola']",2020-05-20 17:59:57+00:00,arxiv,...,799b52ad13c25067d572bcd45244a2d3,html,markdownify,2020-12-18 10:01:34+00:00,"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",NeurIPS 2020. Project page: https://hobbitlong.github.io/InfoMin/,,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/1810.02334,Unsupervised Learning via Meta-Learning,"['Kyle Hsu', 'Sergey Levine', 'Chelsea Finn']",2018-10-04 17:29:17+00:00,arxiv,...,5ca6fa7ad18b2059adaed4f1edf50456,html,markdownify,2019-03-21 23:43:47+00:00,"A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.","ICLR 2019 camera-ready. 24 pages, 2 figures, links to code available
  at https://sites.google.com/view/unsupervised-via-meta",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1907.02544,Large Scale Adversarial Representation Learning,"['Jeff Donahue', 'Karen Simonyan']",2019-07-04 18:00:17+00:00,arxiv,...,5f61ffcec8e527a788fe53f867cf4051,html,markdownify,2019-11-05 18:05:57+00:00,"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).","32 pages. In proceedings of NeurIPS 2019. This is the camera-ready
  version of the paper, with supplementary material included as appendices",,,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1912.07768,Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data,"['Felipe Petroski Such', 'Aditya Rawal', 'Joel Lehman', 'Kenneth O. Stanley', 'Jeff Clune']",2019-12-17 00:57:50+00:00,arxiv,...,69504a1cc93f73f97ae32db6e9d32271,html,markdownify,2019-12-17 00:57:50+00:00,"This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1905.06922,On Variational Bounds of Mutual Information,"['Ben Poole', 'Sherjil Ozair', 'Aaron van den Oord', 'Alexander A. Alemi', 'George Tucker']",2019-05-16 17:31:53+00:00,arxiv,...,ca7acdb9f9ab65ce3d64f4f66d76c298,html,markdownify,2019-05-16 17:31:53+00:00,"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.",ICML 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1807.03748,Representation Learning with Contrastive Predictive Coding,"['Aaron van den Oord', 'Yazhe Li', 'Oriol Vinyals']",2018-07-10 16:52:11+00:00,arxiv,...,a4cfc80ce12e66cad7fff93895bdd33e,html,markdownify,2019-01-22 18:47:12+00:00,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2103.14659v1,Alignment of Language Agents,"['Zachary Kenton', 'Tom Everitt', 'Laura Weidinger', 'Iason Gabriel', 'Vladimir Mikulik', 'Geoffrey Irving']",2021-03-26 18:01:48+00:00,arxiv,...,0ba484e76ec4cbd983ab23e17ea4ee24,html,markdownify,2021-03-26 18:01:48+00:00,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1806.06877,"A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress","['Saurabh Arora', 'Prashant Doshi']",2018-06-18 18:26:29+00:00,arxiv,...,23b9832ddf9bd99ffa27293c5179e7b5,html,markdownify,2020-11-18 18:45:24+00:00,"Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1710.11248,Learning Robust Rewards with Adversarial Inverse Reinforcement Learning,"['Justin Fu', 'Katie Luo', 'Sergey Levine']",2017-10-30 21:22:28+00:00,arxiv,...,f576fe8601894654a43ef76b79dcf58e,html,markdownify,2018-08-13 18:33:24+00:00,"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",,,,cs.LG,['cs.LG']
https://arxiv.org/abs/1804.05296,Adversarial Attacks Against Medical Deep Learning Systems,"['Samuel G. Finlayson', 'Hyung Won Chung', 'Isaac S. Kohane', 'Andrew L. Beam']",2018-04-15 02:33:08+00:00,arxiv,...,d6d1c5839148ceac7d9e7df25351fa8e,html,markdownify,2019-02-04 06:03:22+00:00,"The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.",,,,cs.CR,"['cs.CR', 'cs.CY', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.06256,Learning Robust Representations by Projecting Superficial Statistics Out,"['Haohan Wang', 'Zexue He', 'Zachary C. Lipton', 'Eric P. Xing']",2019-03-02 00:42:03+00:00,arxiv,...,15264c25a5e6bd987d870367d7d1ccec,html,markdownify,2019-03-02 00:42:03+00:00,"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.","To appear at ICLR 2019. Implementation:
  https://github.com/HaohanWang/HEX",,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/1902.07379,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,"['Jun Shu', 'Qi Xie', 'Lixuan Yi', 'Qian Zhao', 'Sanping Zhou', 'Zongben Xu', 'Deyu Meng']",2019-02-20 02:29:55+00:00,arxiv,...,a9e9d830027fc0189ae500eac04452cf,html,markdownify,2019-09-27 02:48:20+00:00,"Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",NeurIPS 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.01032,Reinforcement Learning with Perturbed Rewards,"['Jingkang Wang', 'Yang Liu', 'Bo Li']",2018-10-02 01:43:45+00:00,arxiv,...,230523aca9c7a98a48a5ae2207b12c5b,html,markdownify,2020-02-01 21:15:52+00:00,"Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.",AAAI 2020 (Spotlight),,,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1810.01014,Bayesian Policy Optimization for Model Uncertainty,"['Gilwoo Lee', 'Brian Hou', 'Aditya Mandalika', 'Jeongseok Lee', 'Sanjiban Choudhury', 'Siddhartha S. Srinivasa']",2018-10-01 23:39:25+00:00,arxiv,...,6ea4f5166694a44b1dbad780a85ee96e,html,markdownify,2019-05-08 15:04:45+00:00,"Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.",,,,cs.RO,"['cs.RO', 'cs.LG']"
https://arxiv.org/abs/1811.12231,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"['Robert Geirhos', 'Patricia Rubisch', 'Claudio Michaelis', 'Matthias Bethge', 'Felix A. Wichmann', 'Wieland Brendel']",2018-11-29 15:04:05+00:00,arxiv,...,cd33671701a8ecc263068d3d85496af5,html,markdownify,2019-01-14 13:59:09+00:00,"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",Accepted at ICLR 2019 (oral),,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'q-bio.NC', 'stat.ML']"
https://arxiv.org/abs/1901.10513,Adversarial Examples Are a Natural Consequence of Test Error in Noise,"['Nic Ford', 'Justin Gilmer', 'Nicolas Carlini', 'Dogus Cubuk']",2019-01-29 20:01:39+00:00,arxiv,...,171963cc9dc66646b479211ecd92dfbb,html,markdownify,2019-01-29 20:01:39+00:00,"Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.",,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1903.11680,Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,"['Mingchen Li', 'Mahdi Soltanolkotabi', 'Samet Oymak']",2019-03-27 20:00:15+00:00,arxiv,...,28468eb496738822b09a0ff38bcd919a,html,markdownify,2019-07-03 23:48:05+00:00,"Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this, somewhat paradoxically, neural network models trained via first-order methods continue to predict well on yet unseen test data. This paper takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels despite overparameterization. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) to start to overfit to the noisy labels network must stray rather far from from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2002.05379,The Conditional Entropy Bottleneck,['Ian Fischer'],2020-02-13 07:46:38+00:00,arxiv,...,aeb4a71cd90eb72d5d458123273e5413,html,markdownify,2020-02-13 07:46:38+00:00,"Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2009.00802,Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,['Andrew J. Lohn'],2020-09-02 03:33:40+00:00,arxiv,...,1714d01ef53627705e9d930fb93d30e2,html,markdownify,2020-09-02 03:33:40+00:00,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.CY', 'cs.SE', 'stat.ML']"
https://arxiv.org/abs/2011.03395,Underspecification Presents Challenges for Credibility in Modern Machine Learning,"[""Alexander D'Amour"", 'Katherine Heller', 'Dan Moldovan', 'Ben Adlam', 'Babak Alipanahi', 'Alex Beutel', 'Christina Chen', 'Jonathan Deaton', 'Jacob Eisenstein', 'Matthew D. Hoffman', 'Farhad Hormozdiari', 'Neil Houlsby', 'Shaobo Hou', 'Ghassen Jerfel', 'Alan Karthikesalingam', 'Mario Lucic', 'Yian Ma', 'Cory McLean', 'Diana Mincu', 'Akinori Mitani', 'Andrea Montanari', 'Zachary Nado', 'Vivek Natarajan', 'Christopher Nielson', 'Thomas F. Osborne', 'Rajiv Raman', 'Kim Ramasamy', 'Rory Sayres', 'Jessica Schrouff', 'Martin Seneviratne', 'Shannon Sequeira', 'Harini Suresh', 'Victor Veitch', 'Max Vladymyrov', 'Xuezhi Wang', 'Kellie Webster', 'Steve Yadlowsky', 'Taedong Yun', 'Xiaohua Zhai', 'D. Sculley']",2020-11-06 14:53:13+00:00,arxiv,...,1bc1d410c1d179661e9c35363ba87e55,html,markdownify,2020-11-24 19:16:02+00:00,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.","Updates: Updated statistical analysis in Section 6; Additional
  citations",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2009.08092,Distributional Generalization: A New Kind of Generalization,"['Preetum Nakkiran', 'Yamini Bansal']",2020-09-17 06:26:17+00:00,arxiv,...,bd87ca9b8d27d87daedffb7ecdde4725,html,markdownify,2020-10-15 02:41:52+00:00,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",Co-first authors. V2: Intro shortened; no new results,,,cs.LG,"['cs.LG', 'cs.NE', 'math.ST', 'stat.ML', 'stat.TH']"
https://arxiv.org/abs/1911.04252,Self-training with Noisy Student improves ImageNet classification,"['Qizhe Xie', 'Minh-Thang Luong', 'Eduard Hovy', 'Quoc V. Le']",2019-11-11 18:59:27+00:00,arxiv,...,85bc8621d049fd27f0b8e68f7bd587e9,html,markdownify,2020-06-19 17:36:57+00:00,"We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.   Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.",CVPR 2020,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1809.03956,Abstraction Learning,"['Fei Deng', 'Jinsheng Ren', 'Feng Chen']",2018-09-11 15:02:24+00:00,arxiv,...,97866ba82c17e422789b306f3b8ad66d,html,markdownify,2018-09-11 15:02:24+00:00,"There has been a gap between artificial intelligence and human intelligence. In this paper, we identify three key elements forming human intelligence, and suggest that abstraction learning combines these elements and is thus a way to bridge the gap. Prior researches in artificial intelligence either specify abstraction by human experts, or take abstraction as a qualitative explanation for the model. This paper aims to learn abstraction directly. We tackle three main challenges: representation, objective function, and learning algorithm. Specifically, we propose a partition structure that contains pre-allocated abstraction neurons; we formulate abstraction learning as a constrained optimization problem, which integrates abstraction properties; we develop a network evolution algorithm to solve this problem. This complete framework is named ONE (Optimization via Network Evolution). In our experiments on MNIST, ONE shows elementary human-like intelligence, including low energy consumption, knowledge sharing, and lifelong learning.",,,,cs.AI,"['cs.AI', 'cs.NE']"
https://arxiv.org/abs/1805.12387,Agents and Devices: A Relative Definition of Agency,"['Laurent Orseau', 'Simon McGregor McGill', 'Shane Legg']",2018-05-31 09:12:14+00:00,arxiv,...,f53891b6145aee7540ea7d8109f8b4f6,html,markdownify,2018-05-31 09:12:14+00:00,"According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.00667,Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,"['Yarin Gal', 'Lewis Smith']",2018-06-02 16:43:17+00:00,arxiv,...,fcccc3285e18e8d76181363fb3155910,html,markdownify,2018-06-28 21:25:21+00:00,"We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.",,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1806.09030,On Adversarial Examples for Character-Level Neural Machine Translation,"['Javid Ebrahimi', 'Daniel Lowd', 'Dejing Dou']",2018-06-23 20:08:56+00:00,arxiv,...,eb77cde8255334f3ddf333a073b6825e,html,markdownify,2018-06-23 20:08:56+00:00,"Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.",,COLING 2018,,cs.CL,"['cs.CL', 'cs.AI']"
https://arxiv.org/abs/1811.03571,Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence,"['Luca Bortolussi', 'Guido Sanguinetti']",2018-11-08 17:51:27+00:00,arxiv,...,75fa74eb031dc2dc6e4b221579c5edd1,html,markdownify,2019-01-24 14:13:58+00:00,"The success of modern Artificial Intelligence (AI) technologies depends critically on the ability to learn non-linear functional dependencies from large, high dimensional data sets. Despite recent high-profile successes, empirical evidence indicates that the high predictive performance is often paired with low robustness, making AI systems potentially vulnerable to adversarial attacks. In this report, we provide a simple intuitive argument suggesting that high performance and vulnerability are intrinsically coupled, and largely dependent on the geometry of typical, high-dimensional data sets. Our work highlights a major potential pitfall of modern AI systems, and suggests practical research directions to ameliorate the problem.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1803.06373,Adversarial Logit Pairing,"['Harini Kannan', 'Alexey Kurakin', 'Ian Goodfellow']",2018-03-16 19:03:45+00:00,arxiv,...,3bf7a95659d04d447a84f9f6e85b166f,html,markdownify,2018-03-16 19:03:45+00:00,"In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.",10 pages,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1806.04169,Defense Against the Dark Arts: An overview of adversarial example security research and future research directions,['Ian Goodfellow'],2018-06-11 18:22:45+00:00,arxiv,...,5d610d25df5a9e966ea5fd02f83db9aa,html,markdownify,2018-06-11 18:22:45+00:00,This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.,,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/2011.05623,"Fooling the primate brain with minimal, targeted image manipulation","['Li Yuan', 'Will Xiao', 'Giorgia Dellaferrera', 'Gabriel Kreiman', 'Francis E. H. Tay', 'Jiashi Feng', 'Margaret S. Livingstone']",2020-11-11 08:30:54+00:00,arxiv,...,c5495776f99c0e88f0d00b23f8add759,html,markdownify,2022-03-30 05:36:53+00:00,"Artificial neural networks (ANNs) are considered the current best models of biological vision. ANNs are the best predictors of neural activity in the ventral stream; moreover, recent work has demonstrated that ANN models fitted to neuronal activity can guide the synthesis of images that drive pre-specified response patterns in small neuronal populations. Despite the success in predicting and steering firing activity, these results have not been connected with perceptual or behavioral changes. Here we propose an array of methods for creating minimal, targeted image perturbations that lead to changes in both neuronal activity and perception as reflected in behavior. We generated 'deceptive images' of human faces, monkey faces, and noise patterns so that they are perceived as a different, pre-specified target category, and measured both monkey neuronal responses and human behavior to these images. We found several effective methods for changing primate visual categorization that required much smaller image change compared to untargeted noise. Our work shares the same goal with adversarial attack, namely the manipulation of images with minimal, targeted noise that leads ANN models to misclassify the images. Our results represent a valuable step in quantifying and characterizing the differences in perturbation robustness of biological and artificial vision.",,,,q-bio.NC,"['q-bio.NC', 'cs.CV', 'cs.NE', 'eess.IV']"
https://arxiv.org/abs/1806.11146,Adversarial Reprogramming of Neural Networks,"['Gamaleldin F. Elsayed', 'Ian Goodfellow', 'Jascha Sohl-Dickstein']",2018-06-28 19:06:26+00:00,arxiv,...,cb0c6719362ab6f12148acaa5b119878,html,markdownify,2018-11-29 22:50:01+00:00,"Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.",,International Conference on Learning Representations 2019,,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1901.08573,Theoretically Principled Trade-off between Robustness and Accuracy,"['Hongyang Zhang', 'Yaodong Yu', 'Jiantao Jiao', 'Eric P. Xing', 'Laurent El Ghaoui', 'Michael I. Jordan']",2019-01-24 18:43:57+00:00,arxiv,...,70905dea00b4a338cc81925af80334d1,html,markdownify,2019-06-24 07:04:11+00:00,"We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.","Appeared in ICML 2019; the winning methodology of the NeurIPS 2018
  Adversarial Vision Challenge",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.10396,The LogBarrier adversarial attack: making effective use of decision boundary information,"['Chris Finlay', 'Aram-Alexandre Pooladian', 'Adam M. Oberman']",2019-03-25 15:21:20+00:00,arxiv,...,0a2a37757cfa2a167aac88f9ffa7e426,html,markdownify,2019-03-25 15:21:20+00:00,"Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the established logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks.","12 pages, 4 figures, 6 tables",,,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1906.03973,E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles,"['Markus Kettunen', 'Erik HÃ¤rkÃ¶nen', 'Jaakko Lehtinen']",2019-06-10 13:40:37+00:00,arxiv,...,8cce6fe3aa00122f8f84c7e1264557b6,html,markdownify,2019-06-11 08:58:35+00:00,"It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of ""perceptual convexity"" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.","Code and supplemental material available at
  https://github.com/mkettune/elpips/",,,cs.CV,"['cs.CV', 'cs.NE']"
https://arxiv.org/abs/1807.06732,Motivating the Rules of the Game for Adversarial Example Research,"['Justin Gilmer', 'Ryan P. Adams', 'Ian Goodfellow', 'David Andersen', 'George E. Dahl']",2018-07-18 01:17:27+00:00,arxiv,...,91328a083c368f985c72cc6eedbcbb29,html,markdownify,2018-07-20 01:57:37+00:00,"Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1805.09190,Towards the first adversarially robust neural network model on MNIST,"['Lukas Schott', 'Jonas Rauber', 'Matthias Bethge', 'Wieland Brendel']",2018-05-23 14:16:22+00:00,arxiv,...,450103dc2780778ddc82a05c3b7936b5,html,markdownify,2018-09-20 17:49:14+00:00,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1811.03531,A Geometric Perspective on the Transferability of Adversarial Directions,"['Zachary Charles', 'Harrison Rosenberg', 'Dimitris Papailiopoulos']",2018-11-08 16:23:50+00:00,arxiv,...,ba4e2e57ca957af44e1c55c9ebab41af,html,markdownify,2018-11-08 16:23:50+00:00,"State-of-the-art machine learning models frequently misclassify inputs that have been perturbed in an adversarial manner. Adversarial perturbations generated for a given input and a specific classifier often seem to be effective on other inputs and even different classifiers. In other words, adversarial perturbations seem to transfer between different inputs, models, and even different neural network architectures. In this work, we show that in the context of linear classifiers and two-layer ReLU networks, there provably exist directions that give rise to adversarial perturbations for many classifiers and data points simultaneously. We show that these ""transferable adversarial directions"" are guaranteed to exist for linear separators of a given set, and will exist with high probability for linear classifiers trained on independent sets drawn from the same distribution. We extend our results to large classes of two-layer ReLU networks. We further show that adversarial directions for ReLU networks transfer to linear classifiers while the reverse need not hold, suggesting that adversarial perturbations for more complex models are more likely to transfer to other classifiers. We validate our findings empirically, even for deeper ReLU networks.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1908.08016,Testing Robustness Against Unforeseen Adversaries,"['Daniel Kang', 'Yi Sun', 'Dan Hendrycks', 'Tom Brown', 'Jacob Steinhardt']",2019-08-21 17:36:48+00:00,arxiv,...,5780fae4849652f60f4ea3e89abf0e78,html,markdownify,2020-06-09 05:17:48+00:00,"Most existing adversarial defenses only measure robustness to L_p adversarial attacks. Not only are adversaries unlikely to exclusively create small L_p perturbations, adversaries are unlikely to remain fixed. Adversaries adapt and evolve their attacks; hence adversarial defenses must be robust to a broad range of unforeseen attacks. We address this discrepancy between research and reality by proposing a new evaluation framework called ImageNet-UA. Our framework enables the research community to test ImageNet model robustness against attacks not encountered during training. To create ImageNet-UA's diverse attack suite, we introduce a total of four novel adversarial attacks. We also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf robustness assessments give a narrow account of model robustness. By evaluating current defenses with ImageNet-UA, we find they provide little robustness to unforeseen attacks. We hope the greater variety and realism of ImageNet-UA enables development of more robust defenses which can generalize beyond attacks seen during training.",,,,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1907.07174,Natural Adversarial Examples,"['Dan Hendrycks', 'Kevin Zhao', 'Steven Basart', 'Jacob Steinhardt', 'Dawn Song']",2019-07-16 17:56:30+00:00,arxiv,...,5ba5d70f49de18f67d7afcc793c14421,html,markdownify,2021-03-04 21:56:19+00:00,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.","CVPR 2021; dataset and code available at
  https://github.com/hendrycks/natural-adv-examples",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1906.09453,Image Synthesis with a Single (Robust) Classifier,"['Shibani Santurkar', 'Dimitris Tsipras', 'Brandon Tran', 'Andrew Ilyas', 'Logan Engstrom', 'Aleksander Madry']",2019-06-06 09:12:08+00:00,arxiv,...,c8830f28afac0ab9197f7d4801d4579a,html,markdownify,2019-08-08 15:47:42+00:00,"We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.",,,,cs.CV,"['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1906.00945,Adversarial Robustness as a Prior for Learned Representations,"['Logan Engstrom', 'Andrew Ilyas', 'Shibani Santurkar', 'Dimitris Tsipras', 'Brandon Tran', 'Aleksander Madry']",2019-06-03 17:55:20+00:00,arxiv,...,ef466765d79862ca3767e21845c95cbf,html,markdownify,2019-09-27 17:39:54+00:00,"An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",,,,stat.ML,"['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']"
https://arxiv.org/abs/1905.02175,"Adversarial Examples Are Not Bugs, They Are Features","['Andrew Ilyas', 'Shibani Santurkar', 'Dimitris Tsipras', 'Logan Engstrom', 'Brandon Tran', 'Aleksander Madry']",2019-05-06 17:45:05+00:00,arxiv,...,97ef732c5d436384a717ff6f4f12da1a,html,markdownify,2019-08-12 14:36:10+00:00,"Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",,,,stat.ML,"['stat.ML', 'cs.CR', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/2104.03113,Scaling Scaling Laws with Board Games,['Andy L. Jones'],2021-04-07 13:34:25+00:00,arxiv,...,58ee8875869513fb22f02969cd07002e,html,markdownify,2021-04-15 10:03:37+00:00,"The largest experiments in machine learning now require resources far beyond the budget of all but a few institutions. Fortunately, it has recently been shown that the results of these huge experiments can often be extrapolated from the results of a sequence of far smaller, cheaper experiments. In this work, we show that not only can the extrapolation be done based on the size of the model, but on the size of the problem as well. By conducting a sequence of experiments using AlphaZero and Hex, we show that the performance achievable with a fixed amount of compute degrades predictably as the game gets larger and harder. Along with our main result, we further show that the test-time and train-time compute available to an agent can be traded off while maintaining performance.",,,,cs.LG,"['cs.LG', 'cs.MA']"
https://arxiv.org/abs/1901.00596,A Comprehensive Survey on Graph Neural Networks,"['Zonghan Wu', 'Shirui Pan', 'Fengwen Chen', 'Guodong Long', 'Chengqi Zhang', 'Philip S. Yu']",2019-01-03 03:20:55+00:00,arxiv,...,fbe796ea7f056250f6153df33d5123cb,html,markdownify,2019-12-04 01:43:00+00:00,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",Minor revision (updated tables and references),,10.1109/TNNLS.2020.2978386,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1905.01067,"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","['Hattie Zhou', 'Janice Lan', 'Rosanne Liu', 'Jason Yosinski']",2019-05-03 08:21:07+00:00,arxiv,...,2a844b56f025c375b3ce78f881befe9f,html,markdownify,2020-03-03 05:40:51+00:00,"The recent ""Lottery Ticket Hypothesis"" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).",NeurIPS 2019 camera ready version,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1905.12149,SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,"['Po-Wei Wang', 'Priya L. Donti', 'Bryan Wilder', 'Zico Kolter']",2019-05-29 00:47:35+00:00,arxiv,...,d6989f144705abddeb90c18183ab9265,html,markdownify,2019-05-29 00:47:35+00:00,"Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a ""visual Sudok"" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.","Accepted at ICML'19. The code can be found at
  https://github.com/locuslab/satnet",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1912.07242,More Data Can Hurt for Linear Regression: Sample-wise Double Descent,['Preetum Nakkiran'],2019-12-16 08:28:26+00:00,arxiv,...,a154903895a6050757fc211b8736e17f,html,markdownify,2019-12-16 08:28:26+00:00,"In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples. In other words, more data actually hurts the estimator. This behavior is implicit in a recent line of theoretical works analyzing ""double-descent"" phenomenon in linear models. In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates. In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.",,,,stat.ML,"['stat.ML', 'cs.LG', 'cs.NE', 'math.ST', 'stat.TH']"
https://arxiv.org/abs/2103.05247,Pretrained Transformers as Universal Computation Engines,"['Kevin Lu', 'Aditya Grover', 'Pieter Abbeel', 'Igor Mordatch']",2021-03-09 06:39:56+00:00,arxiv,...,95ae4cc25c8cdd2f3a0920ff82ed5b1e,html,markdownify,2021-06-30 17:34:46+00:00,"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2105.11447,True Few-Shot Learning with Language Models,"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']",2021-05-24 17:55:51+00:00,arxiv,...,f6cc783e9ea93d9996275b7c7cba5de1,html,markdownify,2021-05-24 17:55:51+00:00,"Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (""prompts""). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",Code at https://github.com/ethanjperez/true_few_shot,,,cs.CL,"['cs.CL', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2108.07732,Program Synthesis with Large Language Models,"['Jacob Austin', 'Augustus Odena', 'Maxwell Nye', 'Maarten Bosma', 'Henryk Michalewski', 'David Dohan', 'Ellen Jiang', 'Carrie Cai', 'Michael Terry', 'Quoc Le', 'Charles Sutton']",2021-08-16 03:57:30+00:00,arxiv,...,906c693a77b29d2e6c657f8010788e58,html,markdownify,2021-08-16 03:57:30+00:00,"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",Jacob and Augustus contributed equally,,,cs.PL,"['cs.PL', 'cs.LG']"
https://arxiv.org/abs/2001.09977,Towards a Human-like Open-Domain Chatbot,"['Daniel Adiwardana', 'Minh-Thang Luong', 'David R. So', 'Jamie Hall', 'Noah Fiedel', 'Romal Thoppilan', 'Zi Yang', 'Apoorv Kulshreshtha', 'Gaurav Nemade', 'Yifeng Lu', 'Quoc V. Le']",2020-01-27 18:53:15+00:00,arxiv,...,7ef6ebc4a8aafbc70c1e88028da2efa4,html,markdownify,2020-02-27 07:36:47+00:00,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.","38 pages, 12 figures",,,cs.CL,"['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/2001.08361,Scaling Laws for Neural Language Models,"['Jared Kaplan', 'Sam McCandlish', 'Tom Henighan', 'Tom B. Brown', 'Benjamin Chess', 'Rewon Child', 'Scott Gray', 'Alec Radford', 'Jeffrey Wu', 'Dario Amodei']",2020-01-23 03:59:20+00:00,arxiv,...,9f0c5e424b1f16085fcd3c30e00cf491,html,markdownify,2020-01-23 03:59:20+00:00,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.","19 pages, 15 figures",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.04053v1,The 30-Year Cycle In The AI Debate,['Jean-Marie Chauvet'],2018-10-08 16:35:06+00:00,arxiv,...,f5acb6708a7ea2744bb207855860f6e1,html,markdownify,2018-10-08 16:35:06+00:00,"In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.","31 pages, 5 tables",,,cs.AI,"['cs.AI', 'I.2.0']"
https://arxiv.org/abs/2009.10385,A narrowing of AI research?,"['Joel Klinger', 'Juan Mateos-Garcia', 'Konstantinos Stathoulopoulos']",2020-09-22 08:23:56+00:00,arxiv,...,dd09f0f37c2cab68fdd54b6a102a8231,html,markdownify,2022-01-11 06:19:32+00:00,"The arrival of deep learning techniques able to infer patterns from large datasets has dramatically improved the performance of Artificial Intelligence (AI) systems. Deep learning's rapid development and adoption, in great part led by large technology companies, has however created concerns about a premature narrowing in the technological trajectory of AI research despite its weaknesses, which include lack of robustness, high environmental costs, and potentially unfair outcomes. We seek to improve the evidence base with a semantic analysis of AI research in arXiv, a popular pre-prints database. We study the evolution of the thematic diversity of AI research, compare the thematic diversity of AI research in academia and the private sector and measure the influence of private companies in AI research through the citations they receive and their collaborations with other institutions. Our results suggest that diversity in AI research has stagnated in recent years, and that AI research involving the private sector tends to be less diverse and more influential than research in academia. We also find that private sector AI researchers tend to specialise in data-hungry and computationally intensive deep learning methods at the expense of research involving other AI methods, research that considers the societal and ethical implications of AI, and applications in sectors like health. Our results provide a rationale for policy action to prevent a premature narrowing of AI research that could constrain its societal benefits, but we note the informational, incentive and scale hurdles standing in the way of such interventions.","Fourth version: Includes substantial changes in response to reviewer
  comments such as: alternative strategy to identify AI papers, new robustness
  section, new analysis of private research influence, substantially modified
  literature review and creation of technical annex",,,cs.CY,['cs.CY']
https://arxiv.org/abs/1810.00619,SmartChoices: Hybridizing Programming and Machine Learning,"['Victor Carbune', 'Thierry Coppey', 'Alexander Daryin', 'Thomas Deselaers', 'Nikhil Sarda', 'Jay Yagnik']",2018-10-01 11:14:22+00:00,arxiv,...,8950a55d50dd99e12517ee6d4a0863db,html,markdownify,2019-06-13 18:20:51+00:00,"We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages which we see as one way to lower the entrance cost to applying ML to problems in new domains. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by providing a 3-call API which we expose through an object called SmartChoice. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algorithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems, our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse -- allowing for a safe deployment in critical applications in real life.","published at the Reinforcement Learning for Real Life (RL4RealLife)
  Workshop in the 36th International Conference on Machine Learning (ICML),
  Long Beach, California, USA, 2019",,,cs.LG,"['cs.LG', 'cs.PL', 'stat.ML']"
https://arxiv.org/abs/1810.09591,Applying Deep Learning To Airbnb Search,"['Malay Haldar', 'Mustafa Abdool', 'Prashant Ramanathan', 'Tao Xu', 'Shulin Yang', 'Huizhong Duan', 'Qing Zhang', 'Nick Barrow-Williams', 'Bradley C. Turnbull', 'Brendan M. Collins', 'Thomas Legrand']",2018-10-22 23:11:01+00:00,arxiv,...,f27e585244ec6f850020e31e709d1875,html,markdownify,2018-10-24 18:28:03+00:00,"The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!",8 pages,,10.1145/3292500.3330658,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'stat.ML']"
https://arxiv.org/abs/1904.12901,Challenges of Real-World Reinforcement Learning,"['Gabriel Dulac-Arnold', 'Daniel Mankowitz', 'Todd Hester']",2019-04-29 18:40:15+00:00,arxiv,...,6ae255c5db6a4a57bd545219d11650b7,html,markdownify,2019-04-29 18:40:15+00:00,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1906.05433,Tackling Climate Change with Machine Learning,"['David Rolnick', 'Priya L. Donti', 'Lynn H. Kaack', 'Kelly Kochanski', 'Alexandre Lacoste', 'Kris Sankaran', 'Andrew Slavin Ross', 'Nikola Milojevic-Dupont', 'Natasha Jaques', 'Anna Waldman-Brown', 'Alexandra Luccioni', 'Tegan Maharaj', 'Evan D. Sherwin', 'S. Karthik Mukkavilli', 'Konrad P. Kording', 'Carla Gomes', 'Andrew Y. Ng', 'Demis Hassabis', 'John C. Platt', 'Felix Creutzig', 'Jennifer Chayes', 'Yoshua Bengio']",2019-06-10 17:51:47+00:00,arxiv,...,ab16651895e3225014b2ceced26f8d05,html,markdownify,2019-11-05 17:37:20+00:00,"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","For additional resources, please visit the website that accompanies
  this paper: https://www.climatechange.ai/",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1907.11274,Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning,"['Aviv Ovadya', 'Jess Whittlestone']",2019-07-25 18:51:45+00:00,arxiv,...,5c0b1c2c083c3fc9d1bf55cb2c96fbc3,html,markdownify,2019-07-29 02:01:40+00:00,"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues.   We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations.   Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.",11 pages. Language fixes and tweaks for clarity,,,cs.CY,"['cs.CY', 'cs.LG']"
https://arxiv.org/abs/1905.12616,Defending Against Neural Fake News,"['Rowan Zellers', 'Ari Holtzman', 'Hannah Rashkin', 'Yonatan Bisk', 'Ali Farhadi', 'Franziska Roesner', 'Yejin Choi']",2019-05-29 17:58:52+00:00,arxiv,...,efb022530fcc64916f36cbec5cf23aae,html,markdownify,2020-12-11 16:17:17+00:00,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.","NeurIPS 2019 camera ready version. Project page/code/demo at
  https://rowanzellers.com/grover",,,cs.CL,"['cs.CL', 'cs.CY']"
https://arxiv.org/abs/2102.06701,Explaining Neural Scaling Laws,"['Yasaman Bahri', 'Ethan Dyer', 'Jared Kaplan', 'Jaehoon Lee', 'Utkarsh Sharma']",2021-02-12 18:57:46+00:00,arxiv,...,0f53b24ad6cbc1f46edd7a654e65fa60,html,markdownify,2021-02-12 18:57:46+00:00,"The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.","11 pages, 5 figures + Supplement",,,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'stat.ML']"
https://arxiv.org/abs/2004.10802,A Neural Scaling Law from the Dimension of the Data Manifold,"['Utkarsh Sharma', 'Jared Kaplan']",2020-04-22 19:16:06+00:00,arxiv,...,2104580e7aaa3a569f6340f9b6302902,html,markdownify,2020-04-22 19:16:06+00:00,"When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.","16+12 pages, 11+11 figures",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1808.00508,Neural Arithmetic Logic Units,"['Andrew Trask', 'Felix Hill', 'Scott Reed', 'Jack Rae', 'Chris Dyer', 'Phil Blunsom']",2018-08-01 18:58:53+00:00,arxiv,...,7275b4b816e1ca958022546481187382,html,markdownify,2018-08-01 18:58:53+00:00,"Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.",,,,cs.NE,['cs.NE']
https://arxiv.org/abs/1808.04730,Analyzing Inverse Problems with Invertible Neural Networks,"['Lynton Ardizzone', 'Jakob Kruse', 'Sebastian Wirkert', 'Daniel Rahner', 'Eric W. Pellegrini', 'Ralf S. Klessen', 'Lena Maier-Hein', 'Carsten Rother', 'Ullrich KÃ¶the']",2018-08-14 14:58:59+00:00,arxiv,...,63ff0b6f64566220ae787277182da67a,html,markdownify,2019-02-06 15:45:02+00:00,"In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Although INNs are not new, they have, so far, received little attention in literature. While classical neural networks attempt to solve the ambiguous inverse problem directly, INNs are able to learn it jointly with the well-defined forward process, using additional latent output variables to capture the information otherwise lost. Given a specific measurement and sampled latent variables, the inverse pass of the INN provides a full distribution over parameter space. We verify experimentally, on artificial data and real-world problems from astrophysics and medicine, that INNs are a powerful analysis tool to find multi-modalities in parameter space, to uncover parameter correlations, and to identify unrecoverable parameters.",,,,cs.LG,"['cs.LG', 'stat.ML', '68T01']"
https://arxiv.org/abs/1808.08946,Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,"['Gongbo Tang', 'Mathias MÃ¼ller', 'Annette Rios', 'Rico Sennrich']",2018-08-27 17:51:27+00:00,arxiv,...,9a104affaaf7229e3cb4189aaaf65e28,html,markdownify,2018-11-11 16:49:47+00:00,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.","11 pages, 5 figures, accepted by EMNLP 2018 (v2: corrected author
  names; v3: fix to CNN context-window size, and new post-publication
  experiments in section 6)",,,cs.CL,['cs.CL']
https://arxiv.org/abs/1804.03235,Large scale distributed neural network training through online distillation,"['Rohan Anil', 'Gabriel Pereyra', 'Alexandre Passos', 'Robert Ormandi', 'George E. Dahl', 'Geoffrey E. Hinton']",2018-04-09 20:56:03+00:00,arxiv,...,89333335f0579945d73a000fe248ae81,html,markdownify,2020-08-20 22:04:36+00:00,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","Clarify that implementations should use available parallelism in
  pseudo-code",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1806.01203,Relational inductive bias for physical construction in humans and machines,"['Jessica B. Hamrick', 'Kelsey R. Allen', 'Victor Bapst', 'Tina Zhu', 'Kevin R. McKee', 'Joshua B. Tenenbaum', 'Peter W. Battaglia']",2018-06-04 16:45:19+00:00,arxiv,...,2842571576c6d7206ad20ae224839894,html,markdownify,2018-06-04 16:45:19+00:00,"While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a ""relational inductive bias"": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.","In Proceedings of the Annual Meeting of the Cognitive Science Society
  (CogSci 2018)",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1811.04784,Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations,"['Xander Steenbrugge', 'Sam Leroux', 'Tim Verbelen', 'Bart Dhoedt']",2018-11-12 15:23:26+00:00,arxiv,...,a5626f801bad10cc17ee1e068befb141,html,markdownify,2018-11-12 15:23:26+00:00,"In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.",,,,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1804.09170,Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"['Avital Oliver', 'Augustus Odena', 'Colin Raffel', 'Ekin D. Cubuk', 'Ian J. Goodfellow']",2018-04-24 17:54:44+00:00,arxiv,...,fd4fede8f7a78450052cb165dbcfc10d,html,markdownify,2019-06-17 11:48:53+00:00,"Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.",,NeurIPS 2018 Proceedings,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1901.05761,Attentive Neural Processes,"['Hyunjik Kim', 'Andriy Mnih', 'Jonathan Schwarz', 'Marta Garnelo', 'Ali Eslami', 'Dan Rosenbaum', 'Oriol Vinyals', 'Yee Whye Teh']",2019-01-17 12:37:26+00:00,arxiv,...,dd0d08d2b8045c403707cb7e6939fb90,html,markdownify,2019-07-09 10:49:01+00:00,"Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.03088,Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions,"['Matthew MacKay', 'Paul Vicol', 'Jon Lorraine', 'David Duvenaud', 'Roger Grosse']",2019-03-07 18:26:46+00:00,arxiv,...,f1ca5263f8bc856502005e43e5e74eff,html,markdownify,2019-03-07 18:26:46+00:00,"Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer. We justify this approximation by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. We fit this model using a gradient-based hyperparameter optimization algorithm which alternates between approximating the best-response around the current hyperparameters and optimizing the hyperparameters using the approximate best-response function. Unlike other gradient-based approaches, we do not require differentiating the training loss with respect to the hyperparameters, allowing us to tune discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Because the hyperparameters are adapted online, our approach discovers hyperparameter schedules that can outperform fixed hyperparameter values. Empirically, our approach outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, Self-Tuning Networks (STNs).",Published as a conference paper at ICLR 2019,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1804.09849,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,"['Mia Xu Chen', 'Orhan Firat', 'Ankur Bapna', 'Melvin Johnson', 'Wolfgang Macherey', 'George Foster', 'Llion Jones', 'Niki Parmar', 'Mike Schuster', 'Zhifeng Chen', 'Yonghui Wu', 'Macduff Hughes']",2018-04-26 01:24:39+00:00,arxiv,...,765c85be3a20562bfd714aa6655b63f4,html,markdownify,2018-04-27 02:31:16+00:00,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",,,,cs.CL,"['cs.CL', 'cs.AI']"
https://arxiv.org/abs/1904.11455,Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,"['Tom Schaul', 'Diana Borsa', 'Joseph Modayil', 'Razvan Pascanu']",2019-04-25 16:54:02+00:00,arxiv,...,3f2fa4527ff5f688a8314d701185237d,html,markdownify,2019-04-25 16:54:02+00:00,"Rather than proposing a new method, this paper investigates an issue present in existing learning algorithms. We study the learning dynamics of reinforcement learning (RL), specifically a characteristic coupling between learning and data generation that arises because RL agents control their future data distribution. In the presence of function approximation, this coupling can lead to a problematic type of 'ray interference', characterized by learning dynamics that sequentially traverse a number of performance plateaus, effectively constraining the agent to learn one thing at a time even when learning in parallel is better. We establish the conditions under which ray interference occurs, show its relation to saddle points and obtain the exact learning dynamics in a restricted setting. We characterize a number of its properties and discuss possible remedies.",Full version of RLDM abstract,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1906.08237,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"['Zhilin Yang', 'Zihang Dai', 'Yiming Yang', 'Jaime Carbonell', 'Ruslan Salakhutdinov', 'Quoc V. Le']",2019-06-19 17:35:48+00:00,arxiv,...,4a26d19dd133378114deab4b672a68f8,html,markdownify,2020-01-02 12:48:08+00:00,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.","Pretrained models and code are available at
  https://github.com/zihangdai/xlnet",,,cs.CL,"['cs.CL', 'cs.LG']"
https://arxiv.org/abs/1805.01772,Dynamic Control Flow in Large-Scale Machine Learning,"['Yuan Yu', 'MartÃ­n Abadi', 'Paul Barham', 'Eugene Brevdo', 'Mike Burrows', 'Andy Davis', 'Jeff Dean', 'Sanjay Ghemawat', 'Tim Harley', 'Peter Hawkins', 'Michael Isard', 'Manjunath Kudlur', 'Rajat Monga', 'Derek Murray', 'Xiaoqiang Zheng']",2018-05-04 13:40:07+00:00,arxiv,...,acb43a57d3697dd1a6b2c062f9e3e737,html,markdownify,2018-05-04 13:40:07+00:00,"Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments.   This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations.   We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.","Appeared in EuroSys 2018. 14 pages, 16 figures","EuroSys 2018: Thirteenth EuroSys Conference, April 23-26, 2018,
  Porto, Portugal. ACM, New York, NY, USA",10.1145/3190508.3190551,cs.DC,"['cs.DC', 'cs.LG']"
https://arxiv.org/abs/1906.04358,Weight Agnostic Neural Networks,"['Adam Gaier', 'David Ha']",2019-06-11 02:40:11+00:00,arxiv,...,cd46a248cec31ebbf29cd1b674ccc49e,html,markdownify,2019-09-05 07:54:07+00:00,"Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/","To appear at NeurIPS 2019, selected for a spotlight presentation",,,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1912.01412,Deep Learning for Symbolic Mathematics,"['Guillaume Lample', 'FranÃ§ois Charton']",2019-12-02 15:05:24+00:00,arxiv,...,d3b4150f177b39ba97172a988000b140,html,markdownify,2019-12-02 15:05:24+00:00,"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",,,,cs.SC,"['cs.SC', 'cs.LG']"
https://arxiv.org/abs/1805.08974,Do Better ImageNet Models Transfer Better?,"['Simon Kornblith', 'Jonathon Shlens', 'Quoc V. Le']",2018-05-23 06:12:35+00:00,arxiv,...,a04723147a7e7a1d24958d216a0bfc34,html,markdownify,2019-06-17 16:25:07+00:00,"Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.",CVPR 2019 Oral,,,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1912.05671,Linear Mode Connectivity and the Lottery Ticket Hypothesis,"['Jonathan Frankle', 'Gintare Karolina Dziugaite', 'Daniel M. Roy', 'Michael Carbin']",2019-12-11 22:22:21+00:00,arxiv,...,cdda8cf3a1f8f19e9f60ca679fd2ff28,html,markdownify,2020-07-18 20:31:17+00:00,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).","Published in ICML 2020. This submission subsumes arXiv:1903.01611
  (""Stabilizing the Lottery Ticket Hypothesis"" and ""The Lottery Ticket
  Hypothesis at Scale"")",,,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/2003.03384,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le']",2020-03-06 19:00:04+00:00,arxiv,...,3fbbecaeedf237d2b57e638a3f3c8c16,html,markdownify,2020-06-30 04:32:44+00:00,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.","Accepted for publication at the 37th International Conference on
  Machine Learning (ICML 2020). Near camera-ready version",,,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML', 'I.2.2; I.2.6']"
https://arxiv.org/abs/1804.00645,Universal Planning Networks,"['Aravind Srinivas', 'Allan Jabri', 'Pieter Abbeel', 'Sergey Levine', 'Chelsea Finn']",2018-04-02 17:51:53+00:00,arxiv,...,805bc7f72175f613acd38831d7f515d3,html,markdownify,2018-04-04 17:36:36+00:00,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",Videos available at https://sites.google.com/view/upn-public/home,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2006.16668,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,"['Dmitry Lepikhin', 'HyoukJoong Lee', 'Yuanzhong Xu', 'Dehao Chen', 'Orhan Firat', 'Yanping Huang', 'Maxim Krikun', 'Noam Shazeer', 'Zhifeng Chen']",2020-06-30 10:42:02+00:00,arxiv,...,07720f0b9aaf2fb9360b8761749ec7e9,html,markdownify,2020-06-30 10:42:02+00:00,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",,,,cs.CL,"['cs.CL', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1909.13371,Gradient Descent: The Ultimate Optimizer,"['Kartik Chandra', 'Audrey Xie', 'Jonathan Ragan-Kelley', 'Erik Meijer']",2019-09-29 21:41:49+00:00,arxiv,...,ad68ba990205f25870a4f9f432706ef7,html,markdownify,2022-10-14 18:34:08+00:00,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for ""hypergradients"" ahead of time.   We show how to automatically compute hypergradients with a simple and elegant modification to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefficients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad infinitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2102.04074,Learning Curve Theory,['Marcus Hutter'],2021-02-08 09:25:31+00:00,arxiv,...,5ce2e377c51de2bd0b40d186390091a1,html,markdownify,2021-02-08 09:25:31+00:00,"Recently a number of empirical ""universal"" scaling law papers have been published, most notably by OpenAI. `Scaling laws' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\beta}$ learning curves for arbitrary power $\beta>0$, and determine whether power laws are universal or depend on the data distribution.","26 pages, 6 Figures",Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.06151,Deep Reinforcement Learning with Feedback-based Exploration,"['Jan Scholten', 'Daan Wout', 'Carlos Celemin', 'Jens Kober']",2019-03-14 17:52:46+00:00,arxiv,...,b9fb0da14bf4694a86c1cbe77275bb49,html,markdownify,2019-03-14 17:52:46+00:00,"Deep Reinforcement Learning has enabled the control of increasingly complex and high-dimensional problems. However, the need of vast amounts of data before reasonable performance is attained prevents its widespread application. We employ binary corrective feedback as a general and intuitive manner to incorporate human intuition and domain knowledge in model-free machine learning. The uncertainty in the policy and the corrective feedback is combined directly in the action space as probabilistic conditional exploration. As a result, the greatest part of the otherwise ignorant learning process can be avoided. We demonstrate the proposed method, Predictive Probabilistic Merging of Policies (PPMP), in combination with DDPG. In experiments on continuous control problems of the OpenAI Gym, we achieve drastic improvements in sample efficiency, final performance, and robustness to erroneous feedback, both for human and synthetic feedback. Additionally, we show solutions beyond the demonstrated knowledge.",6 pages,,10.1109/CDC40024.2019.9029503,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1904.09605,Generative Exploration and Exploitation,"['Jiechuan Jiang', 'Zongqing Lu']",2019-04-21 14:15:24+00:00,arxiv,...,108e45499417f3b8657317210aa204c8,html,markdownify,2019-11-20 11:56:23+00:00,"Sparse reward is one of the biggest challenges in reinforcement learning (RL). In this paper, we propose a novel method called Generative Exploration and Exploitation (GENE) to overcome sparse reward. GENE automatically generates start states to encourage the agent to explore the environment and to exploit received reward signals. GENE can adaptively tradeoff between exploration and exploitation according to the varying distributions of states experienced by the agent as the learning progresses. GENE relies on no prior knowledge about the environment and can be combined with any RL algorithm, no matter on-policy or off-policy, single-agent or multi-agent. Empirically, we demonstrate that GENE significantly outperforms existing methods in three tasks with only binary rewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies verify the emergence of progressive exploration and automatic reversing.",AAAI'20,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1810.06530,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,"['David Janz', 'Jiri Hron', 'PrzemysÅaw Mazur', 'Katja Hofmann', 'JosÃ© Miguel HernÃ¡ndez-Lobato', 'Sebastian Tschiatschek']",2018-10-15 17:30:53+00:00,arxiv,...,e4ab6307dfeacef95e774c859becc6b5,html,markdownify,2019-12-03 16:30:17+00:00,"Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.","Camera ready version, NeurIPS 2019",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1811.11298,Exploring Restart Distributions,"['Arash Tavakoli', 'Vitaly Levdik', 'Riashat Islam', 'Christopher M. Smith', 'Petar Kormushev']",2018-11-27 22:40:01+00:00,arxiv,...,31558705e38edd4517beb841de9de8dd,html,markdownify,2020-08-18 03:42:32+00:00,"We consider the generic approach of using an experience memory to help exploration by adapting a restart distribution. That is, given the capacity to reset the state with those corresponding to the agent's past observations, we help exploration by promoting faster state-space coverage via restarting the agent from a more diverse set of initial states, as well as allowing it to restart in states associated with significant past experiences. This approach is compatible with both on-policy and off-policy methods. However, a caveat is that altering the distribution of initial states could change the optimal policies when searching within a restricted class of policies. To reduce this unsought learning bias, we evaluate our approach in deep reinforcement learning which benefits from the high representational capacity of deep neural networks. We instantiate three variants of our approach, each inspired by an idea in the context of experience replay. Using these variants, we show that performance gains can be achieved, especially in hard exploration problems.",RLDM 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1902.07685,World Discovery Models,"['Mohammad Gheshlaghi Azar', 'Bilal Piot', 'Bernardo Avila Pires', 'Jean-Bastien Grill', 'Florent AltchÃ©', 'RÃ©mi Munos']",2019-02-20 18:07:18+00:00,arxiv,...,6edbaf09a5f228c39e7fdcd4969a957c,html,markdownify,2019-03-01 20:25:58+00:00,"As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.",,,,cs.AI,"['cs.AI', 'stat.AP', 'stat.ML']"
https://arxiv.org/abs/1903.01959,Learning Exploration Policies for Navigation,"['Tao Chen', 'Saurabh Gupta', 'Abhinav Gupta']",2019-03-05 18:03:47+00:00,arxiv,...,6b3231ab077448859aa332286aa6773a,html,markdownify,2019-03-05 18:03:47+00:00,"Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that the use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Code and Videos are available at: https://sites.google.com/view/exploration-for-nav.",,,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2005.05960,Planning to Explore via Self-Supervised World Models,"['Ramanan Sekar', 'Oleh Rybkin', 'Kostas Daniilidis', 'Pieter Abbeel', 'Danijar Hafner', 'Deepak Pathak']",2020-05-12 17:59:45+00:00,arxiv,...,a9049af20d6d77399582f56d2e86cc58,html,markdownify,2020-06-30 23:05:50+00:00,"Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/","Accepted at ICML 2020. Videos and code at
  https://ramanans1.github.io/plan2explore/",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2012.11538,Evaluating Agents without Rewards,"['Brendon Matusch', 'Jimmy Ba', 'Danijar Hafner']",2020-12-21 18:00:39+00:00,arxiv,...,1cb3f73813e0a9e0ce8e40d8cd7522fa,html,markdownify,2021-02-09 22:06:26+00:00,"Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.","15 pages, 6 figures, 5 tables",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']"
https://arxiv.org/abs/1901.05856,Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution,"['Gyeong Taek Lee', 'Chang Ouk Kim']",2019-01-17 15:47:12+00:00,arxiv,...,b71d3894ea7acf6c8d506d128b164073,html,markdownify,2019-01-17 15:47:12+00:00,"This paper proposes a new reinforcement learning (RL) algorithm that enhances exploration by amplifying the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We argue that these two algorithms complement each other and that combining these two algorithms can amplify the imitation effect for exploration. In addition, by adding an intrinsic penalty reward to the state that the RL agent frequently visits and using replay memory for learning the feature state when using an exploration bonus, the proposed approach leads to deep exploration and deviates from the current converged policy. We verified the exploration performance of the algorithm through experiments in a two-dimensional grid environment. In addition, we applied the algorithm to a simulated environment of unmanned combat aerial vehicle (UCAV) mission execution, and the empirical results show that AIE is very effective for finding the UCAV's shortest flight path to avoid an enemy's missiles.",9 pages,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1909.01387,Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,"['Tom Le Paine', 'Caglar Gulcehre', 'Bobak Shahriari', 'Misha Denil', 'Matt Hoffman', 'Hubert Soyer', 'Richard Tanburn', 'Steven Kapturowski', 'Neil Rabinowitz', 'Duncan Williams', 'Gabriel Barth-Maron', 'Ziyu Wang', 'Nando de Freitas', 'Worlds Team']",2019-09-03 18:20:48+00:00,arxiv,...,55047b34812270a552e1589e78130225,html,markdownify,2019-09-03 18:20:48+00:00,"This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1808.01174,Generalization Error in Deep Learning,"['Daniel Jakubovitz', 'Raja Giryes', 'Miguel R. D. Rodrigues']",2018-08-03 12:57:12+00:00,arxiv,...,05643f0c814a8940c3c09b92eac5415b,html,markdownify,2019-04-06 15:25:50+00:00,"Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1804.00222,Meta-Learning Update Rules for Unsupervised Representation Learning,"['Luke Metz', 'Niru Maheswaranathan', 'Brian Cheung', 'Jascha Sohl-Dickstein']",2018-03-31 22:44:28+00:00,arxiv,...,96724a38d9e0e6fc0f6e9adf30c11756,html,markdownify,2019-02-26 05:26:00+00:00,"A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",,,,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1804.04241,Capsules for Object Segmentation,"['Rodney LaLonde', 'Ulas Bagci']",2018-04-11 21:57:57+00:00,arxiv,...,9104915042c55ca23bc4992f34cba7dd,html,markdownify,2018-04-11 21:57:57+00:00,"Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/1903.01611,Stabilizing the Lottery Ticket Hypothesis,"['Jonathan Frankle', 'Gintare Karolina Dziugaite', 'Daniel M. Roy', 'Michael Carbin']",2019-03-05 00:52:12+00:00,arxiv,...,4af00d352cc276bd61b828b859c12c95,html,markdownify,2020-07-20 16:50:33+00:00,"Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the ""lottery ticket hypothesis"" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1% to 7% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork ""stability,"" finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis","This article has been subsumed by ""Linear Mode Connectivity and the
  Lottery Ticket Hypothesis"" (arXiv:1912.05671, ICML 2020). Please read/cite
  that article instead",,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1902.06162,Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,"['Longlong Jing', 'Yingli Tian']",2019-02-16 21:30:18+00:00,arxiv,...,916c2e116fbedf5fa5674567013276e5,html,markdownify,2019-02-16 21:30:18+00:00,"Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1812.11118,Reconciling modern machine learning practice and the bias-variance trade-off,"['Mikhail Belkin', 'Daniel Hsu', 'Siyuan Ma', 'Soumik Mandal']",2018-12-28 17:15:38+00:00,arxiv,...,9e4d947fd9387ab2d2d562d30888270a,html,markdownify,2019-09-10 19:51:04+00:00,"Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.   In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ""double descent"" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.",,,10.1073/pnas.1903070116,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1905.10498,Cold Case: The Lost MNIST Digits,"['Chhavi Yadav', 'LÃ©on Bottou']",2019-05-25 01:50:51+00:00,arxiv,...,e84427a39578b74a4733d42229fb09f4,html,markdownify,2019-11-04 21:05:26+00:00,"Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",Final NeurIPS version,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/2008.08076,Deploying Lifelong Open-Domain Dialogue Learning,"['Kurt Shuster', 'Jack Urbanek', 'Emily Dinan', 'Arthur Szlam', 'Jason Weston']",2020-08-18 17:57:26+00:00,arxiv,...,dc53a63ccf811f59dcbfb3233f5fc7b0,html,markdownify,2020-08-19 16:03:27+00:00,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",,,,cs.AI,"['cs.AI', 'cs.CL']"
https://arxiv.org/abs/2010.11929,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby']",2020-10-22 17:55:59+00:00,arxiv,...,4e94420d70f1b52cc245e4a0b8ead8b7,html,markdownify,2021-06-03 13:08:56+00:00,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","Fine-tuning code and pre-trained models are available at
  https://github.com/google-research/vision_transformer. ICLR camera-ready
  version with 2 small modifications: 1) Added a discussion of CLS vs GAP
  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in
  Figure 5 and Table 6 (relative performance of models is basically not
  affected)",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1806.07912,Resource-Efficient Neural Architect,"['Yanqi Zhou', 'Siavash Ebrahimi', 'Sercan Ã. ArÄ±k', 'Haonan Yu', 'Hairong Liu', 'Greg Diamos']",2018-06-12 20:41:32+00:00,arxiv,...,8ce0fecdc716f949aa80c7f7f73d8ee9,html,markdownify,2018-06-12 20:41:32+00:00,"Neural Architecture Search (NAS) is a laborious process. Prior work on automated NAS targets mainly on improving accuracy, but lacks consideration of computational resource use. We propose the Resource-Efficient Neural Architect (RENA), an efficient resource-constrained NAS using reinforcement learning with network embedding. RENA uses a policy network to process the network embeddings to generate new configurations. We demonstrate RENA on image recognition and keyword spotting (KWS) problems. RENA can find novel architectures that achieve high performance even with tight resource constraints. For CIFAR10, it achieves 2.95% test error when compute intensity is greater than 100 FLOPs/byte, and 3.87% test error when model size is less than 3M parameters. For Google Speech Commands Dataset, RENA achieves the state-of-the-art accuracy without resource constraints, and it outperforms the optimized architectures with tight resource constraints.",,,,cs.NE,"['cs.NE', 'cs.AI']"
https://arxiv.org/abs/1806.09055,DARTS: Differentiable Architecture Search,"['Hanxiao Liu', 'Karen Simonyan', 'Yiming Yang']",2018-06-24 00:06:13+00:00,arxiv,...,78f577b9ad31b8201c362fd9559de87a,html,markdownify,2019-04-23 06:29:32+00:00,"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.","Published at ICLR 2019; Code and pretrained models available at
  https://github.com/quark0/darts",,,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1807.03819,Universal Transformers,"['Mostafa Dehghani', 'Stephan Gouws', 'Oriol Vinyals', 'Jakob Uszkoreit', 'Åukasz Kaiser']",2018-07-10 18:39:15+00:00,arxiv,...,1dc0cd74af0e05fc7065e0d0137b5079,html,markdownify,2019-03-05 16:46:19+00:00,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",Published at ICLR2019,,,cs.CL,"['cs.CL', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1807.10875,TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,"['Augustus Odena', 'Ian Goodfellow']",2018-07-28 02:11:40+00:00,arxiv,...,59f1a6aac2ba1623e80f705963029f12,html,markdownify,2018-07-28 02:11:40+00:00,"Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.",Preprint - work in progress,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1809.05188,CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,"['Jiachen Yang', 'Alireza Nakhaei', 'David Isele', 'Kikuo Fujimura', 'Hongyuan Zha']",2018-09-13 21:46:54+00:00,arxiv,...,b5c3599a6a7b66bc1f1dfd788f8ac893,html,markdownify,2020-01-24 21:24:17+00:00,"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.","Published at International Conference on Learning Representations
  2020",,,cs.LG,"['cs.LG', 'cs.MA', 'stat.ML']"
https://arxiv.org/abs/2010.00581,Emergent Social Learning via Multi-agent Reinforcement Learning,"['Kamal Ndousse', 'Douglas Eck', 'Sergey Levine', 'Natasha Jaques']",2020-10-01 17:54:14+00:00,arxiv,...,5395e2b40b3a02a6356eafcd5d9a63e5,html,markdownify,2021-06-22 21:18:59+00:00,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.","14 pages, 19 figures. To be published in ICML 2021",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']"
https://arxiv.org/abs/1807.00196,Modeling Friends and Foes,"['Pedro A. Ortega', 'Shane Legg']",2018-06-30 16:07:43+00:00,arxiv,...,d5790ab768cef8f32a70d511574c96f8,html,markdownify,2018-06-30 16:07:43+00:00,"How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.","13 pages, 9 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1809.01560,Reinforcement Learning under Threats,"['Victor Gallego', 'Roi Naveiro', 'David Rios Insua']",2018-09-05 14:56:09+00:00,arxiv,...,c57fad9bfc292ee7493eaf82882cb25a,html,markdownify,2019-07-30 12:15:05+00:00,"In several reinforcement learning (RL) scenarios, mainly in security settings, there may be adversaries trying to interfere with the reward generating process. In this paper, we introduce Threatened Markov Decision Processes (TMDPs), which provide a framework to support a decision maker against a potential adversary in RL. Furthermore, we propose a level-$k$ thinking scheme resulting in a new learning framework to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries while the agent learns.","Extends the verson published at the Proceedings of the AAAI
  Conference on Artificial Intelligence 33,
  https://www.aaai.org/ojs/index.php/AAAI/article/view/5106",,10.1609/aaai.v33i01.33019939,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1804.05464,On Gradient-Based Learning in Continuous Games,"['Eric Mazumdar', 'Lillian J. Ratliff', 'S. Shankar Sastry']",2018-04-16 01:14:17+00:00,arxiv,...,8f61256baee27673dcf71fd527f329c3,html,markdownify,2020-02-20 18:26:35+00:00,"We formulate a general framework for competitive gradient-based learning that encompasses a wide breadth of multi-agent learning algorithms, and analyze the limiting behavior of competitive gradient-based learning algorithms using dynamical systems theory. For both general-sum and potential games, we characterize a non-negligible subset of the local Nash equilibria that will be avoided if each agent employs a gradient-based learning algorithm. We also shed light on the issue of convergence to non-Nash strategies in general- and zero-sum games, which may have no relevance to the underlying game, and arise solely due to the choice of algorithm. The existence and frequency of such strategies may explain some of the difficulties encountered when using gradient descent in zero-sum games as, e.g., in the training of generative adversarial networks. To reinforce the theoretical contributions, we provide empirical results that highlight the frequency of linear quadratic dynamic games (a benchmark for multi-agent reinforcement learning) that admit global Nash equilibria that are almost surely avoided by policy gradient.",,"SIAM Journal on Mathematics of Data Science 2020 2:1, 103-131",10.1137/18M1231298,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2002.11174,TanksWorld: A Multi-Agent Environment for AI Safety Research,"['Corban G. Rivera', 'Olivia Lyons', 'Arielle Summitt', 'Ayman Fatima', 'Ji Pak', 'William Shao', 'Robert Chalmers', 'Aryeh Englander', 'Edward W. Staley', 'I-Jeng Wang', 'Ashley J. Llorens']",2020-02-25 21:00:52+00:00,arxiv,...,b3a1c11f04c290bf06ca3abe992c170a,html,markdownify,2020-02-25 21:00:52+00:00,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work.",,,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/2009.03300,Measuring Massive Multitask Language Understanding,"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andy Zou', 'Mantas Mazeika', 'Dawn Song', 'Jacob Steinhardt']",2020-09-07 17:59:25+00:00,arxiv,...,e057337c01ea1b872e4419fb80b2dbac,html,markdownify,2021-01-12 18:57:11+00:00,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.","ICLR 2021; the test and code is available at
  https://github.com/hendrycks/test",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/2103.03874,Measuring Mathematical Problem Solving With the MATH Dataset,"['Dan Hendrycks', 'Collin Burns', 'Saurav Kadavath', 'Akul Arora', 'Steven Basart', 'Eric Tang', 'Dawn Song', 'Jacob Steinhardt']",2021-03-05 18:59:39+00:00,arxiv,...,f7a3ed8b0884c5f9783882a5dc35e8d8,html,markdownify,2021-11-08 21:30:18+00:00,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.","NeurIPS 2021. Code and the MATH dataset is available at
  https://github.com/hendrycks/math/",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']"
https://arxiv.org/abs/2010.14701,Scaling Laws for Autoregressive Generative Modeling,"['Tom Henighan', 'Jared Kaplan', 'Mor Katz', 'Mark Chen', 'Christopher Hesse', 'Jacob Jackson', 'Heewoo Jun', 'Tom B. Brown', 'Prafulla Dhariwal', 'Scott Gray', 'Chris Hallacy', 'Benjamin Mann', 'Alec Radford', 'Aditya Ramesh', 'Nick Ryder', 'Daniel M. Ziegler', 'John Schulman', 'Dario Amodei', 'Sam McCandlish']",2020-10-28 02:17:24+00:00,arxiv,...,cc1f5e1f4d629a192346e247244ae42c,html,markdownify,2020-11-06 04:16:36+00:00,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains.   The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions.   We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.","20+17 pages, 33 figures; added appendix with additional language
  results",,,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV']"
https://arxiv.org/abs/2105.09938,Measuring Coding Challenge Competence With APPS,"['Dan Hendrycks', 'Steven Basart', 'Saurav Kadavath', 'Mantas Mazeika', 'Akul Arora', 'Ethan Guo', 'Collin Burns', 'Samir Puranik', 'Horace He', 'Dawn Song', 'Jacob Steinhardt']",2021-05-20 17:58:42+00:00,arxiv,...,7df8ca28e105e96d162fa2219256dc9b,html,markdownify,2021-11-08 21:16:44+00:00,"While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.","NeurIPS 2021. Code and the APPS dataset is available at
  https://github.com/hendrycks/apps",,,cs.SE,"['cs.SE', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/1812.10352,Learning Not to Learn: Training Deep Neural Networks with Biased Data,"['Byungju Kim', 'Hyunwoo Kim', 'Kyungsu Kim', 'Sungjin Kim', 'Junmo Kim']",2018-12-26 16:01:29+00:00,arxiv,...,66c39c3c66b8e5a381a1f833eef785f3,html,markdownify,2019-04-15 08:42:54+00:00,"We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.","CVPR 2019, Accepted",,,cs.CV,['cs.CV']
https://arxiv.org/abs/1901.04966,Identifying and Correcting Label Bias in Machine Learning,"['Heinrich Jiang', 'Ofir Nachum']",2019-01-15 18:40:06+00:00,arxiv,...,a1e60802a5d9ac3109d205803a8fd6d3,html,markdownify,2019-01-15 18:40:06+00:00,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1809.05630,Towards Better Interpretability in Deep Q-Networks,"['Raghuram Mandyam Annasamy', 'Katia Sycara']",2018-09-15 01:34:27+00:00,arxiv,...,af9687a1563929e868847807ab2f6754,html,markdownify,2018-11-15 03:06:56+00:00,"Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.","Accepted at AAAI-19; (16 pages, 18 figures)",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1811.09720,Representer Point Selection for Explaining Deep Neural Networks,"['Chih-Kuan Yeh', 'Joon Sik Kim', 'Ian E. H. Yen', 'Pradeep Ravikumar']",2018-11-23 22:34:17+00:00,arxiv,...,7a90c6d3941adacff84183931234fa40,html,markdownify,2018-11-23 22:34:17+00:00,"We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.",NIPS 2018,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.00869v1,Training Machine Learning Models by Regularizing their Explanations,['Andrew Slavin Ross'],2018-09-29 17:43:21+00:00,arxiv,...,a785246d43796c9241d450b7c48e154b,html,markdownify,2018-09-29 17:43:21+00:00,"Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).","Harvard CSE master's thesis; includes portions of arxiv:1703.03717
  and arxiv:1711.09404",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1902.06787,Regularizing Black-box Models for Improved Interpretability,"['Gregory Plumb', 'Maruan Al-Shedivat', 'Angel Alexander Cabrera', 'Adam Perer', 'Eric Xing', 'Ameet Talwalkar']",2019-02-18 20:23:12+00:00,arxiv,...,c6d1dc1522e9a285a7dce7ed56ec15b6,html,markdownify,2020-11-08 15:49:08+00:00,"Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, ExpO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for ExpO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1703.03717v2,Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"['Andrew Slavin Ross', 'Michael C. Hughes', 'Finale Doshi-Velez']",2017-03-10 15:35:32+00:00,arxiv,...,04637cfb42c4b2a5addd314aa3818290,html,markdownify,2017-05-25 05:38:45+00:00,"Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1805.07468,Unsupervised Learning of Neural Networks to Explain Neural Networks,"['Quanshi Zhang', 'Yu Yang', 'Yuchen Liu', 'Ying Nian Wu', 'Song-Chun Zhu']",2018-05-18 23:02:14+00:00,arxiv,...,15cc8a14548d9865dae25016fa31d455,html,markdownify,2018-05-18 23:02:14+00:00,"This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.",,,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/1802.07810,Manipulating and Measuring Model Interpretability,"['Forough Poursabzi-Sangdeh', 'Daniel G. Goldstein', 'Jake M. Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",2018-02-21 21:11:36+00:00,arxiv,...,3ffeb5f737b5ddf06ef5b69a34e894fa,html,markdownify,2021-08-15 17:22:43+00:00,"With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",,,,cs.AI,"['cs.AI', 'cs.CY', 'I.2']"
https://arxiv.org/abs/1905.12686,"Learning Representations by Humans, for Humans","['Sophie Hilgard', 'Nir Rosenfeld', 'Mahzarin R. Banaji', 'Jack Cao', 'David C. Parkes']",2019-05-29 19:19:09+00:00,arxiv,...,d6278e880a868ee94d10ec3a7c85954a,html,markdownify,2021-09-15 22:03:35+00:00,"When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This ""Mind Composed with Machine"" framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",,,,cs.LG,"['cs.LG', 'cs.HC', 'stat.ML']"
https://arxiv.org/abs/2105.04857,Leveraging Sparse Linear Layers for Debuggable Deep Networks,"['Eric Wong', 'Shibani Santurkar', 'Aleksander MÄdry']",2021-05-11 08:15:25+00:00,arxiv,...,630a2619917feb74c40103f1d0649f4a,html,markdownify,2021-05-11 08:15:25+00:00,"We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at https://github.com/madrylab/debuggabledeepnetworks.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1907.10580,IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL,"['Nirbhay Modhe', 'Prithvijit Chattopadhyay', 'Mohit Sharma', 'Abhishek Das', 'Devi Parikh', 'Dhruv Batra', 'Ramakrishna Vedantam']",2019-07-24 17:30:39+00:00,arxiv,...,3b990f27c202eab5e568fe5290fb12f7,html,markdownify,2021-01-03 17:37:08+00:00,"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor et.al., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.",,,10.24963/ijcai.2020/280,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1907.01657,Dynamics-Aware Unsupervised Discovery of Skills,"['Archit Sharma', 'Shixiang Gu', 'Sergey Levine', 'Vikash Kumar', 'Karol Hausman']",2019-07-02 21:32:19+00:00,arxiv,...,fabe8e38218f59f7f445dcaedf5bce9b,html,markdownify,2020-02-14 23:20:43+00:00,"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",,,,cs.LG,"['cs.LG', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1811.09656,Hierarchical visuomotor control of humanoids,"['Josh Merel', 'Arun Ahuja', 'Vu Pham', 'Saran Tunyasuvunakool', 'Siqi Liu', 'Dhruva Tirumala', 'Nicolas Heess', 'Greg Wayne']",2018-11-23 19:55:55+00:00,arxiv,...,07154d8bea4c8e6ba5efdfcb203daa0c,html,markdownify,2019-01-15 17:28:36+00:00,"We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .",Accepted as a conference paper at ICLR 2019,,,cs.AI,"['cs.AI', 'cs.RO']"
https://arxiv.org/abs/1901.01365,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,"['Takayuki Osa', 'Voot Tangkaratt', 'Masashi Sugiyama']",2019-01-05 04:43:05+00:00,arxiv,...,0fd759f7f55ed1ea382063218354b238,html,markdownify,2019-03-07 06:34:21+00:00,"Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","16 pages, ICLR 2019",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1903.01567,Model Primitive Hierarchical Lifelong Reinforcement Learning,"['Bohan Wu', 'Jayesh K. Gupta', 'Mykel J. Kochenderfer']",2019-03-04 22:14:23+00:00,arxiv,...,0ae722aace7bce9e7a6f45e6219a2deb,html,markdownify,2019-03-04 22:14:23+00:00,"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.","9 pages, 10 figures. Accepted as a full paper at AAMAS 2019","International Conference on Autonomous Agents and Multiagent
  Systems (AAMAS 2019)",,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']"
https://arxiv.org/abs/1904.01033,Multitask Soft Option Learning,"['Maximilian Igl', 'Andrew Gambardella', 'Jinke He', 'Nantas Nardelli', 'N. Siddharth', 'Wendelin BÃ¶hmer', 'Shimon Whiteson']",2019-04-01 18:01:34+00:00,arxiv,...,336439cd5595029e31e26a0bf10632dc,html,markdownify,2020-06-21 10:36:45+00:00,"We present Multitask Soft Option Learning(MSOL), a hierarchical multitask framework based on Planning as Inference. MSOL extends the concept of options, using separate variational posteriors for each task, regularized by a shared prior. This ''soft'' version of options avoids several instabilities during training in a multitask setting, and provides a natural way to learn both intra-option policies and their terminations. Furthermore, it allows fine-tuning of options for new tasks without forgetting their learned policies, leading to faster training without reducing the expressiveness of the hierarchical policy. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines.",Published at UAI 2020,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1805.08180,Hierarchical Reinforcement Learning with Hindsight,"['Andrew Levy', 'Robert Platt', 'Kate Saenko']",2018-05-21 17:02:53+00:00,arxiv,...,bcf320cc223fcb34b49cffd787a27723,html,markdownify,2019-03-08 17:52:47+00:00,"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.","Duplicate. See arXiv:1712.00948 ""Learning Multi-Level Hierarchies
  with Hindsight"" for latest version",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1804.03980,Emergent Communication through Negotiation,"['Kris Cao', 'Angeliki Lazaridou', 'Marc Lanctot', 'Joel Z Leibo', 'Karl Tuyls', 'Stephen Clark']",2018-04-11 13:48:08+00:00,arxiv,...,7ea6c9d9caafc29a4fb80b350b16b591,html,markdownify,2018-04-11 13:48:08+00:00,"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols -- one grounded in the semantics of the game, and one which is \textit{a priori} ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.",Published as a conference paper at ICLR 2018,,,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.MA']"
https://arxiv.org/abs/1803.07612,Generating Multi-Agent Trajectories using Programmatic Weak Supervision,"['Eric Zhan', 'Stephan Zheng', 'Yisong Yue', 'Long Sha', 'Patrick Lucey']",2018-03-20 19:19:13+00:00,arxiv,...,3dc71a4421f00cc7837bc1c87127f98c,html,markdownify,2019-02-22 05:40:13+00:00,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1811.07882,Guiding Policies with Language via Meta-Learning,"['John D. Co-Reyes', 'Abhishek Gupta', 'Suvansh Sanjeev', 'Nick Altieri', 'Jacob Andreas', 'John DeNero', 'Pieter Abbeel', 'Sergey Levine']",2018-11-19 18:58:42+00:00,arxiv,...,14612d6b871eced8b48e4f6da2f63513,html,markdownify,2019-01-29 18:54:15+00:00,"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",Accepted at ICLR 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.HC']"
https://arxiv.org/abs/1810.00821,"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow","['Xue Bin Peng', 'Angjoo Kanazawa', 'Sam Toyer', 'Pieter Abbeel', 'Sergey Levine']",2018-10-01 17:02:24+00:00,arxiv,...,7530c4f689cc435129a9a1226b547039,html,markdownify,2020-08-25 02:41:11+00:00,"Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1902.05542,Unsupervised Visuomotor Control through Distributional Planning Networks,"['Tianhe Yu', 'Gleb Shevchuk', 'Dorsa Sadigh', 'Chelsea Finn']",2019-02-14 18:54:54+00:00,arxiv,...,84b098205ba2baf1d51b8409b7df9855,html,markdownify,2019-02-14 18:54:54+00:00,"While reinforcement learning (RL) has the potential to enable robots to autonomously acquire a wide range of skills, in practice, RL usually requires manual, per-task engineering of reward functions, especially in real world settings where aspects of the environment needed to compute progress are not directly accessible. To enable robots to autonomously learn skills, we instead consider the problem of reinforcement learning without access to rewards. We aim to learn an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our approach explicitly optimizes for a metric space under which action sequences that reach a particular state are optimal when the goal is the final state reached. This enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms. Our experiments on three simulated environments and two real-world manipulation problems show that our method can learn effective goal metrics from unlabeled interaction, and use the learned goal metrics for autonomous reinforcement learning.",Videos available at https://sites.google.com/view/dpn-public/,,,cs.RO,"['cs.RO', 'cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.03877v2,Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,"['Smitha Milli', 'Anca D. Dragan']",2019-03-09 21:58:46+00:00,arxiv,...,b11098f5cbbe63bb3eea8142397ea7a1,html,markdownify,2019-06-29 03:26:48+00:00,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (""robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",Published at UAI 2019,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1902.04257,Deep Reinforcement Learning from Policy-Dependent Human Feedback,"['Dilip Arumugam', 'Jun Ki Lee', 'Sophie Saskin', 'Michael L. Littman']",2019-02-12 06:45:21+00:00,arxiv,...,a5edc14f154b50c4235d59deaa204d58,html,markdownify,2019-02-12 06:45:21+00:00,"To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1905.12888,Imitation Learning as $f$-Divergence Minimization,"['Liyiming Ke', 'Sanjiban Choudhury', 'Matt Barnes', 'Wen Sun', 'Gilwoo Lee', 'Siddhartha Srinivasa']",2019-05-30 07:19:13+00:00,arxiv,...,690fba623164a9deadd530dbe1c50e3b,html,markdownify,2020-05-31 08:38:33+00:00,"We address the problem of imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argue that in many tasks it is sufficient to imitate any one of them. We show that the state-of-the-art methods such as GAIL and behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight is to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We propose a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we are able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results show that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.","International Workshop on the Algorithmic Foundations of Robotics
  (WAFR) 2020",,,cs.LG,"['cs.LG', 'cs.IT', 'cs.RO', 'math.IT', 'stat.ML']"
https://arxiv.org/abs/1709.10163,Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces,"['Garrett Warnell', 'Nicholas Waytowich', 'Vernon Lawhern', 'Peter Stone']",2017-09-28 20:43:40+00:00,arxiv,...,bd3d4e94e4f8ffcb8ecffd85439a9c85,html,markdownify,2018-01-19 20:36:13+00:00,"While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.","9 pages, 6 figures",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1701.06049,Interactive Learning from Policy-Dependent Human Feedback,"['James MacGlashan', 'Mark K Ho', 'Robert Loftin', 'Bei Peng', 'David Roberts', 'Matthew E. Taylor', 'Michael L. Littman']",2017-01-21 16:37:41+00:00,arxiv,...,02aafab1717a262ca5736d886130cb4e,html,markdownify,2017-01-21 16:37:41+00:00,"For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false---whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.","7 pages, 2 figures",,,cs.AI,"['cs.AI', 'I.2.6']"
https://arxiv.org/abs/1912.05652,Learning Human Objectives by Evaluating Hypothetical Behavior,"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine', 'Shane Legg', 'Jan Leike']",2019-12-05 18:25:48+00:00,arxiv,...,fe2a9394bbdc9c67d62b43cf45cc2d6e,html,markdownify,2021-03-24 22:26:35+00:00,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",Published at International Conference on Machine Learning (ICML) 2020,,,cs.CY,"['cs.CY', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2005.09382,Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,"['Felix Hill', 'Sona Mokra', 'Nathaniel Wong', 'Tim Harley']",2020-05-19 12:16:58+00:00,arxiv,...,a1d767163236fa23c6acb8255a61bd41,html,markdownify,2020-05-19 12:16:58+00:00,"Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",,,,cs.CL,['cs.CL']
https://arxiv.org/abs/2006.13208,Feature Expansive Reward Learning: Rethinking Human Input,"['Andreea Bobu', 'Marius Wiggert', 'Claire Tomlin', 'Anca D. Dragan']",2020-06-23 17:59:34+00:00,arxiv,...,5fa2a24140de238751c41f34333c1c66,html,markdownify,2021-01-12 18:59:50+00:00,"When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.","13 pages, 14 figures",,10.1145/3434073.3444667,cs.RO,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.06544,"Deep Imitative Models for Flexible Inference, Planning, and Control","['Nicholas Rhinehart', 'Rowan McAllister', 'Sergey Levine']",2018-10-15 17:51:03+00:00,arxiv,...,f5ad03bbcba6989820f16f420f5d6d3b,html,markdownify,2019-10-01 00:13:58+00:00,"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1906.10187,Learning to Interactively Learn and Assist,"['Mark Woodward', 'Chelsea Finn', 'Karol Hausman']",2019-06-24 19:23:27+00:00,arxiv,...,2fab2c6381be89e4ba54c9104ced6719,html,markdownify,2019-11-19 21:04:13+00:00,"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.","AAAI 2020. Video overview at https://youtu.be/8yBvDBuAPrw, paper
  website with videos and interactive game at
  http://interactive-learning.github.io/",,,cs.AI,"['cs.AI', 'cs.LG', 'cs.MA']"
https://arxiv.org/abs/1805.08010,Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']",2018-05-21 12:15:34+00:00,arxiv,...,11e8395413e1f0d430615acee8548896,html,markdownify,2019-01-05 18:30:29+00:00,"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",Accepted at Neural Information Processing Systems (NeurIPS) 2018,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1805.00899,AI safety via debate,"['Geoffrey Irving', 'Paul Christiano', 'Dario Amodei']",2018-05-02 16:27:32+00:00,arxiv,...,adafe807cab6c49a7ab496564dfe9b8d,html,markdownify,2018-10-22 17:36:07+00:00,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.","24 pages, 6 figures",,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/2002.09758,Unsupervised Question Decomposition for Question Answering,"['Ethan Perez', 'Patrick Lewis', 'Wen-tau Yih', 'Kyunghyun Cho', 'Douwe Kiela']",2020-02-22 19:40:35+00:00,arxiv,...,493990a5397a89eb251789c80acadd21,html,markdownify,2020-10-06 18:47:48+00:00,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.","EMNLP 2020 Camera-Ready. Code available at
  https://github.com/facebookresearch/UnsupervisedDecomposition",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2103.03872,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']",2021-03-05 18:58:32+00:00,arxiv,...,7d152e296dc0632c70040bf81b8485b2,html,markdownify,2021-03-05 18:58:32+00:00,"We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.","Code at https://github.com/ethanjperez/rda along with a script to run
  RDA on your own dataset",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']"
https://arxiv.org/abs/1806.08340,Interpretable Discovery in Large Image Data Sets,"['Kiri L. Wagstaff', 'Jake Lee']",2018-06-21 17:30:26+00:00,arxiv,...,ec8f7dc2a0fb35967e598f2d7fd4c405,html,markdownify,2018-06-21 17:30:26+00:00,"Automated detection of new, interesting, unusual, or anomalous images within large data sets has great value for applications from surveillance (e.g., airport security) to science (observations that don't fit a given theory can lead to new discoveries). Many image data analysis systems are turning to convolutional neural networks (CNNs) to represent image content due to their success in achieving high classification accuracy rates. However, CNN representations are notoriously difficult for humans to interpret. We describe a new strategy that combines novelty detection with CNN image features to achieve rapid discovery with interpretable explanations of novel image content. We applied this technique to familiar images from ImageNet as well as to a scientific image collection from planetary science.","Presented at the 2018 ICML Workshop on Human Interpretability in
  Machine Learning (WHI 2018), Stockholm, Sweden",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1809.06995,Interpretable Reinforcement Learning with Ensemble Methods,"['Alexander Brown', 'Marek Petrik']",2018-09-19 03:23:35+00:00,arxiv,...,f218713bace3bb43fdb186bdf3be0514,html,markdownify,2018-09-19 03:23:35+00:00,"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1803.04765,"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning","['Nicolas Papernot', 'Patrick McDaniel']",2018-03-13 13:02:13+00:00,arxiv,...,182d60d5e513ad9534bad717283c0eea,html,markdownify,2018-03-13 13:02:13+00:00,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.00184,Stakeholders in Explainable AI,"['Alun Preece', 'Dan Harborne', 'Dave Braines', 'Richard Tomsett', 'Supriyo Chakraborty']",2018-09-29 10:15:18+00:00,arxiv,...,966907236321228438d79b9ef527780f,html,markdownify,2018-09-29 10:15:18+00:00,"There is general consensus that it is important for artificial intelligence (AI) and machine learning systems to be explainable and/or interpretable. However, there is no general consensus over what is meant by 'explainable' and 'interpretable'. In this paper, we argue that this lack of consensus is due to there being several distinct stakeholder communities. We note that, while the concerns of the individual communities are broadly compatible, they are not identical, which gives rise to different intents and requirements for explainability/interpretability. We use the software engineering distinction between validation and verification, and the epistemological distinctions between knowns/unknowns, to tease apart the concerns of the stakeholder communities and highlight the areas where their foci overlap or diverge. It is not the purpose of the authors of this paper to 'take sides' - we count ourselves as members, to varying degrees, of multiple communities - but rather to help disambiguate what stakeholders mean when they ask 'Why?' of an AI.","Presented at AAAI FSS-18: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1810.03292,Sanity Checks for Saliency Maps,"['Julius Adebayo', 'Justin Gilmer', 'Michael Muelly', 'Ian Goodfellow', 'Moritz Hardt', 'Been Kim']",2018-10-08 07:27:11+00:00,arxiv,...,f7e0653ef3bdeae39835c67ae5c12220,html,markdownify,2020-11-06 13:40:14+00:00,"Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.","Updating Guided Backprop experiments due to bug. The results and
  conclusions remain the same",,,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1806.05502,Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes,"['Fabian B. Fuchs', 'Oliver Groth', 'Adam R. Kosiorek', 'Alex Bewley', 'Markus Wulfmeier', 'Andrea Vedaldi', 'Ingmar Posner']",2018-06-14 12:35:50+00:00,arxiv,...,76494e95d697ee931f63268c275fbffb,html,markdownify,2019-09-06 13:49:37+00:00,"Visually predicting the stability of block towers is a popular task in the domain of intuitive physics. While previous work focusses on prediction accuracy, a one-dimensional performance measure, we provide a broader analysis of the learned physical understanding of the final model and how the learning process can be guided. To this end, we introduce neural stethoscopes as a general purpose framework for quantifying the degree of importance of specific factors of influence in deep neural networks as well as for actively promoting and suppressing information as appropriate. In doing so, we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We apply neural stethoscopes to analyse the state-of-the-art neural network for stability prediction. We show that the baseline model is susceptible to being misled by incorrect visual cues. This leads to a performance breakdown to the level of random guessing when training on scenarios where visual cues are inversely correlated with stability. Using stethoscopes to promote meaningful feature extraction increases performance from 51% to 90% prediction accuracy. Conversely, training on an easy dataset where visual cues are positively correlated with stability, the baseline model learns a bias leading to poor performance on a harder dataset. Using an adversarial stethoscope, the network is successfully de-biased, leading to a performance increase from 66% to 88%.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/1806.00069,Explaining Explanations: An Overview of Interpretability of Machine Learning,"['Leilani H. Gilpin', 'David Bau', 'Ben Z. Yuan', 'Ayesha Bajwa', 'Michael Specter', 'Lalana Kagal']",2018-05-31 19:48:00+00:00,arxiv,...,773ae54efafaac4c0663411ad2dd672d,html,markdownify,2019-02-03 21:06:50+00:00,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","The 5th IEEE International Conference on Data Science and Advanced
  Analytics (DSAA 2018). [Research Track]",,,cs.AI,"['cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2103.03938v1,Causal Analysis of Agent Behavior for AI Safety,"['GrÃ©goire DÃ©letang', 'Jordi Grau-Moya', 'Miljan Martic', 'Tim Genewein', 'Tom McGrath', 'Vladimir Mikulik', 'Markus Kunesch', 'Shane Legg', 'Pedro A. Ortega']",2021-03-05 20:51:12+00:00,arxiv,...,88fee0579164e3fcfd4ecf453c16e0da,html,markdownify,2021-03-05 20:51:12+00:00,"As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.","16 pages, 16 figures, 6 tables",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1809.03060,Active Inverse Reward Design,"['SÃ¶ren Mindermann', 'Rohin Shah', 'Adam Gleave', 'Dylan Hadfield-Menell']",2018-09-09 23:30:59+00:00,arxiv,...,e6d69b3d9c8fc63a4a448c6c5b4d9db5,html,markdownify,2019-11-06 17:41:15+00:00,"Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.00482,Few-Shot Goal Inference for Visuomotor Learning and Planning,"['Annie Xie', 'Avi Singh', 'Sergey Levine', 'Chelsea Finn']",2018-09-30 22:57:58+00:00,arxiv,...,6f025c2fcef9ed25a4cce3b00cc4d9b2,html,markdownify,2018-09-30 22:57:58+00:00,"Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.",Videos available at https://sites.google.com/view/few-shot-goals,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1809.02925,Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,"['Ilya Kostrikov', 'Kumar Krishna Agrawal', 'Debidatta Dwibedi', 'Sergey Levine', 'Jonathan Tompson']",2018-09-09 05:37:25+00:00,arxiv,...,0cb2cdac2fe6df7f959bf4e63aee8d7a,html,markdownify,2018-10-15 17:27:25+00:00,"We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1810.11043,One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks,"['Tianhe Yu', 'Pieter Abbeel', 'Sergey Levine', 'Chelsea Finn']",2018-10-25 18:05:08+00:00,arxiv,...,8902e46d593837ba1a81f2dad1cb3853,html,markdownify,2018-10-25 18:05:08+00:00,"We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by ""watching"" a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs.",Video results available at https://sites.google.com/view/one-shot-hil,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1905.11108,SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,"['Siddharth Reddy', 'Anca D. Dragan', 'Sergey Levine']",2019-05-27 10:29:31+00:00,arxiv,...,a673a8423161959b5bb64aba66edbd17,html,markdownify,2019-09-25 18:44:47+00:00,"Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1909.13392,Learning from Observations Using a Single Video Demonstration and Human Feedback,"['Sunil Gandhi', 'Tim Oates', 'Tinoosh Mohsenin', 'Nicholas Waytowich']",2019-09-29 22:44:59+00:00,arxiv,...,a552a879a011ad6233175ccfea92a893,html,markdownify,2019-09-29 22:44:59+00:00,"In this paper, we present a method for learning from video demonstrations by using human feedback to construct a mapping between the standard representation of the agent and the visual representation of the demonstration. In this way, we leverage the advantages of both these representations, i.e., we learn the policy using standard state representations, but are able to specify the expected behavior using video demonstration. We train an autonomous agent using a single video demonstration and use human feedback (using numerical similarity rating) to map the standard representation to the visual representation with a neural network. We show the effectiveness of our method by teaching a hopper agent in the MuJoCo to perform a backflip using a single video demonstration generated in MuJoCo as well as from a real-world YouTube video of a person performing a backflip. Additionally, we show that our method can transfer to new tasks, such as hopping, with very little human feedback.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1805.08336,Maximum Causal Tsallis Entropy Imitation Learning,"['Kyungjae Lee', 'Sungjoon Choi', 'Songhwai Oh']",2018-05-22 00:49:47+00:00,arxiv,...,05dbe8badac102db748a6e7b7b56ec8b,html,markdownify,2018-05-28 04:24:06+00:00,"In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1909.12200,Scaling data-driven robotics with reward sketching and batch reinforcement learning,"['Serkan Cabi', 'Sergio GÃ³mez Colmenarejo', 'Alexander Novikov', 'Ksenia Konyushkova', 'Scott Reed', 'Rae Jeong', 'Konrad Zolna', 'Yusuf Aytar', 'David Budden', 'Mel Vecerik', 'Oleg Sushkov', 'David Barker', 'Jonathan Scholz', 'Misha Denil', 'Nando de Freitas', 'Ziyu Wang']",2019-09-26 15:45:23+00:00,arxiv,...,e4f1b40384fa7356b34effcfafddaedc,html,markdownify,2020-06-04 11:00:06+00:00,"We present a framework for data-driven robotics that makes use of a large dataset of recorded robot experience and scales to several tasks using learned reward functions. We show how to apply this framework to accomplish three different object manipulation tasks on a real robot platform. Given demonstrations of a task together with task-agnostic recorded experience, we use a special form of human annotation as supervision to learn a reward function, which enables us to deal with real-world tasks where the reward signal cannot be acquired directly. Learned rewards are used in combination with a large dataset of experience from different tasks to learn a robot policy offline using batch RL. We show that using our approach it is possible to train agents to perform a variety of challenging manipulation tasks including stacking rigid objects and handling cloth.",Project website: https://sites.google.com/view/data-driven-robotics/,Robotics: Science and Systems Conference 2020,,cs.RO,"['cs.RO', 'cs.LG']"
https://arxiv.org/abs/2003.05012,Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning,"['Stephanie Milani', 'Nicholay Topin', 'Brandon Houghton', 'William H. Guss', 'Sharada P. Mohanty', 'Keisuke Nakata', 'Oriol Vinyals', 'Noboru Sean Kuno']",2020-03-10 21:39:52+00:00,arxiv,...,92b4e3f4cd1ee9485caa7647ad2e8856,html,markdownify,2020-06-18 16:54:23+00:00,"To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.","To appear in Proceedings of Machine Learning Research: NeurIPS 2019
  Competition & Demonstration Track Postproceedings. 12 pages, 2 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2005.00582,Learning to Complement Humans,"['Bryan Wilder', 'Eric Horvitz', 'Ece Kamar']",2020-05-01 20:00:23+00:00,arxiv,...,cba130030b691cb391ea2f2c590374b2,html,markdownify,2020-05-01 20:00:23+00:00,"A rising vision for AI in the open world centers on the development of systems that can complement humans for perceptual, diagnostic, and reasoning tasks. To date, systems aimed at complementing the skills of people have employed models trained to be as accurate as possible in isolation. We demonstrate how an end-to-end learning strategy can be harnessed to optimize the combined performance of human-machine teams by considering the distinct abilities of people and machines. The goal is to focus machine learning on problem instances that are difficult for humans, while recognizing instances that are difficult for the machine and seeking human input on them. We demonstrate in two real-world domains (scientific discovery and medical diagnosis) that human-machine teams built via these methods outperform the individual performance of machines and people. We then analyze conditions under which this complementarity is strongest, and which training methods amplify it. Taken together, our work provides the first systematic investigation of how machine learning systems can be trained to complement human reasoning.",Accepted at IJCAI 2020,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/2002.09089,Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"['Daniel S. Brown', 'Russell Coleman', 'Ravi Srinivasan', 'Scott Niekum']",2020-02-21 02:04:54+00:00,arxiv,...,9aae86a616cac32e91dd883767618185,html,markdownify,2020-12-17 21:48:13+00:00,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.",In proceedings ICML 2020,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2102.02872,Feedback in Imitation Learning: The Three Regimes of Covariate Shift,"['Jonathan Spencer', 'Sanjiban Choudhury', 'Arun Venkatraman', 'Brian Ziebart', 'J. Andrew Bagnell']",2021-02-04 20:18:56+00:00,arxiv,...,a9b4b3da01f7078940bf6ee8e59c377e,html,markdownify,2021-02-11 14:55:12+00:00,"Imitation learning practitioners have often noted that conditioning policies on previous actions leads to a dramatic divergence between ""held out"" error and performance of the learner in situ. Interactive approaches can provably address this divergence but require repeated querying of a demonstrator. Recent work identifies this divergence as stemming from a ""causal confound"" in predicting the current action, and seek to ablate causal aspects of current state using tools from causal inference. In this work, we argue instead that this divergence is simply another manifestation of covariate shift, exacerbated particularly by settings of feedback between decisions and input features. The learner often comes to rely on features that are strongly predictive of decisions, but are subject to strong covariate shift.   Our work demonstrates a broad class of problems where this shift can be mitigated, both theoretically and practically, by taking advantage of a simulator but without any further querying of expert demonstration. We analyze existing benchmarks used to test imitation learning approaches and find that these benchmarks are realizable and simple and thus insufficient for capturing the harder regimes of error compounding seen in real-world decision making problems. We find, in a surprising contrast with previous literature, but consistent with our theory, that naive behavioral cloning provides excellent results. We detail the need for new standardized benchmarks that capture the phenomena seen in robotics problems.",,,,cs.LG,"['cs.LG', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2004.03607,TuringAdvice: A Generative and Dynamic Evaluation of Language Use,"['Rowan Zellers', 'Ari Holtzman', 'Elizabeth Clark', 'Lianhui Qin', 'Ali Farhadi', 'Yejin Choi']",2020-04-07 18:00:03+00:00,arxiv,...,1e91ae1905cd3f7020b4eac1dcb31c33,html,markdownify,2021-04-13 01:05:17+00:00,"We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other.   Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.","NAACL 2021 camera ready. Project page at
  https://rowanzellers.com/advice",,,cs.CL,['cs.CL']
https://arxiv.org/abs/2106.00672,What Matters for Adversarial Imitation Learning?,"['Manu Orsini', 'Anton Raichuk', 'LÃ©onard Hussenot', 'Damien Vincent', 'Robert Dadashi', 'Sertan Girgin', 'Matthieu Geist', 'Olivier Bachem', 'Olivier Pietquin', 'Marcin Andrychowicz']",2021-06-01 17:58:08+00:00,arxiv,...,55c620be8d8598ddcee5c2457092e35b,html,markdownify,2021-06-01 17:58:08+00:00,"Adversarial imitation learning has become a popular framework for imitation in continuous control. Over the years, several variations of its components were proposed to enhance the performance of the learned policies as well as the sample complexity of the algorithm. In practice, these choices are rarely tested all together in rigorous empirical studies. It is therefore difficult to discuss and understand what choices, among the high-level algorithmic options as well as low-level implementation details, matter. To tackle this issue, we implement more than 50 of these choices in a generic adversarial imitation learning framework and investigate their impacts in a large-scale study (>500k trained agents) with both synthetic and human-generated demonstrations. While many of our findings confirm common practices, some of them are surprising or even contradict prior work. In particular, our results suggest that artificial demonstrations are not a good proxy for human data and that the very common practice of evaluating imitation algorithms only with synthetic demonstrations may lead to algorithms which perform poorly in the more realistic scenarios with human demonstrations.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']"
https://arxiv.org/abs/2105.12938,Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors,"['Christian Arzate Cruz', 'Takeo Igarashi']",2021-05-27 04:17:48+00:00,arxiv,...,393fda58fa115ce1f0e7763168451b80,html,markdownify,2021-05-27 04:17:48+00:00,"Reinforcement learning techniques successfully generate convincing agent behaviors, but it is still difficult to tailor the behavior to align with a user's specific preferences. What is missing is a communication method for the system to explain the behavior and for the user to repair it. In this paper, we present a novel interaction method that uses interactive explanations using templates of natural language as a communication method. The main advantage of this interaction method is that it enables a two-way communication channel between users and the agent; the bot can explain its thinking procedure to the users, and the users can communicate their behavior preferences to the bot using the same interactive explanations. In this manner, the thinking procedure of the bot is transparent, and users can provide corrections to the bot that include a suggested action to take, a goal to achieve, and the reasons behind these decisions. We tested our proposed method in a clone of the video game named \textit{Super Mario Bros.}, and the results demonstrate that our interactive explanation approach is effective at diagnosing and repairing bot behaviors.",,,,cs.HC,['cs.HC']
https://arxiv.org/abs/1810.07483,O2A: One-shot Observational learning with Action vectors,"['Leo Pauly', 'Wisdom C. Agboh', 'David C. Hogg', 'Raul Fuentes']",2018-10-17 11:33:11+00:00,arxiv,...,b62be2cf9c130f5fb68153d500451725,html,markdownify,2020-12-21 13:43:15+00:00,"We present O2A, a novel method for learning to perform robotic manipulation tasks from a single (one-shot) third-person demonstration video. To our knowledge, it is the first time this has been done for a single demonstration. The key novelty lies in pre-training a feature extractor for creating a perceptual representation for actions that we call 'action vectors'. The action vectors are extracted using a 3D-CNN model pre-trained as an action classifier on a generic action dataset. The distance between the action vectors from the observed third-person demonstration and trial robot executions is used as a reward for reinforcement learning of the demonstrated task. We report on experiments in simulation and on a real robot, with changes in viewpoint of observation, properties of the objects involved, scene background and morphology of the manipulator between the demonstration and the learning domains. O2A outperforms baseline approaches under different domain shifts and has comparable performance with an oracle (that uses an ideal reward function).",,Front. Robot. AI 8:686368 (2021),10.3389/frobt.2021.686368,cs.RO,"['cs.RO', 'cs.LG']"
https://arxiv.org/abs/1810.10593,Inverse reinforcement learning for video games,"['Aaron Tucker', 'Adam Gleave', 'Stuart Russell']",2018-10-24 20:00:50+00:00,arxiv,...,df6b43539d1d15f90cdff7b3534cddd8,html,markdownify,2018-10-24 20:00:50+00:00,"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.","10 pages, 4 figures. Submitted to NIPS Deep RL Workshop",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML', 'I.2.6']"
https://arxiv.org/abs/1902.06766,Parenting: Safe Reinforcement Learning from Human Input,"['Christopher Frye', 'Ilya Feige']",2019-02-18 19:10:18+00:00,arxiv,...,bb7ed5ecac793f8a549486dd2f8060dc,html,markdownify,2019-02-18 19:10:18+00:00,"Autonomous agents trained via reinforcement learning present numerous safety concerns: reward hacking, negative side effects, and unsafe exploration, among others. In the context of near-future autonomous agents, operating in environments where humans understand the existing dangers, human involvement in the learning process has proved a promising approach to AI Safety. Here we demonstrate that a precise framework for learning from human input, loosely inspired by the way humans parent children, solves a broad class of safety problems in this context. We show that our Parenting algorithm solves these problems in the relevant AI Safety gridworlds of Leike et al. (2017), that an agent can learn to outperform its parent as it ""matures"", and that policies learnt through Parenting are generalisable to new environments.","9 pages, 4 figures, 1 table",,,cs.AI,"['cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2003.06066,Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft,"['Christian Scheller', 'Yanick Schraner', 'Manfred Vogel']",2020-03-12 23:46:16+00:00,arxiv,...,476a27df0583713cd29da0fd31c1b534,html,markdownify,2020-03-12 23:46:16+00:00,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.","10 pages, 2 figures",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1905.09335,Imitation Learning from Video by Leveraging Proprioception,"['Faraz Torabi', 'Garrett Warnell', 'Peter Stone']",2019-05-22 19:21:05+00:00,arxiv,...,4a3982967dda63a81cca810ed0b299ea,html,markdownify,2019-06-19 03:59:10+00:00,"Classically, imitation learning algorithms have been developed for idealized situations, e.g., the demonstrations are often required to be collected in the exact same environment and usually include the demonstrator's actions. Recently, however, the research community has begun to address some of these shortcomings by offering algorithmic solutions that enable imitation learning from observation (IfO), e.g., learning to perform a task from visual demonstrations that may be in a different environment and do not include actions. Motivated by the fact that agents often also have access to their own internal states (i.e., proprioception), we propose and study an IfO algorithm that leverages this information in the policy learning process. The proposed architecture learns policies over proprioceptive state representations and compares the resulting trajectories visually to the demonstration data. We experimentally test the proposed technique on several MuJoCo domains and show that it outperforms other imitation from observation algorithms by a large margin.","International Joint Conference on Artificial Intelligence (IJCAI
  2019)",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2010.10181,Robust Imitation Learning from Noisy Demonstrations,"['Voot Tangkaratt', 'Nontawat Charoenphakdee', 'Masashi Sugiyama']",2020-10-20 10:41:37+00:00,arxiv,...,c82bc6423d8540a155283fdbb37f159a,html,markdownify,2021-02-19 13:38:24+00:00,"Robust learning from noisy demonstrations is a practical but highly challenging problem in imitation learning. In this paper, we first theoretically show that robust imitation learning can be achieved by optimizing a classification risk with a symmetric loss. Based on this theoretical finding, we then propose a new imitation learning method that optimizes the classification risk by effectively combining pseudo-labeling with co-training. Unlike existing methods, our method does not require additional labels or strict assumptions about noise distributions. Experimental results on continuous-control benchmarks show that our method is more robust compared to state-of-the-art methods.","16 pages, 9 figures. Accepted to AISTATS 2021",,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1806.09795,Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games,"['Xiaomin Lin', 'Stephen C. Adams', 'Peter A. Beling']",2018-06-26 05:14:13+00:00,arxiv,...,c6a7564eeb4e2a97ffd938b196e24367,html,markdownify,2019-10-11 01:32:22+00:00,"This paper addresses the problem of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurate estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.",30 pages,"Journal of Artificial Intelligence Research 66 (2019), pp 473-502",10.1613/jair.1.11541,cs.LG,"['cs.LG', 'cs.GT', 'stat.ML']"
https://arxiv.org/abs/1806.08479,Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning,"['Xinlei Pan', 'Eshed Ohn-Bar', 'Nicholas Rhinehart', 'Yan Xu', 'Yilin Shen', 'Kris M. Kitani']",2018-06-22 03:24:00+00:00,arxiv,...,710f725e4e38943c1c58947b01c0c787,html,markdownify,2018-06-22 03:24:00+00:00,"Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.",,,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/2012.05862,Understanding Learned Reward Functions,"['Eric J. Michaud', 'Adam Gleave', 'Stuart Russell']",2020-12-10 18:19:48+00:00,arxiv,...,c07cdf01ddf79d84cd623d02c169e34f,html,markdownify,2020-12-10 18:19:48+00:00,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.","Presented at Deep RL Workshop, NeurIPS 2020",,,cs.LG,['cs.LG']
https://arxiv.org/abs/2012.05672,Imitating Interactive Intelligence,"['Josh Abramson', 'Arun Ahuja', 'Iain Barr', 'Arthur Brussee', 'Federico Carnevale', 'Mary Cassin', 'Rachita Chhaparia', 'Stephen Clark', 'Bogdan Damoc', 'Andrew Dudzik', 'Petko Georgiev', 'Aurelia Guy', 'Tim Harley', 'Felix Hill', 'Alden Hung', 'Zachary Kenton', 'Jessica Landon', 'Timothy Lillicrap', 'Kory Mathewson', 'SoÅa MokrÃ¡', 'Alistair Muldal', 'Adam Santoro', 'Nikolay Savinov', 'Vikrant Varma', 'Greg Wayne', 'Duncan Williams', 'Nathaniel Wong', 'Chen Yan', 'Rui Zhu']",2020-12-10 13:55:47+00:00,arxiv,...,94e4c43cfce7df7bb09f6523bea036e5,html,markdownify,2021-01-21 03:25:38+00:00,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA']"
https://arxiv.org/abs/2009.14715v3,Learning Rewards from Linguistic Feedback,"['Theodore R. Sumers', 'Mark K. Ho', 'Robert D. Hawkins', 'Karthik Narasimhan', 'Thomas L. Griffiths']",2020-09-30 14:51:00+00:00,arxiv,...,9e88997359f8a6471d6f505fb9c73082,html,markdownify,2021-07-03 19:03:12+00:00,"We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the ""pragmatic"" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.","9 pages, 4 figures. AAAI '21",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2103.12656,Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,"['Benjamin Eysenbach', 'Sergey Levine', 'Ruslan Salakhutdinov']",2021-03-23 16:19:55+00:00,arxiv,...,89272440b9562547a5fc2847a6975c82,html,markdownify,2021-12-30 20:26:31+00:00,"Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.","NeurIPS 2021 (oral). Website with code, videos, and blog post:
  https://ben-eysenbach.github.io/rce",,,cs.LG,"['cs.LG', 'cs.RO']"
https://arxiv.org/abs/1807.06158,Generative Adversarial Imitation from Observation,"['Faraz Torabi', 'Garrett Warnell', 'Peter Stone']",2018-07-17 00:25:15+00:00,arxiv,...,665e850f758cc9d94905f5c02e90ae3f,html,markdownify,2019-06-18 04:56:56+00:00,"Imitation from observation (IfO) is the problem of learning directly from state-only demonstrations without having access to the demonstrator's actions. The lack of action information both distinguishes IfO from most of the literature in imitation learning, and also sets it apart as a method that may enable agents to learn from a large set of previously inapplicable resources such as internet videos. In this paper, we propose both a general framework for IfO approaches and also a new IfO approach based on generative adversarial networks called generative adversarial imitation from observation (GAIfO). We conduct experiments in two different settings: (1) when demonstrations consist of low-dimensional, manually-defined state features, and (2) when demonstrations consist of high-dimensional, raw visual data. We demonstrate that our approach performs comparably to classical imitation learning approaches (which have access to the demonstrator's actions) and significantly outperforms existing imitation from observation methods in high-dimensional simulation environments.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2111.06956,Human irrationality: both bad and good for reward inference,"['Lawrence Chan', 'Andrew Critch', 'Anca Dragan']",2021-11-12 21:44:15+00:00,arxiv,...,79ccded364c0e050ec98ae609ab629e5,html,markdownify,2021-11-12 21:44:15+00:00,"Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference.   We find that wrongly modeling a systematically irrational human as noisy-rational performs a lot worse than correctly capturing these biases -- so much so that it can be better to skip inference altogether and stick to the prior! More importantly, we show that an irrational human, when correctly modelled, can communicate more information about the reward than a perfectly rational human can. That is, if a robot has the correct model of a human's irrationality, it can make an even stronger inference than it ever could if the human were rational. Irrationality fundamentally helps rather than hinder reward inference, but it needs to be correctly accounted for.","12 pages, 10 figures",,,cs.LG,['cs.LG']
https://arxiv.org/abs/1904.07633,HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,"['Oguzhan Gencoglu', 'Mark van Gils', 'Esin Guldogan', 'Chamin Morikawa', 'Mehmet SÃ¼zen', 'Mathias Gruber', 'Jussi Leinonen', 'Heikki Huttunen']",2019-04-16 13:02:01+00:00,arxiv,...,e234194ac5b38c7c19f1395ccf75f090,html,markdownify,2019-04-16 13:02:01+00:00,"Recent advancements in machine learning research, i.e., deep learning, introduced methods that excel conventional algorithms as well as humans in several complex tasks, ranging from detection of objects in images and speech recognition to playing difficult strategic games. However, the current methodology of machine learning research and consequently, implementations of the real-world applications of such algorithms, seems to have a recurring HARKing (Hypothesizing After the Results are Known) issue. In this work, we elaborate on the algorithmic, economic and social reasons and consequences of this phenomenon. We present examples from current common practices of conducting machine learning research (e.g. avoidance of reporting negative results) and failure of generalization ability of the proposed algorithms and datasets in actual real-life usage. Furthermore, a potential future trajectory of machine learning research and development from the perspective of accountable, unbiased, ethical and privacy-aware algorithmic decision making is discussed. We would like to emphasize that with this discussion we neither claim to provide an exhaustive argumentation nor blame any specific institution or individual on the raised issues. This is simply a discussion put forth by us, insiders of the machine learning field, reflecting on us.",13 pages,,,cs.LG,['cs.LG']
https://arxiv.org/abs/1905.10985,"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",['Jeff Clune'],2019-05-27 06:05:16+00:00,arxiv,...,21cc320d8d3a601891ceba367ac32065,html,markdownify,2020-02-01 04:46:25+00:00,"Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ""manual AI approach"". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1807.05960,Meta-Learning with Latent Embedding Optimization,"['Andrei A. Rusu', 'Dushyant Rao', 'Jakub Sygnowski', 'Oriol Vinyals', 'Razvan Pascanu', 'Simon Osindero', 'Raia Hadsell']",2018-07-16 16:35:29+00:00,arxiv,...,f281d4e3a653c69c72891dd06692f7ec,html,markdownify,2019-03-26 13:36:45+00:00,"Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1805.08462,Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training,"['Boyu Chen', 'Wenlian Lu', 'Ernest Fokoue']",2018-05-22 09:04:52+00:00,arxiv,...,ff1e4b58af12665f1337c651909cf198,html,markdownify,2018-09-07 06:14:05+00:00,"Meta-learning is a promising method to achieve efficient training method towards deep neural net and has been attracting increases interests in recent years. But most of the current methods are still not capable to train complex neuron net model with long-time training process. In this paper, a novel second-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF) approach, is proposed based on the Hessian-Free approach. Two recurrent neural networks are established to generate the damping and the precondition matrix of this Hessian-Free framework. A series of techniques to meta-train the MLHF towards stable and reinforce the meta-training of this optimizer, including the gradient calculation of $H$. Numerical experiments on deep convolution neural nets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and ILSVRC2012, indicate that the MLHF shows good and continuous training performance during the whole long-time training process, i.e., both the rapid-decreasing early stage and the steadily-deceasing later stage, and so is a promising meta-learning framework towards elevating the training efficiency in real-world deep neural nets.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2001.06782,Gradient Surgery for Multi-Task Learning,"['Tianhe Yu', 'Saurabh Kumar', 'Abhishek Gupta', 'Sergey Levine', 'Karol Hausman', 'Chelsea Finn']",2020-01-19 06:33:47+00:00,arxiv,...,94f820ac897943666601538853f63a9a,html,markdownify,2020-12-22 00:35:46+00:00,"While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.","NeurIPS 2020. Code is available at
  https://github.com/tianheyu927/PCGrad",,,cs.LG,"['cs.LG', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1905.03030,Meta-learning of Sequential Strategies,"['Pedro A. Ortega', 'Jane X. Wang', 'Mark Rowland', 'Tim Genewein', 'Zeb Kurth-Nelson', 'Razvan Pascanu', 'Nicolas Heess', 'Joel Veness', 'Alex Pritzel', 'Pablo Sprechmann', 'Siddhant M. Jayakumar', 'Tom McGrath', 'Kevin Miller', 'Mohammad Azar', 'Ian Osband', 'Neil Rabinowitz', 'AndrÃ¡s GyÃ¶rgy', 'Silvia Chiappa', 'Simon Osindero', 'Yee Whye Teh', 'Hado van Hasselt', 'Nando de Freitas', 'Matthew Botvinick', 'Shane Legg']",2019-05-08 12:27:20+00:00,arxiv,...,d6fedff85b42ece19d96bbe8b333d010,html,markdownify,2019-07-18 18:09:19+00:00,"In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.","DeepMind Technical Report (15 pages, 6 figures). Version V1.1",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1810.03642,Fast Context Adaptation via Meta-Learning,"['Luisa M Zintgraf', 'Kyriacos Shiarlis', 'Vitaly Kurin', 'Katja Hofmann', 'Shimon Whiteson']",2018-10-08 18:11:01+00:00,arxiv,...,018fb258f51030ba2149cd0ad908f3d6,html,markdownify,2019-06-10 17:17:53+00:00,"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.","Published at the International Conference on Machine Learning (ICML)
  2019",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1905.01320,Meta-learners' learning dynamics are unlike learners',['Neil C. Rabinowitz'],2019-05-03 18:00:26+00:00,arxiv,...,568e0546f888fb72d5cac1865dabe3bf,html,markdownify,2019-05-03 18:00:26+00:00,"Meta-learning is a tool that allows us to build sample-efficient learning systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just faster learners than their sample-inefficient deep learning (DL) and reinforcement learning (RL) brethren, but that they actually pursue fundamentally different learning trajectories. We study their learning dynamics on three sets of structured tasks for which the corresponding learning dynamics of DL and RL systems have been previously described: linear regression (Saxe et al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and contextual bandits (Schaul et al., 2019). In each case, while sample-inefficient DL and RL Learners uncover the task structure in a staggered manner, meta-trained LSTM Meta-Learners uncover almost all task structure concurrently, congruent with the patterns expected from Bayes-optimal inference algorithms. This has implications for research areas wherever the learning behaviour itself is of interest, such as safety, curriculum design, and human-in-the-loop machine learning.","26 pages, 23 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1805.07722,Task-Agnostic Meta-Learning for Few-shot Learning,"['Muhammad Abdullah Jamal', 'Guo-Jun Qi', 'Mubarak Shah']",2018-05-20 07:50:42+00:00,arxiv,...,ef4c96ef2c32ec3ae47a6cae6bc983d0,html,markdownify,2018-05-20 07:50:42+00:00,"Meta-learning approaches have been proposed to tackle the few-shot learning problem.Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined.Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1912.03820,Meta-Learning without Memorization,"['Mingzhang Yin', 'George Tucker', 'Mingyuan Zhou', 'Sergey Levine', 'Chelsea Finn']",2019-12-09 02:30:46+00:00,arxiv,...,016f420d82f1a665adda77ea1434565c,html,markdownify,2020-04-27 22:33:53+00:00,"The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.",ICLR 2020,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1903.03096,Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,"['Eleni Triantafillou', 'Tyler Zhu', 'Vincent Dumoulin', 'Pascal Lamblin', 'Utku Evci', 'Kelvin Xu', 'Ross Goroshin', 'Carles Gelada', 'Kevin Swersky', 'Pierre-Antoine Manzagol', 'Hugo Larochelle']",2019-03-07 18:48:55+00:00,arxiv,...,2af3d60dc7a80223579c432f3e18de2e,html,markdownify,2020-04-08 15:58:20+00:00,"Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.",Code available at https://github.com/google-research/meta-dataset,International Conference on Learning Representations (2020),,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1910.10897,Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,"['Tianhe Yu', 'Deirdre Quillen', 'Zhanpeng He', 'Ryan Julian', 'Avnish Narayan', 'Hayden Shively', 'Adithya Bellathur', 'Karol Hausman', 'Chelsea Finn', 'Sergey Levine']",2019-10-24 03:19:46+00:00,arxiv,...,d3bf58323df645961283627b1614a84f,html,markdownify,2021-06-14 18:45:16+00:00,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.","This is an update version of a manuscript that originally appeared at
  CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are
  available at: https://github.com/rlworkgroup/metaworld, and the baselines can
  be found at https://github.com/rlworkgroup/garage",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1804.06458,Deep Probabilistic Programming Languages: A Qualitative Study,"['Guillaume Baudart', 'Martin Hirzel', 'Louis Mandel']",2018-04-17 20:03:25+00:00,arxiv,...,bfec37b6222ecabc16ae81db2ff9dd6f,html,markdownify,2018-04-17 20:03:25+00:00,"Deep probabilistic programming languages try to combine the advantages of deep learning with those of probabilistic programming languages. If successful, this would be a big step forward in machine learning and programming languages. Unfortunately, as of now, this new crop of languages is hard to use and understand. This paper addresses this problem directly by explaining deep probabilistic programming languages and indirectly by characterizing their current strengths and weaknesses.",,,,cs.AI,"['cs.AI', 'cs.PL']"
https://arxiv.org/abs/1811.04251,Formal Limitations on the Measurement of Mutual Information,"['David McAllester', 'Karl Stratos']",2018-11-10 13:12:27+00:00,arxiv,...,bfeefa52945c0f617b69296be77e50fe,html,markdownify,2020-05-20 12:03:39+00:00,"Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N ).",,,,cs.IT,"['cs.IT', 'cs.LG', 'math.IT', 'stat.ML']"
https://arxiv.org/abs/1807.00366,Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning,"['Baoxiang Wang', 'Tongfang Sun', 'Xianjun Sam Zheng']",2018-07-01 18:20:23+00:00,arxiv,...,61e8d9ccfab0943ced7c5aee6efe671d,html,markdownify,2018-07-05 09:14:00+00:00,"In recent years, reinforcement learning (RL) methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go, and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding different players' diverse behaviors. In this paper, we present a novel method called Multi-Motivation Behavior Modeling (MMBM) that takes the multifaceted human motivations into consideration and models the underlying value structure of the players using inverse RL. Our approach does not require the access to the dynamic of the system, making it feasible to model complex interactive environments such as massively multiplayer online games. MMBM is tested on the World of Warcraft Avatar History dataset, which recorded over 70,000 users' gameplay spanning three years period. Our model reveals the significant difference of value structures among different player groups. Using the results of motivation modeling, we also predict and explain their diverse gameplay behaviors and provide a quantitative assessment of how the redesign of the game environment impacts players' behaviors.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1807.06583,Interpretable Latent Spaces for Learning from Demonstration,"['Yordan Hristov', 'Alex Lascarides', 'Subramanian Ramamoorthy']",2018-07-17 17:56:09+00:00,arxiv,...,100fe8d3a9725484a4fce5e83aa2d4a2,html,markdownify,2018-10-02 15:27:23+00:00,"Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.","12 pages, 6 figures, accepted at the Conference on Robot Learning
  (CoRL) 2018, Zurich, Switzerland",,,cs.CV,"['cs.CV', 'cs.RO']"
https://arxiv.org/abs/1809.01036,A Roadmap for Robust End-to-End Alignment,['LÃª NguyÃªn Hoang'],2018-09-04 15:19:44+00:00,arxiv,...,c2a7b07ab879e9532e7ea592cb5381ff,html,markdownify,2020-02-25 08:45:45+00:00,"This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.","21 pages, 2 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1809.03447,Expert-augmented actor-critic for ViZDoom and Montezumas Revenge,"['MichaÅ Garmulewicz', 'Henryk Michalewski', 'Piotr MiÅoÅ']",2018-09-10 16:36:22+00:00,arxiv,...,0a9e5e579d815b3a4614322137431f9d,html,markdownify,2018-09-10 16:36:22+00:00,"We propose an expert-augmented actor-critic algorithm, which we evaluate on two environments with sparse rewards: Montezumas Revenge and a demanding maze from the ViZDoom suite. In the case of Montezumas Revenge, an agent trained with our method achieves very good results consistently scoring above 27,000 points (in many experiments beating the first world). With an appropriate choice of hyperparameters, our algorithm surpasses the performance of the expert data. In a number of experiments, we have observed an unreported bug in Montezumas Revenge which allowed the agent to score more than 800,000 points.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1804.08606,Zero-Shot Visual Imitation,"['Deepak Pathak', 'Parsa Mahmoudieh', 'Guanghao Luo', 'Pulkit Agrawal', 'Dian Chen', 'Yide Shentu', 'Evan Shelhamer', 'Jitendra Malik', 'Alexei A. Efros', 'Trevor Darrell']",2018-04-23 17:58:26+00:00,arxiv,...,1688255a2161541311731284fbed74b5,html,markdownify,2018-04-23 17:58:26+00:00,"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/","Oral presentation at ICLR 2018. Website at
  https://pathak22.github.io/zeroshot-imitation/",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1805.07871,A Framework and Method for Online Inverse Reinforcement Learning,"['Saurabh Arora', 'Prashant Doshi', 'Bikramjit Banerjee']",2018-05-21 02:27:58+00:00,arxiv,...,02c93851835b2b9e24c79b842182d042,html,markdownify,2018-05-21 02:27:58+00:00,"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",,"Journal of Autonomous Agents and Multi-Agent Systems, Volume 35,
  Article number: 4 (2021)",10.1007/s10458-020-09485-4,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2009.09266v1,Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,['Johannes Schneider'],2020-09-19 16:30:37+00:00,arxiv,...,306ba257a1afcbd352162f4adac3b8ce,html,markdownify,2020-09-19 16:30:37+00:00,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",,,,cs.HC,"['cs.HC', 'cs.AI', 'cs.CV']"
https://arxiv.org/abs/1810.08700,Safe Reinforcement Learning with Model Uncertainty Estimates,"['BjÃ¶rn LÃ¼tjens', 'Michael Everett', 'Jonathan P. How']",2018-10-19 22:04:59+00:00,arxiv,...,1af6bc07f0a85fced399b28648fb64d8,html,markdownify,2019-03-01 05:03:11+00:00,"Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.","ICRA 2019; Presented at IROS 2018 Workshop on Machine Learning in
  Robot Motion Planning",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2010.15920,Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones,"['Brijen Thananjeyan', 'Ashwin Balakrishna', 'Suraj Nair', 'Michael Luo', 'Krishnan Srinivasan', 'Minho Hwang', 'Joseph E. Gonzalez', 'Julian Ibarz', 'Chelsea Finn', 'Ken Goldberg']",2020-10-29 20:10:02+00:00,arxiv,...,adf9f6ab1ffb87fe4a1769e4a287222e,html,markdownify,2021-05-17 21:20:48+00:00,"Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2 - 20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",RA-L and ICRA 2021. First two authors contributed equally,"Robotics and Automation Letters (RA-L) and International
  Conference on Robotics and Automation (ICRA) 2021",,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']"
https://arxiv.org/abs/2010.14603,Learning to be Safe: Deep RL with a Safety Critic,"['Krishnan Srinivasan', 'Benjamin Eysenbach', 'Sehoon Ha', 'Jie Tan', 'Chelsea Finn']",2020-10-27 20:53:20+00:00,arxiv,...,c82e2b2b77a08c6920aedf06d2c82960,html,markdownify,2020-10-27 20:53:20+00:00,"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.","In submission, 16 pages (including appendix)",,,cs.LG,"['cs.LG', 'cs.RO']"
https://arxiv.org/abs/1808.03644,Building Safer AGI by introducing Artificial Stupidity,"['MichaÃ«l Trazzi', 'Roman V. Yampolskiy']",2018-08-11 00:14:33+00:00,arxiv,...,5e4dcad25425ebb1504a1a784fbb39b9,html,markdownify,2018-08-11 00:14:33+00:00,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1906.03218,Planning With Uncertain Specifications (PUnS),"['Ankit Shah', 'Shen Li', 'Julie Shah']",2019-06-07 16:32:16+00:00,arxiv,...,84c6da4c409bb10cd22b388fa8bbf6b9,html,markdownify,2020-02-25 19:48:00+00:00,"Reward engineering is crucial to high performance in reinforcement learning systems. Prior research into reward design has largely focused on Markovian functions representing the reward. While there has been research into expressing non-Markov rewards as linear temporal logic (LTL) formulas, this has focused on task specifications directly defined by the user. However, in many real-world applications, task specifications are ambiguous, and can only be expressed as a belief over LTL formulas. In this paper, we introduce planning with uncertain specifications (PUnS), a novel formulation that addresses the challenge posed by non-Markovian specifications expressed as beliefs over LTL formulas. We present four criteria that capture the semantics of satisfying a belief over specifications for different applications, and analyze the qualitative implications of these criteria within a synthetic domain. We demonstrate the existence of an equivalent Markov decision process (MDP) for any instance of PUnS. Finally, we demonstrate our approach on the real-world task of setting a dinner table automatically with a robot that inferred task specifications from human demonstrations.","Accepted for publication by IEEE Robotics and Automation Letters.
  Accepted for presentation at the 2020 IEEE International Conference on
  Robotics and Automation",,10.1109/LRA.2020.2977217,cs.RO,['cs.RO']
https://arxiv.org/abs/2006.14796v5,AvE: Assistance via Empowerment,"['Yuqing Du', 'Stas Tiomkin', 'Emre Kiciman', 'Daniel Polani', 'Pieter Abbeel', 'Anca Dragan']",2020-06-26 04:40:11+00:00,arxiv,...,9eddfcc8dd5654fc14d84ee771541d2a,html,markdownify,2021-01-07 20:54:48+00:00,"One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s). Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.",Final version from NeurIPS 2020 Conference Proceedings,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/1602.04938,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']",2016-02-16 08:20:14+00:00,arxiv,...,0e72150511c6eee4c6d996d46ef646a8,html,markdownify,2016-08-09 17:54:52+00:00,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1703.06856,Counterfactual Fairness,"['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']",2017-03-20 17:18:57+00:00,arxiv,...,3df877e6b12178e9c0661a7a65ee1e81,html,markdownify,2018-03-08 11:23:13+00:00,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",,,,stat.ML,"['stat.ML', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/2009.09071,Measurement in AI Policy: Opportunities and Challenges,"['Saurabh Mishra', 'Jack Clark', 'C. Raymond Perrault']",2020-09-10 05:37:40+00:00,arxiv,...,d5655d82a2c2b7d20f9b6ba507d87357,html,markdownify,2020-09-10 05:37:40+00:00,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",Workshop Paper,,,cs.CY,['cs.CY']
https://arxiv.org/abs/1810.02541,PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,"['Perttu HÃ¤mÃ¤lÃ¤inen', 'Amin Babadi', 'Xiaoxiao Ma', 'Jaakko Lehtinen']",2018-10-05 06:59:29+00:00,arxiv,...,6165503e4c7b99147ace85bd504061bc,html,markdownify,2020-11-03 07:51:49+00:00,"Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, we observe that in a continuous action space, PPO can prematurely shrink the exploration variance, which leads to slow progress and may make the algorithm prone to getting stuck in local optima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization method designed for robustness in similar situations, we propose PPO-CMA, a proximal policy optimization approach that adaptively expands the exploration variance to speed up progress. With only minor changes to PPO, our algorithm considerably improves performance in Roboschool continuous control benchmarks. Our results also show that PPO-CMA, as opposed to PPO, is significantly less sensitive to the choice of hyperparameters, allowing one to use it in complex movement optimization tasks without requiring tedious tuning.","This paper has been accepted to IEEE International Workshop on
  Machine Learning for Signal Processing (MLSP 2020). The arxiv version also
  includes an appendix that covers more results",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2012.05876,Neurosymbolic AI: The 3rd Wave,"[""Artur d'Avila Garcez"", 'Luis C. Lamb']",2020-12-10 18:31:38+00:00,arxiv,...,75136156f8c0ca1d90390bcbfb7a8ade,html,markdownify,2020-12-16 23:21:05+00:00,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.",37 pages,,,cs.AI,"['cs.AI', 'cs.LG', 'I.2.4; I.2.6']"
https://arxiv.org/abs/1810.04805,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova']",2018-10-11 00:50:01+00:00,arxiv,...,0e2a587f5ebbafdf078213230a614dbb,html,markdownify,2019-05-24 20:37:26+00:00,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",,,,cs.CL,['cs.CL']
https://arxiv.org/abs/1906.05909,Stand-Alone Self-Attention in Vision Models,"['Prajit Ramachandran', 'Niki Parmar', 'Ashish Vaswani', 'Irwan Bello', 'Anselm Levskaya', 'Jonathon Shlens']",2019-06-13 19:43:01+00:00,arxiv,...,6d63d1c36752df79c87b91e9148dc74b,html,markdownify,2019-06-13 19:43:01+00:00,"Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/2107.12544,"Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning","['Pedro A. Tsividis', 'Joao Loula', 'Jake Burga', 'Nathan Foss', 'Andres Campero', 'Thomas Pouncy', 'Samuel J. Gershman', 'Joshua B. Tenenbaum']",2021-07-27 01:38:13+00:00,arxiv,...,3a919bb8ba68fe2c9832563c3e883fb4,html,markdownify,2021-07-27 01:38:13+00:00,"Reinforcement learning (RL) studies how an agent comes to achieve reward in an environment through interactions over time. Recent advances in machine RL have surpassed human expertise at the world's oldest board games and many classic video games, but they require vast quantities of experience to learn successfully -- none of today's algorithms account for the human ability to learn so many different tasks, so quickly. Here we propose a new approach to this challenge based on a particularly strong form of model-based RL which we call Theory-Based Reinforcement Learning, because it uses human-like intuitive theories -- rich, abstract, causal models of physical objects, intentional agents, and their interactions -- to explore and model an environment, and plan effectively to achieve task goals. We instantiate the approach in a video game playing agent called EMPA (the Exploring, Modeling, and Planning Agent), which performs Bayesian inference to learn probabilistic generative models expressed as programs for a game-engine simulator, and runs internal simulations over these models to support efficient object-based, relational exploration and heuristic planning. EMPA closely matches human learning efficiency on a suite of 90 challenging Atari-style video games, learning new games in just minutes of game play and generalizing robustly to new game situations and new levels. The model also captures fine-grained structure in people's exploration trajectories and learning dynamics. Its design and behavior suggest a way forward for building more general human-like AI systems.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1903.08894,Towards Characterizing Divergence in Deep Q-Learning,"['Joshua Achiam', 'Ethan Knight', 'Pieter Abbeel']",2019-03-21 09:42:41+00:00,arxiv,...,f6715025a064d16f67fa691211dacb89,html,markdownify,2019-03-21 09:42:41+00:00,"Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1903.01973,Learning Latent Plans from Play,"['Corey Lynch', 'Mohi Khansari', 'Ted Xiao', 'Vikash Kumar', 'Jonathan Tompson', 'Sergey Levine', 'Pierre Sermanet']",2019-03-05 18:36:42+00:00,arxiv,...,5634084c08bcfb0552e0bd06352627ce,html,markdownify,2019-12-20 05:03:10+00:00,"Acquiring a diverse repertoire of general-purpose skills remains an open challenge for robotics. In this work, we propose self-supervising control on top of human teleoperated play data as a way to scale up skill learning. Play has two properties that make it attractive compared to conventional task demonstrations. Play is cheap, as it can be collected in large quantities quickly without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, covering ~4x more interaction space than task demonstrations for the same amount of collection time. To learn control from play, we introduce Play-LMP, a self-supervised method that learns to organize play behaviors in a latent space, then reuse them at test time to achieve specific goals. Combining self-supervised control with a diverse play dataset shifts the focus of skill learning from a narrow and discrete set of tasks to the full continuum of behaviors available in an environment. We find that this combination generalizes well empirically---after self-supervising on unlabeled play, our method substantially outperforms individual expert-trained policies on 18 difficult user-specified visual manipulation tasks in a simulated robotic tabletop environment. We additionally find that play-supervised models, unlike their expert-trained counterparts, are more robust to perturbations and exhibit retrying-till-success behaviors. Finally, we find that our agent organizes its latent plan space around functional tasks, despite never being trained with task labels. Videos, code and data are available at learning-from-play.github.io","Published at CoRL 2019 (3rd Conference on Robot Learning, Osaka,
  Japan)",,,cs.RO,['cs.RO']
https://arxiv.org/abs/1911.08265,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","['Julian Schrittwieser', 'Ioannis Antonoglou', 'Thomas Hubert', 'Karen Simonyan', 'Laurent Sifre', 'Simon Schmitt', 'Arthur Guez', 'Edward Lockhart', 'Demis Hassabis', 'Thore Graepel', 'Timothy Lillicrap', 'David Silver']",2019-11-19 13:58:52+00:00,arxiv,...,c4ab8494bb3e95434879e60b06b029f6,html,markdownify,2020-02-21 18:05:30+00:00,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",,,10.1038/s41586-020-03051-4,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1805.07470,Solving the Rubik's Cube Without Human Knowledge,"['Stephen McAleer', 'Forest Agostinelli', 'Alexander Shmakov', 'Pierre Baldi']",2018-05-18 23:07:31+00:00,arxiv,...,6f722e37639e4691de55dac12aecbae0,html,markdownify,2018-05-18 23:07:31+00:00,"A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have achieved superhuman proficiency in Go, Chess, and Shogi without human data or domain knowledge. In these environments, a reward is always received at the end of the game, however, for many combinatorial optimization environments, rewards are sparse and episodes are not guaranteed to terminate. We introduce Autodidactic Iteration: a novel reinforcement learning algorithm that is able to teach itself how to solve the Rubik's Cube with no human assistance. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves -- less than or equal to solvers that employ human domain knowledge.",First three authors contributed equally. Submitted to NIPS 2018,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1805.11592,Playing hard exploration games by watching YouTube,"['Yusuf Aytar', 'Tobias Pfaff', 'David Budden', 'Tom Le Paine', 'Ziyu Wang', 'Nando de Freitas']",2018-05-29 17:19:36+00:00,arxiv,...,900e057d7831d568f242bd03d6eb8861,html,markdownify,2018-11-30 15:59:27+00:00,"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/2109.04083,User Tampering in Reinforcement Learning Recommender Systems,"['Charles Evans', 'Atoosa Kasirzadeh']",2021-09-09 07:53:23+00:00,arxiv,...,84faafec9baaa16d7da65892b9a06121,html,markdownify,2022-11-02 20:57:37+00:00,"This paper provides novel formal methods and empirical demonstrations of a particular safety concern in reinforcement learning (RL)-based recommendation algorithms. We call this safety concern `user tampering' -- a phenomenon whereby an RL-based recommender system might manipulate a media user's opinions via its recommendations as part of a policy to increase long-term user engagement. We then apply techniques from causal modelling to analyse the leading approaches in the literature for implementing scalable RL-based recommenders, and we observe that the current approaches permit user tampering. Additionally, we review the existing mitigation strategies for reward tampering problems and show that they do not transfer well to the user tampering phenomenon found in the recommendation context. Furthermore, we provide a simulation study of a media RL-based recommendation problem constrained to the recommendation of political content. We show that a Q-learning algorithm consistently learns to exploit its opportunities to polarise simulated users with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. This latter contribution calls for urgency in designing safer RL-based recommenders; the former suggests that creating such safe recommenders will require a fundamental shift in design away from the approaches we have seen in the recent literature.","Accepted for presentation at the 4th FAccTRec Workshop on Responsible
  Recommendation (FAccTRec '21)",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2008.12623,From Optimizing Engagement to Measuring Value,"['Smitha Milli', 'Luca Belli', 'Moritz Hardt']",2020-08-21 03:10:45+00:00,arxiv,...,c959117a942aa8f22a32904b12bf4c8b,html,markdownify,2021-07-19 16:32:49+00:00,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of ""value"" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".",Published at FAccT'21,,10.1145/3442188.3445933,cs.SI,"['cs.SI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2006.07495,Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,"['Adrien Ecoffet', 'Jeff Clune', 'Joel Lehman']",2020-06-12 22:28:09+00:00,arxiv,...,f9a8c95d8d4b6679c2ee560f7c3cc651,html,markdownify,2020-06-12 22:28:09+00:00,"Artificial life originated and has long studied the topic of open-ended evolution, which seeks the principles underlying artificial systems that innovate continually, inspired by biological evolution. Recently, interest has grown within the broader field of AI in a generalization of open-ended evolution, here called open-ended search, wherein such questions of open-endedness are explored for advancing AI, whatever the nature of the underlying search algorithm (e.g. evolutionary or gradient-based). For example, open-ended search might design new architectures for neural networks, new reinforcement learning algorithms, or most ambitiously, aim at designing artificial general intelligence. This paper proposes that open-ended evolution and artificial life have much to contribute towards the understanding of open-ended AI, focusing here in particular on the safety of open-ended search. The idea is that AI systems are increasingly applied in the real world, often producing unintended harms in the process, which motivates the growing field of AI safety. This paper argues that open-ended AI has its own safety challenges, in particular, whether the creativity of open-ended systems can be productively and predictably controlled. This paper explains how unique safety problems manifest in open-ended search, and suggests concrete contributions and research questions to explore them. The hope is to inspire progress towards creative, useful, and safe open-ended search algorithms.",,,,cs.NE,['cs.NE']
https://arxiv.org/abs/2108.07258,On the Opportunities and Risks of Foundation Models,"['Rishi Bommasani', 'Drew A. Hudson', 'Ehsan Adeli', 'Russ Altman', 'Simran Arora', 'Sydney von Arx', 'Michael S. Bernstein', 'Jeannette Bohg', 'Antoine Bosselut', 'Emma Brunskill', 'Erik Brynjolfsson', 'Shyamal Buch', 'Dallas Card', 'Rodrigo Castellon', 'Niladri Chatterji', 'Annie Chen', 'Kathleen Creel', 'Jared Quincy Davis', 'Dora Demszky', 'Chris Donahue', 'Moussa Doumbouya', 'Esin Durmus', 'Stefano Ermon', 'John Etchemendy', 'Kawin Ethayarajh', 'Li Fei-Fei', 'Chelsea Finn', 'Trevor Gale', 'Lauren Gillespie', 'Karan Goel', 'Noah Goodman', 'Shelby Grossman', 'Neel Guha', 'Tatsunori Hashimoto', 'Peter Henderson', 'John Hewitt', 'Daniel E. Ho', 'Jenny Hong', 'Kyle Hsu', 'Jing Huang', 'Thomas Icard', 'Saahil Jain', 'Dan Jurafsky', 'Pratyusha Kalluri', 'Siddharth Karamcheti', 'Geoff Keeling', 'Fereshte Khani', 'Omar Khattab', 'Pang Wei Koh', 'Mark Krass', 'Ranjay Krishna', 'Rohith Kuditipudi', 'Ananya Kumar', 'Faisal Ladhak', 'Mina Lee', 'Tony Lee', 'Jure Leskovec', 'Isabelle Levent', 'Xiang Lisa Li', 'Xuechen Li', 'Tengyu Ma', 'Ali Malik', 'Christopher D. Manning', 'Suvir Mirchandani', 'Eric Mitchell', 'Zanele Munyikwa', 'Suraj Nair', 'Avanika Narayan', 'Deepak Narayanan', 'Ben Newman', 'Allen Nie', 'Juan Carlos Niebles', 'Hamed Nilforoshan', 'Julian Nyarko', 'Giray Ogut', 'Laurel Orr', 'Isabel Papadimitriou', 'Joon Sung Park', 'Chris Piech', 'Eva Portelance', 'Christopher Potts', 'Aditi Raghunathan', 'Rob Reich', 'Hongyu Ren', 'Frieda Rong', 'Yusuf Roohani', 'Camilo Ruiz', 'Jack Ryan', 'Christopher RÃ©', 'Dorsa Sadigh', 'Shiori Sagawa', 'Keshav Santhanam', 'Andy Shih', 'Krishnan Srinivasan', 'Alex Tamkin', 'Rohan Taori', 'Armin W. Thomas', 'Florian TramÃ¨r', 'Rose E. Wang', 'William Wang', 'Bohan Wu', 'Jiajun Wu', 'Yuhuai Wu', 'Sang Michael Xie', 'Michihiro Yasunaga', 'Jiaxuan You', 'Matei Zaharia', 'Michael Zhang', 'Tianyi Zhang', 'Xikun Zhang', 'Yuhui Zhang', 'Lucia Zheng', 'Kaitlyn Zhou', 'Percy Liang']",2021-08-16 17:50:08+00:00,arxiv,...,31f361c9263ce144e282117e2e3d6071,html,markdownify,2022-07-12 23:45:14+00:00,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.","Authored by the Center for Research on Foundation Models (CRFM) at
  the Stanford Institute for Human-Centered Artificial Intelligence (HAI).
  Report page with citation guidelines: https://crfm.stanford.edu/report.html",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/2109.07958,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"['Stephanie Lin', 'Jacob Hilton', 'Owain Evans']",2021-09-08 17:15:27+00:00,arxiv,...,599f896d171b6443ad397b27ac114ef1,html,markdownify,2022-05-08 02:43:02+00:00,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","ACL 2022 (main conference); the TruthfulQA benchmark and evaluation
  code is available at https://github.com/sylinrl/TruthfulQA",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/1810.04538,Secure Deep Learning Engineering: A Software Quality Assurance Perspective,"['Lei Ma', 'Felix Juefei-Xu', 'Minhui Xue', 'Qiang Hu', 'Sen Chen', 'Bo Li', 'Yang Liu', 'Jianjun Zhao', 'Jianxiong Yin', 'Simon See']",2018-10-10 14:04:08+00:00,arxiv,...,5e67006d76f37974bbfade2b7db9ee82,html,markdownify,2018-10-10 14:04:08+00:00,"Over the past decades, deep learning (DL) systems have achieved tremendous success and gained great popularity in various applications, such as intelligent machines, image processing, speech processing, and medical diagnostics. Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. This brings up many open safety and security issues with enormous and urgent demands on rigorous methodologies and engineering practice for quality enhancement. A plethora of studies have shown that the state-of-the-art DL systems suffer from defects and vulnerabilities that can lead to severe loss and tragedies, especially when applied to real-world safety-critical applications. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning. We, from a software quality assurance perspective, pinpoint challenges and future opportunities towards universal secure deep learning engineering. We hope this work and the accompanied paper repository can pave the path for the software engineering community towards addressing the pressing industrial demand of secure intelligent applications.",,,,cs.SE,"['cs.SE', 'cs.AI', 'cs.CR', 'cs.LG']"
https://arxiv.org/abs/1811.01134,A Marauder's Map of Security and Privacy in Machine Learning,['Nicolas Papernot'],2018-11-03 00:25:50+00:00,arxiv,...,e1fd5a49fdf5dbc7ecd2cb67c27ddd87,html,markdownify,2018-11-03 00:25:50+00:00,"There is growing recognition that machine learning (ML) exposes new security and privacy vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited but expanding. In this talk, we explore the threat model space of ML algorithms through the lens of Saltzer and Schroeder's principles for the design of secure computer systems. This characterization of the threat space prompts an investigation of current and future research directions. We structure our discussion around three of these directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. Key insights resulting from these three directions pursued both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by systematizing best practices in our community.","This report summarizes the keynote presented by the author in October
  2018 at AISec (colocated with ACM CCS) on security and privacy in machine
  learning",,,cs.CR,['cs.CR']
https://arxiv.org/abs/2012.07805,Extracting Training Data from Large Language Models,"['Nicholas Carlini', 'Florian Tramer', 'Eric Wallace', 'Matthew Jagielski', 'Ariel Herbert-Voss', 'Katherine Lee', 'Adam Roberts', 'Tom Brown', 'Dawn Song', 'Ulfar Erlingsson', 'Alina Oprea', 'Colin Raffel']",2020-12-14 18:39:09+00:00,arxiv,...,d610889ff222ab37f91057e154267739,html,markdownify,2021-06-15 17:45:26+00:00,"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.   We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.   We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",,,,cs.CR,"['cs.CR', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/1807.08060,Safe Option-Critic: Learning Safety in the Option-Critic Architecture,"['Arushi Jain', 'Khimya Khetarpal', 'Doina Precup']",2018-07-21 00:39:23+00:00,arxiv,...,34e9c279cdfc7aec92b4a0ef64fb6e4a,html,markdownify,2021-03-02 11:07:34+00:00,"Designing hierarchical reinforcement learning algorithms that exhibit safe behaviour is not only vital for practical applications but also, facilitates a better understanding of an agent's decisions. We tackle this problem in the options framework, a particular way to specify temporally abstract actions which allow an agent to use sub-policies with start and end conditions. We consider a behaviour as safe that avoids regions of state-space with high uncertainty in the outcomes of actions. We propose an optimization objective that learns safe options by encouraging the agent to visit states with higher behavioural consistency. The proposed objective results in a trade-off between maximizing the standard expected return and minimizing the effect of model uncertainty in the return. We propose a policy gradient algorithm to optimize the constrained objective function. We examine the quantitative and qualitative behaviour of the proposed approach in a tabular grid-world, continuous-state puddle-world, and three games from the Arcade Learning Environment: Ms.Pacman, Amidar, and Q*Bert. Our approach achieves a reduction in the variance of return, boosts performance in environments with intrinsic variability in the reward structure, and compares favorably both with primitive actions as well as with risk-neutral options.","To appear at The Knowledge Engineering Review (KER), 2021. Previous
  draft appeared in Adaptive Learning Agents (ALA) 2018 workshop held at ICML,
  AAMAS in Stockholm. Corrected typos, added references and added extra figures",The Knowledge Engineering Review 36 (2021) e4,10.1017/S0269888921000035,cs.AI,['cs.AI']
https://arxiv.org/abs/2010.02846v1,Safety Aware Reinforcement Learning (SARL),"['Santiago Miret', 'Somdeb Majumdar', 'Carroll Wainwright']",2020-10-06 16:08:28+00:00,arxiv,...,3618ea69395baf69bdf53a0d42f4b486,html,markdownify,2020-10-06 16:08:28+00:00,"As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2008.12146v3,Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,"['Sandhya Saisubramanian', 'Shlomo Zilberstein', 'Ece Kamar']",2020-08-24 16:48:46+00:00,arxiv,...,6a49c1fc5ae44394d8f122788c20e6a4,html,markdownify,2021-10-18 18:40:37+00:00,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model -- handcrafted or machine acquired -- is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of an agent's actions is critical to improve the safety and reliability of autonomous systems. Mitigating negative side effects is an emerging research topic that is attracting increased attention due to the rapid growth in the deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. The article concludes with a discussion of open questions and suggestions for future research directions.",9 pages,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1606.06565v2,Concrete Problems in AI Safety,"['Dario Amodei', 'Chris Olah', 'Jacob Steinhardt', 'Paul Christiano', 'John Schulman', 'Dan ManÃ©']",2016-06-21 13:37:05+00:00,arxiv,...,89a7410428f59923ab5c2e80137cc64b,html,markdownify,2016-07-25 17:23:29+00:00,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",29 pages,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1902.09980v7,Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,"['Tom Everitt', 'Pedro A. Ortega', 'Elizabeth Barnes', 'Shane Legg']",2019-02-26 14:54:09+00:00,arxiv,...,267567fa95206743d1a66719cc492469,html,markdownify,2022-01-20 17:39:06+00:00,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",Mostly superseded by arXiv:2102.01685,,,cs.AI,"['cs.AI', 'cs.LG', 'I.2.6; I.2.8']"
https://arxiv.org/abs/1711.06782,Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,"['Benjamin Eysenbach', 'Shixiang Gu', 'Julian Ibarz', 'Sergey Levine']",2017-11-18 00:53:20+00:00,arxiv,...,0a378efd124019d5b96e0e6d7f92ce8c,html,markdownify,2017-11-18 00:53:20+00:00,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.","Videos of our experiments are available at:
  https://sites.google.com/site/mlleavenotrace/",,,cs.LG,"['cs.LG', 'cs.RO']"
https://arxiv.org/abs/1705.10720v1,Low Impact Artificial Intelligences,"['Stuart Armstrong', 'Benjamin Levinstein']",2017-05-30 16:15:16+00:00,arxiv,...,8ca6f14d7837e0c4b7fde9551c1d473b,html,markdownify,2017-05-30 16:15:16+00:00,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1611.08219v3,The Off-Switch Game,"['Dylan Hadfield-Menell', 'Anca Dragan', 'Pieter Abbeel', 'Stuart Russell']",2016-11-24 15:23:48+00:00,arxiv,...,4ce2bdaa70193147efd085ab496c85ff,html,markdownify,2017-06-16 01:41:59+00:00,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1803.03453,The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities,"['Joel Lehman', 'Jeff Clune', 'Dusan Misevic', 'Christoph Adami', 'Lee Altenberg', 'Julie Beaulieu', 'Peter J. Bentley', 'Samuel Bernard', 'Guillaume Beslon', 'David M. Bryson', 'Patryk Chrabaszcz', 'Nick Cheney', 'Antoine Cully', 'Stephane Doncieux', 'Fred C. Dyer', 'Kai Olav Ellefsen', 'Robert Feldt', 'Stephan Fischer', 'Stephanie Forrest', 'Antoine FrÃ©noy', 'Christian GagnÃ©', 'Leni Le Goff', 'Laura M. Grabowski', 'Babak Hodjat', 'Frank Hutter', 'Laurent Keller', 'Carole Knibbe', 'Peter Krcah', 'Richard E. Lenski', 'Hod Lipson', 'Robert MacCurdy', 'Carlos Maestre', 'Risto Miikkulainen', 'Sara Mitri', 'David E. Moriarty', 'Jean-Baptiste Mouret', 'Anh Nguyen', 'Charles Ofria', 'Marc Parizeau', 'David Parsons', 'Robert T. Pennock', 'William F. Punch', 'Thomas S. Ray', 'Marc Schoenauer', 'Eric Shulte', 'Karl Sims', 'Kenneth O. Stanley', 'FranÃ§ois Taddei', 'Danesh Tarapore', 'Simon Thibault', 'Westley Weimer', 'Richard Watson', 'Jason Yosinski']",2018-03-09 10:17:18+00:00,arxiv,...,5f7f4f399184ee5b5c7bcaff9c2dbebd,html,markdownify,2019-11-21 23:58:46+00:00,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.",,,,cs.NE,['cs.NE']
https://arxiv.org/abs/1707.06354v2,Pragmatic-Pedagogic Value Alignment,"['Jaime F. Fisac', 'Monica A. Gates', 'Jessica B. Hamrick', 'Chang Liu', 'Dylan Hadfield-Menell', 'Malayandi Palaniappan', 'Dhruv Malik', 'S. Shankar Sastry', 'Thomas L. Griffiths', 'Anca D. Dragan']",2017-07-20 03:07:19+00:00,arxiv,...,a9bea869c3ce8e9e6ef9d9303c3c602e,html,markdownify,2018-02-05 20:44:09+00:00,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.","Published at the International Symposium on Robotics Research (ISRR
  2017)","International Symposium on Robotics Research, 2017",,cs.AI,"['cs.AI', 'cs.HC', 'cs.LG', 'cs.RO', '68T05', 'I.2.0; I.2.6; I.2.8; I.2.9']"
https://arxiv.org/abs/1606.03137,Cooperative Inverse Reinforcement Learning,"['Dylan Hadfield-Menell', 'Anca Dragan', 'Pieter Abbeel', 'Stuart Russell']",2016-06-09 22:39:54+00:00,arxiv,...,02b4aea44266893cc43e172a0f354cfc,html,markdownify,2016-11-12 20:33:43+00:00,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1512.05832v1,"Learning the Preferences of Ignorant, Inconsistent Agents","['Owain Evans', 'Andreas Stuhlmueller', 'Noah D. Goodman']",2015-12-18 00:24:08+00:00,arxiv,...,097fd1be5a147511d95927ff6ff53b20,html,markdownify,2015-12-18 00:24:08+00:00,"An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.",AAAI 2016,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1810.08575,Supervising strong learners by amplifying weak experts,"['Paul Christiano', 'Buck Shlegeris', 'Dario Amodei']",2018-10-19 16:30:48+00:00,arxiv,...,70e177cd92dc523583db7083999d2b43,html,markdownify,2018-10-19 16:30:48+00:00,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1706.03741,Deep reinforcement learning from human preferences,"['Paul Christiano', 'Jan Leike', 'Tom B. Brown', 'Miljan Martic', 'Shane Legg', 'Dario Amodei']",2017-06-12 17:23:59+00:00,arxiv,...,99e2b183954746fd368ee9e93aec1928,html,markdownify,2017-07-13 20:18:41+00:00,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.HC', 'cs.LG']"
https://arxiv.org/abs/1602.04184,Parametric Bounded LÃ¶b's Theorem and Robust Cooperation of Bounded Agents,['Andrew Critch'],2016-02-12 19:51:54+00:00,arxiv,...,94aea5fe98f5e7b04a61cc6da0260710,html,markdownify,2016-08-24 05:22:57+00:00,"L\""ob's theorem and G\""odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.","Corrected typos, added grant acknowledgement, updated citation style
  to author-year",,,cs.GT,"['cs.GT', 'cs.LO']"
https://arxiv.org/abs/1711.09883,AI Safety Gridworlds,"['Jan Leike', 'Miljan Martic', 'Victoria Krakovna', 'Pedro A. Ortega', 'Tom Everitt', 'Andrew Lefrancq', 'Laurent Orseau', 'Shane Legg']",2017-11-27 18:57:13+00:00,arxiv,...,914104978ed5d9a281d3f1d8998b4cfa,html,markdownify,2017-11-28 17:40:36+00:00,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1312.6114,Auto-Encoding Variational Bayes,"['Diederik P Kingma', 'Max Welling']",2013-12-20 20:58:10+00:00,arxiv,...,8cece5d098685a13dd24d4342f827aa6,html,markdownify,2014-05-01 15:43:28+00:00,"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1310.4546,Distributed Representations of Words and Phrases and their Compositionality,"['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean']",2013-10-16 23:28:53+00:00,arxiv,...,9e613d4717566c2d80dae6aad1707fc4,html,markdownify,2013-10-16 23:28:53+00:00,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",,,,cs.CL,"['cs.CL', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1406.2661,Generative Adversarial Networks,"['Ian J. Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio']",2014-06-10 18:58:17+00:00,arxiv,...,d142dce12fe1fedf041b8227ef8cd922,html,markdownify,2014-06-10 18:58:17+00:00,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/1409.0473,Neural Machine Translation by Jointly Learning to Align and Translate,"['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio']",2014-09-01 16:33:02+00:00,arxiv,...,79fd335fa5eaba74cfe3fdaf8517911b,html,markdownify,2016-05-19 21:53:22+00:00,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",Accepted at ICLR 2015 as oral presentation,,,cs.CL,"['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']"
https://arxiv.org/abs/1412.6980,Adam: A Method for Stochastic Optimization,"['Diederik P. Kingma', 'Jimmy Ba']",2014-12-22 13:54:29+00:00,arxiv,...,b7b78dd60f6d440c51540a8f643280a4,html,markdownify,2017-01-30 01:27:54+00:00,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","Published as a conference paper at the 3rd International Conference
  for Learning Representations, San Diego, 2015",,,cs.LG,['cs.LG']
https://arxiv.org/abs/1706.03762,Attention Is All You Need,"['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin']",2017-06-12 17:57:34+00:00,arxiv,...,26234c1adadc4370bb1bdf27fc000d86,html,markdownify,2017-12-06 03:30:32+00:00,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","15 pages, 5 figures",,,cs.CL,"['cs.CL', 'cs.LG']"
https://arxiv.org/abs/1609.08144,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"['Yonghui Wu', 'Mike Schuster', 'Zhifeng Chen', 'Quoc V. Le', 'Mohammad Norouzi', 'Wolfgang Macherey', 'Maxim Krikun', 'Yuan Cao', 'Qin Gao', 'Klaus Macherey', 'Jeff Klingner', 'Apurva Shah', 'Melvin Johnson', 'Xiaobing Liu', 'Åukasz Kaiser', 'Stephan Gouws', 'Yoshikiyo Kato', 'Taku Kudo', 'Hideto Kazawa', 'Keith Stevens', 'George Kurian', 'Nishant Patil', 'Wei Wang', 'Cliff Young', 'Jason Smith', 'Jason Riesa', 'Alex Rudnick', 'Oriol Vinyals', 'Greg Corrado', 'Macduff Hughes', 'Jeffrey Dean']",2016-09-26 19:59:55+00:00,arxiv,...,e880b4acdfd4f09431ccf42930121dd4,html,markdownify,2016-10-08 19:10:41+00:00,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1611.01578,Neural Architecture Search with Reinforcement Learning,"['Barret Zoph', 'Quoc V. Le']",2016-11-05 00:41:37+00:00,arxiv,...,bfed7ceb1c74461027ecfdfbc8e837f2,html,markdownify,2017-02-15 05:28:05+00:00,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']"
https://arxiv.org/abs/1610.02357,Xception: Deep Learning with Depthwise Separable Convolutions,['FranÃ§ois Chollet'],2016-10-07 17:51:51+00:00,arxiv,...,a1fbc2ecb6ef06e39c20166ef16f40d6,html,markdownify,2017-04-04 18:40:27+00:00,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1512.02595,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,"['Dario Amodei', 'Rishita Anubhai', 'Eric Battenberg', 'Carl Case', 'Jared Casper', 'Bryan Catanzaro', 'Jingdong Chen', 'Mike Chrzanowski', 'Adam Coates', 'Greg Diamos', 'Erich Elsen', 'Jesse Engel', 'Linxi Fan', 'Christopher Fougner', 'Tony Han', 'Awni Hannun', 'Billy Jun', 'Patrick LeGresley', 'Libby Lin', 'Sharan Narang', 'Andrew Ng', 'Sherjil Ozair', 'Ryan Prenger', 'Jonathan Raiman', 'Sanjeev Satheesh', 'David Seetapun', 'Shubho Sengupta', 'Yi Wang', 'Zhiqian Wang', 'Chong Wang', 'Bo Xiao', 'Dani Yogatama', 'Jun Zhan', 'Zhenyao Zhu']",2015-12-08 19:13:50+00:00,arxiv,...,d0f9c1fe1e88d68cdeb1902ddaf802d1,html,markdownify,2015-12-08 19:13:50+00:00,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",,,,cs.CL,['cs.CL']
https://arxiv.org/abs/1409.1556,Very Deep Convolutional Networks for Large-Scale Image Recognition,"['Karen Simonyan', 'Andrew Zisserman']",2014-09-04 19:48:04+00:00,arxiv,...,a72a4739afd198c19fca613e09fa3f4b,html,markdownify,2015-04-10 16:25:04+00:00,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1409.3215,Sequence to Sequence Learning with Neural Networks,"['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V. Le']",2014-09-10 19:55:35+00:00,arxiv,...,1741c7c7a8a11516c77034e87f5eb14f,html,markdownify,2014-12-14 20:59:51+00:00,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",9 pages,,,cs.CL,"['cs.CL', 'cs.LG']"
https://arxiv.org/abs/1312.5602,Playing Atari with Deep Reinforcement Learning,"['Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver', 'Alex Graves', 'Ioannis Antonoglou', 'Daan Wierstra', 'Martin Riedmiller']",2013-12-19 16:00:08+00:00,arxiv,...,a7e3c565797834704f1ae04ff1adc256,html,markdownify,2013-12-19 16:00:08+00:00,"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",NIPS Deep Learning Workshop 2013,,,cs.LG,['cs.LG']
https://arxiv.org/abs/1311.2901,Visualizing and Understanding Convolutional Networks,"['Matthew D Zeiler', 'Rob Fergus']",2013-11-12 20:02:22+00:00,arxiv,...,62a97438f581582c89f9b6c7a1565db5,html,markdownify,2013-11-28 23:04:01+00:00,"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1207.0580,Improving neural networks by preventing co-adaptation of feature detectors,"['Geoffrey E. Hinton', 'Nitish Srivastava', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan R. Salakhutdinov']",2012-07-03 06:35:15+00:00,arxiv,...,ff8563f9d1dc688187926d8dfaca8cab,html,markdownify,2012-07-03 06:35:15+00:00,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",,,,cs.NE,"['cs.NE', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/2012.07532v1,An overview of 11 proposals for building safe advanced AI,['Evan Hubinger'],2020-12-04 22:53:18+00:00,arxiv,...,b8705b05bebfca29acb4a4b1c5f5b314,html,markdownify,2020-12-04 22:53:18+00:00,"This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2201.08102,Safe Deep RL in 3D Environments using Human Feedback,"['Matthew Rahtz', 'Vikrant Varma', 'Ramana Kumar', 'Zachary Kenton', 'Shane Legg', 'Jan Leike']",2022-01-20 10:26:34+00:00,arxiv,...,2a2ddc42ed80100e4480174679e3fe0d,html,markdownify,2022-01-21 16:10:14+00:00,"Agents should avoid unsafe behaviour during both training and deployment. This typically requires a simulator and a procedural specification of unsafe behaviour. Unfortunately, a simulator is not always available, and procedurally specifying constraints can be difficult or impossible for many real-world tasks. A recently introduced technique, ReQueST, aims to solve this problem by learning a neural simulator of the environment from safe human trajectories, then using the learned simulator to efficiently learn a reward model from human feedback. However, it is yet unknown whether this approach is feasible in complex 3D environments with feedback obtained from real humans - whether sufficient pixel-based neural simulator quality can be achieved, and whether the human data requirements are viable in terms of both quantity and quality. In this paper we answer this question in the affirmative, using ReQueST to train an agent to perform a 3D first-person object collection task using data entirely from human contractors. We show that the resulting agent exhibits an order of magnitude reduction in unsafe behaviour compared to standard reinforcement learning.",,,,cs.LG,['cs.LG']
https://arxiv.org/abs/2201.03544v2,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"['Alexander Pan', 'Kush Bhatia', 'Jacob Steinhardt']",2022-01-10 18:58:52+00:00,arxiv,...,94d5d1af9597150bc04b80f7566c83b3,html,markdownify,2022-02-14 09:05:38+00:00,"Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",ICLR 2022; 19 pages,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2202.01197,VOS: Learning What You Don't Know by Virtual Outlier Synthesis,"['Xuefeng Du', 'Zhaoning Wang', 'Mu Cai', 'Yixuan Li']",2022-02-02 18:43:01+00:00,arxiv,...,50840a253ea3df16cbfc1f4d848da2c9,html,markdownify,2022-05-09 20:12:32+00:00,"Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models, reducing the FPR95 by up to 9.36% compared to the previous best method on object detectors. Code is available at https://github.com/deeplearning-wisc/vos.",ICLR 2022,,,cs.LG,"['cs.LG', 'cs.CV']"
https://arxiv.org/abs/2106.04260,Provably Robust Detection of Out-of-distribution Data (almost) for free,"['Alexander Meinke', 'Julian Bitterwolf', 'Matthias Hein']",2021-06-08 11:40:49+00:00,arxiv,...,3ca7cc8d39a3ca9749ed7466a72bfb45,html,markdownify,2022-10-18 11:40:06+00:00,"The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. We show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data. Since the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy. Moreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks. We provide code for all our experiments.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']"
https://arxiv.org/abs/2109.01903,Robust fine-tuning of zero-shot models,"['Mitchell Wortsman', 'Gabriel Ilharco', 'Jong Wook Kim', 'Mike Li', 'Simon Kornblith', 'Rebecca Roelofs', 'Raphael Gontijo-Lopes', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Hongseok Namkoong', 'Ludwig Schmidt']",2021-09-04 17:11:28+00:00,arxiv,...,283348a656b0e9fd0c67e0a85961468d,html,markdownify,2022-06-21 21:50:28+00:00,"Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.",CVPR 2022,,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/2007.03244,Robust Learning with Frequency Domain Regularization,"['Weiyu Guo', 'Yidong Ouyang']",2020-07-07 07:29:20+00:00,arxiv,...,2f2004aa9897fcdd195a887b8d4dce45,html,markdownify,2020-07-07 07:29:20+00:00,"Convolution neural networks have achieved remarkable performance in many tasks of computing vision. However, CNN tends to bias to low frequency components. They prioritize capturing low frequency patterns which lead them fail when suffering from application scenario transformation. While adversarial example implies the model is very sensitive to high frequency perturbations. In this paper, we introduce a new regularization method by constraining the frequency spectra of the filter of the model. Different from band-limit training, our method considers the valid frequency range probably entangles in different layers rather than continuous and trains the valid frequency range end-to-end by backpropagation. We demonstrate the effectiveness of our regularization by (1) defensing to adversarial perturbations; (2) reducing the generalization gap in different architecture; (3) improving the generalization ability in transfer learning scenario without fine-tune.",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2111.10493,Discrete Representations Strengthen Vision Transformer Robustness,"['Chengzhi Mao', 'Lu Jiang', 'Mostafa Dehghani', 'Carl Vondrick', 'Rahul Sukthankar', 'Irfan Essa']",2021-11-20 01:49:56+00:00,arxiv,...,bcc7924e5dd5421777c227b1a728d7eb,html,markdownify,2022-04-02 01:51:00+00:00,"Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/2107.06882,Conservative Objective Models for Effective Offline Model-Based Optimization,"['Brandon Trabucco', 'Aviral Kumar', 'Xinyang Geng', 'Sergey Levine']",2021-07-14 17:55:28+00:00,arxiv,...,54193034eb718b7389643b64c9514a0d,html,markdownify,2021-07-14 17:55:28+00:00,"Computational design problems arise in a number of settings, from synthetic biology to computer architectures. In this paper, we aim to solve data-driven model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function provided access to only a static dataset of prior experiments. Such data-driven optimization procedures are the only practical methods in many real-world domains where active data collection is expensive (e.g., when optimizing over proteins) or dangerous (e.g., when optimizing over aircraft designs). Typical methods for MBO that optimize the design against a learned model suffer from distributional shift: it is easy to find a design that ""fools"" the model into predicting a high value. To overcome this, we propose conservative objective models (COMs), a method that learns a model of the objective function that lower bounds the actual value of the ground-truth objective on out-of-distribution inputs, and uses it for optimization. Structurally, COMs resemble adversarial training methods used to overcome adversarial examples. COMs are simple to implement and outperform a number of existing methods on a wide range of MBO problems, including optimizing protein sequences, robot morphologies, neural network weights, and superconducting materials.","ICML 2021. First two authors contributed equally. Code at:
  https://github.com/brandontrabucco/design-baselines/blob/c65a53fe1e6567b740f0adf60c5db9921c1f2330/design_baselines/coms_cleaned/__init__.py",,,cs.LG,['cs.LG']
https://arxiv.org/abs/2111.03913,Linguistic Cues of Deception in a Multilingual April Fools' Day Context,"['Katerina Papantoniou', 'Panagiotis Papadakos', 'Giorgos Flouris', 'Dimitris Plexousakis']",2021-11-06 16:28:12+00:00,arxiv,...,263a1c72a3bf3b97b6636ad4ca078536,html,markdownify,2022-02-28 06:50:12+00:00,"In this work we consider the collection of deceptive April Fools' Day(AFD) news articles as a useful addition in existing datasets for deception detection tasks. Such collections have an established ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current research thread, we also discuss the individualism/collectivism dimension in deception with respect to these two datasets. Lastly, we build classifiers by testing various monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection studies, and are in alignment with the observations of other deception detection works.","Accepted for publication in the proceedings of the Eighth Italian
  Conference on Computational Linguistics (CLIC-it 2021)",,,cs.CL,['cs.CL']
https://arxiv.org/abs/2107.11264,Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation,"['Sanghun Jung', 'Jungsoo Lee', 'Daehoon Gwak', 'Sungha Choi', 'Jaegul Choo']",2021-07-23 14:25:02+00:00,arxiv,...,9094702d9ad824e728307b051f55bd75,html,markdownify,2021-10-11 11:10:41+00:00,"Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leaderboard with a large margin. Our code is publicly available at this $\href{https://github.com/shjung13/Standardized-max-logits}{link}$.",Accepted to ICCV 2021 (Oral Presentation),,,cs.CV,['cs.CV']
https://arxiv.org/abs/2108.01634,Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation,"['Victor Besnier', 'Andrei Bursuc', 'David Picard', 'Alexandre Briot']",2021-08-03 17:09:56+00:00,arxiv,...,e1a37393b4f380489a016ee750915161,html,markdownify,2021-08-03 17:09:56+00:00,"In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/2111.02840,Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models,"['Boxin Wang', 'Chejian Xu', 'Shuohang Wang', 'Zhe Gan', 'Yu Cheng', 'Jianfeng Gao', 'Ahmed Hassan Awadallah', 'Bo Li']",2021-11-04 12:59:55+00:00,arxiv,...,9d3e34a325259348414377deac1fe5f2,html,markdownify,2022-01-10 06:05:16+00:00,"Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.","Oral Presentation in NeurIPS 2021 (Datasets and Benchmarks Track). 24
  pages, 4 figures, 12 tables",,,cs.CL,"['cs.CL', 'cs.CR', 'cs.LG']"
https://arxiv.org/abs/2111.15121,Pyramid Adversarial Training Improves ViT Performance,"['Charles Herrmann', 'Kyle Sargent', 'Lu Jiang', 'Ramin Zabih', 'Huiwen Chang', 'Ce Liu', 'Dilip Krishnan', 'Deqing Sun']",2021-11-30 04:38:14+00:00,arxiv,...,7264fe80e7d674e2644cb8e6bc0924e1,html,markdownify,2022-09-02 21:24:06+00:00,"Aggressive data augmentation is a key component of the strong generalization capabilities of Vision Transformer (ViT). One such data augmentation technique is adversarial training (AT); however, many prior works have shown that this often results in poor clean accuracy. In this work, we present pyramid adversarial training (PyramidAT), a simple and effective technique to improve ViT's overall performance. We pair it with a ""matched"" Dropout and stochastic depth regularization, which adopts the same Dropout and stochastic depth configuration for the clean and adversarial samples. Similar to the improvements on CNNs by AdvProp (not directly applicable to ViT), our pyramid adversarial training breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. It leads to 1.82% absolute improvement on ImageNet clean accuracy for the ViT-B model when trained only on ImageNet-1K data, while simultaneously boosting performance on 7 ImageNet robustness metrics, by absolute numbers ranging from 1.76% to 15.68%. We set a new state-of-the-art for ImageNet-C (41.42 mCE), ImageNet-R (53.92%), and ImageNet-Sketch (41.04%) without extra data, using only the ViT-B/16 backbone and our pyramid adversarial training. Our code is publicly available at pyramidat.github.io.","Accepted to CVPR22 (oral, best paper finalist). 33 pages, including
  references & supplementary material",,,cs.CV,['cs.CV']
https://arxiv.org/abs/2010.05150,Safe Reinforcement Learning with Natural Language Constraints,"['Tsung-Yen Yang', 'Michael Hu', 'Yinlam Chow', 'Peter J. Ramadge', 'Karthik Narasimhan']",2020-10-11 03:41:56+00:00,arxiv,...,5c9f95f14f58a3dd45d8cd6a5e9ff757,html,markdownify,2021-08-04 02:46:48+00:00,"While safe reinforcement learning (RL) holds great promise for many practical applications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such specifications demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we first introduce HazardWorld, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HazardWorld, we show that our method achieves higher rewards (up to11x) and fewer constraint violations (by 1.8x) compared to existing approaches. However, in terms of absolute performance, HazardWorld still poses significant challenges for agents to learn efficiently, motivating the need for future work.",The first two authors contributed equally,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/2203.02155v1,Training language models to follow instructions with human feedback,"['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L. Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",2022-03-04 07:04:42+00:00,arxiv,...,21a59b4b3db83aa48df4924ab498c107,html,markdownify,2022-03-04 07:04:42+00:00,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2111.03026,B-Pref: Benchmarking Preference-Based Reinforcement Learning,"['Kimin Lee', 'Laura Smith', 'Anca Dragan', 'Pieter Abbeel']",2021-11-04 17:32:06+00:00,arxiv,...,e5e8b123c8df6769003a90552afa163f,html,markdownify,2021-11-04 17:32:06+00:00,"Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.","NeurIPS Datasets and Benchmarks Track 2021. Code is available at
  https://github.com/rll-research/B-Pref",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']"
https://arxiv.org/abs/2011.04864,Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts,"['Hanmeng Liu', 'Leyang Cui', 'Jian Liu', 'Yue Zhang']",2020-11-10 02:31:31+00:00,arxiv,...,171cfd8509993b2f1f3f7b464eb03278,html,markdownify,2020-11-10 02:31:31+00:00,"Natural language inference (NLI) is a fundamental NLP task, investigating the entailment relationship between two texts. Popular NLI datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce ConTRoL, a new dataset for ConTextual Reasoning over Long texts. Consisting of 8,325 expert-designed ""context-hypothesis"" pairs with gold labels, ConTRoL is a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous NLI benchmarks, the materials in ConTRoL are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.",,,,cs.CL,['cs.CL']
https://arxiv.org/abs/2106.07998,Revisiting the Calibration of Modern Neural Networks,"['Matthias Minderer', 'Josip Djolonga', 'Rob Romijnders', 'Frances Hubis', 'Xiaohua Zhai', 'Neil Houlsby', 'Dustin Tran', 'Mario Lucic']",2021-06-15 09:24:43+00:00,arxiv,...,a60c9a83cba125b8b0e14b23aa7ec8b8,html,markdownify,2021-10-26 12:08:13+00:00,"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.","35th Conference on Neural Information Processing Systems (NeurIPS
  2021)",,,cs.LG,"['cs.LG', 'cs.CV']"
https://arxiv.org/abs/2108.00106,Soft Calibration Objectives for Neural Networks,"['Archit Karandikar', 'Nicholas Cain', 'Dustin Tran', 'Balaji Lakshminarayanan', 'Jonathon Shlens', 'Michael C. Mozer', 'Becca Roelofs']",2021-07-30 23:30:20+00:00,arxiv,...,907912542aebc77832e58c06a7b27d83,html,markdownify,2021-12-07 18:31:29+00:00,"Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.","17 pages total, 10 page main paper, 5 page appendix, 10 figures
  total, 8 figures in main paper, 2 figures in appendix",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2111.12797,ReAct: Out-of-distribution Detection With Rectified Activations,"['Yiyou Sun', 'Chuan Guo', 'Yixuan Li']",2021-11-24 21:02:07+00:00,arxiv,...,b49cb38187fd4216c16198717b8ed2ce,html,markdownify,2021-11-24 21:02:07+00:00,"Out-of-distribution (OOD) detection has received much attention lately due to its practical importance in enhancing the safe deployment of neural networks. One of the primary challenges is that models often produce highly confident predictions on OOD data, which undermines the driving principle in OOD detection that the model should only be confident about in-distribution samples. In this work, we propose ReAct--a simple and effective technique for reducing model overconfidence on OOD data. Our method is motivated by novel analysis on internal activations of neural networks, which displays highly distinctive signature patterns for OOD distributions. Our method can generalize effectively to different network architectures and different OOD detection scores. We empirically demonstrate that ReAct achieves competitive detection performance on a comprehensive suite of benchmark datasets, and give theoretical explication for our method's efficacy. On the ImageNet benchmark, ReAct reduces the false positive rate (FPR95) by 25.05% compared to the previous best method.",NeurIPS 2021,,,cs.LG,['cs.LG']
https://arxiv.org/abs/1911.11132,Scaling Out-of-Distribution Detection for Real-World Settings,"['Dan Hendrycks', 'Steven Basart', 'Mantas Mazeika', 'Andy Zou', 'Joe Kwon', 'Mohammadreza Mostajabi', 'Jacob Steinhardt', 'Dawn Song']",2019-11-25 18:58:23+00:00,arxiv,...,841c19dd2f873dddd9dd318195c366d3,html,markdownify,2022-05-15 16:44:03+00:00,"Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.","ICML 2022; The Species dataset and code are available at
  https://github.com/hendrycks/anomaly-seg",,,cs.CV,"['cs.CV', 'cs.LG']"
https://arxiv.org/abs/2201.00762,Execute Order 66: Targeted Data Poisoning for Reinforcement Learning,"['Harrison Foley', 'Liam Fowl', 'Tom Goldstein', 'Gavin Taylor']",2022-01-03 17:09:32+00:00,arxiv,...,35417d6ed353b3b65bf23d5db9678e5b,html,markdownify,2022-07-28 17:30:20+00:00,"Data poisoning for reinforcement learning has historically focused on general performance degradation, and targeted attacks have been successful via perturbations that involve control of the victim's policy and rewards. We introduce an insidious poisoning attack for reinforcement learning which causes agent misbehavior only at specific target states - all while minimally modifying a small fraction of training observations without assuming any control over policy or reward. We accomplish this by adapting a recent technique, gradient alignment, to reinforcement learning. We test our method and demonstrate success in two Atari games of varying difficulty.","Workshop on Safe and Robust Control of Uncertain Systems at the 35th
  Conference on Neural Information Processing Systems",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']"
https://arxiv.org/abs/2201.01763,Robust Self-Supervised Audio-Visual Speech Recognition,"['Bowen Shi', 'Wei-Ning Hsu', 'Abdelrahman Mohamed']",2022-01-05 18:50:50+00:00,arxiv,...,fcfd1ac0d5505071cb1eecfeb2045466,html,markdownify,2022-07-14 23:05:52+00:00,"Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by ~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75% (25.8% vs. 5.8%) on average.",Interspeech 2022,,,cs.SD,"['cs.SD', 'cs.CV', 'cs.LG', 'eess.AS']"
https://arxiv.org/abs/2202.11233,Retrieval Augmented Classification for Long-Tail Visual Recognition,"['Alexander Long', 'Wei Yin', 'Thalaiyasingam Ajanthan', 'Vu Nguyen', 'Pulak Purkait', 'Ravi Garg', 'Alan Blair', 'Chunhua Shen', 'Anton van den Hengel']",2022-02-22 23:40:51+00:00,arxiv,...,aa4d98c377e06424e85135a4ff95bfc2,html,markdownify,2022-02-22 23:40:51+00:00,"We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/2202.09931,Deconstructing Distributions: A Pointwise Framework of Learning,"['Gal Kaplun', 'Nikhil Ghosh', 'Saurabh Garg', 'Boaz Barak', 'Preetum Nakkiran']",2022-02-20 23:25:28+00:00,arxiv,...,ef20cf873caa07c79779768adf372515,html,markdownify,2022-06-07 06:32:29+00:00,"In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ""compatible"" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts ""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021)","GK and NG contributed equally. v2: Added Figures 4, 5",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/2202.03286,Red Teaming Language Models with Language Models,"['Ethan Perez', 'Saffron Huang', 'Francis Song', 'Trevor Cai', 'Roman Ring', 'John Aslanides', 'Amelia Glaese', 'Nat McAleese', 'Geoffrey Irving']",2022-02-07 15:22:17+00:00,arxiv,...,2468d15bc285d108e1e2c9047dc301f1,html,markdownify,2022-02-07 15:22:17+00:00,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CR', 'cs.LG']"
https://arxiv.org/abs/2004.09984,BERT-ATTACK: Adversarial Attack Against BERT Using BERT,"['Linyang Li', 'Ruotian Ma', 'Qipeng Guo', 'Xiangyang Xue', 'Xipeng Qiu']",2020-04-21 13:30:02+00:00,arxiv,...,d34a8b156c4c213e8c9ae8fc9fa9a0e2,html,markdownify,2020-10-02 03:08:04+00:00,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.",Accepted by EMNLP2020,,,cs.CL,['cs.CL']
https://arxiv.org/abs/2111.05328,Data Augmentation Can Improve Robustness,"['Sylvestre-Alvise Rebuffi', 'Sven Gowal', 'Dan A. Calian', 'Florian Stimberg', 'Olivia Wiles', 'Timothy Mann']",2021-11-09 18:57:00+00:00,arxiv,...,439fab87a1a0bd936cad7c8d3c905b91,html,markdownify,2021-11-09 18:57:00+00:00,"Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$, our model reaches 60.07% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.","Accepted at NeurIPS 2021. arXiv admin note: substantial text overlap
  with arXiv:2103.01946; text overlap with arXiv:2110.09468",,,cs.CV,"['cs.CV', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2112.05135,PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures,"['Dan Hendrycks', 'Andy Zou', 'Mantas Mazeika', 'Leonard Tang', 'Bo Li', 'Dawn Song', 'Jacob Steinhardt']",2021-12-09 18:59:31+00:00,arxiv,...,ec820edfe1eeafdb16c4590db03f87a2,html,markdownify,2022-03-29 04:33:54+00:00,"In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.","CVPR 2022. Code and models are available at
  https://github.com/andyzoujm/pixmix",,,cs.LG,"['cs.LG', 'cs.CV']"
https://arxiv.org/abs/2105.08475v1,AI and Shared Prosperity,"['Katya Klinova', 'Anton Korinek']",2021-05-18 12:37:18+00:00,arxiv,...,a112f1734f1827133cd0db4ddae37b01,html,markdownify,2021-05-18 12:37:18+00:00,"Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
  Society (AIES '21)",10.1145/3461702.3462619,cs.AI,"['cs.AI', 'cs.CY', 'econ.GN', 'q-fin.EC', 'J.4; K.4.1']"
https://arxiv.org/abs/2106.05091v1,PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training,"['Kimin Lee', 'Laura Smith', 'Pieter Abbeel']",2021-06-09 14:10:50+00:00,arxiv,...,a7b4e46faa1a56b9fdbc29a6e5aaf6b3,html,markdownify,2021-06-09 14:10:50+00:00,"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.","ICML 2021. First two authors contributed equally. Website:
  https://sites.google.com/view/icml21pebble Code:
  https://github.com/pokaxpoka/B_Pref",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2102.04255v1,AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks,"['McKane Andrus', 'Sarah Dean', 'Thomas Krendl Gilbert', 'Nathan Lambert', 'Tom Zick']",2021-02-04 18:54:20+00:00,arxiv,...,dd45c509524c460a137a07b8eadd2ecb,html,markdownify,2021-02-04 18:54:20+00:00,"Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI.",8 Pages,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1701.01302,Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making,['Andrew Critch'],2017-01-05 13:00:05+00:00,arxiv,...,c92d142a0deeadc497ca493626131113,html,markdownify,2017-05-13 08:33:46+00:00,"Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\""{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.",,,,cs.AI,"['cs.AI', 'cs.GT', 'cs.LG']"
https://arxiv.org/abs/1705.04226,Robot Planning with Mathematical Models of Human State and Action,['Anca D. Dragan'],2017-05-11 15:02:34+00:00,arxiv,...,0b91ee789e3438b263b0d1f88c559916,html,markdownify,2017-07-04 02:08:23+00:00,"Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.",,,,cs.RO,['cs.RO']
https://arxiv.org/abs/2003.04881v4,Pruned Neural Networks are Surprisingly Modular,"['Daniel Filan', 'Shlomi Hod', 'Cody Wild', 'Andrew Critch', 'Stuart Russell']",2020-03-10 17:51:33+00:00,arxiv,...,7d4ca925b245b7a3b01c1e270d0eba9c,html,markdownify,2020-08-11 21:50:05+00:00,"The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.","25 pages, 12 figures",,,cs.NE,"['cs.NE', 'cs.LG']"
https://arxiv.org/abs/1811.09246v1,Oversight of Unsafe Systems via Dynamic Safety Envelopes,['David Manheim'],2018-11-22 17:31:41+00:00,arxiv,...,1313427e95a689da259c5cca2699adcc,html,markdownify,2018-11-22 17:31:41+00:00,"This paper reviews the reasons that Human-in-the-Loop is both critical for preventing widely-understood failure modes for machine learning, and not a practical solution. Following this, we review two current heuristic methods for addressing this. The first is provable safety envelopes, which are possible only when the dynamics of the system are fully known, but can be useful safety guarantees when optimal behavior is based on machine learning with poorly-understood safety characteristics. The second is the simpler circuit breaker model, which can forestall or prevent catastrophic outcomes by stopping the system, without any specific model of the system. This paper proposes using heuristic, dynamic safety envelopes, which are a plausible halfway point between these approaches that allows human oversight without some of the more difficult problems faced by Human-in-the-Loop systems. Finally, the paper concludes with how this approach can be used for governance of systems where otherwise unsafe systems are deployed.",,,,cs.AI,"['cs.AI', 'K.3.5; K.4.1']"
https://arxiv.org/abs/2011.06275v2,Performance of Bounded-Rational Agents With the Ability to Self-Modify,"['Jakub TÄtek', 'Marek Sklenka', 'TomÃ¡Å¡ GavenÄiak']",2020-11-12 09:25:08+00:00,arxiv,...,9efc199d383b3c914ab941b3e19cf2a9,html,markdownify,2021-01-18 09:55:26+00:00,"Self-modification of agents embedded in complex environments is hard to avoid, whether it happens via direct means (e.g. own code modification) or indirectly (e.g. influencing the operator, exploiting bugs or the environment). It has been argued that intelligent agents have an incentive to avoid modifying their utility function so that their future instances work towards the same goals.   Everitt et al. (2016) formally show that providing an option to self-modify is harmless for perfectly rational agents. We show that this result is no longer true for agents with bounded rationality. In such agents, self-modification may cause exponential deterioration in performance and gradual misalignment of a previously aligned agent. We investigate how the size of this effect depends on the type and magnitude of imperfections in the agent's rationality (1-4 below). We also discuss model assumptions and the wider problem and framing space.   We examine four ways in which an agent can be bounded-rational: it either (1) doesn't always choose the optimal action, (2) is not perfectly aligned with human values, (3) has an inaccurate model of the environment, or (4) uses the wrong temporal discounting factor. We show that while in the cases (2)-(4) the misalignment caused by the agent's imperfection does not increase over time, with (1) the misalignment may grow exponentially.",Fixed minor problems; To appear in SafeAI @ AAAI 2021,,,cs.AI,"['cs.AI', 'cs.CY']"
https://arxiv.org/abs/1711.05541v5,Good and safe uses of AI Oracles,"['Stuart Armstrong', ""Xavier O'Rorke""]",2017-11-15 12:47:17+00:00,arxiv,...,56f83ea9b9ffb5ec3c120c7eb9d8490c,html,markdownify,2018-06-05 11:13:48+00:00,"It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.","11 pages, 2 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1609.04994v3,Exploration Potential,['Jan Leike'],2016-09-16 10:55:27+00:00,arxiv,...,50fd3e3cfcc7bd236d3b18a5c8853716,html,markdownify,2016-11-18 11:17:56+00:00,"We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.","10 pages, including proofs",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1806.02404,Dissolving the Fermi Paradox,"['Anders Sandberg', 'Eric Drexler', 'Toby Ord']",2018-06-06 19:51:21+00:00,arxiv,...,564a53cf0850e61567acf20e1ab63c60,html,markdownify,2018-06-06 19:51:21+00:00,"The Fermi paradox is the conflict between an expectation of a high {\em ex ante} probability of intelligent life elsewhere in the universe and the apparently lifeless universe we in fact observe. The expectation that the universe should be teeming with intelligent life is linked to models like the Drake equation, which suggest that even if the probability of intelligent life developing at a given site is small, the sheer multitude of possible sites should nonetheless yield a large number of potentially observable civilizations. We show that this conflict arises from the use of Drake-like equations, which implicitly assume certainty regarding highly uncertain parameters. We examine these parameters, incorporating models of chemical and genetic transitions on paths to the origin of life, and show that extant scientific knowledge corresponds to uncertainties that span multiple orders of magnitude. This makes a stark difference. When the model is recast to represent realistic distributions of uncertainty, we find a substantial {\em ex ante} probability of there being no other intelligent life in our observable universe, and thus that there should be little surprise when we fail to detect any signs of it. This result dissolves the Fermi paradox, and in doing so removes any need to invoke speculative mechanisms by which civilizations would inevitably fail to have observable effects upon the universe.","Submitted to Proceedings of the Royal Society of London A; 4
  supplements",,,physics.pop-ph,['physics.pop-ph']
https://arxiv.org/abs/1812.01647,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,"['Jonathan Uesato', 'Ananya Kumar', 'Csaba Szepesvari', 'Tom Erez', 'Avraham Ruderman', 'Keith Anderson', 'Krishmamurthy', 'Dvijotham', 'Nicolas Heess', 'Pushmeet Kohli']",2018-12-04 19:39:53+00:00,arxiv,...,ef069c4c5d6abf0c3f79b8232dc27d3d,html,markdownify,2018-12-04 19:39:53+00:00,"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",,,,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/1801.08757,Safe Exploration in Continuous Action Spaces,"['Gal Dalal', 'Krishnamurthy Dvijotham', 'Matej Vecerik', 'Todd Hester', 'Cosmin Paduraru', 'Yuval Tassa']",2018-01-26 11:11:18+00:00,arxiv,...,6bd774c51ce3317fd4e654edd2e42ebd,html,markdownify,2018-01-26 11:11:18+00:00,"We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2011.08820v1,REALab: An Embedded Perspective on Tampering,"['Ramana Kumar', 'Jonathan Uesato', 'Richard Ngo', 'Tom Everitt', 'Victoria Krakovna', 'Shane Legg']",2020-11-17 18:37:20+00:00,arxiv,...,dc213dca88c03e269d285eeb78c0d816,html,markdownify,2020-11-17 18:37:20+00:00,"This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1809.07802,Playing the Game of Universal Adversarial Perturbations,"['Julien Perolat', 'Mateusz Malinowski', 'Bilal Piot', 'Olivier Pietquin']",2018-09-20 18:48:36+00:00,arxiv,...,bbeea6a2cebbaa7a3f99f5b1c599579d,html,markdownify,2018-09-25 20:16:45+00:00,"We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.",,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1705.03394v1,That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox,"['Anders Sandberg', 'Stuart Armstrong', 'Milan M. Cirkovic']",2017-04-27 15:41:00+00:00,arxiv,...,c0e14da82cbbadab306a3f0650dc87b4,html,markdownify,2017-04-27 15:41:00+00:00,"If a civilization wants to maximize computation it appears rational to aestivate until the far future in order to exploit the low temperature environment: this can produce a $10^{30}$ multiplier of achievable computation. We hence suggest the ""aestivation hypothesis"": the reason we are not observing manifestations of alien civilizations is that they are currently (mostly) inactive, patiently waiting for future cosmic eras. This paper analyzes the assumptions going into the hypothesis and how physical law and observational evidence constrain the motivations of aliens compatible with the hypothesis.",Submitted to Journal of the British Interplanetary Society,,,physics.pop-ph,['physics.pop-ph']
https://arxiv.org/abs/1703.10987,On the Impossibility of Supersized Machines,"['Ben Garfinkel', 'Miles Brundage', 'Daniel Filan', 'Carrick Flynn', 'Jelena Luketina', 'Michael Page', 'Anders Sandberg', 'Andrew Snyder-Beattie', 'Max Tegmark']",2017-03-31 17:14:39+00:00,arxiv,...,1dbad4a970961f0ca1d6e4c2a3b442e1,html,markdownify,2017-03-31 17:14:39+00:00,"In recent years, a number of prominent computer scientists, along with academics in fields such as philosophy and physics, have lent credence to the notion that machines may one day become as large as humans. Many have further argued that machines could even come to exceed human size by a significant margin. However, there are at least seven distinct arguments that preclude this outcome. We show that it is not only implausible that machines will ever exceed human size, but in fact impossible.","9 pages, 2 figures",,,cs.CY,"['cs.CY', 'physics.pop-ph']"
https://arxiv.org/abs/astro-ph/0512204v2,How unlikely is a doomsday catastrophe?,"['Max Tegmark', 'Nick Bostrom']",2005-12-08 05:17:15+00:00,arxiv,...,12342bffdec481644d895a1d4584f942,html,markdownify,2005-12-21 14:28:48+00:00,"Numerous Earth-destroying doomsday scenarios have recently been analyzed, including breakdown of a metastable vacuum state and planetary destruction triggered by a ""strangelet'' or microscopic black hole. We point out that many previous bounds on their frequency give a false sense of security: one cannot infer that such events are rare from the the fact that Earth has survived for so long, because observers are by definition in places lucky enough to have avoided destruction. We derive a new upper bound of one per 10^9 years (99.9% c.l.) on the exogenous terminal catastrophe rate that is free of such selection bias, using planetary age distributions and the relatively late formation time of Earth.","Substantially expanded discussion to better explain key argument. 4
  pages, 1 fig",,,astro-ph,['astro-ph']
https://arxiv.org/abs/1712.06365v4,Indifference' methods for managing agent rewards,"['Stuart Armstrong', ""Xavier O'Rourke""]",2017-12-18 12:28:45+00:00,arxiv,...,d59ec4762a4aac48cb3a3caf3a66be34,html,markdownify,2018-06-05 11:10:23+00:00,"`Indifference' refers to a class of methods used to control reward based agents. Indifference techniques aim to achieve one or more of three distinct goals: rewards dependent on certain events (without the agent being motivated to manipulate the probability of those events), effective disbelief (where agents behave as if particular events could never happen), and seamless transition from one reward function to another (with the agent acting as if this change is unanticipated). This paper presents several methods for achieving these goals in the POMDP setting, establishing their uses, strengths, and requirements. These methods of control work even when the implications of the agent's reward are otherwise not fully understood.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2103.12021,Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism,"['Paria Rashidinejad', 'Banghua Zhu', 'Cong Ma', 'Jiantao Jiao', 'Stuart Russell']",2021-03-22 17:27:08+00:00,arxiv,...,dba8c148cda21811ea2de1ef52b25fef,html,markdownify,2021-03-22 17:27:08+00:00,"Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.   Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.","84 pages, 6 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']"
https://arxiv.org/abs/1901.10031,Lyapunov-based Safe Policy Optimization for Continuous Control,"['Yinlam Chow', 'Ofir Nachum', 'Aleksandra Faust', 'Edgar Duenez-Guzman', 'Mohammad Ghavamzadeh']",2019-01-28 23:14:58+00:00,arxiv,...,7fe3e71f765f63eab6ec9062cb81bb22,html,markdownify,2019-02-11 20:52:42+00:00,"We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1912.02757,Deep Ensembles: A Loss Landscape Perspective,"['Stanislav Fort', 'Huiyi Hu', 'Balaji Lakshminarayanan']",2019-12-05 17:48:18+00:00,arxiv,...,a95ff17592ca56ba110b79b98cb054dc,html,markdownify,2020-06-25 03:57:04+00:00,"Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.",,,,stat.ML,"['stat.ML', 'cs.LG']"
https://arxiv.org/abs/2011.08827,Avoiding Tampering Incentives in Deep RL via Decoupled Approval,"['Jonathan Uesato', 'Ramana Kumar', 'Victoria Krakovna', 'Tom Everitt', 'Richard Ngo', 'Shane Legg']",2020-11-17 18:48:59+00:00,arxiv,...,e3a389d50a69b307bbcce76226225798,html,markdownify,2020-11-17 18:48:59+00:00,"How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2003.11881,An empirical investigation of the challenges of real-world reinforcement learning,"['Gabriel Dulac-Arnold', 'Nir Levine', 'Daniel J. Mankowitz', 'Jerry Li', 'Cosmin Paduraru', 'Sven Gowal', 'Todd Hester']",2020-03-24 11:05:41+00:00,arxiv,...,6698057ac42f6c1a4782367c37ad0b2f,html,markdownify,2021-03-04 13:02:59+00:00,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called the realworldrl-suite which we propose an as an open-source benchmark.",arXiv admin note: text overlap with arXiv:1904.12901,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1910.09338,An Alternative Surrogate Loss for PGD-based Adversarial Testing,"['Sven Gowal', 'Jonathan Uesato', 'Chongli Qin', 'Po-Sen Huang', 'Timothy Mann', 'Pushmeet Kohli']",2019-10-21 13:08:54+00:00,arxiv,...,d051098be44f6ad94f11f77c3c898c67,html,markdownify,2019-10-21 13:08:54+00:00,"Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36% (with $\ell_\infty$ perturbations of $\epsilon = 0.3$) and the accuracy of their CIFAR-10 model to 44.03% (at $\epsilon = 8/255$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07% (with $\ell_\infty$ perturbations of $\epsilon = 0.031$).",,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1705.04630,Forecasting using incomplete models,['Vanessa Kosoy'],2017-05-12 15:38:57+00:00,arxiv,...,027d591d78bc8cba0732f48abf51106f,html,markdownify,2019-05-16 12:54:56+00:00,"We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is ""suspected"" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.",29 pages,,,cs.LG,"['cs.LG', '68Q32, 62M10, 62G08', 'G.3; I.2.6']"
https://arxiv.org/abs/1604.05280,Asymptotic Convergence in Online Learning with Unbounded Delays,"['Scott Garrabrant', 'Nate Soares', 'Jessica Taylor']",2016-04-18 19:04:59+00:00,arxiv,...,78f494174ac02d6a82137078a4c513b5,html,markdownify,2016-09-07 18:43:24+00:00,"We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.",,,,cs.LG,"['cs.LG', 'cs.AI', 'math.PR']"
https://arxiv.org/abs/2002.10657,Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,['Satrajit Chatterjee'],2020-02-25 03:59:31+00:00,arxiv,...,d85829cb6dff93e17ddcf6581cf6341d,html,markdownify,2020-02-25 03:59:31+00:00,"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",To appear in ICLR 2020,,,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1907.04534v1,The Role of Cooperation in Responsible AI Development,"['Amanda Askell', 'Miles Brundage', 'Gillian Hadfield']",2019-07-10 06:51:04+00:00,arxiv,...,af45c5295eea0e8919156c7b309f1978,html,markdownify,2019-07-10 06:51:04+00:00,"In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.","23 pages, 1 table",,,cs.CY,"['cs.CY', 'cs.AI', 'K.4.1; K.1']"
https://arxiv.org/abs/1905.01034,Transfer of Adversarial Robustness Between Perturbation Types,"['Daniel Kang', 'Yi Sun', 'Tom Brown', 'Dan Hendrycks', 'Jacob Steinhardt']",2019-05-03 04:51:07+00:00,arxiv,...,eda729c11ff461578a4ca5eff8ef0bef,html,markdownify,2019-05-03 04:51:07+00:00,"We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type may not always imply and may sometimes hurt robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.","11 pages, 6 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']"
https://arxiv.org/abs/2102.02503,"Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models","['Alex Tamkin', 'Miles Brundage', 'Jack Clark', 'Deep Ganguli']",2021-02-04 09:27:04+00:00,arxiv,...,a698b98a7f3c89a5365fa1b138752602,html,markdownify,2021-02-04 09:27:04+00:00,"On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.",,,,cs.CL,"['cs.CL', 'cs.LG']"
https://arxiv.org/abs/1608.04112,Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm,"['Vanessa Kosoy', 'Alexander Appel']",2016-08-14 15:34:24+00:00,arxiv,...,bd7b446c21631b6ee53bbda253ac3a58,html,markdownify,2019-06-04 19:53:27+00:00,"We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of ""optimal polynomial-time estimators."" We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with ""classical"" probability theory.",86 pages,,,cs.CC,['cs.CC']
https://arxiv.org/abs/1105.3821v1,Ontological Crises in Artificial Agents' Value Systems,['Peter de Blanc'],2011-05-19 09:32:46+00:00,arxiv,...,f4003ea83019b0a6362eb8eacef54f2f,html,markdownify,2011-05-19 09:32:46+00:00,"Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals.   We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1604.05288,Inductive Coherence,"['Scott Garrabrant', 'Benya Fallenstein', 'Abram Demski', 'Nate Soares']",2016-04-18 19:37:46+00:00,arxiv,...,30cf59d6d2f0a2b3e84743f0683d99f2,html,markdownify,2016-10-07 17:00:38+00:00,"While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.",,,,cs.AI,"['cs.AI', 'cs.LG', 'math.PR']"
https://arxiv.org/abs/1902.09469,Embedded Agency,"['Abram Demski', 'Scott Garrabrant']",2019-02-25 17:38:48+00:00,arxiv,...,1805141cda7ec1f938147f8c6ad6f9b4,html,markdownify,2020-10-06 21:20:37+00:00,"Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts.   We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type ""function""; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1809.08343,Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration,"['Ritesh Noothigattu', 'Djallel Bouneffouf', 'Nicholas Mattei', 'Rachita Chandra', 'Piyush Madan', 'Kush Varshney', 'Murray Campbell', 'Moninder Singh', 'Francesca Rossi']",2018-09-21 23:38:17+00:00,arxiv,...,6ffe351f771a4179372e5300bfc3bb4e,html,markdownify,2018-09-21 23:38:17+00:00,"Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.","8 pages, 3 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1805.08347,How To Solve Moral Conundrums with Computability Theory,['Min Baek'],2018-05-22 01:47:16+00:00,arxiv,...,24246a65a04467668bafb30a1102e558,html,markdownify,2021-04-25 23:48:42+00:00,"Various moral conundrums plague population ethics: the Non-Identity Problem, the Procreation Asymmetry, the Repugnant Conclusion, and more. I argue that the aforementioned moral conundrums have a structure neatly accounted for, and solved by, some ideas in computability theory. I introduce a mathematical model based on computability theory and show how previous arguments pertaining to these conundrums fit into the model. This paper proceeds as follows. First, I do a very brief survey of the history of computability theory in moral philosophy. Second, I follow various papers, and show how their arguments fit into, or don't fit into, our model. Third, I discuss the implications of our model to the question why the human race should or should not continue to exist. Finally, I show that our model may be interpreted according to a Confucian-Taoist moral principle.",Conclusion is incorrect,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LO']"
https://arxiv.org/abs/1904.01318,Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,"['Christian Rupprecht', 'Cyril Ibrahim', 'Christopher J. Pal']",2019-04-02 10:21:23+00:00,arxiv,...,f41379e839976d5096bd1fc053b3f097,html,markdownify,2019-04-02 10:21:23+00:00,"As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",,,,cs.CV,['cs.CV']
https://arxiv.org/abs/1910.01074v3,Formal Language Constraints for Markov Decision Processes,"['Eleanor Quint', 'Dong Xu', 'Samuel Flint', 'Stephen Scott', 'Matthew Dwyer']",2019-10-02 16:45:23+00:00,arxiv,...,191b4e93eaa186d87cd05546fde79391,html,markdownify,2020-10-13 18:00:26+00:00,"In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.",NeurIPS 2019 Workshop on Safety and Robustness in Decision Making,,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/2004.14990,Reinforcement Learning with Augmented Data,"['Michael Laskin', 'Kimin Lee', 'Adam Stooke', 'Lerrel Pinto', 'Pieter Abbeel', 'Aravind Srinivas']",2020-04-30 17:35:32+00:00,arxiv,...,ddd837e1ad9441d38ededc82a54427d1,html,markdownify,2020-11-05 06:04:50+00:00,"Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.","NeurIPS 2020 camera-ready version. First two authors contributed
  equally, website: https://mishalaskin.github.io/rad code:
  https://github.com/MishaLaskin/rad and
  https://github.com/pokaxpoka/rad_procgen",,,cs.LG,"['cs.LG', 'stat.ML']"
https://arxiv.org/abs/1807.10299,Variational Option Discovery Algorithms,"['Joshua Achiam', 'Harrison Edwards', 'Dario Amodei', 'Pieter Abbeel']",2018-07-26 18:05:45+00:00,arxiv,...,021104d1104e3164d2aad77cc9f22fe9,html,markdownify,2018-07-26 18:05:45+00:00,"We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1907.09273v2,Why Build an Assistant in Minecraft?,"['Arthur Szlam', 'Jonathan Gray', 'Kavya Srinet', 'Yacine Jernite', 'Armand Joulin', 'Gabriel Synnaeve', 'Douwe Kiela', 'Haonan Yu', 'Zhuoyuan Chen', 'Siddharth Goyal', 'Demi Guo', 'Danielle Rothermel', 'C. Lawrence Zitnick', 'Jason Weston']",2019-07-22 12:32:15+00:00,arxiv,...,d31afb79cc3303878666bb2913fa3058,html,markdownify,2019-07-25 21:52:08+00:00,"In this document we describe a rationale for a research program aimed at building an open ""assistant"" in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.",,,,cs.AI,"['cs.AI', 'cs.CL']"
https://arxiv.org/abs/1905.07861v1,Perceptual Values from Observation,"['Ashley D. Edwards', 'Charles L. Isbell']",2019-05-20 03:59:44+00:00,arxiv,...,1fd278bc93322aa3bc50123b659e3885,html,markdownify,2019-05-20 03:59:44+00:00,"Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.",Accepted into the Workshop on Self-Supervised Learning at ICML 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1809.05214,Model-Based Reinforcement Learning via Meta-Policy Optimization,"['Ignasi Clavera', 'Jonas Rothfuss', 'John Schulman', 'Yasuhiro Fujita', 'Tamim Asfour', 'Pieter Abbeel']",2018-09-14 01:15:28+00:00,arxiv,...,97844ed0f23f52179ffb7b25812e1232,html,markdownify,2018-09-14 01:15:28+00:00,"Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.","First 2 authors contributed equally. Accepted for Conference on Robot
  Learning (CoRL)",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2004.13649,Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels,"['Ilya Kostrikov', 'Denis Yarats', 'Rob Fergus']",2020-04-28 16:48:16+00:00,arxiv,...,d0c75d0e6748db77fe354bf11a114ffd,html,markdownify,2021-03-07 16:37:37+00:00,"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.",,,,cs.LG,"['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']"
https://arxiv.org/abs/1609.03543,Logical Induction,"['Scott Garrabrant', 'Tsvi Benson-Tilsen', 'Andrew Critch', 'Nate Soares', 'Jessica Taylor']",2016-09-12 19:30:56+00:00,arxiv,...,d2ff193c557b5e6a6136f3fd9f04cfd4,html,markdownify,2020-12-07 22:26:59+00:00,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit.   These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]",,,,cs.AI,"['cs.AI', 'cs.LO', 'math.LO', 'math.PR']"
https://arxiv.org/abs/0912.5073v2,A Rational Decision Maker with Ordinal Utility under Uncertainty: Optimism and Pessimism,['Ji Han'],2009-12-27 14:09:43+00:00,arxiv,...,20775b9c1c5621cc9587bccbc936c839,html,markdownify,2010-06-11 12:25:13+00:00,"In game theory and artificial intelligence, decision making models often involve maximizing expected utility, which does not respect ordinal invariance. In this paper, the author discusses the possibility of preserving ordinal invariance and still making a rational decision under uncertainty.",This paper has been withdrawn by the author,,,cs.AI,"['cs.AI', 'cs.GT', 'I.2.0; I.2.11']"
https://arxiv.org/abs/1107.5537v1,Asymptotically Optimal Agents,"['Tor Lattimore', 'Marcus Hutter']",2011-07-27 16:51:48+00:00,arxiv,...,376ccee42afbf323f0b9f308568acf8a,html,markdownify,2011-07-27 16:51:48+00:00,"Artificial general intelligence aims to create agents capable of learning to solve arbitrary interesting problems. We define two versions of asymptotic optimality and prove that no agent can satisfy the strong version while in some cases, depending on discounting, there does exist a non-computable weak asymptotically optimal agent.",21 LaTeX pages,"Proc. 22nd International Conf. on Algorithmic Learning Theory
  (ALT-2011) pages 368-382",,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1509.07582v1,Constructing Abstraction Hierarchies Using a Skill-Symbol Loop,['George Konidaris'],2015-09-25 04:07:22+00:00,arxiv,...,d4a13c76a1b2a518801a1a93248e48fe,html,markdownify,2015-09-25 04:07:22+00:00,"We describe a framework for building abstraction hierarchies whereby an agent alternates skill- and representation-acquisition phases to construct a sequence of increasingly abstract Markov decision processes. Our formulation builds on recent results showing that the appropriate abstract representation of a problem is specified by the agent's skills. We describe how such a hierarchy can be used for fast planning, and illustrate the construction of an appropriate hierarchy for the Taxi domain.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1510.09033v1,Turing's Red Flag,['Toby Walsh'],2015-10-30 10:04:11+00:00,arxiv,...,8eaefb70477e1ff4f00b2b8fd484e242,html,markdownify,2015-10-30 10:04:11+00:00,"Sometime in the future we will have to deal with the impact of AI's being mistaken for humans. For this reason, I propose that any autonomous system should be designed so that it is unlikely to be mistaken for anything besides an autonomous sysem, and should identify itself at the start of any interaction with another agent.",To appear in the Communications of the ACM,,,cs.AI,"['cs.AI', 'cs.CY', 'I.2.0']"
https://arxiv.org/abs/1910.06636v1,Solving Logic Grid Puzzles with an Algorithm that Imitates Human Behavior,"['Guillaume Escamocher', ""Barry O'Sullivan""]",2019-10-15 10:18:07+00:00,arxiv,...,d8a6f3600e8a0028adc175eb4ecf74d1,html,markdownify,2019-10-15 10:18:07+00:00,"We present in this paper our solver for logic grid puzzles. The approach used by our algorithm mimics the way a human would try to solve the same problem. Every progress made during the solving process is accompanied by a detailed explanation of our program's reasoning. Since this reasoning is based on the same heuristics that a human would employ, the user can easily follow the given explanation.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2105.03414v1,Using reinforcement learning to design an AI assistantfor a satisfying co-op experience,"['Ajay Krishnan', 'Niranj Jyothish', 'Xun Jia']",2021-05-07 17:44:02+00:00,arxiv,...,cb3aac92665edb436453d7dbd86ef1f8,html,markdownify,2021-05-07 17:44:02+00:00,"In this project, we designed an intelligent assistant player for the single-player game Space Invaders with the aim to provide a satisfying co-op experience. The agent behaviour was designed using reinforcement learning techniques and evaluated based on several criteria. We validate the hypothesis that an AI-driven computer player can provide a satisfying co-op experience.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/cs/0307069v1,A logic for reasoning about upper probabilities,"['Joseph Y. Halpern', 'Riccardo Pucella']",2003-07-30 21:08:54+00:00,arxiv,...,23fac3d8ca9fa691fa14a9e6d97f5641,html,markdownify,2003-07-30 21:08:54+00:00,"We present a propositional logic %which can be used to reason about the uncertainty of events, where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event. We give a sound and complete axiomatization for the logic, and show that the satisfiability problem is NP-complete, no harder than satisfiability for propositional logic.","A preliminary version of this paper appeared in Proc. of the 17th
  Conference on Uncertainty in AI, 2001","Journal of AI Research 17, 2001, pp. 57-81",,cs.AI,"['cs.AI', 'cs.LO', 'I.2.4; F.2.1']"
https://arxiv.org/abs/cs/0312040v1,Diagnostic reasoning with A-Prolog,"['Marcello Balduccini', 'Michael Gelfond']",2003-12-18 13:38:49+00:00,arxiv,...,1eb41530f36325ecc43f143ed70bb117,html,markdownify,2003-12-18 13:38:49+00:00,"In this paper we suggest an architecture for a software agent which operates a physical device and is capable of making observations and of testing and repairing the device's components. We present simplified definitions of the notions of symptom, candidate diagnosis, and diagnosis which are based on the theory of action language ${\cal AL}$. The definitions allow one to give a simple account of the agent's behavior in which many of the agent's tasks are reduced to computing stable models of logic programs.","46 pages, 1 Postscript figure",TPLP Vol 3(4&5) (2003) 425-461,,cs.AI,"['cs.AI', 'F.4.1; F.2.2']"
https://arxiv.org/abs/cs/0605024v1,A Formal Measure of Machine Intelligence,"['Shane Legg', 'Marcus Hutter']",2006-05-06 16:56:43+00:00,arxiv,...,6652a85295ba9bc2f57f9dc1d2d2d408,html,markdownify,2006-05-06 16:56:43+00:00,"A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this measure formally captures the concept of machine intelligence in the broadest reasonable sense.",8 two-column pages,"Proc. 15th Annual Machine Learning Conference of {B}elgium and The
  Netherlands (Benelearn 2006) pages 73-80",,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1207.3874v1,Reasoning about Agent Programs using ATL-like Logics,"['Nitin Yadav', 'Sebastian Sardina']",2012-07-17 04:10:28+00:00,arxiv,...,cd4e6aad60e23f9bff3e12d533f8b123,html,markdownify,2012-07-17 04:10:28+00:00,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the agents' operational know-how, as defined by their libraries of abstract plans. Inspired by ATLES, a variant itself of ATL, it is possible in our logic to explicitly refer to ""rational"" strategies for agents developed under the Belief-Desire-Intention agent programming paradigm. This allows us to express and verify properties of BDI systems using ATL-type logical frameworks.",,"In Proceedings of the European Conference on Logics in Artificial
  Intelligence (JELIA), volume 7519 of LNCS, pages 437-449, 2012",,cs.AI,['cs.AI']
https://arxiv.org/abs/1312.5713v2,Giving the AI definition a form suitable for the engineer,['Dimiter Dobrev'],2013-12-19 19:28:18+00:00,arxiv,...,5d3efaa08b107909cdb6edf83db5324f,html,markdownify,2015-03-31 10:26:48+00:00,"Artificial Intelligence - what is this? That is the question! In earlier papers we already gave a formal definition for AI, but if one desires to build an actual AI implementation, the following issues require attention and are treated here: the data format to be used, the idea of Undef and Nothing symbols, various ways for defining the ""meaning of life"", and finally, a new notion of ""incorrect move"". These questions are of minor importance in the theoretical discussion, but we already know the answer of the question ""Does AI exist?"" Now we want to make the next step and to create this program.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1602.03506v1,Research Priorities for Robust and Beneficial Artificial Intelligence,"['Stuart Russell', 'Daniel Dewey', 'Max Tegmark']",2016-02-10 20:29:25+00:00,arxiv,...,36a2ab85f463fefb82f95d40f287cb6c,html,markdownify,2016-02-10 20:29:25+00:00,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","This article gives examples of the type of research advocated by the
  open letter for robust & beneficial AI at
  http://futureoflife.org/ai-open-letter",AI Magazine 36:4 (2015),,cs.AI,"['cs.AI', 'stat.ML']"
https://arxiv.org/abs/1702.08495v2,Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument,['Sebastian Benthall'],2017-02-27 19:57:17+00:00,arxiv,...,96d475600e2a0e4c8e435777bb1c87f6,html,markdownify,2017-03-04 20:43:32+00:00,"In recent years prominent intellectuals have raised ethical concerns about the consequences of artificial intelligence. One concern is that an autonomous agent might modify itself to become ""superintelligent"" and, in supremely effective pursuit of poorly specified goals, destroy all of humanity. This paper considers and rejects the possibility of this outcome. We argue that this scenario depends on an agent's ability to rapidly improve its ability to predict its environment through self-modification. Using a Bayesian model of a reasoning agent, we show that there are important limitations to how an agent may improve its predictive ability through self-modification alone. We conclude that concern about this artificial intelligence outcome is misplaced and better directed at policy questions around data access and storage.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1706.02513v1,Responsible Autonomy,['Virginia Dignum'],2017-06-08 11:06:52+00:00,arxiv,...,d523c8400a71eef681405b67457092f5,html,markdownify,2017-06-08 11:06:52+00:00,"As intelligent systems are increasingly making decisions that directly affect society, perhaps the most important upcoming research direction in AI is to rethink the ethical implications of their actions. Means are needed to integrate moral, societal and legal values with technological developments in AI, both during the design process as well as part of the deliberation algorithms employed by these systems. In this paper, we describe leading ethics theories and propose alternative ways to ensure ethical behavior by artificial systems. Given that ethics are dependent on the socio-cultural context and are often only implicit in deliberation processes, methodologies are needed to elicit the values held by designers and stakeholders, and to make these explicit leading to better understanding and trust on artificial autonomous systems.",IJCAI2017 (International Joint Conference on Artificial Intelligence),,,cs.AI,['cs.AI']
https://arxiv.org/abs/1707.08759v1,Together We Know How to Achieve: An Epistemic Logic of Know-How (Extended Abstract),"['Pavel Naumov', 'Jia Tao']",2017-07-27 07:53:29+00:00,arxiv,...,117da1d012b01b044a448a1994ecf6a0,html,markdownify,2017-07-27 07:53:29+00:00,"The existence of a coalition strategy to achieve a goal does not necessarily mean that the coalition has enough information to know how to follow the strategy. Neither does it mean that the coalition knows that such a strategy exists. The paper studies an interplay between the distributed knowledge, coalition strategies, and coalition ""know-how"" strategies. The main technical result is a sound and complete trimodal logical system that describes the properties of this interplay.","In Proceedings TARK 2017, arXiv:1707.08250","EPTCS 251, 2017, pp. 441-453",10.4204/EPTCS.251.32,cs.AI,"['cs.AI', 'cs.LO']"
https://arxiv.org/abs/1803.02912v1,A Brandom-ian view of Reinforcement Learning towards strong-AI,['Atrisha Sarkar'],2018-03-07 23:26:49+00:00,arxiv,...,6cc9fcccf230ecd7b555cc5e1635e577,html,markdownify,2018-03-07 23:26:49+00:00,"The analytic philosophy of Robert Brandom, based on the ideas of pragmatism, paints a picture of sapience, through inferentialism. In this paper, we present a theory, that utilizes essential elements of Brandom's philosophy, towards the objective of achieving strong-AI. We do this by connecting the constitutive elements of reinforcement learning and the Game Of Giving and Asking For Reasons. Further, following Brandom's prescriptive thoughts, we restructure the popular reinforcement learning algorithm A3C, and show that RL algorithms can be tuned towards the objective of strong-AI.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1806.05234v2,Understanding the Meaning of Understanding,['Daniele Funaro'],2018-06-13 19:26:55+00:00,arxiv,...,29a0d3369b2675a2b0a5c3f978318f90,html,markdownify,2019-02-05 13:31:59+00:00,"Can we train a machine to detect if another machine has understood a concept? In principle, this is possible by conducting tests on the subject of that concept. However we want this procedure to be done by avoiding direct questions. In other words, we would like to isolate the absolute meaning of an abstract idea by putting it into a class of equivalence, hence without adopting straight definitions or showing how this idea ""works"" in practice. We discuss the metaphysical implications hidden in the above question, with the aim of providing a plausible reference framework.",9 pages,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1901.02918v3,Making AI meaningful again,"['Jobst Landgrebe', 'Barry Smith']",2019-01-09 20:16:44+00:00,arxiv,...,ab89eda813ca9928066be1f256238a08,html,markdownify,2019-03-23 06:17:08+00:00,"Artificial intelligence (AI) research enjoyed an initial period of enthusiasm in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of frustration when genuinely useful AI applications failed to be forthcoming. Today, we are experiencing once again a period of enthusiasm, fired above all by the successes of the technology of deep neural networks or deep machine learning. In this paper we draw attention to what we take to be serious problems underlying current views of artificial intelligence encouraged by these successes, especially in the domain of language processing. We then show an alternative approach to language-centric AI, in which we identify a role for philosophy.","23 pages, 1 Table",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1905.13178v1,Better Future through AI: Avoiding Pitfalls and Guiding AI Towards its Full Potential,"['Risto Miikkulainen', 'Bret Greenstein', 'Babak Hodjat', 'Jerry Smith']",2019-05-30 17:06:11+00:00,arxiv,...,326e7800aab29b98258948235b2ae955,html,markdownify,2019-05-30 17:06:11+00:00,"Artificial Intelligence (AI) technology is rapidly changing many areas of society. While there is tremendous potential in this transition, there are several pitfalls as well. Using the history of computing and the world-wide web as a guide, in this article we identify those pitfalls and actions that lead AI development to its full potential. If done right, AI will be instrumental in achieving the goals we set for economy, society, and the world in general.",,,,cs.AI,"['cs.AI', 'cs.CY']"
https://arxiv.org/abs/1910.04527v1,The Quest for Interpretable and Responsible Artificial Intelligence,['Vaishak Belle'],2019-10-10 12:56:14+00:00,arxiv,...,03964b8cfa91667a642eca1f02a89c63,html,markdownify,2019-10-10 12:56:14+00:00,"Artificial Intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in computational biology, finance, law and robotics. However, such a highly positive impact is coupled with significant challenges: How do we understand the decisions suggested by these systems in order that we can trust them? How can they be held accountable for those decisions?   In this short survey, we cover some of the motivations and trends in the area that attempt to address such questions.","This is a slightly edited version of an article to appear in The
  Biochemist, Portland Press, October 2019",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2010.07738v1,Do's and Don'ts for Human and Digital Worker Integration,"['Vinod Muthusamy', 'Merve Unuvar', 'Hagen VÃ¶lzer', 'Justin D. Weisz']",2020-10-15 13:30:23+00:00,arxiv,...,8ebbe45ce114867bf7c2c6271faca796,html,markdownify,2020-10-15 13:30:23+00:00,"Robotic process automation (RPA) and its next evolutionary stage, intelligent process automation, promise to drive improvements in efficiencies and process outcomes. However, how can business leaders evaluate how to integrate intelligent automation into business processes? What is an appropriate division of labor between humans and machines? How should combined human-AI teams be evaluated? For RPA, often the human labor cost and the robotic labor cost are directly compared to make an automation decision. In this position paper, we argue for a broader view that incorporates the potential for multiple levels of autonomy and human involvement, as well as a wider range of metrics beyond productivity when integrating digital workers into a business process",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2103.15294v1,"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest Value for us?",['Bin Liu'],2021-03-29 02:57:48+00:00,arxiv,...,90a7a36173935b22697fdc61faf6edb4,html,markdownify,2021-03-29 02:57:48+00:00,"AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, ""Starcraft"" and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance to develop into ""strong AI"".",7 pages,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2108.11004v1,"Reasoning about Counterfactuals and Explanations: Problems, Results and Directions",['Leopoldo Bertossi'],2021-08-25 01:04:49+00:00,arxiv,...,980d7e3dbd6bbea3a65925683a9f9f75,html,markdownify,2021-08-25 01:04:49+00:00,"There are some recent approaches and results about the use of answer-set programming for specifying counterfactual interventions on entities under classification, and reasoning about them. These approaches are flexible and modular in that they allow the seamless addition of domain knowledge. Reasoning is enabled by query answering from the answer-set program. The programs can be used to specify and compute responsibility-based numerical scores as attributive explanations for classification results.","To appear in informal proceedings of 2nd Workshop on Explainable
  Logic-Based Knowledge Representation (XLoKR 2021), co-located with KR 2021.
  arXiv admin note: substantial text overlap with arXiv:2107.10159",,,cs.AI,"['cs.AI', 'cs.LO']"
https://arxiv.org/abs/2006.07558v1,Ethical Considerations for AI Researchers,['Kyle Dent'],2020-06-13 04:31:42+00:00,arxiv,...,c30eb3855525a578df2221c504f89fec,html,markdownify,2020-06-13 04:31:42+00:00,"Use of artificial intelligence is growing and expanding into applications that impact people's lives. People trust their technology without really understanding it or its limitations. There is the potential for harm and we are already seeing examples of that in the world. AI researchers have an obligation to consider the impact of intelligent applications they work on. While the ethics of AI is not clear-cut, there are guidelines we can consider to minimize the harm we might introduce.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/cs/0001015v1,Multi-Agent Only Knowing,"['Joseph Y. Halpern', 'Gerhard Lakemeyer']",2000-01-19 22:13:38+00:00,arxiv,...,175d27237802231623744d8ac5043206,html,markdownify,2000-01-19 22:13:38+00:00,"Levesque introduced a notion of ``only knowing'', with the goal of capturing certain types of nonmonotonic reasoning. Levesque's logic dealt with only the case of a single agent. Recently, both Halpern and Lakemeyer independently attempted to extend Levesque's logic to the multi-agent case. Although there are a number of similarities in their approaches, there are some significant differences. In this paper, we reexamine the notion of only knowing, going back to first principles. In the process, we simplify Levesque's completeness proof, and point out some problems with the earlier definitions. This leads us to reconsider what the properties of only knowing ought to be. We provide an axiom system that captures our desiderata, and show that it has a semantics that corresponds to it. The axiom system has an added feature of interest: it includes a modal operator for satisfiability, and thus provides a complete axiomatization for satisfiability in the logic K45.","To appear, Journal of Logic and Computation",,,cs.AI,"['cs.AI', 'cs.LO', 'I.2.4, F.4.1']"
https://arxiv.org/abs/cs/0004001v1,A Theory of Universal Artificial Intelligence based on Algorithmic Complexity,['Marcus Hutter'],2000-04-03 06:16:16+00:00,arxiv,...,8ce41aa78902e36d00ea9c98b225a5a5,html,markdownify,2000-04-03 06:16:16+00:00,"Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI-tl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXI-tl is of the order tx2^l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.","62 pages, LaTeX",,,cs.AI,"['cs.AI', 'cs.IT', 'cs.LG', 'math.IT', 'I.2; F.1.3; E.4']"
https://arxiv.org/abs/cs/0701125v1,Universal Algorithmic Intelligence: A mathematical top->down approach,['Marcus Hutter'],2007-01-20 00:18:06+00:00,arxiv,...,fab8df235c1a0568f710d4b6b5bfb899,html,markdownify,2007-01-20 00:18:06+00:00,"Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2^l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.",70 pages,"In Artificial General Intelligence, Springer (2007) 227-290",,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1003.5305v2,Rational Value of Information Estimation for Measurement Selection,"['David Tolpin', 'Solomon Eyal Shimony']",2010-03-27 14:56:16+00:00,arxiv,...,e77e4d8fec9a8245a8d5b3c61307c21e,html,markdownify,2010-04-16 08:52:06+00:00,"Computing value of information (VOI) is a crucial task in various aspects of decision-making under uncertainty, such as in meta-reasoning for search; in selecting measurements to make, prior to choosing a course of action; and in managing the exploration vs. exploitation tradeoff. Since such applications typically require numerous VOI computations during a single run, it is essential that VOI be computed efficiently. We examine the issue of anytime estimation of VOI, as frequently it suffices to get a crude estimate of the VOI, thus saving considerable computational resources. As a case study, we examine VOI estimation in the measurement selection problem. Empirical evaluation of the proposed scheme in this domain shows that computational resources can indeed be significantly reduced, at little cost in expected rewards achieved in the overall decision problem.","7 pages, 2 figures, presented at URPDM2010; plots fixed",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1012.5506v1,Ontology-based Queries over Cancer Data,"['Alejandra Gonzalez-Beltran', 'Ben Tagger', 'Anthony Finkelstein']",2010-12-26 10:49:52+00:00,arxiv,...,fe1ab5a6cb7736572bb9ac6289dccc39,html,markdownify,2010-12-26 10:49:52+00:00,"The ever-increasing amount of data in biomedical research, and in cancer research in particular, needs to be managed to support efficient data access, exchange and integration. Existing software infrastructures, such caGrid, support access to distributed information annotated with a domain ontology. However, caGrid's current querying functionality depends on the structure of individual data resources without exploiting the semantic annotations. In this paper, we present the design and development of an ontology-based querying functionality that consists of: the generation of OWL2 ontologies from the underlying data resources metadata and a query rewriting and translation process based on reasoning, which converts a query at the domain ontology level into queries at the software infrastructure level. We present a detailed analysis of our approach as well as an extensive performance evaluation. While the implementation and evaluation was performed for the caGrid infrastructure, the approach could be applicable to other model and metadata-driven environments for data sharing.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010",,,cs.AI,"['cs.AI', 'cs.DB', 'cs.IR', 'J.3']"
https://arxiv.org/abs/1203.0699v1,Ambiguous Language and Differences in Beliefs,"['Joseph Y. Halpern', 'Willemien Kets']",2012-03-04 01:55:09+00:00,arxiv,...,aed7171463a75b56c532654f90072ef3,html,markdownify,2012-03-04 01:55:09+00:00,"Standard models of multi-agent modal logic do not capture the fact that information is often ambiguous, and may be interpreted in different ways by different agents. We propose a framework that can model this, and consider different semantics that capture different assumptions about the agents' beliefs regarding whether or not there is ambiguity. We consider the impact of ambiguity on a seminal result in economics: Aumann's result saying that agents with a common prior cannot agree to disagree. This result is known not to hold if agents do not have a common prior; we show that it also does not hold in the presence of ambiguity. We then consider the tradeoff between assuming a common interpretation (i.e., no ambiguity) and a common prior (i.e., shared initial beliefs).",,,,cs.AI,"['cs.AI', 'cs.GT']"
https://arxiv.org/abs/1304.5159v1,Interactive POMDP Lite: Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Agents,"['Trong Nghia Hoang', 'Kian Hsiang Low']",2013-04-18 15:11:25+00:00,arxiv,...,5d05732e0c0cfe327008027a13e4b8db,html,markdownify,2013-04-18 15:11:25+00:00,"A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to interact and perform effectively among boundedly rational, self-interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions of the other agents' behavior, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance losses incurred by the resulting planning policies are linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our policies can achieve better and more robust performance than the state-of-the-art algorithms.","23rd International Joint Conference on Artificial Intelligence (IJCAI
  2013), Extended version with proofs, 24 pages",,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/1312.0144v3,Knowing Whether,"['Jie Fan', 'Yanjing Wang', 'Hans van Ditmarsch']",2013-11-30 19:18:49+00:00,arxiv,...,8d675e3cc4719f2fd553882e9009e0b9,html,markdownify,2013-12-12 10:02:26+00:00,"Knowing whether a proposition is true means knowing that it is true or knowing that it is false. In this paper, we study logics with a modal operator Kw for knowing whether but without a modal operator K for knowing that. This logic is not a normal modal logic, because we do not have Kw (phi -> psi) -> (Kw phi -> Kw psi). Knowing whether logic cannot define many common frame properties, and its expressive power less than that of basic modal logic over classes of models without reflexivity. These features make axiomatizing knowing whether logics non-trivial. We axiomatize knowing whether logic over various frame classes. We also present an extension of knowing whether logic with public announcement operators and we give corresponding reduction axioms for that. We compare our work in detail to two recent similar proposals.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1504.03592v1,Towards Verifiably Ethical Robot Behaviour,"['Louise A. Dennis', 'Michael Fisher', 'Alan F. T. Winfield']",2015-04-14 15:49:40+00:00,arxiv,...,751d960c397d5b4a4c9c9818bcdeaa4e,html,markdownify,2015-04-14 15:49:40+00:00,"Ensuring that autonomous systems work ethically is both complex and difficult. However, the idea of having an additional `governor' that assesses options the system has, and prunes them to select the most ethical choices is well understood. Recent work has produced such a governor consisting of a `consequence engine' that assesses the likely future outcomes of actions then applies a Safety/Ethical logic to select actions. Although this is appealing, it is impossible to be certain that the most ethical options are actually taken. In this paper we extend and apply a well-known agent verification approach to our consequence engine, allowing us to verify the correctness of its ethical decision-making.","Presented at the 1st International Workshop on AI and Ethics, Sunday
  25th January 2015, Hill Country A, Hyatt Regency Austin. Will appear in the
  workshop proceedings published by AAAI",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1505.02449v1,Automating change of representation for proofs in discrete mathematics,"['Daniel Raggi', 'Alan Bundy', 'Gudmund Grov', 'Alison Pease']",2015-05-10 22:14:55+00:00,arxiv,...,28af2c63ccb0fdb774aaa33ba113c48d,html,markdownify,2015-05-10 22:14:55+00:00,"Representation determines how we can reason about a specific problem. Sometimes one representation helps us find a proof more easily than others. Most current automated reasoning tools focus on reasoning within one representation. There is, therefore, a need for the development of better tools to mechanise and automate formal and logically sound changes of representation.   In this paper we look at examples of representational transformations in discrete mathematics, and show how we have used Isabelle's Transfer tool to automate the use of these transformations in proofs. We give a brief overview of a general theory of transformations that we consider appropriate for thinking about the matter, and we explain how it relates to the Transfer package. We show our progress towards developing a general tactic that incorporates the automatic search for representation within the proving process.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1508.03032v1,OOASP: Connecting Object-oriented and Logic Programming,"['Andreas Falkner', 'Anna Ryabokon', 'Gottfried Schenner', 'Kostyantyn Shchekotykhin']",2015-08-12 18:59:41+00:00,arxiv,...,efc431dd4272b1dd7e47ce77f518c852,html,markdownify,2015-08-12 18:59:41+00:00,"Most of contemporary software systems are implemented using an object-oriented approach. Modeling phases -- during which software engineers analyze requirements to the future system using some modeling language -- are an important part of the development process, since modeling errors are often hard to recognize and correct.   In this paper we present a framework which allows the integration of Answer Set Programming into the object-oriented software development process. OOASP supports reasoning about object-oriented software models and their instantiations. Preliminary results of the OOASP application in CSL Studio, which is a Siemens internal modeling environment for product configurators, show that it can be used as a lightweight approach to verify, create and transform instantiations of object models at runtime and to support the software development process during design and testing.","13 pages, 4 figures, accepted for publication at LPNMR 2015",,,cs.AI,"['cs.AI', 'cs.SE']"
https://arxiv.org/abs/1602.06462v1,The Singularity May Never Be Near,['Toby Walsh'],2016-02-20 21:09:07+00:00,arxiv,...,7273683e99f7dc2ca9483cb294266cad,html,markdownify,2016-02-20 21:09:07+00:00,"There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent species starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.",Under review,,,cs.AI,"['cs.AI', 'I.2.0']"
https://arxiv.org/abs/1607.00061v1,Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain By Its End-Users,"['I. Dan Melamed', 'Nobal B. Niraula']",2016-06-30 22:04:26+00:00,arxiv,...,e95db113a7057752eadd3005fc693369,html,markdownify,2016-06-30 22:04:26+00:00,"The challenge stated in the title can be divided into two main problems. The first problem is to reliably mimic the way that users interact with user interfaces. The second problem is to build an instructible agent, i.e. one that can be taught to execute tasks expressed as previously unseen natural language commands. This paper proposes a solution to the second problem, a system we call Helpa. End-users can teach Helpa arbitrary new tasks whose level of complexity is similar to the tasks available from today's most popular virtual assistants. Teaching Helpa does not involve any programming. Instead, users teach Helpa by providing just one example of a command paired with a demonstration of how to execute that command. Helpa does not rely on any pre-existing domain-specific knowledge. It is therefore completely domain-independent. Our usability study showed that end-users can teach Helpa many new tasks in less than a minute each, often much less.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1607.00656v1,A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching,"['Gavin Rens', 'Deshendran Moodley']",2016-07-03 17:11:52+00:00,arxiv,...,0f37f42b4790929f63392761d94176ea,html,markdownify,2016-07-03 17:11:52+00:00,"This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.","26 pages, 3 figures, unpublished version",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1609.03765v1,Graph Aggregation,"['Ulle Endriss', 'Umberto Grandi']",2016-09-13 11:08:23+00:00,arxiv,...,31d098117b6fd7a5eeeb5170341d8c50,html,markdownify,2016-09-13 11:08:23+00:00,"Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow's seminal result for the aggregation of preference orders to a large collection of different types of graphs.",,"Artificial Intelligence, Volume 245, pages 86-114, 2017",10.1016/j.artint.2017.01.001,cs.AI,['cs.AI']
https://arxiv.org/abs/1609.08524v2,UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS,"['Tathagata Chakraborti', 'Kartik Talamadupula', 'Kshitij P. Fadnis', 'Murray Campbell', 'Subbarao Kambhampati']",2016-09-27 16:42:30+00:00,arxiv,...,a3a1f4708b41a4bdfa929486b58e2f53,html,markdownify,2017-08-12 21:31:02+00:00,"In this paper, we present UbuntuWorld 1.0 LTS - a platform for developing automated technical support agents in the Ubuntu operating system. Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu environment for a learning-based agent and demonstrate the usefulness of adopting reinforcement learning (RL) techniques for basic problem solving and troubleshooting in this environment. We provide a plug-and-play interface to the simulator as a python package where different types of agents can be plugged in and evaluated, and provide pathways for integrating data from online support forums like AskUbuntu into an automated agent's learning process. Finally, we show that the use of this data significantly improves the agent's learning efficiency. We believe that this platform can be adopted as a real-world test bed for research on automated technical support.",Appeared (under the same title) in AAAI/IAAI 2017,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1701.08317v5,Plan Explanations as Model Reconciliation: Moving Beyond Explanation as Soliloquy,"['Tathagata Chakraborti', 'Sarath Sreedharan', 'Yu Zhang', 'Subbarao Kambhampati']",2017-01-28 19:22:52+00:00,arxiv,...,53c218ab2f21aca36dced7da520c36d6,html,markdownify,2017-05-30 21:31:24+00:00,"When AI systems interact with humans in the loop, they are often called on to provide explanations for their plans and behavior. Past work on plan explanations primarily involved the AI system explaining the correctness of its plan and the rationale for its decision in terms of its own model. Such soliloquy is wholly inadequate in most realistic scenarios where the humans have domain and task models that differ significantly from that used by the AI system. We posit that the explanations are best studied in light of these differing models. In particular, we show how explanation can be seen as a ""model reconciliation problem"" (MRP), where the AI system in effect suggests changes to the human's model, so as to make its plan be optimal with respect to that changed human model. We will study the properties of such explanations, present algorithms for automatically computing them, and evaluate the performance of the algorithms.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1702.08222v1,Synergistic Team Composition,"['Ewa Andrejczuk', 'Juan A. Rodriguez-Aguilar', 'Carme Roig', 'Carles Sierra']",2017-02-27 10:36:36+00:00,arxiv,...,6d8127dffec85efa7b14c8d6d9cfda99,html,markdownify,2017-02-27 10:36:36+00:00,"Effective teams are crucial for organisations, especially in environments that require teams to be constantly created and dismantled, such as software development, scientific experiments, crowd-sourcing, or the classroom. Key factors influencing team performance are competences and personality of team members. Hence, we present a computational model to compose proficient and congenial teams based on individuals' personalities and their competences to perform tasks of different nature. With this purpose, we extend Wilde's post-Jungian method for team composition, which solely employs individuals' personalities. The aim of this study is to create a model to partition agents into teams that are balanced in competences, personality and gender. Finally, we present some preliminary empirical results that we obtained when analysing student performance. Results show the benefits of a more informed team composition that exploits individuals' competences besides information about their personalities.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1705.09349v2,Together We Know How to Achieve: An Epistemic Logic of Know-How,"['Pavel Naumov', 'Jia Tao']",2017-05-25 20:22:16+00:00,arxiv,...,f0afc0c6ca9c7ce8a5a694066029047c,html,markdownify,2017-06-15 18:59:15+00:00,"The existence of a coalition strategy to achieve a goal does not necessarily mean that the coalition has enough information to know how to follow the strategy. Neither does it mean that the coalition knows that such a strategy exists. The article studies an interplay between the distributed knowledge, coalition strategies, and coalition ""know-how"" strategies. The main technical result is a sound and complete trimodal logical system that describes the properties of this interplay.","An extended abstract of this paper will appear in Proceedings of 16th
  conference on Theoretical Aspects of Rationality and Knowledge (TARK-17),
  Liverpool, United Kingdom, July 24-26, 2017",,,cs.AI,"['cs.AI', 'cs.GT', 'cs.LO', 'cs.MA']"
https://arxiv.org/abs/1708.00376v1,Using Program Induction to Interpret Transition System Dynamics,"['Svetlin Penkov', 'Subramanian Ramamoorthy']",2017-07-26 12:49:04+00:00,arxiv,...,d75a93d1cf6ce257b54b73fa368926bc,html,markdownify,2017-07-26 12:49:04+00:00,"Explaining and reasoning about processes which underlie observed black-box phenomena enables the discovery of causal mechanisms, derivation of suitable abstract representations and the formulation of more robust predictions. We propose to learn high level functional programs in order to represent abstract models which capture the invariant structure in the observed data. We introduce the $\pi$-machine (program-induction machine) -- an architecture able to induce interpretable LISP-like programs from observed data traces. We propose an optimisation procedure for program learning based on backpropagation, gradient descent and A* search. We apply the proposed method to two problems: system identification of dynamical systems and explaining the behaviour of a DQN agent. Our results show that the $\pi$-machine can efficiently induce interpretable programs from individual data traces.","Presented at 2017 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2017), Sydney, NSW, Australia. arXiv admin note: substantial
  text overlap with arXiv:1705.08320",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1708.05448v1,On Ensuring that Intelligent Machines Are Well-Behaved,"['Philip S. Thomas', 'Bruno Castro da Silva', 'Andrew G. Barto', 'Emma Brunskill']",2017-08-17 21:53:47+00:00,arxiv,...,02d91250d64645c4cccf15264c0e51d6,html,markdownify,2017-08-17 21:53:47+00:00,"Machine learning algorithms are everywhere, ranging from simple data analysis and pattern recognition tools used across the sciences to complex systems that achieve super-human performance on various tasks. Ensuring that they are well-behaved---that they do not, for example, cause harm to humans or act in a racist or sexist way---is therefore not a hypothetical problem to be dealt with in the future, but a pressing one that we address here. We propose a new framework for designing machine learning algorithms that simplifies the problem of specifying and regulating undesirable behaviors. To show the viability of this new framework, we use it to create new machine learning algorithms that preclude the sexist and harmful behaviors exhibited by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1709.01547v2,Knowledge Transfer Between Artificial Intelligence Systems,"['Ivan Y. Tyukin', 'Alexander N. Gorban', 'Konstantin Sofeikov', 'Ilya Romanenko']",2017-09-05 18:38:07+00:00,arxiv,...,67aa402d99cc60fcf30fab5767d99d71,html,markdownify,2017-11-14 08:21:13+00:00,"We consider the fundamental question: how a legacy ""student"" Artificial Intelligent (AI) system could learn from a legacy ""teacher"" AI system or a human expert without complete re-training and, most importantly, without requiring significant computational resources. Here ""learning"" is understood as an ability of one system to mimic responses of the other and vice-versa. We call such learning an Artificial Intelligence knowledge transfer. We show that if internal variables of the ""student"" Artificial Intelligent system have the structure of an $n$-dimensional topological vector space and $n$ is sufficiently high then, with probability close to one, the required knowledge transfer can be implemented by simple cascades of linear functionals. In particular, for $n$ sufficiently large, with probability close to one, the ""student"" system can successfully and non-iteratively learn $k\ll n$ new examples from the ""teacher"" (or correct the same number of mistakes) at the cost of two additional inner products. The concept is illustrated with an example of knowledge transfer from a pre-trained convolutional neural network to a simple linear classifier with HOG features.",,Front Neurorobot. 2018; 12: 49,10.3389/fnbot.2018.00049,cs.AI,"['cs.AI', '68T05, 68T30']"
https://arxiv.org/abs/1709.08071v2,Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems,"['Stefano V. Albrecht', 'Peter Stone']",2017-09-23 16:10:52+00:00,arxiv,...,4e6dc1fdfbbd261bfa765d9a695ad169,html,markdownify,2018-02-09 15:54:18+00:00,"Much research in artificial intelligence is concerned with the development of autonomous agents that can interact effectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the different sub-communities within which they were developed and reflecting the different practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research.","Final manuscript (46 pages), published in Artificial Intelligence
  Journal. The arXiv version also contains a table of contents after the
  abstract, but is otherwise identical to the AIJ version. Keywords: autonomous
  agents, multiagent systems, modelling other agents, opponent modelling",,10.1016/j.artint.2018.01.002,cs.AI,"['cs.AI', 'cs.MA', 'I.2.11']"
https://arxiv.org/abs/1710.08191v1,Human-in-the-loop Artificial Intelligence,['Fabio Massimo Zanzotto'],2017-10-23 10:37:50+00:00,arxiv,...,3adb5f8ff9bd05df46943fd54c2178f6,html,markdownify,2017-10-23 10:37:50+00:00,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves.   In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.",,"Journal of Artificial Intelligence Research, 2019",10.1613/jair.1.11345,cs.AI,"['cs.AI', 'I.2; I.2.6']"
https://arxiv.org/abs/1711.06431v2,Using KL-divergence to focus Deep Visual Explanation,"['Housam Khalifa Bashier Babiker', 'Randy Goebel']",2017-11-17 06:53:17+00:00,arxiv,...,0254548952a2728958c5ca403f67d5ac,html,markdownify,2018-01-25 06:18:18+00:00,"We present a method for explaining the image classification predictions of deep convolution neural networks, by highlighting the pixels in the image which influence the final class prediction. Our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction, and here we use Kullback-Leibler divergence to provide this focus. Overall, our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks. In this brief paper, our experiments evaluate the performance of two popular networks in this context of interpretability.",Presented at NIPS 2017 Symposium on Interpretable Machine Learning,,,cs.AI,"['cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1711.08068v1,Deterministic Policy Optimization by Combining Pathwise and Score Function Estimators for Discrete Action Spaces,"['Daniel Levy', 'Stefano Ermon']",2017-11-21 22:05:18+00:00,arxiv,...,4e42f8d9dce3cc2ed306bdf31da9e6fa,html,markdownify,2017-11-21 22:05:18+00:00,"Policy optimization methods have shown great promise in solving complex reinforcement and imitation learning tasks. While model-free methods are broadly applicable, they often require many samples to optimize complex policies. Model-based methods greatly improve sample-efficiency but at the cost of poor generalization, requiring a carefully handcrafted model of the system dynamics for each task. Recently, hybrid methods have been successful in trading off applicability for improved sample-complexity. However, these have been limited to continuous action spaces. In this work, we present a new hybrid method based on an approximation of the dynamics as an expectation over the next state under the current policy. This relaxation allows us to derive a novel hybrid policy gradient estimator, combining score function and pathwise derivative estimators, that is applicable to discrete action spaces. We show significant gains in sample complexity, ranging between $1.7$ and $25\times$, when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and Hand Mass. Our method is applicable to both discrete and continuous action spaces, when competing pathwise methods are limited to the latter.",In AAAI 2018 proceedings,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1802.06306v8,Learning Data-Driven Objectives to Optimize Interactive Systems,"['Ziming Li', 'Julia Kiseleva', 'Alekh Agarwal', 'Maarten de Rijke']",2018-02-17 23:04:15+00:00,arxiv,...,855a8dac081745c1f08e0ad1f6d15299,html,markdownify,2019-12-13 22:11:21+00:00,"Effective optimization is essential for interactive systems to provide a satisfactory user experience. However, it is often challenging to find an objective to optimize for. Generally, such objectives are manually crafted and rarely capture complex user needs in an accurate manner. We propose an approach that infers the objective directly from observed user interactions. These inferences can be made regardless of prior knowledge and across different types of user behavior. We introduce interactive system optimization, a novel algorithm that uses these inferred objectives for optimization. Our main contribution is a new general principled approach to optimizing interactive systems using data-driven objectives. We demonstrate the high effectiveness of interactive system optimization over several simulations.",10 pages,,,cs.AI,"['cs.AI', 'cs.HC', 'cs.IR', 'cs.LG']"
https://arxiv.org/abs/1802.09159v1,Antifragility for Intelligent Autonomous Systems,"['Anusha Mujumdar', 'Swarup Kumar Mohalik', 'Ramamurthy Badrinath']",2018-02-26 04:58:55+00:00,arxiv,...,d24dcc46ac65b7bbb24263978cbaced1,html,markdownify,2018-02-26 04:58:55+00:00,"Antifragile systems grow measurably better in the presence of hazards. This is in contrast to fragile systems which break down in the presence of hazards, robust systems that tolerate hazards up to a certain degree, and resilient systems that -- like self-healing systems -- revert to their earlier expected behavior after a period of convalescence. The notion of antifragility was introduced by Taleb for economics systems, but its applicability has been illustrated in biological and engineering domains as well. In this paper, we propose an architecture that imparts antifragility to intelligent autonomous systems, specifically those that are goal-driven and based on AI-planning. We argue that this architecture allows the system to self-improve by uncovering new capabilities obtained either through the hazards themselves (opportunistic) or through deliberation (strategic). An AI planning-based case study of an autonomous wheeled robot is presented. We show that with the proposed architecture, the robot develops antifragile behaviour with respect to an oil spill hazard.",Under Review. Consists of seven pages and four figures,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1803.03407v2,Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions,"['Alexander Boer', 'Giovanni Sileno']",2018-03-09 07:59:21+00:00,arxiv,...,183c9caa2b21ec267a9385538fce1110,html,markdownify,2021-06-15 19:11:23+00:00,"Artificial Intelligence (AI) started out with an ambition to reproduce the human mind, but, as the sheer scale of that ambition became manifest, it quickly retreated into either studying specialized intelligent behaviours, or proposing over-arching architectural concepts for interfacing specialized intelligent behaviour components, conceived of as agents in a kind of organization. This agent-based modeling paradigm, in turn, proves to have interesting applications in understanding, simulating, and predicting the behaviour of social and legal structures on an aggregate level. For these reasons, this chapter examines a number of relevant cross-cutting concerns, conceptualizations, modeling problems and design challenges in large-scale distributed Artificial Intelligence, as well as in institutional systems, and identifies potential grounds for novel advances.","invited chapter, before proofread",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1803.04263v3,The Challenge of Crafting Intelligible Intelligence,"['Daniel S. Weld', 'Gagan Bansal']",2018-03-09 06:38:55+00:00,arxiv,...,c637a4f9df0a00fb9a58e776d6f8b82b,html,markdownify,2018-10-15 06:10:30+00:00,"Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.",arXiv admin note: text overlap with arXiv:1603.08507 by other authors,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1804.10817v1,A Logic of Agent Organizations,"['Virginia Dignum', 'Frank Dignum']",2018-04-28 15:09:10+00:00,arxiv,...,3126fc3228b83ccc7720d68816964d1e,html,markdownify,2018-04-28 15:09:10+00:00,"Organization concepts and models are increasingly being adopted for the design and specification of multi-agent systems. Agent organizations can be seen as mechanisms of social order, created to achieve global (or organizational) objectives by more or less autonomous agents. In order to develop a theory on the relation between organizational structures, organizational objectives and the actions of agents fulfilling roles in the organization a theoretical framework is needed to describe organizational structures and actions of (groups of) agents. Current logical formalisms focus on specific aspects of organizations (e.g. power, delegation, agent actions, or normative issues) but a framework that integrates and relates different aspects is missing. Given the amount of aspects involved and the subsequent complexity of a formalism encompassing them all, it is difficult to realize. In this paper, a first step is taken to solve this problem. We present a generic formal model that enables to specify and relate the main concepts of an organization (including, activity, structure, environment and others) so that organizations can be analyzed at a high level of abstraction. However, for some aspects we use a simplified model in order to avoid the complexity of combining many different types of (modal) operators.",,"Logic Journal of the IGPL, vol. 20, no. 1, pp. 283-316, Feb. 2012",10.1093/jigpal/jzr041,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/1806.08874v1,The Foundations of Deep Learning with a Path Towards General Intelligence,['Eray Ãzkural'],2018-06-22 22:52:12+00:00,arxiv,...,e94a06a5e7cccef797d6a531392c7623,html,markdownify,2018-06-22 22:52:12+00:00,"Like any field of empirical science, AI may be approached axiomatically. We formulate requirements for a general-purpose, human-level AI system in terms of postulates. We review the methodology of deep learning, examining the explicit and tacit assumptions in deep learning research. Deep Learning methodology seeks to overcome limitations in traditional machine learning research as it combines facets of model richness, generality, and practical applicability. The methodology so far has produced outstanding results due to a productive synergy of function approximation, under plausible assumptions of irreducibility and the efficiency of back-propagation family of algorithms. We examine these winning traits of deep learning, and also observe the various known failure modes of deep learning. We conclude by giving recommendations on how to extend deep learning methodology to cover the postulates of general-purpose AI including modularity, and cognitive architecture. We also relate deep learning to advances in theoretical neuroscience research.",Submitted to AGI 2018,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1810.07311v3,Finding Options that Minimize Planning Time,"['Yuu Jinnai', 'David Abel', 'D Ellis Hershkowitz', 'Michael Littman', 'George Konidaris']",2018-10-16 23:24:18+00:00,arxiv,...,47ba0957eb7cbc7f6e58822a3e57336c,html,markdownify,2019-03-16 20:08:18+00:00,"We formalize the problem of selecting the optimal set of options for planning as that of computing the smallest set of options so that planning converges in less than a given maximum of value-iteration passes. We first show that the problem is NP-hard, even if the task is constrained to be deterministic---the first such complexity result for option discovery. We then present the first polynomial-time boundedly suboptimal approximation algorithm for this setting, and empirically evaluate it against both the optimal options and a representative collection of heuristic approaches in simple grid-based domains including the classic four-rooms problem.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1811.06606v2,Economics of Human-AI Ecosystem: Value Bias and Lost Utility in Multi-Dimensional Gaps,['Daniel Muller'],2018-11-15 21:59:41+00:00,arxiv,...,0c5d33ad1dd6e7cb72bd4f53fba6fe29,html,markdownify,2018-11-19 01:48:49+00:00,"In recent years, artificial intelligence (AI) decision-making and autonomous systems became an integrated part of the economy, industry, and society. The evolving economy of the human-AI ecosystem raising concerns regarding the risks and values inherited in AI systems. This paper investigates the dynamics of creation and exchange of values and points out gaps in perception of cost-value, knowledge, space and time dimensions. It shows aspects of value bias in human perception of achievements and costs that encoded in AI systems. It also proposes rethinking hard goals definitions and cost-optimal problem-solving principles in the lens of effectiveness and efficiency in the development of trusted machines. The paper suggests a value-driven with cost awareness strategy and principles for problem-solving and planning of effective research progress to address real-world problems that involve diverse forms of achievements, investments, and survival scenarios.","8 pages, typos corrected, examples added to Table 1",,,cs.AI,"['cs.AI', 'econ.GN', 'q-fin.EC']"
https://arxiv.org/abs/1812.03868v2,Toward the Engineering of Virtuous Machines,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord', 'Rikhiya Ghosh']",2018-12-07 16:30:20+00:00,arxiv,...,398a6dc61842cf736354a167108174b5,html,markdownify,2018-12-30 05:37:19+00:00,"While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.","To appear in the proceedings of AAAI/ACM Conference on AI, Ethics,
  and Society (AIES) 2019 (http://www.aies-conference.com/accepted-papers/).
  This subsumes and completes the earlier partial formalization described in
  arXiv:1805.07797",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1903.09328v1,Improving Safety in Reinforcement Learning Using Model-Based Architectures and Human Intervention,"['Bharat Prakash', 'Mohit Khatwani', 'Nicholas Waytowich', 'Tinoosh Mohsenin']",2019-03-22 02:48:21+00:00,arxiv,...,d7016476a193f03a467abd6a348f8a3e,html,markdownify,2019-03-22 02:48:21+00:00,"Recent progress in AI and Reinforcement learning has shown great success in solving complex problems with high dimensional state spaces. However, most of these successes have been primarily in simulated environments where failure is of little or no consequence. Most real-world applications, however, require training solutions that are safe to operate as catastrophic failures are inadmissible especially when there is human interaction involved. Currently, Safe RL systems use human oversight during training and exploration in order to make sure the RL agent does not go into a catastrophic state. These methods require a large amount of human labor and it is very difficult to scale up. We present a hybrid method for reducing the human intervention time by combining model-based approaches and training a supervised learner to improve sample efficiency while also ensuring safety. We evaluate these methods on various grid-world environments using both standard and visual representations and show that our approach achieves better performance in terms of sample efficiency, number of catastrophic states reached as well as overall task performance compared to traditional model-free approaches",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1905.00547v1,The relationship between Biological and Artificial Intelligence,['George Cevora'],2019-05-01 13:41:35+00:00,arxiv,...,f6c00a03aff38afeab364c60bbaa9fd4,html,markdownify,2019-05-01 13:41:35+00:00,"Intelligence can be defined as a predominantly human ability to accomplish tasks that are generally hard for computers and animals. Artificial Intelligence [AI] is a field attempting to accomplish such tasks with computers. AI is becoming increasingly widespread, as are claims of its relationship with Biological Intelligence. Often these claims are made to imply higher chances of a given technology succeeding, working on the assumption that AI systems which mimic the mechanisms of Biological Intelligence should be more successful.   In this article I will discuss the similarities and differences between AI and the extent of our knowledge about the mechanisms of intelligence in biology, especially within humans. I will also explore the validity of the assumption that biomimicry in AI systems aids their advancement, and I will argue that existing similarity to biological systems in the way Artificial Neural Networks [ANNs] tackle tasks is due to design decisions, rather than inherent similarity of underlying mechanisms. This article is aimed at people who understand the basics of AI (especially ANNs), and would like to be better able to evaluate the often wild claims about the value of biomimicry in AI.",,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.NE']"
https://arxiv.org/abs/1905.09730v1,On modelling the emergence of logical thinking,"['Cristian Ivan', 'Bipin Indurkhya']",2019-05-23 15:46:13+00:00,arxiv,...,c71ebc9b6157eebede3b7938cfe5c170,html,markdownify,2019-05-23 15:46:13+00:00,"Recent progress in machine learning techniques have revived interest in building artificial general intelligence using these particular tools. There has been a tremendous success in applying them for narrow intellectual tasks such as pattern recognition, natural language processing and playing Go. The latter application vastly outperforms the strongest human player in recent years. However, these tasks are formalized by people in such ways that it has become ""easy"" for automated recipes to find better solutions than humans do. In the sense of John Searle's Chinese Room Argument, the computer playing Go does not actually understand anything from the game. Thinking like a human mind requires to go beyond the curve fitting paradigm of current systems. There is a fundamental limit to what they can achieve currently as only very specific problem formalization can increase their performances in particular tasks. In this paper, we argue than one of the most important aspects of the human mind is its capacity for logical thinking, which gives rise to many intellectual expressions that differentiate us from animal brains. We propose to model the emergence of logical thinking based on Piaget's theory of cognitive development.",,,,cs.AI,"['cs.AI', 'cs.GL']"
https://arxiv.org/abs/1907.05447v1,Grounding Value Alignment with Ethical Principles,"['Tae Wan Kim', 'Thomas Donaldson', 'John Hooker']",2019-07-11 18:55:47+00:00,arxiv,...,3ff9ee719bd95e65c6c14b99b3660e0d,html,markdownify,2019-07-11 18:55:47+00:00,"An important step in the development of value alignment (VA) systems in AI is understanding how values can interrelate with facts. Designers of future VA systems will need to utilize a hybrid approach in which ethical reasoning and empirical observation interrelate successfully in machine behavior. In this article we identify two problems about this interrelation that have been overlooked by AI discussants and designers. The first problem is that many AI designers commit inadvertently a version of what has been called by moral philosophers the ""naturalistic fallacy,"" that is, they attempt to derive an ""ought"" from an ""is."" We illustrate when and why this occurs. The second problem is that AI designers adopt training routines that fail fully to simulate human ethical reasoning in the integration of ethical principles and facts. Using concepts of quantified modal logic, we proceed to offer an approach that promises to simulate ethical reasoning in humans by connecting ethical principles on the one hand and propositions about states of affairs on the other.",,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/1910.14436v1,How can AI Automate End-to-End Data Science?,"['Charu Aggarwal', 'Djallel Bouneffouf', 'Horst Samulowitz', 'Beat Buesser', 'Thanh Hoang', 'Udayan Khurana', 'Sijia Liu', 'Tejaswini Pedapati', 'Parikshit Ram', 'Ambrish Rawat', 'Martin Wistuba', 'Alexander Gray']",2019-10-22 12:54:48+00:00,arxiv,...,6f42bc8ae4904cea5fa217eb40d06b58,html,markdownify,2019-10-22 12:54:48+00:00,"Data science is labor-intensive and human experts are scarce but heavily involved in every aspect of it. This makes data science time consuming and restricted to experts with the resulting quality heavily dependent on their experience and skills. To make data science more accessible and scalable, we need its democratization. Automated Data Science (AutoDS) is aimed towards that goal and is emerging as an important research and business topic. We introduce and define the AutoDS challenge, followed by a proposal of a general AutoDS framework that covers existing approaches but also provides guidance for the development of new methods. We categorize and review the existing literature from multiple aspects of the problem setup and employed techniques. Then we provide several views on how AI could succeed in automating end-to-end AutoDS. We hope this survey can serve as insightful guideline for the AutoDS field and provide inspiration for future research.",,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1912.09571v1,Measuring the intelligence of an idealized mechanical knowing agent,['Samuel Allen Alexander'],2019-12-03 02:03:00+00:00,arxiv,...,88460777e5bede8698e49f80c7e03298,html,markdownify,2019-12-03 02:03:00+00:00,"We define a notion of the intelligence level of an idealized mechanical knowing agent. This is motivated by efforts within artificial intelligence research to define real-number intelligence levels of complicated intelligent systems. Our agents are more idealized, which allows us to define a much simpler measure of intelligence level for them. In short, we define the intelligence level of a mechanical knowing agent to be the supremum of the computable ordinals that have codes the agent knows to be codes of computable ordinals. We prove that if one agent knows certain things about another agent, then the former necessarily has a higher intelligence level than the latter. This allows our intelligence notion to serve as a stepping stone to obtain results which, by themselves, are not stated in terms of our intelligence notion (results of potential interest even to readers totally skeptical that our notion correctly captures intelligence). As an application, we argue that these results comprise evidence against the possibility of intelligence explosion (that is, the notion that sufficiently intelligent machines will eventually be capable of designing even more intelligent machines, which can then design even more intelligent machines, and so on).","17 pages, CIFMA 2019",,,cs.AI,"['cs.AI', 'cs.LO', 'math.LO', '03E10, 03B42']"
https://arxiv.org/abs/1912.10305v2,Questions to Guide the Future of Artificial Intelligence Research,['Jordan Ott'],2019-12-21 17:48:31+00:00,arxiv,...,dd9f8508e7c45cedd1c82d36f5eb44ae,html,markdownify,2020-03-10 15:31:34+00:00,"The field of machine learning has focused, primarily, on discretized sub-problems (i.e. vision, speech, natural language) of intelligence. While neuroscience tends to be observation heavy, providing few guiding theories. It is unlikely that artificial intelligence will emerge through only one of these disciplines. Instead, it is likely to be some amalgamation of their algorithmic and observational findings. As a result, there are a number of problems that should be addressed in order to select the beneficial aspects of both fields. In this article, we propose leading questions to guide the future of artificial intelligence research. There are clear computational principles on which the brain operates. The problem is finding these computational needles in a haystack of biological complexity. Biology has clear constraints but by not using it as a guide we are constraining ourselves.",,,,cs.AI,"['cs.AI', 'cs.NE']"
https://arxiv.org/abs/2001.08823v2,What's a Good Prediction? Challenges in evaluating an agent's knowledge,"['Alex Kearney', 'Anna Koop', 'Patrick M. Pilarski']",2020-01-23 21:44:43+00:00,arxiv,...,8d02ffaa9c1972a63b3a8a88bb6a00e0,html,markdownify,2021-04-13 23:44:37+00:00,"Constructing general knowledge by learning task-independent models of the world can help agents solve challenging problems. However, both constructing and evaluating such models remains an open challenge. The most common approaches to evaluating models is to assess their accuracy with respect to observable values. However, the prevailing reliance on estimator accuracy as a proxy for the usefulness of the knowledge has the potential to lead us astray. We demonstrate the conflict between accuracy and usefulness through a series of illustrative examples including both a thought experiment and empirical example in MineCraft, using the General Value Function framework (GVF). Having identified challenges in assessing an agent's knowledge, we propose an alternate evaluation approach that arises continually in the online continual learning setting we recommend evaluation by examining internal learning processes, specifically the relevance of a GVF's features to the prediction task at hand. This paper contributes a first look into evaluation of predictions through their use, an integral component of predictive knowledge which is as of yet unexplored.",In preparation for submission to Adaptive Behaviour,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/2002.01080v4,Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations,"['Sarath Sreedharan', 'Utkarsh Soni', 'Mudit Verma', 'Siddharth Srivastava', 'Subbarao Kambhampati']",2020-02-04 01:37:56+00:00,arxiv,...,c1ba9ab22ae143bf8668dffc0890f3e5,html,markdownify,2022-03-19 22:47:40+00:00,"As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the vocabulary mismatch between the user and the AI system. This paper introduces methods for providing contrastive explanations in terms of user-specified concepts for sequential decision-making settings where the system's model of the task may be best represented as an inscrutable model. We do this by building partial symbolic models of a local approximation of the task that can be leveraged to answer the user queries. We test these methods on a popular Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning benchmark) and report the results of user studies to evaluate whether people find explanations generated in this form useful.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2002.02938v1,Student/Teacher Advising through Reward Augmentation,['Cameron Reid'],2020-02-07 18:15:51+00:00,arxiv,...,90e39c1e02b091f1d736c0ff0b9e17ac,html,markdownify,2020-02-07 18:15:51+00:00,"Transfer learning is an important new subfield of multiagent reinforcement learning that aims to help an agent learn about a problem by using knowledge that it has gained solving another problem, or by using knowledge that is communicated to it by an agent who already knows the problem. This is useful when one wishes to change the architecture or learning algorithm of an agent (so that the new knowledge need not be built ""from scratch""), when new agents are frequently introduced to the environment with no knowledge, or when an agent must adapt to similar but different problems. Great progress has been made in the agent-to-agent case using the Teacher/Student framework proposed by (Torrey and Taylor 2013). However, that approach requires that learning from a teacher be treated differently from learning in every other reinforcement learning context. In this paper, I propose a method which allows the teacher/student framework to be applied in a way that fits directly and naturally into the more general reinforcement learning framework by integrating the teacher feedback into the reward signal received by the learning agent. I show that this approach can significantly improve the rate of learning for an agent playing a one-player stochastic game; I give examples of potential pitfalls of the approach; and I propose further areas of research building on this framework.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2006.05133v2,Contestable Black Boxes,"['Andrea Aler Tubella', 'Andreas Theodorou', 'Virginia Dignum', 'Loizos Michael']",2020-06-09 09:09:00+00:00,arxiv,...,2e07fcf819051abe2493ee236f62a68c,html,markdownify,2020-06-30 14:49:12+00:00,"The right to contest a decision with consequences on individuals or the society is a well-established democratic right. Despite this right also being explicitly included in GDPR in reference to automated decision-making, its study seems to have received much less attention in the AI literature compared, for example, to the right for explanation. This paper investigates the type of assurances that are needed in the contesting process when algorithmic black-boxes are involved, opening new questions about the interplay of contestability and explainability. We argue that specialised complementary methodologies to evaluate automated decision-making in the case of a particular decision being contested need to be developed. Further, we propose a combination of well-established software engineering and rule-based approaches as a possible socio-technical solution to the issue of contestability, one of the new democratic challenges posed by the automation of decision making.",Accepted at RuleML 2020 as a short paper,,10.1007/978-3-030-57977-7_12,cs.AI,"['cs.AI', 'cs.CY', 'I.2.m; K.4.1']"
https://arxiv.org/abs/2006.14779v3,Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,"['Gagan Bansal', 'Tongshuang Wu', 'Joyce Zhou', 'Raymond Fok', 'Besmira Nushi', 'Ece Kamar', 'Marco Tulio Ribeiro', 'Daniel S. Weld']",2020-06-26 03:34:04+00:00,arxiv,...,033d1c2feca71d480a4b964dfeed23d0,html,markdownify,2021-01-12 22:50:34+00:00,"Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI's recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?",CHI'21,,,cs.AI,"['cs.AI', 'cs.CL', 'cs.HC', 'cs.LG']"
https://arxiv.org/abs/2008.07667v1,Runtime-Safety-Guided Policy Repair,"['Weichao Zhou', 'Ruihan Gao', 'BaekGyu Kim', 'Eunsuk Kang', 'Wenchao Li']",2020-08-17 23:31:48+00:00,arxiv,...,2deea9dc5a96f68917c6e86544a1ca80,html,markdownify,2020-08-17 23:31:48+00:00,"We study the problem of policy repair for learning-based control policies in safety-critical settings. We consider an architecture where a high-performance learning-based control policy (e.g. one trained as a neural network) is paired with a model-based safety controller. The safety controller is endowed with the abilities to predict whether the trained policy will lead the system to an unsafe state, and take over control when necessary. While this architecture can provide added safety assurances, intermittent and frequent switching between the trained policy and the safety controller can result in undesirable behaviors and reduced performance. We propose to reduce or even eliminate control switching by `repairing' the trained policy based on runtime data produced by the safety controller in a way that deviates minimally from the original policy. The key idea behind our approach is the formulation of a trajectory optimization problem that allows the joint reasoning of policy update and safety constraints. Experimental results demonstrate that our approach is effective even when the system model in the safety controller is unknown and only approximated.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2009.08644v2,Efficient Reinforcement Learning Development with RLzoo,"['Zihan Ding', 'Tianyang Yu', 'Yanhua Huang', 'Hongming Zhang', 'Guo Li', 'Quancheng Guo', 'Luo Mai', 'Hao Dong']",2020-09-18 06:18:49+00:00,arxiv,...,33c6f03ab80cf7383c0629d74b9ea5ac,html,markdownify,2021-08-19 01:59:29+00:00,"Many researchers and developers are exploring for adopting Deep Reinforcement Learning (DRL) techniques in their applications. They however often find such an adoption challenging. Existing DRL libraries provide poor support for prototyping DRL agents (i.e., models), customising the agents, and comparing the performance of DRL agents. As a result, the developers often report low efficiency in developing DRL agents. In this paper, we introduce RLzoo, a new DRL library that aims to make the development of DRL agents efficient. RLzoo provides developers with (i) high-level yet flexible APIs for prototyping DRL agents, and further customising the agents for best performance, (ii) a model zoo where users can import a wide range of DRL agents and easily compare their performance, and (iii) an algorithm that can automatically construct DRL agents with custom components (which are critical to improve agent's performance in custom applications). Evaluation results show that RLzoo can effectively reduce the development cost of DRL agents, while achieving comparable performance with existing DRL libraries.",Accepted by ACM Multimedia Open Source Software Competition,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2011.05064v1,What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes,"['Herman Yau', 'Chris Russell', 'Simon Hadfield']",2020-11-10 12:05:08+00:00,arxiv,...,9ef240961bab55b6ee32f8f65e41f37c,html,markdownify,2020-11-10 12:05:08+00:00,"We present a novel form of explanation for Reinforcement Learning, based around the notion of intended outcome. These explanations describe the outcome an agent is trying to achieve by its actions. We provide a simple proof that general methods for post-hoc explanations of this nature are impossible in traditional reinforcement learning. Rather, the information needed for the explanations must be collected in conjunction with training the agent. We derive approaches designed to extract local explanations based on intention for several variants of Q-function approximation and prove consistency between the explanations and the Q-values learned. We demonstrate our method on multiple reinforcement learning problems, and provide code to help researchers introspecting their RL environments and algorithms.",,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/2011.12863v1,European Strategy on AI: Are we truly fostering social good?,"['Francesca Foffano', 'Teresa Scantamburlo', 'Atia CortÃ©s', 'Chiara Bissolo']",2020-11-25 16:39:12+00:00,arxiv,...,58d0aee9290c3f4adea5147ef992a961,html,markdownify,2020-11-25 16:39:12+00:00,"Artificial intelligence (AI) is already part of our daily lives and is playing a key role in defining the economic and social shape of the future. In 2018, the European Commission introduced its AI strategy able to compete in the next years with world powers such as China and US, but relying on the respect of European values and fundamental rights. As a result, most of the Member States have published their own National Strategy with the aim to work on a coordinated plan for Europe. In this paper, we present an ongoing study on how European countries are approaching the field of Artificial Intelligence, with its promises and risks, through the lens of their national AI strategies. In particular, we aim to investigate how European countries are investing in AI and to what extent the stated plans can contribute to the benefit of the whole society. This paper reports the main findings of a qualitative analysis of the investment plans reported in 15 European National Strategies","6 pages, 1 figures, submitted at IJCAI 2020 Workshop on AI for Social
  Good",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2102.07017v1,Mitigating Negative Side Effects via Environment Shaping,"['Sandhya Saisubramanian', 'Shlomo Zilberstein']",2021-02-13 22:15:00+00:00,arxiv,...,a72675c9bf0ed4d0f834c3c07cda4166,html,markdownify,2021-02-13 22:15:00+00:00,"Agents operating in unstructured environments often produce negative side effects (NSE), which are difficult to identify at design time. While the agent can learn to mitigate the side effects from human feedback, such feedback is often expensive and the rate of learning is sensitive to the agent's state representation. We examine how humans can assist an agent, beyond providing feedback, and exploit their broader scope of knowledge to mitigate the impacts of NSE. We formulate this problem as a human-agent team with decoupled objectives. The agent optimizes its assigned task, during which its actions may produce NSE. The human shapes the environment through minor reconfiguration actions so as to mitigate the impacts of the agent's side effects, without affecting the agent's ability to complete its assigned task. We present an algorithm to solve this problem and analyze its theoretical properties. Through experiments with human subjects, we assess the willingness of users to perform minor environment modifications to mitigate the impacts of NSE. Empirical evaluation of our approach shows that the proposed framework can successfully mitigate NSE, without affecting the agent's ability to complete its assigned task.",9 pages,,,cs.AI,"['cs.AI', 'cs.MA', 'cs.RO']"
https://arxiv.org/abs/2102.10985v1,Software Architecture for Next-Generation AI Planning Systems,"['Sebastian Graef', 'Ilche Georgievski']",2021-02-22 13:43:45+00:00,arxiv,...,a8324c78501f057de02333f94d0b710f,html,markdownify,2021-02-22 13:43:45+00:00,"Artificial Intelligence (AI) planning is a flourishing research and development discipline that provides powerful tools for searching a course of action that achieves some user goal. While these planning tools show excellent performance on benchmark planning problems, they represent challenging software systems when it comes to their use and integration in real-world applications. In fact, even in-depth understanding of their internal mechanisms does not guarantee that one can successfully set up, use and manipulate existing planning tools. We contribute toward alleviating this situation by proposing a service-oriented planning architecture to be at the core of the ability to design, develop and use next-generation AI planning systems. We collect and classify common planning capabilities to form the building blocks of the planning architecture. We incorporate software design principles and patterns into the architecture to allow for usability, interoperability and reusability of the planning capabilities. Our prototype planning system demonstrates the potential of our approach for rapid prototyping and flexibility of system composition. Finally, we provide insight into the qualitative advantages of our approach when compared to a typical planning tool.",,,,cs.AI,"['cs.AI', 'cs.SE']"
https://arxiv.org/abs/2104.12871v2,Why AI is Harder Than We Think,['Melanie Mitchell'],2021-04-26 20:39:18+00:00,arxiv,...,e5d3791519825dec9465817313c3aeca,html,markdownify,2021-04-28 15:51:25+00:00,"Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (""AI spring"") and periods of disappointment, loss of confidence, and reduced funding (""AI winter""). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.",12 pages; typos corrected in newest version,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2105.00525v2,Planning for Proactive Assistance in Environments with Partial Observability,"['Anagha Kulkarni', 'Siddharth Srivastava', 'Subbarao Kambhampati']",2021-05-02 18:12:06+00:00,arxiv,...,d66b9e2e47fa8fddaa82fab42d3029e0,html,markdownify,2021-09-04 15:29:50+00:00,"This paper addresses the problem of synthesizing the behavior of an AI agent that provides proactive task assistance to a human in settings like factory floors where they may coexist in a common environment. Unlike in the case of requested assistance, the human may not be expecting proactive assistance and hence it is crucial for the agent to ensure that the human is aware of how the assistance affects her task. This becomes harder when there is a possibility that the human may neither have full knowledge of the AI agent's capabilities nor have full observability of its activities. Therefore, our \textit{proactive assistant} is guided by the following three principles: \textbf{(1)} its activity decreases the human's cost towards her goal; \textbf{(2)} the human is able to recognize the potential reduction in her cost; \textbf{(3)} its activity optimizes the human's overall cost (time/resources) of achieving her goal. Through empirical evaluation and user studies, we demonstrate the usefulness of our approach.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2107.04303v2,"Integrating Planning, Execution and Monitoring in the presence of Open World Novelties: Case Study of an Open World Monopoly Solver","['Sriram Gopalakrishnan', 'Utkarsh Soni', 'Tung Thai', 'Panagiotis Lymperopoulos', 'Matthias Scheutz', 'Subbarao Kambhampati']",2021-07-09 08:26:28+00:00,arxiv,...,beacfc94597a9a053b31fdf1a1b6a588,html,markdownify,2021-08-09 21:22:15+00:00,"The game of monopoly is an adversarial multi-agent domain where there is no fixed goal other than to be the last player solvent, There are useful subgoals like monopolizing sets of properties, and developing them. There is also a lot of randomness from dice rolls, card-draws, and adversaries' strategies. This unpredictability is made worse when unknown novelties are added during gameplay. Given these challenges, Monopoly was one of the test beds chosen for the DARPA-SAILON program which aims to create agents that can detect and accommodate novelties. To handle the game complexities, we developed an agent that eschews complete plans, and adapts it's policy online as the game evolves. In the most recent independent evaluation in the SAILON program, our agent was the best performing agent on most measures. We herein present our approach and results.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2201.07719v1,Improving Behavioural Cloning with Human-Driven Dynamic Dataset Augmentation,"['Federico Malato', 'Joona Jehkonen', 'Ville HautamÃ¤ki']",2022-01-19 16:57:17+00:00,arxiv,...,e2d76474ebb81828bf3859555b9d67cc,html,markdownify,2022-01-19 16:57:17+00:00,"Behavioural cloning has been extensively used to train agents and is recognized as a fast and solid approach to teach general behaviours based on expert trajectories. Such method follows the supervised learning paradigm and it strongly depends on the distribution of the data. In our paper, we show how combining behavioural cloning with human-in-the-loop training solves some of its flaws and provides an agent task-specific corrections to overcome tricky situations while speeding up the training time and lowering the required resources. To do this, we introduce a novel approach that allows an expert to take control of the agent at any moment during a simulation and provide optimal solutions to its problematic situations. Our experiments show that this approach leads to better policies both in terms of quantitative evaluation and in human-likeliness.","6 pages, 5 figures, 2 code snippets, accepted at the AAAI-22 Workshop
  on Interactive Machine Learning",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2201.08111v2,Safety-Aware Multi-Agent Apprenticeship Learning,['Junchen Zhao'],2022-01-20 11:01:01+00:00,arxiv,...,21c4dbe8837eb2e8eac06bb0e36e44e2,html,markdownify,2022-01-24 17:16:02+00:00,"Our objective of this project is to make the extension based on the technique mentioned in the paper ""Safety-Aware Apprenticeship Learning"" to improve the utility and the efficiency of the existing Reinforcement Learning model from a Single-Agent Learning framework to a Multi-Agent Learning framework. Our contributions to the project are presented in the following bullet points: 1. Regarding the fact that we will add an extension to the Inverse Reinforcement Learning model from a Single-Agent scenario to a Multi-Agentscenario. Our first contribution to this project is considering the case of extracting safe reward functions from expert behaviors in a Multi-Agent scenario instead of being from the Single-Agent scenario. 2. Our second contribution is extending the Single-Agent Learning Framework to a Multi-Agent Learning framework and designing a novel Learning Framework based on the extension in the end. 3. Our final contribution to this project is evaluating empirically the performance of my extension to the Single-Agent Inverse Reinforcement Learning framework.","58 pages, 4 figures. Master's report. arXiv admin note: text overlap
  with arXiv:1710.07983 by other authors",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/2202.03192v2,Reward is not enough: can we liberate AI from the reinforcement learning paradigm?,['Vacslav Glukhov'],2022-02-03 18:31:48+00:00,arxiv,...,cf6d2a77bb71eda46e5fa55a48a51a30,html,markdownify,2022-02-08 19:04:08+00:00,"I present arguments against the hypothesis put forward by Silver, Singh, Precup, and Sutton ( https://www.sciencedirect.com/science/article/pii/S0004370221000862 ) : reward maximization is not enough to explain many activities associated with natural and artificial intelligence including knowledge, learning, perception, social intelligence, evolution, language, generalisation and imitation. I show such reductio ad lucrum has its intellectual origins in the political economy of Homo economicus and substantially overlaps with the radical version of behaviourism. I show why the reinforcement learning paradigm, despite its demonstrable usefulness in some practical application, is an incomplete framework for intelligence -- natural and artificial. Complexities of intelligent behaviour are not simply second-order complications on top of reward maximisation. This fact has profound implications for the development of practically usable, smart, safe and robust artificially intelligent agents.","25 pages, 1 figure",,,cs.AI,"['cs.AI', 'I.2.0']"
https://arxiv.org/abs/2202.11798v2,Drawing Inductor Layout with a Reinforcement Learning Agent: Method and Application for VCO Inductors,"['Cameron Haigh', 'Zichen Zhang', 'Negar Hassanpour', 'Khurram Javed', 'Yingying Fu', 'Shayan Shahramian', 'Shawn Zhang', 'Jun Luo']",2022-02-23 21:44:40+00:00,arxiv,...,37d9bf7dd93183c77ff5bc8feac5180e,html,markdownify,2022-02-25 20:29:25+00:00,"Design of Voltage-Controlled Oscillator (VCO) inductors is a laborious and time-consuming task that is conventionally done manually by human experts. In this paper, we propose a framework for automating the design of VCO inductors, using Reinforcement Learning (RL). We formulate the problem as a sequential procedure, where wire segments are drawn one after another, until a complete inductor is created. We then employ an RL agent to learn to draw inductors that meet certain target specifications. In light of the need to tweak the target specifications throughout the circuit design cycle, we also develop a variant in which the agent can learn to quickly adapt to draw new inductors for moderately different target specifications. Our empirical results show that the proposed framework is successful at automatically generating VCO inductors that meet or exceed the target specification.",,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/2202.13985v2,The dangers in algorithms learning humans' values and irrationalities,"['Rebecca Gorman', 'Stuart Armstrong']",2022-02-28 17:41:39+00:00,arxiv,...,cacb30687fee3fd7b44cf5b714ce422a,html,markdownify,2022-03-01 11:23:04+00:00,"For an artificial intelligence (AI) to be aligned with human values (or human preferences), it must first learn those values. AI systems that are trained on human behavior, risk miscategorising human irrationalities as human values -- and then optimising for these irrationalities. Simply learning human values still carries risks: AI learning them will inevitably also gain information on human irrationalities and human behaviour/policy. Both of these can be dangerous: knowing human policy allows an AI to become generically more powerful (whether it is partially aligned or not aligned at all), while learning human irrationalities allows it to exploit humans without needing to provide value in return. This paper analyses the danger in developing artificial intelligence that learns about human irrationalities and human policy, and constructs a model recommendation system with various levels of information about human biases, human policy, and human values. It concludes that, whatever the power and knowledge of the AI, it is more dangerous for it to know human irrationalities than human values. Thus it is better for the AI to learn human values directly, rather than learning human biases and then deducing values from behaviour.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2203.12918v1,A Rationale-Centric Framework for Human-in-the-loop Machine Learning,"['Jinghui Lu', 'Linyi Yang', 'Brian Mac Namee', 'Yue Zhang']",2022-03-24 08:12:57+00:00,arxiv,...,52f875f605a7d660b2c7d12f74e9ec0b,html,markdownify,2022-03-24 08:12:57+00:00,"We present a novel rationale-centric framework with human-in-the-loop -- Rationales-centric Double-robustness Learning (RDL) -- to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks -- especially for few-shot learning scenarios. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",Accepted to ACL 2022,,,cs.AI,"['cs.AI', 'cs.CL', 'cs.HC']"
https://arxiv.org/abs/1204.2601v1,Detecting lateral genetic material transfer,"['C. CalderÃ³n', 'L. Delaye', 'V. Mireles', 'P. Miramontes']",2012-04-12 01:45:21+00:00,arxiv,...,8851f32c673bbb42bcab7086aead39ab,html,markdownify,2012-04-12 01:45:21+00:00,"The bioinformatical methods to detect lateral gene transfer events are mainly based on functional coding DNA characteristics. In this paper, we propose the use of DNA traits not depending on protein coding requirements. We introduce several semilocal variables that depend on DNA primary sequence and that reflect thermodynamic as well as physico-chemical magnitudes that are able to tell apart the genome of different organisms. After combining these variables in a neural classificator, we obtain results whose power of resolution go as far as to detect the exchange of genomic material between bacteria that are phylogenetically close.",Submitted to Applied Computational Intelligence and Soft Computing,,,cs.NE,"['cs.NE', 'cs.AI', 'q-bio.GN']"
https://arxiv.org/abs/1902.01580v1,PUTWorkbench: Analysing Privacy in AI-intensive Systems,"['Saurabh Srivastava', 'Vinay P. Namboodiri', 'T. V. Prabhakar']",2019-02-05 08:09:33+00:00,arxiv,...,678013961bd7429e354b9e76ceacd80a,html,markdownify,2019-02-05 08:09:33+00:00,AI intensive systems that operate upon user data face the challenge of balancing data utility with privacy concerns. We propose the idea and present the prototype of an open-source tool called Privacy Utility Trade-off (PUT) Workbench which seeks to aid software practitioners to take such crucial decisions. We pick a simple privacy model that doesn't require any background knowledge in Data Science and show how even that can achieve significant results over standard and real-life datasets. The tool and the source code is made freely available for extensions and usage.,,,,cs.CR,"['cs.CR', 'cs.AI', 'cs.SE']"
https://arxiv.org/abs/1909.06965v1,Better AI through Logical Scaffolding,"['Nikos Arechiga', 'Jonathan DeCastro', 'Soonho Kong', 'Karen Leung']",2019-09-12 05:41:25+00:00,arxiv,...,9fdb462f90bf5dcde05da0ef530caec3,html,markdownify,2019-09-12 05:41:25+00:00,"We describe the concept of logical scaffolds, which can be used to improve the quality of software that relies on AI components. We explain how some of the existing ideas on runtime monitors for perception systems can be seen as a specific instance of logical scaffolds. Furthermore, we describe how logical scaffolds may be useful for improving AI programs beyond perception systems, to include general prediction systems and agent behavior models.",CAV Workshop on Formal Methods for ML-enabled Autonomous Systems 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2002.10221v2,The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI,['Samuel Allen Alexander'],2020-02-15 22:01:56+00:00,arxiv,...,491c4acab43d419065cafe41af00c78d,html,markdownify,2020-10-20 03:39:28+00:00,"After generalizing the Archimedean property of real numbers in such a way as to make it adaptable to non-numeric structures, we demonstrate that the real numbers cannot be used to accurately measure non-Archimedean structures. We argue that, since an agent with Artificial General Intelligence (AGI) should have no problem engaging in tasks that inherently involve non-Archimedean rewards, and since traditional reinforcement learning rewards are real numbers, therefore traditional reinforcement learning probably will not lead to AGI. We indicate two possible ways traditional reinforcement learning could be altered to remove this roadblock.",16 pages,Journal of Artificial General Intelligence 11(1): 70--85 (2020),10.2478/jagi-2020-0004,cs.LG,"['cs.LG', 'cs.AI', '97R40']"
https://arxiv.org/abs/2009.05260v1,The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and Industrial Q Needs,['Markus Borg'],2020-09-11 07:31:23+00:00,arxiv,...,31a861f93745cb5bd2648b9168520829,html,markdownify,2020-09-11 07:31:23+00:00,"AI solutions seem to appear in any and all application domains. As AI becomes more pervasive, the importance of quality assurance increases. Unfortunately, there is no consensus on what artificial intelligence means and interpretations range from simple statistical analysis to sentient humanoid robots. On top of that, quality is a notoriously hard concept to pinpoint. What does this mean for AI quality? In this paper, we share our working definition and a pragmatic approach to address the corresponding quality assurance with a focus on testing. Finally, we present our ongoing work on establishing the AIQ Meta-Testbed.","Accepted for publication in the Proc. of the Software Quality Days
  2021, Vienna, Austria",,,cs.SE,"['cs.SE', 'cs.AI']"
https://arxiv.org/abs/1201.6583v1,Empowerment for Continuous Agent-Environment Systems,"['Tobias Jung', 'Daniel Polani', 'Peter Stone']",2012-01-31 15:46:27+00:00,arxiv,...,26325a65256d8cf8286ee0dd56f3032b,html,markdownify,2012-01-31 15:46:27+00:00,"This paper develops generalizations of empowerment to continuous states. Empowerment is a recently introduced information-theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms, but also from considerations stemming from curiosity-driven learning. Empowemerment measures, for agent-environment systems with stochastic transitions, how much influence an agent has on its environment, but only that influence that can be sensed by the agent sensors. It is an information-theoretic generalization of joint controllability (influence on environment) and observability (measurement by sensors) of the environment by the agent, both controllability and observability being usually defined in control theory as the dimensionality of the control/observation spaces. Earlier work has shown that empowerment has various interesting and relevant properties, e.g., it allows us to identify salient states using only the dynamics, and it can act as intrinsic reward without requiring an external reward. However, in this previous work empowerment was limited to the case of small-scale and discrete domains and furthermore state transition probabilities were assumed to be known. The goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector-valued state spaces and initially unknown state transition probabilities. The continuous state space is addressed by Monte-Carlo approximation; the unknown transitions are addressed by model learning and prediction for which we apply Gaussian processes regression with iterated forecasting. In a number of well-known continuous control tasks we examine the dynamics induced by empowerment and include an application to exploration and online model learning.",,"Adaptive Behavior 19(1),2011",,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1209.3734v1,RIO: Minimizing User Interaction in Ontology Debugging,"['Patrick Rodler', 'Kostyantyn Shchekotykhin', 'Philipp Fleiss', 'Gerhard Friedrich']",2012-09-17 18:02:50+00:00,arxiv,...,7fb908e4bbc9caf12754c7223f224022,html,markdownify,2012-09-17 18:02:50+00:00,"Efficient ontology debugging is a cornerstone for many activities in the context of the Semantic Web, especially when automatic tools produce (parts of) ontologies such as in the field of ontology matching. The best currently known interactive debugging systems rely upon some meta information in terms of fault probabilities, which can speed up the debugging procedure in the good case, but can also have negative impact on the performance in the bad case. The problem is that assessment of the meta information is only possible a-posteriori. Consequently, as long as the actual fault is unknown, there is always some risk of suboptimal interactive diagnoses discrimination. As an alternative, one might prefer to rely on a tool which pursues a no-risk strategy. In this case, however, possibly well-chosen meta information cannot be exploited, resulting again in inefficient debugging actions. In this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information. Therefore, this method is suitable for application scenarios where reliable a-priori fault estimates are difficult to obtain. Using problematic ontologies in the field of ontology matching, we show that the proposed risk-aware query strategy outperforms both active learning approaches and no-risk strategies on average in terms of required amount of user interaction.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1505.04813v1,What is Learning? A primary discussion about information and Representation,['Hao Wu'],2015-05-19 01:17:47+00:00,arxiv,...,5b625f46842b6d8818c6bce2d4c700c9,html,markdownify,2015-05-19 01:17:47+00:00,"Nowadays, represented by Deep Learning techniques, the field of machine learning is experiencing unprecedented prosperity and its influence is demonstrated in academia, industry and civil society. ""Intelligent"" has become a label which could not be neglected for most applications; celebrities and scientists also warned that the development of full artificial intelligence may spell the end of the human race. It seems that the answer to building a computer system that could automatically improve with experience is right on the next corner. While for AI and machine learning researchers, it is a consensus that we are not anywhere near the core technique which could bring the Terminator, Number 5 or R2D2 into real life, and there is not even a formal definition about what is intelligence, or one of its basic properties: Learning. Therefore, even though researchers know these concerns are not necessary currently, there is no generalized explanation about why these concerns are not necessary, and what properties people should take into account that would make these concerns to be necessary. In this paper, starts from analysing the relation between information and its representation, a necessary condition for a model to be a learning model is proposed. This condition and related future works could be used to verify whether a system is able to learn or not, and enrich our understanding of learning: one important property of Intelligence.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1507.05895v1,Decision Maker based on Atomic Switches,"['Song-Ju Kim', 'Tohru Tsuruoka', 'Tsuyoshi Hasegawa', 'Masakazu Aono']",2015-07-21 16:22:53+00:00,arxiv,...,c7bae4ec66dce747c0ae4f81a047cd94,html,markdownify,2015-07-21 16:22:53+00:00,"We propose a simple model for an atomic switch-based decision maker (ASDM), and show that, as long as its total volume of precipitated Ag atoms is conserved when coupled with suitable operations, an atomic switch system provides a sophisticated ""decision-making"" capability that is known to be one of the most important intellectual abilities in human beings. We considered the multi-armed bandit problem (MAB); the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards. These decisions are made as dictated by each volume of precipitated Ag atoms, which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game. The ""tug-of-war (TOW) dynamics"" of the ASDM exhibits higher efficiency than conventional MAB solvers. We show analytical calculations that validate the statistical reasons for the ASDM dynamics to produce such high performance, despite its simplicity. These results imply that various physical systems, in which some conservation law holds, can be used to implement efficient ""decision-making objects."" Efficient MAB solvers are useful for many practical applications, because MAB abstracts a variety of decision-making problems in real- world situations where an efficient trial-and-error is required. The proposed scheme will introduce a new physics-based analog computing paradigm, which will include such things as ""intelligent nano devices"" and ""intelligent information networks"" based on self-detection and self-judgment.","10 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1412.6141",,,cs.AI,"['cs.AI', 'cond-mat.mtrl-sci']"
https://arxiv.org/abs/1601.03411v5,Analysis of Algorithms and Partial Algorithms,['Andrew MacFie'],2016-01-13 21:17:42+00:00,arxiv,...,5f0bd99e140b873fe92d866c777c4845,html,markdownify,2017-08-07 01:30:46+00:00,"We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention an approach to self-improving AGI enabled by this methodology.   Aug 2017 addendum: This article was originally written with multiple audiences in mind. It is really best put in the following terms. Goertzel, Hutter, Legg, and others have developed a definition of an intelligence score for a general abstract agent: expected lifetime reward in a random environment. AIXI is generally the optimal agent according to this score, but there may be reasons to analyze other agents and compare score values. If we want to use this definition of intelligence in practice, perhaps we can start by analyzing some simple agents. Common algorithms can be thought of as simple agents (environment is input, reward is based on running time) so we take the goal of applying the agent intelligence score to algorithms. That is, we want to find, what are the IQ scores of algorithms? We can do some very simple analysis, but the real answer is that even for simple algorithms, the intelligence score is too difficult to work with in practice.",,"Artificial General Intelligence 2016, New York, USA, July 16-19,
  2016, Proceedings, 284-293",10.1007/978-3-319-41649-6_29,cs.AI,"['cs.AI', 'cs.DS']"
https://arxiv.org/abs/1602.03924v1,Modeling Human Ad Hoc Coordination,"['Peter M. Krafft', 'Chris L. Baker', 'Alex Pentland', 'Joshua B. Tenenbaum']",2016-02-11 22:48:59+00:00,arxiv,...,b8e4f9aea9945db7d1f6648c5c2b1325,html,markdownify,2016-02-11 22:48:59+00:00,"Whether in groups of humans or groups of computer agents, collaboration is most effective between individuals who have the ability to coordinate on a joint strategy for collective action. However, in general a rational actor will only intend to coordinate if that actor believes the other group members have the same intention. This circular dependence makes rational coordination difficult in uncertain environments if communication between actors is unreliable and no prior agreements have been made. An important normative question with regard to coordination in these ad hoc settings is therefore how one can come to believe that other actors will coordinate, and with regard to systems involving humans, an important empirical question is how humans arrive at these expectations. We introduce an exact algorithm for computing the infinitely recursive hierarchy of graded beliefs required for rational coordination in uncertain environments, and we introduce a novel mechanism for multiagent coordination that uses it. Our algorithm is valid in any environment with a finite state space, and extensions to certain countably infinite state spaces are likely possible. We test our mechanism for multiagent coordination as a model for human decisions in a simple coordination game using existing experimental data. We then explore via simulations whether modeling humans in this way may improve human-agent collaboration.",AAAI 2016,,,cs.AI,"['cs.AI', 'cs.GT', 'cs.MA', 'I.2.0; I.2.11; J.4']"
https://arxiv.org/abs/1604.04721v1,An artificial intelligence tool for heterogeneous team formation in the classroom,"['Juan M. Alberola', 'Elena Del Val', 'Victor Sanchez-Anguix', 'Alberto Palomares', 'Maria Dolores Teruel']",2016-04-16 10:50:02+00:00,arxiv,...,fa932fc42adb5435467ae1ebc03d8acd,html,markdownify,2016-04-16 10:50:02+00:00,"Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organize their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimized. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin's role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student's predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.",,"Knowledge-Based Systems, 2016",10.1016/j.knosys.2016.02.010,cs.AI,"['cs.AI', 'cs.CY', 'cs.HC', 'I.2.8; K.3.1; J.4']"
https://arxiv.org/abs/1712.05855v1,A Berkeley View of Systems Challenges for AI,"['Ion Stoica', 'Dawn Song', 'Raluca Ada Popa', 'David Patterson', 'Michael W. Mahoney', 'Randy Katz', 'Anthony D. Joseph', 'Michael Jordan', 'Joseph M. Hellerstein', 'Joseph E. Gonzalez', 'Ken Goldberg', 'Ali Ghodsi', 'David Culler', 'Pieter Abbeel']",2017-12-15 22:01:52+00:00,arxiv,...,5b72dd0f387951420392108e3d378b57,html,markdownify,2017-12-15 22:01:52+00:00,"With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, AI (Artificial Intelligence) has moved from research labs to production. These changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems software and architectures, and by the broad accessibility of these technologies.   The next generation of AI systems promises to accelerate these developments and increasingly impact our lives via frequent interactions and making (often mission-critical) decisions on our behalf, often in highly personalized contexts. Realizing this promise, however, raises daunting challenges. In particular, we need AI systems that make timely and safe decisions in unpredictable environments, that are robust against sophisticated adversaries, and that can process ever increasing amounts of data across organizations and individuals without compromising confidentiality. These challenges will be exacerbated by the end of the Moore's Law, which will constrain the amount of data these technologies can store and process. In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society.",Berkeley Technical Report,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1712.07752v3,Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI],['Rajesh Chidambaram'],2017-12-21 00:29:48+00:00,arxiv,...,b452e554a09b0db0a1f892e516adc3cf,html,markdownify,2018-06-28 22:24:09+00:00,"Artificial Intelligence (AI), is once again in the phase of drastic advancements. Unarguably, the technology itself can revolutionize the way we live our everyday life. But the exponential growth of technology poses a daunting task for policy researchers and law makers in making amendments to the existing norms. In addition, not everyone in the society is studying the potential socio-economic intricacies and cultural drifts that AI can bring about. It is prudence to reflect from our historical past to propel the development of technology in the right direction. To benefit the society of the present and future, I scientifically explore the societal impact of AI. While there are many public and private partnerships working on similar aspects, here I describe the necessity for an Unanimous International Regulatory Body for all applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such an organization. To combat any drawbacks in the formation of an UIRB-AI, both idealistic and pragmatic perspectives are discussed alternatively. The paper further advances the discussion by proposing novel policies on how such organization should be structured and how it can bring about a win-win situation for everyone in the society.","The paper covers a diverse range of topics but doesn't get into the
  details of any and hence the proposals remain pragmatically irrelevant",,,cs.AI,"['cs.AI', 'cs.CY']"
https://arxiv.org/abs/1804.01396v1,Artificial Intelligence and its Role in Near Future,"['Jahanzaib Shabbir', 'Tarique Anwer']",2018-04-01 23:12:30+00:00,arxiv,...,64faf430e7fd3cadc228682dad1cf0d2,html,markdownify,2018-04-01 23:12:30+00:00,"AI technology has a long history which is actively and constantly changing and growing. It focuses on intelligent agents, which contain devices that perceive the environment and based on which takes actions in order to maximize goal success chances. In this paper, we will explain the modern AI basics and various representative applications of AI. In the context of the modern digitalized world, AI is the property of machines, computer programs, and systems to perform the intellectual and creative functions of a person, independently find ways to solve problems, be able to draw conclusions and make decisions. Most artificial intelligence systems have the ability to learn, which allows people to improve their performance over time. The recent research on AI tools, including machine learning, deep learning and predictive analysis intended toward increasing the planning, learning, reasoning, thinking and action taking ability. Based on which, the proposed research intends towards exploring on how the human intelligence differs from the artificial intelligence. Moreover, we critically analyze what AI of today is capable of doing, why it still cannot reach human intelligence and what are the open challenges existing in front of AI to reach and outperform human level of intelligence. Furthermore, it will explore the future predictions for artificial intelligence and based on which potential solution will be recommended to solve it within next decades.",,,,cs.AI,"['cs.AI', 'cs.CV']"
https://arxiv.org/abs/1805.03382v2,Automated Mechanism Design via Neural Networks,"['Weiran Shen', 'Pingzhong Tang', 'Song Zuo']",2018-05-09 05:57:29+00:00,arxiv,...,2a007550f51bcdb7ae1eec1b27b7c947,html,markdownify,2021-05-03 13:26:26+00:00,"Using AI approaches to automatically design mechanisms has been a central research mission at the interface of AI and economics [Conitzer and Sandholm, 2002]. Previous approaches that attempt to design revenue optimal auctions for the multi-dimensional settings fall short in at least one of the three aspects: 1) representation -- search in a space that probably does not even contain the optimal mechanism; 2) exactness -- finding a mechanism that is either not truthful or far from optimal; 3) domain dependence -- need a different design for different environment settings.   To resolve the three difficulties, in this paper, we put forward -- MenuNet -- a unified neural network based framework that automatically learns to design revenue optimal mechanisms. Our framework consists of a mechanism network that takes an input distribution for training and outputs a mechanism, as well as a buyer network that takes a mechanism as input and output an action. Such a separation in design mitigates the difficulty to impose incentive compatibility constraints on the mechanism, by making it a rational choice of the buyer. As a result, our framework easily overcomes the previously mentioned difficulty in incorporating IC constraints and always returns exactly incentive compatible mechanisms.   We then apply our framework to a number of multi-item revenue optimal design settings, for a few of which the theoretically optimal mechanisms are unknown. We then go on to theoretically prove that the mechanisms found by our framework are indeed optimal.   To the best of our knowledge, we are the first to apply neural networks to discover optimal auction mechanisms with provable optimality.",Published at AAMAS 2019,,,cs.AI,"['cs.AI', 'cs.GT']"
https://arxiv.org/abs/1810.06721v2,Optimizing Agent Behavior over Long Time Scales by Transporting Value,"['Chia-Chun Hung', 'Timothy Lillicrap', 'Josh Abramson', 'Yan Wu', 'Mehdi Mirza', 'Federico Carnevale', 'Arun Ahuja', 'Greg Wayne']",2018-10-15 22:01:28+00:00,arxiv,...,f3c4ba3bad00b50ed9561bb8c014aab5,html,markdownify,2018-12-21 13:56:03+00:00,"Humans spend a remarkable fraction of waking life engaged in acts of ""mental time travel"". We dwell on our actions in the past and experience satisfaction or regret. More than merely autobiographical storytelling, we use these event recollections to change how we will act in similar scenarios in the future. This process endows us with a computationally important ability to link actions and consequences across long spans of time, which figures prominently in addressing the problem of long-term temporal credit assignment; in artificial intelligence (AI) this is the question of how to evaluate the utility of the actions within a long-duration behavioral sequence leading to success or failure in a task. Existing approaches to shorter-term credit assignment in AI cannot solve tasks with long delays between actions and consequences. Here, we introduce a new paradigm for reinforcement learning where agents use recall of specific memories to credit actions from the past, allowing them to solve problems that are intractable for existing algorithms. This paradigm broadens the scope of problems that can be investigated in AI and offers a mechanistic account of behaviors that may inspire computational models in neuroscience, psychology, and behavioral economics.",,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1902.03245v2,"Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of Task Delegability","['Brian Lubars', 'Chenhao Tan']",2019-02-08 19:00:02+00:00,arxiv,...,d7d8316b36b094ff2604b908c29bf8a5,html,markdownify,2019-11-01 18:00:00+00:00,"While artificial intelligence (AI) holds promise for addressing societal challenges, issues of exactly which tasks to automate and to what extent to do so remain understudied. We approach this problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to AI. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life, and administer a survey based on our proposed framework. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Among the four factors, trust is the most correlated with human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of AI automation across tasks. We hope this work encourages future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development.","19 pages, 3 figures, 5 tables, NeurIPS 2019, dataset available at
  https://delegability.github.io",,,cs.AI,"['cs.AI', 'cs.CY', 'cs.HC']"
https://arxiv.org/abs/1904.08166v1,Analysing Neural Network Topologies: a Game Theoretic Approach,"['Julian Stier', 'Gabriele Gianini', 'Michael Granitzer', 'Konstantin Ziegler']",2019-04-17 10:28:21+00:00,arxiv,...,f69ee4296c7d78da598cb3a088774594,html,markdownify,2019-04-17 10:28:21+00:00,"Artificial Neural Networks have shown impressive success in very different application cases. Choosing a proper network architecture is a critical decision for a network's success, usually done in a manual manner. As a straightforward strategy, large, mostly fully connected architectures are selected, thereby relying on a good optimization strategy to find proper weights while at the same time avoiding overfitting. However, large parts of the final network are redundant. In the best case, large parts of the network become simply irrelevant for later inferencing. In the worst case, highly parameterized architectures hinder proper optimization and allow the easy creation of adverserial examples fooling the network. A first step in removing irrelevant architectural parts lies in identifying those parts, which requires measuring the contribution of individual components such as neurons. In previous work, heuristics based on using the weight distribution of a neuron as contribution measure have shown some success, but do not provide a proper theoretical understanding. Therefore, in our work we investigate game theoretic measures, namely the Shapley value (SV), in order to separate relevant from irrelevant parts of an artificial neural network. We begin by designing a coalitional game for an artificial neural network, where neurons form coalitions and the average contributions of neurons to coalitions yield to the Shapley value. In order to measure how well the Shapley value measures the contribution of individual neurons, we remove low-contributing neurons and measure its impact on the network performance. In our experiments we show that the Shapley value outperforms other heuristics for measuring the contribution of neurons.",,Procedia Computer Science 126 (2018): 234-243,10.1016/j.procs.2018.07.257,cs.AI,"['cs.AI', 'cs.GT']"
https://arxiv.org/abs/1904.11737v2,Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in Discrete Environments,"['Ramon Fraga Pereira', 'Nir Oren', 'Felipe Meneguzzi']",2019-04-26 09:36:26+00:00,arxiv,...,97267b0d316b03582282f4afb561b077,html,markdownify,2020-07-28 16:38:44+00:00,"Assessing whether an agent has abandoned a goal or is actively pursuing it is important when multiple agents are trying to achieve joint goals, or when agents commit to achieving goals for each other. Making such a determination for a single goal by observing only plan traces is not trivial as agents often deviate from optimal plans for various reasons, including the pursuit of multiple goals or the inability to act optimally. In this article, we develop an approach based on domain independent heuristics from automated planning, landmarks, and fact partitions to identify sub-optimal action steps - with respect to a plan - within a plan execution trace. Such capability is very important in domains where multiple agents cooperate and delegate tasks among themselves, e.g. through social commitments, and need to ensure that a delegating agent can infer whether or not another agent is actually progressing towards a delegated task. We demonstrate how an agent can use our technique to determine - by observing a trace - whether an agent is honouring a commitment. We empirically show, for a number of representative domains, that our approach infers sub-optimal action steps with very high accuracy and detects commitment abandonment in nearly all cases.",,,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/1907.13275v1,Towards a Theory of Intentions for Human-Robot Collaboration,"['Rocio Gomez', 'Mohan Sridharan', 'Heather Riley']",2019-07-31 01:31:04+00:00,arxiv,...,d98f258b8985b410ae2652ce32b4d072,html,markdownify,2019-07-31 01:31:04+00:00,"The architecture described in this paper encodes a theory of intentions based on the the key principles of non-procrastination, persistence, and automatically limiting reasoning to relevant knowledge and observations. The architecture reasons with transition diagrams of any given domain at two different resolutions, with the fine-resolution description defined as a refinement of, and hence tightly-coupled to, a coarse-resolution description. Non-monotonic logical reasoning with the coarse-resolution description computes an activity (i.e., plan) comprising abstract actions for any given goal. Each abstract action is implemented as a sequence of concrete actions by automatically zooming to and reasoning with the part of the fine-resolution transition diagram relevant to the current coarse-resolution transition and the goal. Each concrete action in this sequence is executed using probabilistic models of the uncertainty in sensing and actuation, and the corresponding fine-resolution outcomes are used to infer coarse-resolution observations that are added to the coarse-resolution history. The architecture's capabilities are evaluated in the context of a simulated robot assisting humans in an office domain, on a physical robot (Baxter) manipulating tabletop objects, and on a wheeled robot (Turtlebot) moving objects to particular places or people. The experimental results indicate improvements in reliability and computational efficiency compared with an architecture that does not include the theory of intentions, and an architecture that does not include zooming for fine-resolution reasoning.","25 pages, 4 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1908.00528v2,Neural Simplex Architecture,"['Dung T. Phan', 'Radu Grosu', 'Nils Jansen', 'Nicola Paoletti', 'Scott A. Smolka', 'Scott D. Stoller']",2019-08-01 17:39:18+00:00,arxiv,...,b7e4bd4ea12da8d993c3f36b4e61e163,html,markdownify,2020-03-24 19:11:09+00:00,"We present the Neural Simplex Architecture (NSA), a new approach to runtime assurance that provides safety guarantees for neural controllers (obtained e.g. using reinforcement learning) of autonomous and other complex systems without unduly sacrificing performance. NSA is inspired by the Simplex control architecture of Sha et al., but with some significant differences. In the traditional approach, the advanced controller (AC) is treated as a black box; when the decision module switches control to the baseline controller (BC), the BC remains in control forever. There is relatively little work on switching control back to the AC, and there are no techniques for correcting the AC's behavior after it generates a potentially unsafe control input that causes a failover to the BC. Our NSA addresses both of these limitations. NSA not only provides safety assurances in the presence of a possibly unsafe neural controller, but can also improve the safety of such a controller in an online setting via retraining, without overly degrading its performance. To demonstrate NSA's benefits, we have conducted several significant case studies in the continuous control domain. These include a target-seeking ground rover navigating an obstacle field, and a neural controller for an artificial pancreas system.",12th NASA Formal Methods Symposium (NFM 2020),,,cs.AI,"['cs.AI', 'cs.SY', 'eess.SY']"
https://arxiv.org/abs/1912.05907v1,Anti-Alignments -- Measuring The Precision of Process Models and Event Logs,"['Thomas Chatain', 'Mathilde Boltenhagen', 'Josep Carmona']",2019-11-28 07:39:23+00:00,arxiv,...,02f412ebb0893f2a9af78c166ce4752a,html,markdownify,2019-11-28 07:39:23+00:00,"Processes are a crucial artefact in organizations, since they coordinate the execution of activities so that products and services are provided. The use of models to analyse the underlying processes is a well-known practice. However, due to the complexity and continuous evolution of their processes, organizations need an effective way of analysing the relation between processes and models. Conformance checking techniques asses the suitability of a process model in representing an underlying process, observed through a collection of real executions. One important metric in conformance checking is to asses the precision of the model with respect to the observed executions, i.e., characterize the ability of the model to produce behavior unrelated to the one observed. In this paper we present the notion of anti-alignment as a concept to help unveiling runs in the model that may deviate significantly from the observed behavior. Using anti-alignments, a new metric for precision is proposed. In contrast to existing metrics, anti-alignment based precision metrics satisfy most of the required axioms highlighted in a recent publication. Moreover, a complexity analysis of the problem of computing anti-alignments is provided, which sheds light into the practicability of using anti-alignment to estimate precision. Experiments are provided that witness the validity of the concepts introduced in this paper.",,,,cs.AI,"['cs.AI', 'cs.FL']"
https://arxiv.org/abs/2002.06100v2,Analyzing Differentiable Fuzzy Logic Operators,"['Emile van Krieken', 'Erman Acar', 'Frank van Harmelen']",2020-02-14 16:11:36+00:00,arxiv,...,ebdf32b0d64625d690a8a6af29d04f4c,html,markdownify,2021-08-24 08:25:41+00:00,"The AI community is increasingly putting its attention towards combining symbolic and neural approaches, as it is often argued that the strengths and weaknesses of these approaches are complementary. One recent trend in the literature are weakly supervised learning techniques that employ operators from fuzzy logics. In particular, these use prior background knowledge described in such logics to help the training of a neural network from unlabeled and noisy data. By interpreting logical symbols using neural networks, this background knowledge can be added to regular loss functions, hence making reasoning a part of learning. We study, both formally and empirically, how a large collection of logical operators from the fuzzy logic literature behave in a differentiable learning setting. We find that many of these operators, including some of the most well-known, are highly unsuitable in this setting. A further finding concerns the treatment of implication in these fuzzy logics, and shows a strong imbalance between gradients driven by the antecedent and the consequent of the implication. Furthermore, we introduce a new family of fuzzy implications (called sigmoidal implications) to tackle this phenomenon. Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised learning, and compare how different operators behave in practice. We find that, to achieve the largest performance improvement over a supervised baseline, we have to resort to non-standard combinations of logical operators which perform well in learning, but no longer satisfy the usual logical laws.","47 pages, 18 figures. V2: Added analysis for existential
  quantification. Improved experiments and writing",,10.1016/j.artint.2021.103602,cs.AI,"['cs.AI', 'cs.LG', 'cs.LO']"
https://arxiv.org/abs/2002.09044v1,A Road Map to Strong Intelligence,['Philip Paquette'],2020-02-20 22:22:50+00:00,arxiv,...,669418cec0f66cfdc569177ad1f14153,html,markdownify,2020-02-20 22:22:50+00:00,"I wrote this paper because technology can really improve people's lives. With it, we can live longer in a healthy body, save time through increased efficiency and automation, and make better decisions. To get to the next level, we need to start looking at intelligence from a much broader perspective, and promote international interdisciplinary collaborations. Section 1 of this paper delves into sociology and social psychology to explain that the mechanisms underlying intelligence are inherently social. Section 2 proposes a method to classify intelligence, and describes the differences between weak and strong intelligence. Section 3 examines the Chinese Room argument from a different perspective. It demonstrates that a Turing-complete machine cannot have strong intelligence, and considers the modifications necessary for a computer to be intelligent and have understanding. Section 4 argues that the existential risk caused by the technological explosion of a single agent should not be of serious concern. Section 5 looks at the AI control problem and argues that it is impossible to build a super-intelligent machine that will do what it creators want. By using insights from biology, it also proposes a solution to the control problem. Section 6 discusses some of the implications of strong intelligence. Section 7 lists the main challenges with deep learning, and asserts that radical changes will be required to reach strong intelligence. Section 8 examines a neuroscience framework that could help explain how a cortical column works. Section 9 lays out the broad strokes of a road map towards strong intelligence. Finally, section 10 analyzes the impacts and the challenges of greater intelligence.",,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/2004.00470v2,Counterfactual Multi-Agent Reinforcement Learning with Graph Convolution Communication,"['Jianyu Su', 'Stephen Adams', 'Peter A. Beling']",2020-04-01 14:36:13+00:00,arxiv,...,469636ee237827cca56e97d6cb5d8b20,html,markdownify,2020-12-29 00:57:59+00:00,"We consider a fully cooperative multi-agent system where agents cooperate to maximize a system's utility in a partial-observable environment. We propose that multi-agent systems must have the ability to (1) communicate and understand the inter-plays between agents and (2) correctly distribute rewards based on an individual agent's contribution. In contrast, most work in this setting considers only one of the above abilities. In this study, we develop an architecture that allows for communication among agents and tailors the system's reward for each individual agent. Our architecture represents agent communication through graph convolution and applies an existing credit assignment structure, counterfactual multi-agent policy gradient (COMA), to assist agents to learn communication by back-propagation. The flexibility of the graph structure enables our method to be applicable to a variety of multi-agent systems, e.g. dynamic systems that consist of varying numbers of agents and static systems with a fixed number of agents. We evaluate our method on a range of tasks, demonstrating the advantage of marrying communication with credit assignment. In the experiments, our proposed method yields better performance than the state-of-art methods, including COMA. Moreover, we show that the communication strategies offers us insights and interpretability of the system's cooperative policies.",,,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/2004.13102v3,Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork,"['Gagan Bansal', 'Besmira Nushi', 'Ece Kamar', 'Eric Horvitz', 'Daniel S. Weld']",2020-04-27 19:06:28+00:00,arxiv,...,030570faabe1f01c4592efedac9a48bc,html,markdownify,2021-02-19 20:22:20+00:00,"AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue ""No"" -- predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team's expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",v2,,,cs.AI,"['cs.AI', 'cs.HC', 'cs.LG']"
https://arxiv.org/abs/2005.13601v1,"The Adversarial Resilience Learning Architecture for AI-based Modelling, Exploration, and Operation of Complex Cyber-Physical Systems","['Eric MSP Veith', 'Nils Wenninghoff', 'Emilie Frost']",2020-05-27 19:19:57+00:00,arxiv,...,3f3bf13e0e51d43f763e71e54479d487,html,markdownify,2020-05-27 19:19:57+00:00,"Modern algorithms in the domain of Deep Reinforcement Learning (DRL) demonstrated remarkable successes; most widely known are those in game-based scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time strategy game. However, applications in the domain of modern Cyber-Physical Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We assume that the benefits would be considerable: Modern CPS have become increasingly complex and evolved beyond traditional methods of modelling and analysis. At the same time, these CPS are confronted with an increasing amount of stochastic inputs, from volatile energy sources in power grids to broad user participation stemming from markets. Approaches of system modelling that use techniques from the domain of Artificial Intelligence (AI) do not focus on analysis and operation. In this paper, we describe the concept of Adversarial Resilience Learning (ARL) that formulates a new approach to complex environment checking and resilient operation: It defines two agent classes, attacker and defender agents. The quintessence of ARL lies in both agents exploring the system and training each other without any domain knowledge. Here, we introduce the ARL software architecture that allows to use a wide range of model-free as well as model-based DRL-based algorithms, and document results of concrete experiment runs on a complex power grid.",Submitted to NIPS 2020,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2106.01826v2,Towards a Mathematical Theory of Abstraction,['Beren Millidge'],2021-06-03 13:23:49+00:00,arxiv,...,976b2b3a3f29164e074213c68aa6d7c8,html,markdownify,2021-06-25 19:37:18+00:00,"While the utility of well-chosen abstractions for understanding and predicting the behaviour of complex systems is well appreciated, precisely what an abstraction $\textit{is}$ has so far has largely eluded mathematical formalization. In this paper, we aim to set out a mathematical theory of abstraction. We provide a precise characterisation of what an abstraction is and, perhaps more importantly, suggest how abstractions can be learnt directly from data both for static datasets and for dynamical systems. We define an abstraction to be a small set of `summaries' of a system which can be used to answer a set of queries about the system or its behaviour. The difference between the ground truth behaviour of the system on the queries and the behaviour of the system predicted only by the abstraction provides a measure of the `leakiness' of the abstraction which can be used as a loss function to directly learn abstractions from data. Our approach can be considered a generalization of classical statistics where we are not interested in reconstructing `the data' in full, but are instead only concerned with answering a set of arbitrary queries about the data. While highly theoretical, our results have deep implications for statistical inference and machine learning and could be used to develop explicit methods for learning precise kinds of abstractions directly from data.",03/06/21 initial upload. 25/06/21 minor fixes and corrections,,,cs.AI,"['cs.AI', 'stat.ML']"
https://arxiv.org/abs/2106.15764v1,The Threat of Offensive AI to Organizations,"['Yisroel Mirsky', 'Ambra Demontis', 'Jaidip Kotak', 'Ram Shankar', 'Deng Gelei', 'Liu Yang', 'Xiangyu Zhang', 'Wenke Lee', 'Yuval Elovici', 'Battista Biggio']",2021-06-30 01:03:28+00:00,arxiv,...,1536ed0cbc9bced603e895ec8fffd7e8,html,markdownify,2021-06-30 01:03:28+00:00,"AI has provided us with the ability to automate tasks, extract information from vast amounts of data, and synthesize media that is nearly indistinguishable from the real thing. However, positive tools can also be used for negative purposes. In particular, cyber adversaries can use AI (such as machine learning) to enhance their attacks and expand their campaigns.   Although offensive AI has been discussed in the past, there is a need to analyze and understand the threat in the context of organizations. For example, how does an AI-capable adversary impact the cyber kill chain? Does AI benefit the attacker more than the defender? What are the most significant AI threats facing organizations today and what will be their impact on the future?   In this survey, we explore the threat of offensive AI on organizations. First, we present the background and discuss how AI changes the adversary's methods, strategies, goals, and overall attack model. Then, through a literature review, we identify 33 offensive AI capabilities which adversaries can use to enhance their attacks. Finally, through a user study spanning industry and academia, we rank the AI threats and provide insights on the adversaries.",,,,cs.AI,"['cs.AI', 'cs.CR', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/2110.01834v1,Thinking Fast and Slow in AI: the Role of Metacognition,"['Marianna Bergamaschi Ganapini', 'Murray Campbell', 'Francesco Fabiano', 'Lior Horesh', 'Jon Lenchner', 'Andrea Loreggia', 'Nicholas Mattei', 'Francesca Rossi', 'Biplav Srivastava', 'Kristen Brent Venable']",2021-10-05 06:05:38+00:00,arxiv,...,99e0995e0c2e0b445b670528a798ce42,html,markdownify,2021-10-05 06:05:38+00:00,"AI systems have seen dramatic advancement in recent years, bringing many applications that pervade our everyday life. However, we are still mostly seeing instances of narrow AI: many of these recent developments are typically focused on a very limited set of competencies and goals, e.g., image interpretation, natural language processing, classification, prediction, and many others. Moreover, while these successes can be accredited to improved algorithms and techniques, they are also tightly linked to the availability of huge datasets and computational power. State-of-the-art AI still lacks many capabilities that would naturally be included in a notion of (human) intelligence.   We argue that a better study of the mechanisms that allow humans to have these capabilities can help us understand how to imbue AI systems with these competencies. We focus especially on D. Kahneman's theory of thinking fast and slow, and we propose a multi-agent AI architecture where incoming problems are solved by either system 1 (or ""fast"") agents, that react by exploiting only past experience, or by system 2 (or ""slow"") agents, that are deliberately activated when there is the need to reason and search for optimal solutions beyond what is expected from the system 1 agent. Both kinds of agents are supported by a model of the world, containing domain knowledge about the environment, and a model of ""self"", containing information about past actions of the system and solvers' skills.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2202.13252v3,The Quest for a Common Model of the Intelligent Decision Maker,['Richard S. Sutton'],2022-02-26 23:40:42+00:00,arxiv,...,e5f912b9cd51f2653aa0ac84ee9705bd,html,markdownify,2022-06-05 22:15:16+00:00,"The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the ""common model of the intelligent agent"". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.","Will appear as an extended abstract at the fifth Multi-disciplinary
  Conference on Reinforcement Learning and Decision Making, held in Providence,
  Rhode Island, June 8-11, 2022",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2204.05091v1,Linguistic communication as (inverse) reward design,"['Theodore R. Sumers', 'Robert D. Hawkins', 'Mark K. Ho', 'Thomas L. Griffiths', 'Dylan Hadfield-Menell']",2022-04-11 13:50:34+00:00,arxiv,...,fa0511558db7f0001f1de4d2e8f4c562,html,markdownify,2022-04-11 13:50:34+00:00,"Natural language is an intuitive and expressive way to communicate reward information to autonomous agents. It encompasses everything from concrete instructions to abstract descriptions of the world. Despite this, natural language is often challenging to learn from: it is difficult for machine learning methods to make appropriate inferences from such a wide range of input. This paper proposes a generalization of reward design as a unifying principle to ground linguistic communication: speakers choose utterances to maximize expected rewards from the listener's future behaviors. We first extend reward design to incorporate reasoning about unknown future states in a linear bandit setting. We then define a speaker model which chooses utterances according to this objective. Simulations show that short-horizon speakers (reasoning primarily about a single, known state) tend to use instructions, while long-horizon speakers (reasoning primarily about unknown, future states) tend to describe the reward function. We then define a pragmatic listener which performs inverse reward design by jointly inferring the speaker's latent horizon and rewards. Our findings suggest that this extension of reward design to linguistic communication, including the notion of a latent speaker horizon, is a promising direction for achieving more robust alignment outcomes from natural language supervision.","6 pages, 3 figures. Accepted at Learning from Natural Language
  Supervision workshop (ACL 2022)",,,cs.AI,"['cs.AI', 'cs.CL']"
https://arxiv.org/abs/2204.06117v1,AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,"['Huili Chen', 'Xinqiao Zhang', 'Ke Huang', 'Farinaz Koushanfar']",2022-04-12 23:56:59+00:00,arxiv,...,30cbe4fe88ca8d161410f5e2c2842392,html,markdownify,2022-04-12 23:56:59+00:00,"This paper proposes AdaTest, a novel adaptive test pattern generation framework for efficient and reliable Hardware Trojan (HT) detection. HT is a backdoor attack that tampers with the design of victim integrated circuits (ICs). AdaTest improves the existing HT detection techniques in terms of scalability and accuracy of detecting smaller Trojans in the presence of noise and variations. To achieve high trigger coverage, AdaTest leverages Reinforcement Learning (RL) to produce a diverse set of test inputs. Particularly, we progressively generate test vectors with high reward values in an iterative manner. In each iteration, the test set is evaluated and adaptively expanded as needed. Furthermore, AdaTest integrates adaptive sampling to prioritize test samples that provide more information for HT detection, thus reducing the number of samples while improving the sample quality for faster exploration. We develop AdaTest with a Software/Hardware co-design principle and provide an optimized on-chip architecture solution. AdaTest's architecture minimizes the hardware overhead in two ways:(i) Deploying circuit emulation on programmable hardware to accelerate reward evaluation of the test input; (ii) Pipelining each computation stage in AdaTest by automatically constructing auxiliary circuit for test input generation, reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on various HT benchmarks and compare it with two prior works that use logic testing for HT detection. Experimental results show that AdaTest engenders up to two orders of test generation speedup and two orders of test set size reduction compared to the prior works while achieving the same level or higher Trojan detection rate.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2205.03824v1,A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges,"['Zhenghua Chen', 'Min Wu', 'Alvin Chan', 'Xiaoli Li', 'Yew-Soon Ong']",2022-05-08 09:38:35+00:00,arxiv,...,94eee45661af7c51496a998b7498983b,html,markdownify,2022-05-08 09:38:35+00:00,"Artificial Intelligence (AI) is a fast-growing research and development (R&D) discipline which is attracting increasing attention because of its promises to bring vast benefits for consumers and businesses, with considerable benefits promised in productivity growth and innovation. To date it has reported significant accomplishments in many areas that have been deemed as challenging for machines, ranging from computer vision, natural language processing, audio analysis to smart sensing and many others. The technical trend in realizing the successes has been towards increasing complex and large size AI models so as to solve more complex problems at superior performance and robustness. This rapid progress, however, has taken place at the expense of substantial environmental costs and resources. Besides, debates on the societal impacts of AI, such as fairness, safety and privacy, have continued to grow in intensity. These issues have presented major concerns pertaining to the sustainable development of AI. In this work, we review major trends in machine learning approaches that can address the sustainability problem of AI. Specifically, we examine emerging AI methodologies and algorithms for addressing the sustainability issue of AI in two major aspects, i.e., environmental sustainability and social sustainability of AI. We will also highlight the major limitations of existing studies and propose potential research challenges and directions for the development of next generation of sustainable AI techniques. We believe that this technical review can help to promote a sustainable development of AI R&D activities for the research community.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2205.15241v2,Multi-Game Decision Transformers,"['Kuang-Huei Lee', 'Ofir Nachum', 'Mengjiao Yang', 'Lisa Lee', 'Daniel Freeman', 'Winnie Xu', 'Sergio Guadarrama', 'Ian Fischer', 'Eric Jang', 'Henryk Michalewski', 'Igor Mordatch']",2022-05-30 16:55:38+00:00,arxiv,...,cd6a8aeb821e1776f8b806fc333a06ba,html,markdownify,2022-10-15 07:31:27+00:00,"A longstanding goal of the field of AI is a method for learning a highly capable, generalist agent from diverse experience. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction.","NeurIPS 2022. 24 pages, 16 figures. Additional information, videos
  and code can be seen at https://sites.google.com/view/multi-game-transformers",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1507.07688v3,Belief and Truth in Hypothesised Behaviours,"['Stefano V. Albrecht', 'Jacob W. Crandall', 'Subramanian Ramamoorthy']",2015-07-28 08:52:45+00:00,arxiv,...,5001dbed7fa0445d81bf5b15efca71a5,html,markdownify,2016-03-02 18:38:32+00:00,"There is a long history in game theory on the topic of Bayesian or ""rational"" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.",44 pages; final manuscript published in Artificial Intelligence (AIJ),,10.1016/j.artint.2016.02.004,cs.AI,"['cs.AI', 'cs.GT', 'I.2.11']"
https://arxiv.org/abs/1612.06528v1,Neuro-symbolic EDA-based Optimisation using ILP-enhanced DBNs,"['Sarmimala Saikia', 'Lovekesh Vig', 'Ashwin Srinivasan', 'Gautam Shroff', 'Puneet Agarwal', 'Richa Rawat']",2016-12-20 06:56:12+00:00,arxiv,...,2add9e9b2949fa08cfef8756f4006389,html,markdownify,2016-12-20 06:56:12+00:00,"We investigate solving discrete optimisation problems using the estimation of distribution (EDA) approach via a novel combination of deep belief networks(DBN) and inductive logic programming (ILP).While DBNs are used to learn the structure of successively better feasible solutions,ILP enables the incorporation of domain-based background knowledge related to the goodness of solutions.Recent work showed that ILP could be an effective way to use domain knowledge in an EDA scenario.However,in a purely ILP-based EDA,sampling successive populations is either inefficient or not straightforward.In our Neuro-symbolic EDA,an ILP engine is used to construct a model for good solutions using domain-based background knowledge.These rules are introduced as Boolean features in the last hidden layer of DBNs used for EDA-based optimization.This incorporation of logical ILP features requires some changes while training and sampling from DBNs: (a)our DBNs need to be trained with data for units at the input layer as well as some units in an otherwise hidden layer, and (b)we would like the samples generated to be drawn from instances entailed by the logical model.We demonstrate the viability of our approach on instances of two optimisation problems: predicting optimal depth-of-win for the KRK endgame,and jobshop scheduling.Our results are promising: (i)On each iteration of distribution estimation,samples obtained with an ILP-assisted DBN have a substantially greater proportion of good solutions than samples generated using a DBN without ILP features, and (ii)On termination of distribution estimation,samples obtained using an ILP-assisted DBN contain more near-optimal samples than samples from a DBN without ILP features.These results suggest that the use of ILP-constructed theories could be useful for incorporating complex domain-knowledge into deep models for estimation of distribution based procedures.","9 pages, 7 figures, Cognitive Computation: Integrating Neural and
  Symbolic Approaches (Workshop at 30th Conference on Neural Information
  Processing Systems (NIPS 2016), Barcelona, Spain.),
  http://daselab.cs.wright.edu/nesy/CoCo2016/coco_nips_2016_pre-proceedings.pdf
  (page 78-86). arXiv admin note: substantial text overlap with
  arXiv:1608.01093",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1703.08922v5,On Automating the Doctrine of Double Effect,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']",2017-03-27 04:03:56+00:00,arxiv,...,25ac3d279d405d8713789e42bf12dee4,html,markdownify,2017-07-17 23:12:54+00:00,"The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.This is preliminary work to illustrate the feasibility of the second mode, and we hope that our initial sketches can be useful for other researchers in incorporating DDE in their own frameworks.","26th International Joint Conference on Artificial Intelligence 2017;
  Special Track on AI & Autonomy",,,cs.AI,"['cs.AI', 'cs.LO', 'cs.RO']"
https://arxiv.org/abs/2105.08489v2,Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising,"['Dongbo Xi', 'Zhen Chen', 'Peng Yan', 'Yinger Zhang', 'Yongchun Zhu', 'Fuzhen Zhuang', 'Yu Chen']",2021-05-18 13:07:12+00:00,arxiv,...,c5e2d9e9a75be8409746e7e871fd15a2,html,markdownify,2021-05-24 02:46:16+00:00,"In most real-world large-scale online applications (e.g., e-commerce or finance), customer acquisition is usually a multi-step conversion process of audiences. For example, an impression->click->purchase process is usually performed of audiences for e-commerce platforms. However, it is more difficult to acquire customers in financial advertising (e.g., credit card advertising) than in traditional advertising. On the one hand, the audience multi-step conversion path is longer. On the other hand, the positive feedback is sparser (class imbalance) step by step, and it is difficult to obtain the final positive feedback due to the delayed feedback of activation. Multi-task learning is a typical solution in this direction. While considerable multi-task efforts have been made in this direction, a long-standing challenge is how to explicitly model the long-path sequential dependence among audience multi-step conversions for improving the end-to-end conversion. In this paper, we propose an Adaptive Information Transfer Multi-task (AITM) framework, which models the sequential dependence among audience multi-step conversions via the Adaptive Information Transfer (AIT) module. The AIT module can adaptively learn what and how much information to transfer for different conversion stages. Besides, by combining the Behavioral Expectation Calibrator in the loss function, the AITM framework can yield more accurate end-to-end conversion identification. The proposed framework is deployed in Meituan app, which utilizes it to real-timely show a banner to the audience with a high end-to-end conversion rate for Meituan Co-Branded Credit Cards. Offline experimental results on both industrial and public real-world datasets clearly demonstrate that the proposed framework achieves significantly better performance compared with state-of-the-art baselines.",accepted by KDD21,,,cs.AI,"['cs.AI', 'cs.IR']"
https://arxiv.org/abs/2111.11276v2,Branching Time Active Inference: empirical study and complexity class analysis,"['ThÃ©ophile Champion', 'Howard Bowman', 'Marek GrzeÅ']",2021-11-22 15:30:35+00:00,arxiv,...,135b83f27fa6e4d21c92cbe3a1f123fb,html,markdownify,2022-05-24 14:31:35+00:00,"Active inference is a state-of-the-art framework for modelling the brain that explains a wide range of mechanisms such as habit formation, dopaminergic discharge and curiosity. However, recent implementations suffer from an exponential complexity class when computing the prior over all the possible policies up to the time horizon. Fountas et al (2020) used Monte Carlo tree search to address this problem, leading to very good results in two different tasks. Additionally, Champion et al (2021a) proposed a tree search approach based on (temporal) structure learning. This was enabled by the development of a variational message passing approach to active inference, which enables compositional construction of Bayesian networks for active inference. However, this message passing tree search approach, which we call branching-time active inference (BTAI), has never been tested empirically. In this paper, we present an experimental study of BTAI in the context of a maze solving agent. In this context, we show that both improved prior preferences and deeper search help mitigate the vulnerability to local minima. Then, we compare BTAI to standard active inference (AcI) on a graph navigation task. We show that for small graphs, both BTAI and AcI successfully solve the task. For larger graphs, AcI exhibits an exponential (space) complexity class, making the approach intractable. However, BTAI explores the space of policies more efficiently, successfully scaling to larger graphs. Then, BTAI was compared to the POMCP algorithm on the frozen lake environment. The experiments suggest that BTAI and the POMCP algorithm accumulate a similar amount of reward. Also, we describe when BTAI receives more rewards than the POMCP agent, and when the opposite is true. Finally, we compared BTAI to the approach of Fountas et al (2020) on the dSprites dataset, and we discussed the pros and cons of each approach.","39 pages, 11 figures, accepted for publication in Neural Networks",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1003.0617v1,Agent Based Approaches to Engineering Autonomous Space Software,"['Louise A. Dennis', 'Michael Fisher', 'Nicholas Lincoln', 'Alexei Lisitsa', 'Sandor M. Veres']",2010-03-02 15:38:48+00:00,arxiv,...,91f33fcb771319e4ca9eaf1fe131879a,html,markdownify,2010-03-02 15:38:48+00:00,Current approaches to the engineering of space software such as satellite control systems are based around the development of feedback controllers using packages such as MatLab's Simulink toolbox. These provide powerful tools for engineering real time systems that adapt to changes in the environment but are limited when the controller itself needs to be adapted.   We are investigating ways in which ideas from temporal logics and agent programming can be integrated with the use of such control systems to provide a more powerful layer of autonomous decision making. This paper will discuss our initial approaches to the engineering of such systems.,"3 pages, 1 Figure, Formal Methods in Aerospace","EPTCS 20, 2010, pp. 63-67",10.4204/EPTCS.20.6,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/1110.6437v3,Anthropic decision theory,['Stuart Armstrong'],2011-10-28 13:34:36+00:00,arxiv,...,30207c1f671ceb62d625a85a9b5b4b4e,html,markdownify,2017-09-20 15:52:49+00:00,"This paper sets out to resolve how agents ought to act in the Sleeping Beauty problem and various related anthropic (self-locating belief) problems, not through the calculation of anthropic probabilities, but through finding the correct decision to make. It creates an anthropic decision theory (ADT) that decides these problems from a small set of principles. By doing so, it demonstrates that the attitude of agents with regards to each other (selfish or altruistic) changes the decisions they reach, and that it is very important to take this into account. To illustrate ADT, it is then applied to two major anthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday problems, thus resolving some issues about the probability of human extinction.",,,,physics.data-an,"['physics.data-an', 'cs.AI', 'hep-th', 'physics.pop-ph', '62C05']"
https://arxiv.org/abs/1209.2355v5,Counterfactual Reasoning and Learning Systems,"['LÃ©on Bottou', 'Jonas Peters', 'Joaquin QuiÃ±onero-Candela', 'Denis X. Charles', 'D. Max Chickering', 'Elon Portugaly', 'Dipankar Ray', 'Patrice Simard', 'Ed Snelson']",2012-09-11 15:47:43+00:00,arxiv,...,39e8d45a5f9aaeef6449d5d6ac2e121b,html,markdownify,2013-07-27 18:02:46+00:00,This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems. This work is illustrated by experiments carried out on the ad placement system associated with the Bing search engine.,revised version,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'math.ST', 'stat.TH']"
https://arxiv.org/abs/1211.2290v1,Dating Texts without Explicit Temporal Cues,"['Abhimanu Kumar', 'Jason Baldridge', 'Matthew Lease', 'Joydeep Ghosh']",2012-11-10 05:12:31+00:00,arxiv,...,e1e042af58adf7c86374d506cda497cb,html,markdownify,2012-11-10 05:12:31+00:00,"This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely {\it solely} on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals' lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for non-biographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.",,,,cs.CL,"['cs.CL', 'cs.AI']"
https://arxiv.org/abs/1308.4526v5,"Formalization, Mechanization and Automation of GÃ¶del's Proof of God's Existence","['Christoph BenzmÃ¼ller', 'Bruno Woltzenlogel Paleo']",2013-08-21 09:56:57+00:00,arxiv,...,8b4de844cf01504c205e77761a1619cd,html,markdownify,2017-09-03 18:43:28+00:00,"G\""odel's ontological proof has been analysed for the first-time with an unprecedent degree of detail and formality with the help of higher-order theorem provers. The following has been done (and in this order): A detailed natural deduction proof. A formalization of the axioms, definitions and theorems in the TPTP THF syntax. Automatic verification of the consistency of the axioms and definitions with Nitpick. Automatic demonstration of the theorems with the provers LEO-II and Satallax. A step-by-step formalization using the Coq proof assistant. A formalization using the Isabelle proof assistant, where the theorems (and some additional lemmata) have been automated with Sledgehammer and Metis.",2 pages,"Frontiers in Artificial Intelligence and Applications, Volume 263:
  ECAI 2014",10.3233/978-1-61499-419-0-93,cs.LO,"['cs.LO', 'cs.AI', 'math.LO', '03Axx, 68T27, 68T30, 68T15', 'F.4.1; I.2.3; I.2.4']"
https://arxiv.org/abs/1404.0854v1,Enabling Automatic Certification of Online Auctions,"['Wei Bai', 'Emmanuel M. Tadjouddine', 'Yu Guo']",2014-04-03 10:45:01+00:00,arxiv,...,aad88e81ac52abd0f438382a1fd4672f,html,markdownify,2014-04-03 10:45:01+00:00,"We consider the problem of building up trust in a network of online auctions by software agents. This requires agents to have a deeper understanding of auction mechanisms and be able to verify desirable properties of a given mechanism. We have shown how these mechanisms can be formalised as semantic web services in OWL-S, a good enough expressive machine-readable formalism enabling software agents, to discover, invoke, and execute a web service. We have also used abstract interpretation to translate the auction's specifications from OWL-S, based on description logic, to COQ, based on typed lambda calculus, in order to enable automatic verification of desirable properties of the auction by the software agents. For this language translation, we have discussed the syntactic transformation as well as the semantics connections between both concrete and abstract domains. This work contributes to the implementation of the vision of agent-mediated e-commerce systems.","In Proceedings FESCA 2014, arXiv:1404.0436","EPTCS 147, 2014, pp. 123-132",10.4204/EPTCS.147.9,cs.LO,"['cs.LO', 'cs.AI']"
https://arxiv.org/abs/1602.07029v1,Latent Skill Embedding for Personalized Lesson Sequence Recommendation,"['Siddharth Reddy', 'Igor Labutov', 'Thorsten Joachims']",2016-02-23 04:20:40+00:00,arxiv,...,66e42845f436ac12f1244064d0efe82a,html,markdownify,2016-02-23 04:20:40+00:00,"Students in online courses generate large amounts of data that can be used to personalize the learning process and improve quality of education. In this paper, we present the Latent Skill Embedding (LSE), a probabilistic model of students and educational content that can be used to recommend personalized sequences of lessons with the goal of helping students prepare for specific assessments. Akin to collaborative filtering for recommender systems, the algorithm does not require students or content to be described by features, but it learns a representation using access traces. We formulate this problem as a regularized maximum-likelihood embedding of students, lessons, and assessments from historical student-content interactions. An empirical evaluation on large-scale data from Knewton, an adaptive learning technology company, shows that this approach predicts assessment results competitively with benchmark models and is able to discriminate between lesson sequences that lead to mastery and failure.","Under review by the ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/1604.08153v3,Classifying Options for Deep Reinforcement Learning,"['Kai Arulkumaran', 'Nat Dilokthanakul', 'Murray Shanahan', 'Anil Anthony Bharath']",2016-04-27 17:48:39+00:00,arxiv,...,6bd9d1666b2f870d090b30600e384c28,html,markdownify,2017-06-19 15:34:58+00:00,"In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different ""option heads"" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer.","IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and
  Challenges",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1607.05540v2,Exploiting Vagueness for Multi-Agent Consensus,"['Michael Crosscombe', 'Jonathan Lawry']",2016-07-19 12:19:35+00:00,arxiv,...,510a27dfd737ef4bbdc883bd5c844d41,html,markdownify,2016-09-20 14:26:12+00:00,"A framework for consensus modelling is introduced using Kleene's three valued logic as a means to express vagueness in agents' beliefs. Explicitly borderline cases are inherent to propositions involving vague concepts where sentences of a propositional language may be absolutely true, absolutely false or borderline. By exploiting these intermediate truth values, we can allow agents to adopt a more vague interpretation of underlying concepts in order to weaken their beliefs and reduce the levels of inconsistency, so as to achieve consensus. We consider a consensus combination operation which results in agents adopting the borderline truth value as a shared viewpoint if they are in direct conflict. Simulation experiments are presented which show that applying this operator to agents chosen at random (subject to a consistency threshold) from a population, with initially diverse opinions, results in convergence to a smaller set of more precise shared beliefs. Furthermore, if the choice of agents for combination is dependent on the payoff of their beliefs, this acting as a proxy for performance or usefulness, then the system converges to beliefs which, on average, have higher payoff.","Submitted to the second international workshop on Smart Simulation
  and Modelling for Complex Systems (SSMCS) at IJCAI 2015",,10.1007/978-981-10-2564-8_5,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/1609.04904v2,Long-Term Trends in the Public Perception of Artificial Intelligence,"['Ethan Fast', 'Eric Horvitz']",2016-09-16 03:45:15+00:00,arxiv,...,dcb7cdaa7b9fb641e04c2e419a243f52,html,markdownify,2016-12-02 17:18:42+00:00,"Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.",In AAAI 2017,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/1611.03372v1,A stochastically verifiable autonomous control architecture with reasoning,"['Paolo Izzo', 'Hongyang Qu', 'Sandor M. Veres']",2016-11-10 16:06:31+00:00,arxiv,...,47a74ef4a9f5cdfae59de4da26b91162,html,markdownify,2016-11-10 16:06:31+00:00,A new agent architecture called Limited Instruction Set Agent (LISA) is introduced for autonomous control. The new architecture is based on previous implementations of AgentSpeak and it is structurally simpler than its predecessors with the aim of facilitating design-time and run-time verification methods. The process of abstracting the LISA system to two different types of discrete probabilistic models (DTMC and MDP) is investigated and illustrated. The LISA system provides a tool for complete modelling of the agent and the environment for probabilistic verification. The agent program can be automatically compiled into a DTMC or a MDP model for verification with Prism. The automatically generated Prism model can be used for both design-time and run-time verification. The run-time verification is investigated and illustrated in the LISA system as an internal modelling mechanism for prediction of future outcomes.,"Accepted at IEEE Conf. Decision and Control, 2016",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.SE', 'cs.SY']"
https://arxiv.org/abs/1611.09321v3,Improving Policy Gradient by Exploring Under-appreciated Rewards,"['Ofir Nachum', 'Mohammad Norouzi', 'Dale Schuurmans']",2016-11-28 20:15:55+00:00,arxiv,...,006b17a78060e1250f0ce6adb520a5a9,html,markdownify,2017-03-15 22:55:17+00:00,"This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.",Published as a conference paper at ICLR 2017,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1707.00183v2,Teacher-Student Curriculum Learning,"['Tambet Matiisen', 'Avital Oliver', 'Taco Cohen', 'John Schulman']",2017-07-01 18:13:17+00:00,arxiv,...,5d262f6690b695737de9ba32cb4cd626,html,markdownify,2017-11-29 20:57:09+00:00,"We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1803.02852v1,"Value Alignment, Fair Play, and the Rights of Service Robots",['Daniel Estrada'],2018-03-07 19:33:08+00:00,arxiv,...,c144aedfa370cc79e1c1c04e4f586bca,html,markdownify,2018-03-07 19:33:08+00:00,"Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment"" with human values and interests. I argue that Turing's call for ""fair play for machines"" is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play"" motivate a novel interpretation of Turing's notorious ""imitation game"" as a condition not of intelligence but instead of value alignment: a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair"" in precisely the sense that it encourages an alignment of interests between humans and machines.",,ACM/AIES 2018,10.1145/3278721.3278730,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1805.11447v1,Virtuously Safe Reinforcement Learning,"['Henrik Aslund', 'El Mahdi El Mhamdi', 'Rachid Guerraoui', 'Alexandre Maurer']",2018-05-29 13:34:39+00:00,arxiv,...,36b646b2edddb01739541180e476d187,html,markdownify,2018-05-29 13:34:39+00:00,"We show that when a third party, the adversary, steps into the two-party setting (agent and operator) of safely interruptible reinforcement learning, a trade-off has to be made between the probability of following the optimal policy in the limit, and the probability of escaping a dangerous situation created by the adversary. So far, the work on safely interruptible agents has assumed a perfect perception of the agent about its environment (no adversary), and therefore implicitly set the second probability to zero, by explicitly seeking a value of one for the first probability. We show that (1) agents can be made both interruptible and adversary-resilient, and (2) the interruptibility can be made safe in the sense that the agent itself will not seek to avoid it. We also solve the problem that arises when the agent does not go completely greedy, i.e. issues with safe exploration in the limit. Resilience to perturbed perception, safe exploration in the limit, and safe interruptibility are the three pillars of what we call \emph{virtuously safe reinforcement learning}.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']"
https://arxiv.org/abs/1807.06919v5,"Backplay: ""Man muss immer umkehren""","['Cinjon Resnick', 'Roberta Raileanu', 'Sanyam Kapoor', 'Alexander Peysakhovich', 'Kyunghyun Cho', 'Joan Bruna']",2018-07-18 13:28:59+00:00,arxiv,...,22151977f71f59ef31576026d3dfcb21,html,markdownify,2022-04-21 14:03:32+00:00,"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.","AAAI-19 Workshop on Reinforcement Learning in Games;
  0xd1a80a702b8170f6abeaabcf32a0c4c4401e9177",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1809.10283v3,Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees,"['Christopher Iliffe Sprague', 'Petter Ãgren']",2018-09-26 12:23:19+00:00,arxiv,...,efa3e33c9115379243c04cff762a6d94,html,markdownify,2022-07-25 09:14:25+00:00,"In this paper, we show how Behavior Trees that have performance guarantees, in terms of safety and goal convergence, can be extended with components that were designed using machine learning, without destroying those performance guarantees.   Machine learning approaches such as reinforcement learning or learning from demonstration can be very appealing to AI designers that want efficient and realistic behaviors in their agents. However, those algorithms seldom provide guarantees for solving the given task in all different situations while keeping the agent safe. Instead, such guarantees are often easier to find for manually designed model-based approaches. In this paper we exploit the modularity of behavior trees to extend a given design with an efficient, but possibly unreliable, machine learning component in a way that preserves the guarantees. The approach is illustrated with an inverted pendulum example.","Accepted as Regular Paper to The 61th IEEE Conference on Decision and
  Control (CDC 2022)",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.NE']"
https://arxiv.org/abs/1810.06519v2,Factorized Machine Self-Confidence for Decision-Making Agents,"['Brett W Israelsen', 'Nisar R Ahmed', 'Eric Frew', 'Dale Lawrence', 'Brian Argrow']",2018-10-15 17:06:38+00:00,arxiv,...,e17788ae8ba5c1c29f03d65d66564b88,html,markdownify,2019-01-09 18:31:04+00:00,"Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately. Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance. The idea of `machine self-confidence' is introduced for autonomous systems. Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems. Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems. A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes. A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models. Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter, and environment conditions indicate that the self-confidence metric exhibits the desired properties. Discussion of results, and avenues for future investigation are included.","title change, leaving as stand-alone tech report",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY', 'cs.HC', 'cs.RO']"
https://arxiv.org/abs/1811.06284v1,Guiding the One-to-one Mapping in CycleGAN via Optimal Transport,"['Guansong Lu', 'Zhiming Zhou', 'Yuxuan Song', 'Kan Ren', 'Yong Yu']",2018-11-15 10:34:33+00:00,arxiv,...,a7c38921eaa79ac48998ba3ea62fc7b9,html,markdownify,2018-11-15 10:34:33+00:00,"CycleGAN is capable of learning a one-to-one mapping between two data distributions without paired examples, achieving the task of unsupervised data translation. However, there is no theoretical guarantee on the property of the learned one-to-one mapping in CycleGAN. In this paper, we experimentally find that, under some circumstances, the one-to-one mapping learned by CycleGAN is just a random one within the large feasible solution space. Based on this observation, we explore to add extra constraints such that the one-to-one mapping is controllable and satisfies more properties related to specific tasks. We propose to solve an optimal transport mapping restrained by a task-specific cost function that reflects the desired properties, and use the barycenters of optimal transport mapping to serve as references for CycleGAN. Our experiments indicate that the proposed algorithm is capable of learning a one-to-one mapping with the desired properties.","The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI
  2019)",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1901.03327v1,A New Tensioning Method using Deep Reinforcement Learning for Surgical Pattern Cutting,"['Thanh Thi Nguyen', 'Ngoc Duy Nguyen', 'Fernando Bello', 'Saeid Nahavandi']",2019-01-10 13:30:46+00:00,arxiv,...,c6e9bf6ac0ff271badaf8190d5e465be,html,markdownify,2019-01-10 13:30:46+00:00,"Surgeons normally need surgical scissors and tissue grippers to cut through a deformable surgical tissue. The cutting accuracy depends on the skills to manipulate these two tools. Such skills are part of basic surgical skills training as in the Fundamentals of Laparoscopic Surgery. The gripper is used to pinch a point on the surgical sheet and pull the tissue to a certain direction to maintain the tension while the scissors cut through a trajectory. As the surgical materials are deformable, it requires a comprehensive tensioning policy to yield appropriate tensioning direction at each step of the cutting process. Automating a tensioning policy for a given cutting trajectory will support not only the human surgeons but also the surgical robots to improve the cutting accuracy and reliability. This paper presents a multiple pinch point approach to modelling an autonomous tensioning planner based on a deep reinforcement learning algorithm. Experiments on a simulator show that the proposed method is superior to existing methods in terms of both performance and robustness.","2019 IEEE International Conference on Industrial Technology (ICIT),
  Melbourne, Australia (to appear)",2019 IEEE International Conference on Industrial Technology (ICIT),10.1109/ICIT.2019.8755235,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'stat.ML']"
https://arxiv.org/abs/1903.01003v3,Hacking Google reCAPTCHA v3 using Reinforcement Learning,"['Ismail Akrout', 'Amal Feriani', 'Mohamed Akrout']",2019-03-03 22:10:47+00:00,arxiv,...,9aff7a67a29e678367dd7aa6be6ae7ee,html,markdownify,2019-04-18 16:22:33+00:00,"We present a Reinforcement Learning (RL) methodology to bypass Google reCAPTCHA v3. We formulate the problem as a grid world where the agent learns how to move the mouse and click on the reCAPTCHA button to receive a high score. We study the performance of the agent when we vary the cell size of the grid world and show that the performance drops when the agent takes big steps toward the goal. Finally, we used a divide and conquer strategy to defeat the reCAPTCHA system for any grid resolution. Our proposed method achieves a success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen resolution.","Accepted for the Conference on Reinforcement Learning and Decision
  Making (RLDM) 2019",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1904.07451v2,Counterfactual Visual Explanations,"['Yash Goyal', 'Ziyan Wu', 'Jan Ernst', 'Dhruv Batra', 'Devi Parikh', 'Stefan Lee']",2019-04-16 04:16:11+00:00,arxiv,...,b5058f3e6023a205f3710a2b043ff10c,html,markdownify,2019-06-11 16:49:55+00:00,"In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a 'distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/1905.05675v1,The Algonauts Project: A Platform for Communication between the Sciences of Biological and Artificial Intelligence,"['Radoslaw Martin Cichy', 'Gemma Roig', 'Alex Andonian', 'Kshitij Dwivedi', 'Benjamin Lahner', 'Alex Lascelles', 'Yalda Mohsenzadeh', 'Kandan Ramakrishnan', 'Aude Oliva']",2019-05-14 15:37:22+00:00,arxiv,...,de88d27364cd3c64f3e317242207671c,html,markdownify,2019-05-14 15:37:22+00:00,"In the last decade, artificial intelligence (AI) models inspired by the brain have made unprecedented progress in performing real-world perceptual tasks like object classification and speech recognition. Recently, researchers of natural intelligence have begun using those AI models to explore how the brain performs such tasks. These developments suggest that future progress will benefit from increased interaction between disciplines. Here we introduce the Algonauts Project as a structured and quantitative communication channel for interdisciplinary interaction between natural and artificial intelligence researchers. The project's core is an open challenge with a quantitative benchmark whose goal is to account for brain data through computational models. This project has the potential to provide better models of natural intelligence and to gather findings that advance AI. The 2019 Algonauts Project focuses on benchmarking computational models predicting human brain activity when people look at pictures of objects. The 2019 edition of the Algonauts Project is available online: http://algonauts.csail.mit.edu/.","4 pages, 2 figures",,,cs.CV,"['cs.CV', 'cs.AI', 'q-bio.NC']"
https://arxiv.org/abs/1906.02299v1,Teaching AI to Explain its Decisions Using Embeddings and Multi-Task Learning,"['Noel C. F. Codella', 'Michael Hind', 'Karthikeyan Natesan Ramamurthy', 'Murray Campbell', 'Amit Dhurandhar', 'Kush R. Varshney', 'Dennis Wei', 'Aleksandra MojsiloviÄ']",2019-06-05 20:42:14+00:00,arxiv,...,bf22ea3cf9996ffcda80b88913af6901,html,markdownify,2019-06-05 20:42:14+00:00,"Using machine learning in high-stakes applications often requires predictions to be accompanied by explanations comprehensible to the domain user, who has ultimate responsibility for decisions and outcomes. Recently, a new framework for providing explanations, called TED, has been proposed to provide meaningful explanations for predictions. This framework augments training data to include explanations elicited from domain users, in addition to features and labels. This approach ensures that explanations for predictions are tailored to the complexity expectations and domain knowledge of the consumer. In this paper, we build on this foundational work, by exploring more sophisticated instantiations of the TED framework and empirically evaluate their effectiveness in two diverse domains, chemical odor and skin cancer prediction. Results demonstrate that meaningful explanations can be reliably taught to machine learning algorithms, and in some cases, improving modeling accuracy.","presented at 2019 ICML Workshop on Human in the Loop Learning (HILL
  2019), Long Beach, USA. arXiv admin note: substantial text overlap with
  arXiv:1805.11648",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1907.09106v1,"A Conceptually Well-Founded Characterization of Iterated Admissibility Using an ""All I Know"" Operator","['Joseph Y. Halpern', 'Rafael Pass']",2019-07-22 03:16:38+00:00,arxiv,...,82c0af9d0675a2ab1e23582a5a23985e,html,markdownify,2019-07-22 03:16:38+00:00,"Brandenburger, Friedenberg, and Keisler provide an epistemic characterization of iterated admissibility (IA), also known as iterated deletion of weakly dominated strategies, where uncertainty is represented using LPSs (lexicographic probability sequences). Their characterization holds in a rich structure called a complete structure, where all types are possible. In earlier work, we gave a characterization of iterated admissibility using an ""all I know"" operator, that captures the intuition that ""all the agent knows"" is that agents satisfy the appropriate rationality assumptions. That characterization did not need complete structures and used probability structures, not LPSs. However, that characterization did not deal with Samuelson's conceptual concern regarding IA, namely, that at higher levels, players do not consider possible strategies that were used to justify their choice of strategy at lower levels. In this paper, we give a characterization of IA using the all I know operator that does deal with Samuelson's concern. However, it uses LPSs. We then show how to modify the characterization using notions of ""approximate belief"" and ""approximately all I know"" so as to deal with Samuelson's concern while still working with probability structures.","In Proceedings TARK 2019, arXiv:1907.08335","EPTCS 297, 2019, pp. 221-232",10.4204/EPTCS.297.15,cs.GT,"['cs.GT', 'cs.AI', 'cs.LO']"
https://arxiv.org/abs/1909.07528v2,Emergent Tool Use From Multi-Agent Autocurricula,"['Bowen Baker', 'Ingmar Kanitscheider', 'Todor Markov', 'Yi Wu', 'Glenn Powell', 'Bob McGrew', 'Igor Mordatch']",2019-09-17 00:17:02+00:00,arxiv,...,86b9db66725c40dbe318cd222f2aa931,html,markdownify,2020-02-11 00:56:50+00:00,"Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']"
https://arxiv.org/abs/1909.08068v1,From the Internet of Information to the Internet of Intelligence,['F. Richard Yu'],2019-08-30 22:50:10+00:00,arxiv,...,387f511a7221baa786a20421a82426ba,html,markdownify,2019-08-30 22:50:10+00:00,"In the era of the Internet of information, we have gone through layering, cross-layer, and cross-system design paradigms. Recently, the ``curse of modeling"" and ``curse of dimensionality"" of the cross-system design paradigm have resulted in the popularity of using artificial intelligence (AI) to optimize the Internet of information. However, many significant research challenges remain to be addressed for the AI approach, including the lack of high-quality training data due to privacy and resources constraints in this data-driven approach. To address these challenges, we need to take a look at humans' cooperation in a larger time scale. To facilitate cooperation in modern history, we have built three major technologies: ``grid of transportation"", ``grid of energy"", and ``the Internet of information"". In this paper, we argue that the next cooperation paradigm could be the ``Internet of intelligence (Intelligence-Net)"", where intelligence can be easily obtained like energy and information, enabled by the recent advances in blockchain technology. We present some recent advances in these areas, and discuss some open issues and challenges that need to be addressed in the future.",,,,cs.NI,"['cs.NI', 'cs.AI']"
https://arxiv.org/abs/1910.02330v2,Towards Deployment of Robust AI Agents for Human-Machine Partnerships,"['Ahana Ghosh', 'Sebastian Tschiatschek', 'Hamed Mahdavi', 'Adish Singla']",2019-10-05 21:04:27+00:00,arxiv,...,095976c60497fa7a08d566eab2c1bd53,html,markdownify,2020-06-15 23:19:40+00:00,"We study the problem of designing AI agents that can robustly cooperate with people in human-machine partnerships. Our work is inspired by real-life scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate with new users after its deployment. We model this problem via a parametric MDP framework where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of unknown type. Our approach to designing a robust AI agent relies on observing the user's actions to make inferences about the user's type and adapting its policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. We develop two algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our approach in solving a two-agent collaborative task.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1910.04098v2,Improving Generalization in Meta Reinforcement Learning using Learned Objectives,"['Louis Kirsch', 'Sjoerd van Steenkiste', 'JÃ¼rgen Schmidhuber']",2019-10-09 16:20:48+00:00,arxiv,...,bdaaef436e9b1485a4c1221b0a30239b,html,markdownify,2020-02-14 16:56:33+00:00,"Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.",Accepted to ICLR 2020,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML', 'I.2.6']"
https://arxiv.org/abs/1910.04365v1,Asking Easy Questions: A User-Friendly Approach to Active Reward Learning,"['Erdem BÄ±yÄ±k', 'Malayandi Palan', 'Nicholas C. Landolfi', 'Dylan P. Losey', 'Dorsa Sadigh']",2019-10-10 04:52:46+00:00,arxiv,...,4cb5e8a4bb6434aed9833cd104c2b3f2,html,markdownify,2019-10-10 04:52:46+00:00,"Robots can learn the right reward function by querying a human expert. Existing approaches attempt to choose questions where the robot is most uncertain about the human's response; however, they do not consider how easy it will be for the human to answer! In this paper we explore an information gain formulation for optimally selecting questions that naturally account for the human's ability to answer. Our approach identifies questions that optimize the trade-off between robot and human uncertainty, and determines when these questions become redundant or costly. Simulations and a user study show our method not only produces easy questions, but also ultimately results in faster reward learning.","Proceedings of the 3rd Conference on Robot Learning (CoRL), October
  2019",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1910.06266v1,Using AI/ML to gain situational understanding from passive network observations,"['D. Verma', 'S. Calo']",2019-10-14 16:46:33+00:00,arxiv,...,21eb6c5d34ef62f3866f479c428db427,html,markdownify,2019-10-14 16:46:33+00:00,"The data available in the network traffic fromany Government building contains a significant amount ofinformation. An analysis of the traffic can yield insightsand situational understanding about what is happening inthe building. However, the use of traditional network packet inspection, either deep or shallow, is useful for only a limited understanding of the environment, with applicability limited to some aspects of network and security management. If weuse AI/ML based techniques to understand the network traffic, we can gain significant insights which increase our situational awareness of what is happening in the environment.At IBM, we have created a system which uses a combination of network domain knowledge and machine learning techniques to convert network traffic into actionable insights about the on premise environment. These insights include characterization of the communicating devices, discovering unauthorized devices that may violate policy requirements, identifying hidden components and vulnerability points, detecting leakage of sensitive information, and identifying the presence of people and devices.In this paper, we will describe the overall design of this system, the major use-cases that have been identified for it, and the lessons learnt when deploying this system for some of those use-cases","Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA",,,cs.CR,"['cs.CR', 'cs.AI']"
https://arxiv.org/abs/1910.06428v1,Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks,"['Bairavi Venkatesh', 'Tosha Shah', 'Antong Chen', 'Soheil Ghafurian']",2019-10-14 21:22:54+00:00,arxiv,...,130d4a32ffe8e356bd264d31811c4aa2,html,markdownify,2019-10-14 21:22:54+00:00,"It is common for pathologists to annotate specific regions of the tissue, such as tumor, directly on the glass slide with markers. Although this practice was helpful prior to the advent of histology whole slide digitization, it often occludes important details which are increasingly relevant to immuno-oncology due to recent advancements in digital pathology imaging techniques. The current work uses a generative adversarial network with cycle loss to remove these annotations while still maintaining the underlying structure of the tissue by solving an image-to-image translation problem. We train our network on up to 300 whole slide images with marker inks and show that 70% of the corrected image patches are indistinguishable from originally uncontaminated image tissue to a human expert. This portion increases 97% when we replace the human expert with a deep residual network. We demonstrated the fidelity of the method to the original image by calculating the correlation between image gradient magnitudes. We observed a revival of up to 94,000 nuclei per slide in our dataset, the majority of which were located on tissue border.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']"
https://arxiv.org/abs/1910.09508v1,Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination,"['Dongge Han', 'Wendelin Boehmer', 'Michael Wooldridge', 'Alex Rogers']",2019-10-21 16:54:49+00:00,arxiv,...,c28d4c4b4769de12e4846425de9948d2,html,markdownify,2019-10-21 16:54:49+00:00,"In a multi-agent system, an agent's optimal policy will typically depend on the policies chosen by others. Therefore, a key issue in multi-agent systems research is that of predicting the behaviours of others, and responding promptly to changes in such behaviours. One obvious possibility is for each agent to broadcast their current intention, for example, the currently executed option in a hierarchical reinforcement learning framework. However, this approach results in inflexibility of agents if options have an extended duration and are dynamic. While adjusting the executed option at each step improves flexibility from a single-agent perspective, frequent changes in options can induce inconsistency between an agent's actual behaviour and its broadcast intention. In order to balance flexibility and predictability, we propose a dynamic termination Bellman equation that allows the agents to flexibly terminate their options. We evaluate our model empirically on a set of multi-agent pursuit and taxi tasks, and show that our agents learn to adapt flexibly across scenarios that require different termination behaviours.",PRICAI 2019,,10.1007/978-3-030-29911-8_7,cs.MA,"['cs.MA', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1911.00061v1,DeepLine: AutoML Tool for Pipelines Generation using Deep Reinforcement Learning and Hierarchical Actions Filtering,"['Yuval Heffetz', 'Roman Vainstein', 'Gilad Katz', 'Lior Rokach']",2019-10-31 19:06:14+00:00,arxiv,...,01602b17c85ddbf7ad53c4f9ab52007e,html,markdownify,2019-10-31 19:06:14+00:00,"Automatic machine learning (AutoML) is an area of research aimed at automating machine learning (ML) activities that currently require human experts. One of the most challenging tasks in this field is the automatic generation of end-to-end ML pipelines: combining multiple types of ML algorithms into a single architecture used for end-to-end analysis of previously-unseen data. This task has two challenging aspects: the first is the need to explore a large search space of algorithms and pipeline architectures. The second challenge is the computational cost of training and evaluating multiple pipelines. In this study we present DeepLine, a reinforcement learning based approach for automatic pipeline generation. Our proposed approach utilizes an efficient representation of the search space and leverages past knowledge gained from previously-analyzed datasets to make the problem more tractable. Additionally, we propose a novel hierarchical-actions algorithm that serves as a plugin, mediating the environment-agent interaction in deep reinforcement learning problems. The plugin significantly speeds up the training process of our model. Evaluation on 56 datasets shows that DeepLine outperforms state-of-the-art approaches both in accuracy and in computational cost.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1911.00226v1,Generating Justifications for Norm-Related Agent Decisions,"['Daniel Kasenberg', 'Antonio Roque', 'Ravenna Thielstrom', 'Meia Chita-Tegmark', 'Matthias Scheutz']",2019-11-01 06:53:12+00:00,arxiv,...,f851fd444ba2f7b7e34d01fefcf2ac0c,html,markdownify,2019-11-01 06:53:12+00:00,"We present an approach to generating natural language justifications of decisions derived from norm-based reasoning. Assuming an agent which maximally satisfies a set of rules specified in an object-oriented temporal logic, the user can ask factual questions (about the agent's rules, actions, and the extent to which the agent violated the rules) as well as ""why"" questions that require the agent comparing actual behavior to counterfactual trajectories with respect to these rules. To produce natural-sounding explanations, we focus on the subproblem of producing natural language clauses from statements in a fragment of temporal logic, and then describe how to embed these clauses into explanatory sentences. We use a human judgment evaluation on a testbed task to compare our approach to variants in terms of intelligibility, mental model and perceived trust.","Accepted to the Proceedings of the 12th International Conference on
  Natural Language Generation (INLG 2019)",,,cs.CL,"['cs.CL', 'cs.AI']"
https://arxiv.org/abs/1911.13152v1,Induction of Subgoal Automata for Reinforcement Learning,"['Daniel Furelos-Blanco', 'Mark Law', 'Alessandra Russo', 'Krysia Broda', 'Anders Jonsson']",2019-11-29 15:28:54+00:00,arxiv,...,cd5329c5569156b448580204b2674b0c,html,markdownify,2019-11-29 15:28:54+00:00,"In this work we present ISA, a novel approach for learning and exploiting subgoals in reinforcement learning (RL). Our method relies on inducing an automaton whose transitions are subgoals expressed as propositional formulas over a set of observable events. A state-of-the-art inductive logic programming system is used to learn the automaton from observation traces perceived by the RL agent. The reinforcement learning and automaton learning processes are interleaved: a new refined automaton is learned whenever the RL agent generates a trace not recognized by the current automaton. We evaluate ISA in several gridworld problems and show that it performs similarly to a method for which automata are given in advance. We also show that the learned automata can be exploited to speed up convergence through reward shaping and transfer learning across multiple tasks. Finally, we analyze the running time and the number of traces that ISA needs to learn an automata, and the impact that the number of observable events has on the learner's performance.","Preprint accepted for publication to the 34th AAAI Conference on
  Artificial Intelligence (AAAI-20)",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.LO', 'stat.ML']"
https://arxiv.org/abs/1912.00782v2,The relationship between trust in AI and trustworthy machine learning technologies,"['Ehsan Toreini', 'Mhairi Aitken', 'Kovila Coopamootoo', 'Karen Elliott', 'Carlos Gonzalez Zelaya', 'Aad van Moorsel']",2019-11-27 16:36:13+00:00,arxiv,...,06a7f7ce2c9788756e320e584b99e304,html,markdownify,2019-12-03 11:59:43+00:00,"To build AI-based systems that users and the public can justifiably trust one needs to understand how machine learning technologies impact trust put in these services. To guide technology developments, this paper provides a systematic approach to relate social science concepts of trust with the technologies used in AI-based services and products. We conceive trust as discussed in the ABI (Ability, Benevolence, Integrity) framework and use a recently proposed mapping of ABI on qualities of technologies. We consider four categories of machine learning technologies, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these possess the required qualities. Trust can be impacted throughout the life cycle of AI-based systems, and we introduce the concept of Chain of Trust to discuss technological needs for trust in different stages of the life cycle. FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international Principled AI policy and technology frameworks that have emerged in recent years.",This submission has been accepted in ACM FAT* 2020 Conference,,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1912.01188v2,Adaptive Online Planning for Continual Lifelong Learning,"['Kevin Lu', 'Igor Mordatch', 'Pieter Abbeel']",2019-12-03 04:29:01+00:00,arxiv,...,1da22784eb4b41de3a74e3ba067dc8e5,html,markdownify,2020-06-27 05:28:56+00:00,"We study learning control in an online reset-free lifelong learning scenario, where mistakes can compound catastrophically into the future and the underlying dynamics of the environment may change. Traditional model-free policy learning methods have achieved successes in difficult tasks due to their broad flexibility, but struggle in this setting, as they can activate failure modes early in their lifetimes which are difficult to recover from and face performance degradation as dynamics change. On the other hand, model-based planning methods learn and adapt quickly, but require prohibitive levels of computational resources. We present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By approximating the uncertainty of the model-free components and the planner performance, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times, while still gracefully adapting behaviors in the face of unpredictable changes in the world -- even when traditional RL fails.",Originally published in NeurIPS Deep RL 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/1912.05284v1,Interactive AI with a Theory of Mind,"['Mustafa Mert Ãelikok', 'Tomi Peltola', 'Pedram Daee', 'Samuel Kaski']",2019-12-01 19:26:48+00:00,arxiv,...,2cbd79dd72e52609575386aacd11a02b,html,markdownify,2019-12-01 19:26:48+00:00,"Understanding each other is the key to success in collaboration. For humans, attributing mental states to others, the theory of mind, provides the crucial advantage. We argue for formulating human--AI interaction as a multi-agent problem, endowing AI with a computational theory of mind to understand and anticipate the user. To differentiate the approach from previous work, we introduce a categorisation of user modelling approaches based on the level of agency learnt in the interaction. We describe our recent work in using nested multi-agent modelling to formulate user models for multi-armed bandit based interactive AI systems, including a proof-of-concept user study.","This is a slightly updated version of a manuscript that appeared in
  ACM CHI 2019 Workshop: Computational Modeling in Human-Computer Interaction",,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/1912.05453v1,Value-of-Information based Arbitration between Model-based and Model-free Control,"['Krishn Bera', 'Yash Mandilwar', 'Bapi Raju']",2019-12-08 07:26:33+00:00,arxiv,...,181f51880714223911578341c2d34f02,html,markdownify,2019-12-08 07:26:33+00:00,"There have been numerous attempts in explaining the general learning behaviours using model-based and model-free methods. While the model-based control is flexible yet computationally expensive in planning, the model-free control is quick but inflexible. The model-based control is therefore immune from reward devaluation and contingency degradation. Multiple arbitration schemes have been suggested to achieve the data efficiency and computational efficiency of model-based and model-free control respectively. In this context, we propose a quantitative 'value of information' based arbitration between both the controllers in order to establish a general computational framework for skill learning. The interacting model-based and model-free reinforcement learning processes are arbitrated using an uncertainty-based value of information. We further show that our algorithm performs better than Q-learning as well as Q-learning with experience replay.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1912.11095v1,Defining AI in Policy versus Practice,"['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Karen Huang', 'Ghislain Bugingo']",2019-12-23 20:18:21+00:00,arxiv,...,2815b85722fe5857b7b581a73a503f71,html,markdownify,2019-12-23 20:18:21+00:00,"Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.",,,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'physics.soc-ph']"
https://arxiv.org/abs/2001.06691v1,Teaching Software Engineering for AI-Enabled Systems,"['Christian KÃ¤stner', 'Eunsuk Kang']",2020-01-18 15:24:17+00:00,arxiv,...,bcd0b62303b0bbbb620754d699066f12,html,markdownify,2020-01-18 15:24:17+00:00,"Software engineers have significant expertise to offer when building intelligent systems, drawing on decades of experience and methods for building systems that are scalable, responsive and robust, even when built on unreliable components. Systems with artificial-intelligence or machine-learning (ML) components raise new challenges and require careful engineering. We designed a new course to teach software-engineering skills to students with a background in ML. We specifically go beyond traditional ML courses that teach modeling techniques under artificial conditions and focus, in lecture and assignments, on realism with large and changing datasets, robust and evolvable infrastructure, and purposeful requirements engineering that considers ethics and fairness as well. We describe the course and our infrastructure and share experience and all material from teaching the course for the first time.",to be published in ICSE-SEET 2020,,,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2001.07455v1,Designing for the Long Tail of Machine Learning,"['Martin Lindvall', 'Jesper Molin']",2020-01-21 11:53:28+00:00,arxiv,...,b4045b660a81317d38087902f7aee6be,html,markdownify,2020-01-21 11:53:28+00:00,"Recent technical advances has made machine learning (ML) a promising component to include in end user facing systems. However, user experience (UX) practitioners face challenges in relating ML to existing user-centered design processes and how to navigate the possibilities and constraints of this design space. Drawing on our own experience, we characterize designing within this space as navigating trade-offs between data gathering, model development and designing valuable interactions for a given model performance. We suggest that the theoretical description of how machine learning performance scales with training data can guide designers in these trade-offs as well as having implications for prototyping. We exemplify the learning curve's usage by arguing that a useful pattern is to design an initial system in a bootstrap phase that aims to exploit the training effect of data collected at increasing orders of magnitude.","Accepted for presentation in poster format for the ACM CHI'19
  Workshop <Emerging Perspectives in Human-Centered Machine Learning>",,,cs.HC,"['cs.HC', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2001.07522v2,Engineering AI Systems: A Research Agenda,"['Jan Bosch', 'Ivica Crnkovic', 'Helena HolmstrÃ¶m Olsson']",2020-01-16 20:29:48+00:00,arxiv,...,f31cd7cc76f67b17e527de6b29d6525e,html,markdownify,2020-06-03 12:59:36+00:00,"Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry, However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this paper, we provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that we have studied. The main contribution of the paper is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.","8 pages, 4 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']"
https://arxiv.org/abs/2001.08016v1,Subjective Knowledge and Reasoning about Agents in Multi-Agent Systems,"['Shikha Singh', 'Deepak Khemani']",2020-01-22 13:50:26+00:00,arxiv,...,e652da6d545619edf329e1695a75d11a,html,markdownify,2020-01-22 13:50:26+00:00,"Though a lot of work in multi-agent systems is focused on reasoning about knowledge and beliefs of artificial agents, an explicit representation and reasoning about the presence/absence of agents, especially in the scenarios where agents may be unaware of other agents joining in or going offline in a multi-agent system, leading to partial knowledge/asymmetric knowledge of the agents is mostly overlooked by the MAS community. Such scenarios lay the foundations of cases where an agent can influence other agents' mental states by (mis)informing them about the presence/absence of collaborators or adversaries. In this paper, we investigate how Kripke structure-based epistemic models can be extended to express the above notion based on an agent's subjective knowledge and we discuss the challenges that come along.",,,,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/2003.06507v3,The Conflict Between People's Urge to Punish AI and Legal Systems,"['Gabriel Lima', 'Meeyoung Cha', 'Chihyung Jeon', 'Kyungsin Park']",2020-03-13 23:19:58+00:00,arxiv,...,aae8643f04680aa3104ebe98a3de58b5,html,markdownify,2021-11-11 02:36:45+00:00,"Regulating artificial intelligence (AI) has become necessary in light of its deployment in high-risk scenarios. This paper explores the proposal to extend legal personhood to AI and robots, which had not yet been examined through the lens of the general public. We present two studies (N = 3,559) to obtain people's views of electronic legal personhood vis-\`a-vis existing liability models. Our study reveals people's desire to punish automated agents even though these entities are not recognized any mental state. Furthermore, people did not believe automated agents' punishment would fulfill deterrence nor retribution and were unwilling to grant them legal punishment preconditions, namely physical independence and assets. Collectively, these findings suggest a conflict between the desire to punish automated agents and its perceived impracticability. We conclude by discussing how future design and legal decisions may influence how the public reacts to automated agents' wrongdoings.","Published at Frontiers in AI and Robots - Ethics in Robotics and
  Artificial Intelligence Section",,10.3389/frobt.2021.756242,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2006.06870v4,Multi-Agent Informational Learning Processes,"['J. K. Terry', 'Nathaniel Grammel']",2020-06-11 23:18:50+00:00,arxiv,...,e1b618b3df0f88425bafc8b82c7b6d1c,html,markdownify,2021-02-25 21:43:47+00:00,"We introduce a new mathematical model of multi-agent reinforcement learning, the Multi-Agent Informational Learning Processor ""MAILP"" model. The model is based on the notion that agents have policies for a certain amount of information, models how this information iteratively evolves and propagates through many agents. This model is very general, and the only meaningful assumption made is that learning for individual agents progressively slows over time.","We are withdrawing this paper as section 2.1.1 implicitly assumes
  information gain at all points is homogenous. A researcher has provided us an
  example showing that this assumption causes our model to make unexpected and
  pathological predictions, and we are aware of now way to remove this
  assumption from our work",,,cs.MA,"['cs.MA', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2006.09436v1,SAMBA: Safe Model-Based & Active Reinforcement Learning,"['Alexander I. Cowen-Rivers', 'Daniel Palenicek', 'Vincent Moens', 'Mohammed Abdullah', 'Aivar Sootla', 'Jun Wang', 'Haitham Ammar']",2020-06-12 10:40:46+00:00,arxiv,...,575df2961ced4103ebd1d1859d7be1d5,html,markdownify,2020-06-12 10:40:46+00:00,"In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel(semi-)metrics for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our active metrics and safety constraints.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
https://arxiv.org/abs/2007.02092v2,Customized Handling of Unintended Interface Operation in Assistive Robots,"['Deepak Gopinath', 'Mahdieh Nejati Javaremi', 'Brenna D. Argall']",2020-07-04 13:23:22+00:00,arxiv,...,4f22140931d217633c8df790e9b0ee4e,html,markdownify,2020-11-05 20:03:43+00:00,"We present an assistance system that reasons about a human's intended actions during robot teleoperation in order to provide appropriate corrections for unintended behavior. We model the human's physical interaction with a control interface during robot teleoperation and distinguish between intended and measured physical actions explicitly. By reasoning over the unobserved intentions using model-based inference techniques, our assistive system provides customized corrections on a user's issued commands. We validate our algorithm with a 10-person human subject study in which we evaluate the performance of the proposed assistance paradigms. Our results show that the assistance paradigms helped to significantly reduce task completion time, number of mode switches, cognitive workload, and user frustration and improve overall user satisfaction.","10 pages, 7 figures, preprint",,,cs.RO,"['cs.RO', 'cs.AI']"
https://arxiv.org/abs/2008.01339v1,Collecting the Public Perception of AI and Robot Rights,"['Gabriel Lima', 'Changyeon Kim', 'Seungho Ryu', 'Chihyung Jeon', 'Meeyoung Cha']",2020-08-04 05:35:29+00:00,arxiv,...,b9615896be135e0e1a4f4661c4fde210,html,markdownify,2020-08-04 05:35:29+00:00,"Whether to give rights to artificial intelligence (AI) and robots has been a sensitive topic since the European Parliament proposed advanced robots could be granted ""electronic personalities."" Numerous scholars who favor or disfavor its feasibility have participated in the debate. This paper presents an experiment (N=1270) that 1) collects online users' first impressions of 11 possible rights that could be granted to autonomous electronic agents of the future and 2) examines whether debunking common misconceptions on the proposal modifies one's stance toward the issue. The results indicate that even though online users mainly disfavor AI and robot rights, they are supportive of protecting electronic agents from cruelty (i.e., favor the right against cruel treatment). Furthermore, people's perceptions became more positive when given information about rights-bearing non-human entities or myth-refuting statements. The style used to introduce AI and robot rights significantly affected how the participants perceived the proposal, similar to the way metaphors function in creating laws. For robustness, we repeated the experiment over a more representative sample of U.S. residents (N=164) and found that perceptions gathered from online users and those by the general population are similar.",Conditionally Accepted to ACM CSCW 2020,,,cs.CY,"['cs.CY', 'cs.AI', 'cs.RO']"
https://arxiv.org/abs/2008.09043v1,"Considerations, Good Practices, Risks and Pitfalls in Developing AI Solutions Against COVID-19","['Alexandra Luccioni', 'Joseph Bullock', 'Katherine Hoffmann Pham', 'Cynthia Sin Nga Lam', 'Miguel Luengo-Oroz']",2020-08-13 12:37:37+00:00,arxiv,...,a75a0e338ab4ed77e80a39ba13c43776,html,markdownify,2020-08-13 12:37:37+00:00,"The COVID-19 pandemic has been a major challenge to humanity, with 12.7 million confirmed cases as of July 13th, 2020 [1]. In previous work, we described how Artificial Intelligence can be used to tackle the pandemic with applications at the molecular, clinical, and societal scales [2]. In the present follow-up article, we review these three research directions, and assess the level of maturity and feasibility of the approaches used, as well as their potential for operationalization. We also summarize some commonly encountered risks and practical pitfalls, as well as guidelines and best practices for formulating and deploying AI applications at different scales.","4 pages, 1 figure","Harvard CRCS Workshop on AI for Social Good, United States, 2020",,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'cs.SI']"
https://arxiv.org/abs/2008.09293v2,A Composable Specification Language for Reinforcement Learning Tasks,"['Kishor Jothimurugan', 'Rajeev Alur', 'Osbert Bastani']",2020-08-21 03:40:57+00:00,arxiv,...,4fae4837fad82821e2039c66208fb013,html,markdownify,2020-10-29 15:02:43+00:00,"Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.",,"In Advances in Neural Information Processing Systems, pp.
  13041-13051. 2019",,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2008.12566v2,How Researchers Use Diagrams in Communicating Neural Network Systems,"['Guy Clarke Marshall', 'AndrÃ© Freitas', 'Caroline Jay']",2020-08-28 10:21:03+00:00,arxiv,...,517e81e68fe74a053f3cb6e42468e3db,html,markdownify,2020-08-31 09:59:39+00:00,"Neural networks are a prevalent and effective machine learning component, and their application is leading to significant scientific progress in many domains. As the field of neural network systems is fast growing, it is important to understand how advances are communicated. Diagrams are key to this, appearing in almost all papers describing novel systems. This paper reports on a study into the use of neural network system diagrams, through interviews, card sorting, and qualitative feedback structured around ecologically-derived examples. We find high diversity of usage, perception and preference in both creation and interpretation of diagrams, examining this in the context of existing design, information visualisation, and user experience guidelines. Considering the interview data alongside existing guidance, we propose guidelines aiming to improve the way in which neural network system diagrams are constructed.","19 pages, 6 tables, 3 figures",,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/2009.04875v2,Importance Weighted Policy Learning and Adaptation,"['Alexandre Galashov', 'Jakub Sygnowski', 'Guillaume Desjardins', 'Jan Humplik', 'Leonard Hasenclever', 'Rae Jeong', 'Yee Whye Teh', 'Nicolas Heess']",2020-09-10 14:16:58+00:00,arxiv,...,c30ca98d39ae4e704803117d3c0edfc9,html,markdownify,2021-06-04 13:21:40+00:00,"The ability to exploit prior experience to solve novel problems rapidly is a hallmark of biological learning systems and of great practical importance for artificial ones. In the meta reinforcement learning literature much recent work has focused on the problem of optimizing the learning process itself. In this paper we study a complementary approach which is conceptually simple, general, modular and built on top of recent improvements in off-policy learning. The framework is inspired by ideas from the probabilistic inference literature and combines robust off-policy learning with a behavior prior, or default behavior that constrains the space of solutions and serves as a bias for exploration; as well as a representation for the value function, both of which are easily learned from a number of training tasks in a multi-task scenario. Our approach achieves competitive adaptation performance on hold-out tasks compared to meta reinforcement learning baselines and can scale to complex sparse-reward scenarios.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2010.02229v1,Learning to Generalize for Sequential Decision Making,"['Xusen Yin', 'Ralph Weischedel', 'Jonathan May']",2020-10-05 18:00:03+00:00,arxiv,...,4eaa594e58054f21e87f9febd4573b52,html,markdownify,2020-10-05 18:00:03+00:00,"We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7% on in-domain problems and 24% on out-of-domain problems.","Findings of EMNLP2020, 18 pages",,,cs.CL,"['cs.CL', 'cs.AI']"
https://arxiv.org/abs/2010.02419v1,Providing Actionable Feedback in Hiring Marketplaces using Generative Adversarial Networks,"['Daniel Nemirovsky', 'Nicolas Thiebaut', 'Ye Xu', 'Abhishek Gupta']",2020-10-06 01:26:00+00:00,arxiv,...,d5b21d9912254722e4e087a30fb21b53,html,markdownify,2020-10-06 01:26:00+00:00,"Machine learning predictors have been increasingly applied in production settings, including in one of the world's largest hiring platforms, Hired, to provide a better candidate and recruiter experience. The ability to provide actionable feedback is desirable for candidates to improve their chances of achieving success in the marketplace. Until recently, however, methods aimed at providing actionable feedback have been limited in terms of realism and latency. In this work, we demonstrate how, by applying a newly introduced method based on Generative Adversarial Networks (GANs), we are able to overcome these limitations and provide actionable feedback in real-time to candidates in production settings. Our experimental results highlight the significant benefits of utilizing a GAN-based approach on our dataset relative to two other state-of-the-art approaches (including over 1000x latency gains). We also illustrate the potential impact of this approach in detail on two real candidate profile examples.","5 pages, 2 figures",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2011.06619v1,Learning Latent Representations to Influence Multi-Agent Interaction,"['Annie Xie', 'Dylan P. Losey', 'Ryan Tolsma', 'Chelsea Finn', 'Dorsa Sadigh']",2020-11-12 19:04:26+00:00,arxiv,...,ed4cb69e545f05f675a3a3346fa8cb2f,html,markdownify,2020-11-12 19:04:26+00:00,"Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.","Conference on Robot Learning (CoRL) 2020. Supplementary website at
  https://sites.google.com/view/latent-strategies/",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2012.12469v4,Augmenting Policy Learning with Routines Discovered from a Single Demonstration,"['Zelin Zhao', 'Chuang Gan', 'Jiajun Wu', 'Xiaoxiao Guo', 'Joshua B. Tenenbaum']",2020-12-23 03:15:21+00:00,arxiv,...,3e3202a2135e0c174c81cf1131b942c5,html,markdownify,2021-05-02 06:55:54+00:00,"Humans can abstract prior knowledge from very little data and use it to boost skill learning. In this paper, we propose routine-augmented policy learning (RAPL), which discovers routines composed of primitive actions from a single demonstration and uses discovered routines to augment policy learning. To discover routines from the demonstration, we first abstract routine candidates by identifying grammar over the demonstrated action trajectory. Then, the best routines measured by length and frequency are selected to form a routine library. We propose to learn policy simultaneously at primitive-level and routine-level with discovered routines, leveraging the temporal structure of routines. Our approach enables imitating expert behavior at multiple temporal scales for imitation learning and promotes reinforcement learning exploration. Extensive experiments on Atari games demonstrate that RAPL improves the state-of-the-art imitation learning method SQIL and reinforcement learning method A2C. Further, we show that discovered routines can generalize to unseen levels and difficulties on the CoinRun benchmark.","AAAI 2021. Code is available at
  https://github.com/sjtuytc/AAAI21-RoutineAugmentedPolicyLearning",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']"
https://arxiv.org/abs/2101.02500v1,Bridging In- and Out-of-distribution Samples for Their Better Discriminability,"['Engkarat Techapanurak', 'Anh-Chuong Dang', 'Takayuki Okatani']",2021-01-07 11:34:18+00:00,arxiv,...,61d1f2ef48493b94d337c9e3a44b9372,html,markdownify,2021-01-07 11:34:18+00:00,"This paper proposes a method for OOD detection. Questioning the premise of previous studies that ID and OOD samples are separated distinctly, we consider samples lying in the intermediate of the two and use them for training a network. We generate such samples using multiple image transformations that corrupt inputs in various ways and with different severity levels. We estimate where the generated samples by a single image transformation lie between ID and OOD using a network trained on clean ID samples. To be specific, we make the network classify the generated samples and calculate their mean classification accuracy, using which we create a soft target label for them. We train the same network from scratch using the original ID samples and the generated samples with the soft labels created for them. We detect OOD samples by thresholding the entropy of the predicted softmax probability. The experimental results show that our method outperforms the previous state-of-the-art in the standard benchmark tests. We also analyze the effect of the number and particular combinations of image corrupting transformations on the performance.",,,,cs.CV,"['cs.CV', 'cs.AI']"
https://arxiv.org/abs/2102.06741v1,Discovery of Options via Meta-Learned Subgoals,"['Vivek Veeriah', 'Tom Zahavy', 'Matteo Hessel', 'Zhongwen Xu', 'Junhyuk Oh', 'Iurii Kemaev', 'Hado van Hasselt', 'David Silver', 'Satinder Singh']",2021-02-12 19:50:40+00:00,arxiv,...,64ed37e5810410e4ca8a4d445017cd97,html,markdownify,2021-02-12 19:50:40+00:00,"Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2102.07024v2,Interactive Learning from Activity Description,"['Khanh Nguyen', 'Dipendra Misra', 'Robert Schapire', 'Miro DudÃ­k', 'Patrick Shafto']",2021-02-13 22:51:11+00:00,arxiv,...,fbacba0e00330c1c91b252aed2c9d4b2,html,markdownify,2021-06-14 23:40:40+00:00,"We present a novel interactive learning protocol that enables training request-fulfilling agents by verbally describing their activities. Unlike imitation learning (IL), our protocol allows the teaching agent to provide feedback in a language that is most appropriate for them. Compared with reward in reinforcement learning (RL), the description feedback is richer and allows for improved sample complexity. We develop a probabilistic framework and an algorithm that practically implements our protocol. Empirical results in two challenging request-fulfilling problems demonstrate the strengths of our approach: compared with RL baselines, it is more sample-efficient; compared with IL baselines, it achieves competitive success rates without requiring the teaching agent to be able to demonstrate the desired behavior using the learning agent's actions. Apart from empirical evaluation, we also provide theoretical guarantees for our algorithm under certain assumptions about the teacher and the environment.",ICML 2021,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']"
https://arxiv.org/abs/2103.06076v2,"Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs","['Solon Barocas', 'Anhong Guo', 'Ece Kamar', 'Jacquelyn Krones', 'Meredith Ringel Morris', 'Jennifer Wortman Vaughan', 'Duncan Wadsworth', 'Hanna Wallach']",2021-03-10 14:26:14+00:00,arxiv,...,89fa739adaf2b1bd738aea3055a31b73,html,markdownify,2021-12-01 20:38:18+00:00,"Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts -- both beneficial and harmful -- that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",,,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2103.07460v1,Towards Risk Modeling for Collaborative AI,"['Matteo Camilli', 'Michael Felderer', 'Andrea Giusti', 'Dominik T. Matt', 'Anna Perini', 'Barbara Russo', 'Angelo Susi']",2021-03-12 18:53:06+00:00,arxiv,...,bd9df97579e1e740330c2e349143b41b,html,markdownify,2021-03-12 18:53:06+00:00,"Collaborative AI systems aim at working together with humans in a shared space to achieve a common goal. This setting imposes potentially hazardous circumstances due to contacts that could harm human beings. Thus, building such systems with strong assurances of compliance with requirements domain specific standards and regulations is of greatest importance. Challenges associated with the achievement of this goal become even more severe when such systems rely on machine learning components rather than such as top-down rule-based AI. In this paper, we introduce a risk modeling approach tailored to Collaborative AI systems. The risk model includes goals, risk events and domain specific indicators that potentially expose humans to hazards. The risk model is then leveraged to drive assurance methods that feed in turn the risk model through insights extracted from run-time evidence. Our envisioned approach is described by means of a running example in the domain of Industry 4.0, where a robotic arm endowed with a visual perception component, implemented with machine learning, collaborates with a human operator for a production-relevant task.","4 pages, 2 figures",,,cs.SE,"['cs.SE', 'cs.AI']"
https://arxiv.org/abs/2103.09230v1,Lyapunov Barrier Policy Optimization,"['Harshit Sikchi', 'Wenxuan Zhou', 'David Held']",2021-03-16 17:58:27+00:00,arxiv,...,d84e75deb4cea7f99a6ab72983209f6b,html,markdownify,2021-03-16 17:58:27+00:00,"Deploying Reinforcement Learning (RL) agents in the real-world require that the agents satisfy safety constraints. Current RL agents explore the environment without considering these constraints, which can lead to damage to the hardware or even other agents in the environment. We propose a new method, LBPO, that uses a Lyapunov-based barrier function to restrict the policy update to a safe set for each training iteration. Our method also allows the user to control the conservativeness of the agent with respect to the constraints in the environment. LBPO significantly outperforms state-of-the-art baselines in terms of the number of constraint violations during training while being competitive in terms of performance. Further, our analysis reveals that baselines like CPO and SDDPG rely mostly on backtracking to ensure safety rather than safe projection, which provides insight into why previous methods might not have effectively limit the number of constraint violations.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']"
https://arxiv.org/abs/2103.13107v1,W2WNet: a two-module probabilistic Convolutional Neural Network with embedded data cleansing functionality,"['Francesco Ponzio', 'Enrico Macii', 'Elisa Ficarra', 'Santa Di Cataldo']",2021-03-24 11:28:59+00:00,arxiv,...,80923a63b652385969db0d81bcee8433,html,markdownify,2021-03-24 11:28:59+00:00,"Convolutional Neural Networks (CNNs) are supposed to be fed with only high-quality annotated datasets. Nonetheless, in many real-world scenarios, such high quality is very hard to obtain, and datasets may be affected by any sort of image degradation and mislabelling issues. This negatively impacts the performance of standard CNNs, both during the training and the inference phase. To address this issue we propose Wise2WipedNet (W2WNet), a new two-module Convolutional Neural Network, where a Wise module exploits Bayesian inference to identify and discard spurious images during the training, and a Wiped module takes care of the final classification while broadcasting information on the prediction confidence at inference time. The goodness of our solution is demonstrated on a number of public benchmarks addressing different image classification tasks, as well as on a real-world case study on histological image analysis. Overall, our experiments demonstrate that W2WNet is able to identify image degradation and mislabelling issues both at training and at inference time, with a positive impact on the final classification accuracy.",,,,cs.CV,"['cs.CV', 'cs.AI']"
https://arxiv.org/abs/2104.08441v1,Action Advising with Advice Imitation in Deep Reinforcement Learning,"['Ercument Ilhan', 'Jeremy Gow', 'Diego Perez-Liebana']",2021-04-17 04:24:04+00:00,arxiv,...,539a568b2f6fd4224d068a2a8d935d77,html,markdownify,2021-04-17 04:24:04+00:00,"Action advising is a peer-to-peer knowledge exchange technique built on the teacher-student paradigm to alleviate the sample inefficiency problem in deep reinforcement learning. Recently proposed student-initiated approaches have obtained promising results. However, due to being in the early stages of development, these also have some substantial shortcomings. One of the abilities that are absent in the current methods is further utilising advice by reusing, which is especially crucial in the practical settings considering the budget and cost constraints in peer-to-peer. In this study, we present an approach to enable the student agent to imitate previously acquired advice to reuse them directly in its exploration policy, without any interventions in the learning mechanism itself. In particular, we employ a behavioural cloning module to imitate the teacher policy and use dropout regularisation to have a notion of epistemic uncertainty to keep track of which state-advice pairs are actually collected. As the results of experiments we conducted in three Atari games show, advice reusing via generalisation is indeed a feasible option in deep RL and our approach can successfully achieve this while significantly improving the learning performance, even when paired with a simple early advising heuristic.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2105.00385v2,pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models,"['Anirudhan Badrinath', 'Frederic Wang', 'Zachary Pardos']",2021-05-02 03:08:53+00:00,arxiv,...,2b89010fc4233239e38fae2c5895a22f,html,markdownify,2021-05-29 04:20:30+00:00,"Bayesian Knowledge Tracing, a model used for cognitive mastery estimation, has been a hallmark of adaptive learning research and an integral component of deployed intelligent tutoring systems (ITS). In this paper, we provide a brief history of knowledge tracing model research and introduce pyBKT, an accessible and computationally efficient library of model extensions from the literature. The library provides data generation, fitting, prediction, and cross-validation routines, as well as a simple to use data helper interface to ingest typical tutor log dataset formats. We evaluate the runtime with various dataset sizes and compare to past implementations. Additionally, we conduct sanity checks of the model using experiments with simulated data to evaluate the accuracy of its EM parameter learning and use real-world data to validate its predictions, comparing pyBKT's supported model variants with results from the papers in which they were originally introduced. The library is open source and open license for the purpose of making knowledge tracing more accessible to communities of research and practice and to facilitate progress in the field through easier replication of past approaches.",Accepted to the 2021 Conference on Educational Data Mining (EDM '21),,,cs.MS,"['cs.MS', 'cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/2106.08492v1,Developing a Fidelity Evaluation Approach for Interpretable Machine Learning,"['Mythreyi Velmurugan', 'Chun Ouyang', 'Catarina Moreira', 'Renuka Sindhgatta']",2021-06-16 00:21:16+00:00,arxiv,...,096dd9d51618636d0109b9ee834c112e,html,markdownify,2021-06-16 00:21:16+00:00,"Although modern machine learning and deep learning methods allow for complex and in-depth data analytics, the predictive models generated by these methods are often highly complex, and lack transparency. Explainable AI (XAI) methods are used to improve the interpretability of these complex models, and in doing so improve transparency. However, the inherent fitness of these explainable methods can be hard to evaluate. In particular, methods to evaluate the fidelity of the explanation to the underlying black box require further development, especially for tabular data. In this paper, we (a) propose a three phase approach to developing an evaluation method; (b) adapt an existing evaluation method primarily for image and text data to evaluate models trained on tabular data; and (c) evaluate two popular explainable methods using this evaluation method. Our evaluations suggest that the internal mechanism of the underlying predictive model, the internal mechanism of the explainable method used and model and data complexity all affect explanation fidelity. Given that explanation fidelity is so sensitive to context and tools and data used, we could not clearly identify any specific explainable method as being superior to another.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2107.04409v1,An Orchestration Platform that Puts Radiologists in the Driver's Seat of AI Innovation: A Methodological Approach,"['Raphael Y. Cohen', 'Aaron D. Sodickson']",2021-07-06 20:32:14+00:00,arxiv,...,873b8c1b2c739b329d36eff34c0556e4,html,markdownify,2021-07-06 20:32:14+00:00,"Current AI-driven research in radiology requires resources and expertise that are often inaccessible to small and resource-limited labs. The clinicians who are able to participate in AI research are frequently well-funded, well-staffed, and either have significant experience with AI and computing, or have access to colleagues or facilities that do. Current imaging data is clinician-oriented and is not easily amenable to machine learning initiatives, resulting in inefficient, time consuming, and costly efforts that rely upon a crew of data engineers and machine learning scientists, and all too often preclude radiologists from driving AI research and innovation. We present the system and methodology we have developed to address infrastructure and platform needs, while reducing the staffing and resource barriers to entry. We emphasize a data-first and modular approach that streamlines the AI development and deployment process while providing efficient and familiar interfaces for radiologists, such that they can be the drivers of new AI innovations.",,,,cs.SE,"['cs.SE', 'cs.AI', 'cs.DC', 'eess.IV']"
https://arxiv.org/abs/2107.04457v2,Aligning an optical interferometer with beam divergence control and continuous action space,"['Stepan Makarenko', 'Dmitry Sorokin', 'Alexander Ulanov', 'A. I. Lvovsky']",2021-07-09 14:23:01+00:00,arxiv,...,ea596910426938e539956ae1e84f4f27,html,markdownify,2021-11-16 07:44:54+00:00,"Reinforcement learning is finding its way to real-world problem application, transferring from simulated environments to physical setups. In this work, we implement vision-based alignment of an optical Mach-Zehnder interferometer with a confocal telescope in one arm, which controls the diameter and divergence of the corresponding beam. We use a continuous action space; exponential scaling enables us to handle actions within a range of over two orders of magnitude. Our agent trains only in a simulated environment with domain randomizations. In an experimental evaluation, the agent significantly outperforms an existing solution and a human expert.","12 pages, 5 figures",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.LG', 'physics.optics']"
https://arxiv.org/abs/2107.12806v2,Towards Industrial Private AI: A two-tier framework for data and model security,"['Sunder Ali Khowaja', 'Kapal Dev', 'Nawab Muhammad Faseeh Qureshi', 'Parus Khuwaja', 'Luca Foschini']",2021-07-27 13:28:07+00:00,arxiv,...,10b77f6d11d5dae92c436e3093527ab7,html,markdownify,2022-01-18 13:52:12+00:00,"With the advances in 5G and IoT devices, the industries are vastly adopting artificial intelligence (AI) techniques for improving classification and prediction-based services. However, the use of AI also raises concerns regarding privacy and security that can be misused or leaked. Private AI was recently coined to address the data security issue by combining AI with encryption techniques, but existing studies have shown that model inversion attacks can be used to reverse engineer the images from model parameters. In this regard, we propose a Federated Learning and Encryption-based Private (FLEP) AI framework that provides two-tier security for data and model parameters in an IIoT environment. We proposed a three-layer encryption method for data security and provide a hypothetical method to secure the model parameters. Experimental results show that the proposed method achieves better encryption quality at the expense of slightly increased execution time. We also highlight several open issues and challenges regarding the FLEP AI framework's realization.","9 pages, 4 figures, 2 tables, Magazine article",IEEE Wireless Communications 2022,,cs.CR,"['cs.CR', 'cs.AI']"
https://arxiv.org/abs/2108.12427v2,Why and How Governments Should Monitor AI Development,"['Jess Whittlestone', 'Jack Clark']",2021-08-28 19:41:22+00:00,arxiv,...,d07425fdf5729dfdf535fe4b52420d26,html,markdownify,2021-08-31 12:49:31+00:00,"In this paper we outline a proposal for improving the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this would give governments greater information about the AI ecosystem, equipping them to more effectively direct AI development and deployment in the most societally and economically beneficial directions. It would also create infrastructure that could rapidly identify potential threats or harms that could occur as a consequence of changes in the AI ecosystem, such as the emergence of strategically transformative capabilities, or the deployment of harmful systems.   We begin by outlining the problem which motivates this proposal: in brief, traditional governance approaches struggle to keep pace with the speed of progress in AI. We then present our proposal for addressing this problem: governments must invest in measurement and monitoring infrastructure. We discuss this proposal in detail, outlining what specific things governments could focus on measuring and monitoring, and the kinds of benefits this would generate for policymaking. Finally, we outline some potential pilot projects and some considerations for implementing this in practice.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2110.10024v1,Risks of AI Foundation Models in Education,"['Su Lin Blodgett', 'Michael Madaio']",2021-10-19 14:44:02+00:00,arxiv,...,e0cf7d72c7a22cfbb77b2eb6c60bf9d9,html,markdownify,2021-10-19 14:44:02+00:00,"If the authors of a recent Stanford report (Bommasani et al., 2021) on the opportunities and risks of ""foundation models"" are to be believed, these models represent a paradigm shift for AI and for the domains in which they will supposedly be used, including education. Although the name is new (and contested (Field, 2021)), the term describes existing types of algorithmic models that are ""trained on broad data at scale"" and ""fine-tuned"" (i.e., adapted) for particular downstream tasks, and is intended to encompass large language models such as BERT or GPT-3 and computer vision models such as CLIP. Such technologies have the potential for harm broadly speaking (e.g., Bender et al., 2021), but their use in the educational domain is particularly fraught, despite the potential benefits for learners claimed by the authors. In section 3.3 of the Stanford report, Malik et al. argue that achieving the goal of providing education for all learners requires more efficient computational approaches that can rapidly scale across educational domains and across educational contexts, for which they argue foundation models are uniquely well-suited. However, evidence suggests that not only are foundation models not likely to achieve the stated benefits for learners, but their use may also introduce new risks for harm.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2111.06420v1,Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities,"['Waddah Saeed', 'Christian Omlin']",2021-11-11 19:06:13+00:00,arxiv,...,a049b8f7aa608090f2cf474f26a15efe,html,markdownify,2021-11-11 19:06:13+00:00,"The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that identified challenges and potential research directions in XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey for challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions in XAI and (2) challenges and research directions in XAI based on machine learning life cycle's phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.","29 pages, 2 figures, 4 tables",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2111.14874v1,Weighing the Milky Way and Andromeda with Artificial Intelligence,"['Pablo Villanueva-Domingo', 'Francisco Villaescusa-Navarro', 'Shy Genel', 'Daniel AnglÃ©s-AlcÃ¡zar', 'Lars Hernquist', 'Federico Marinacci', 'David N. Spergel', 'Mark Vogelsberger', 'Desika Narayanan']",2021-11-29 19:00:04+00:00,arxiv,...,e907710ded71d7337cb444cd0495972f,html,markdownify,2021-11-29 19:00:04+00:00,"We present new constraints on the masses of the halos hosting the Milky Way and Andromeda galaxies derived using graph neural networks. Our models, trained on thousands of state-of-the-art hydrodynamic simulations of the CAMELS project, only make use of the positions, velocities and stellar masses of the galaxies belonging to the halos, and are able to perform likelihood-free inference on halo masses while accounting for both cosmological and astrophysical uncertainties. Our constraints are in agreement with estimates from other traditional methods.","2 figures, 2 tables, 7 pages. Code publicly available at
  https://github.com/PabloVD/HaloGraphNet",,,astro-ph.GA,"['astro-ph.GA', 'astro-ph.CO', 'astro-ph.IM', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2112.10925v1,"DB-BERT: a Database Tuning Tool that ""Reads the Manual""",['Immanuel Trummer'],2021-12-21 01:04:08+00:00,arxiv,...,5a01780692f19e7e070694c95c3e71b1,html,markdownify,2021-12-21 01:04:08+00:00,"DB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specifically, the BERT model) for text analysis. During an initial training phase, it fine-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specific database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT finds the best parameter settings among all compared methods. The code of DB-BERT is available online at https://itrummer.github.io/dbbert/.",,,,cs.DB,"['cs.DB', 'cs.AI', 'cs.CL']"
https://arxiv.org/abs/2201.04632v1,The Concept of Criticality in AI Safety,"['Yitzhak Spielberg', 'Amos Azaria']",2022-01-12 17:44:22+00:00,arxiv,...,01e39e280f6d18a994829e2f9b35d55f,html,markdownify,2022-01-12 17:44:22+00:00,"When AI agents don't align their actions with human values they may cause serious harm. One way to solve the value alignment problem is by including a human operator who monitors all of the agent's actions. Despite the fact, that this solution guarantees maximal safety, it is very inefficient, since it requires the human operator to dedicate all of his attention to the agent. In this paper, we propose a much more efficient solution that allows an operator to be engaged in other activities without neglecting his monitoring task. In our approach the AI agent requests permission from the operator only for critical actions, that is, potentially harmful actions. We introduce the concept of critical actions with respect to AI safety and discuss how to build a model that measures action criticality. We also discuss how the operator's feedback could be used to make the agent smarter.",,,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/2201.04633v1,Revelation of Task Difficulty in AI-aided Education,"['Yitzhak Spielberg', 'Amos Azaria']",2022-01-12 18:10:30+00:00,arxiv,...,c886d27b4e72a0d0faede5512750d25f,html,markdownify,2022-01-12 18:10:30+00:00,"When a student is asked to perform a given task, her subjective estimate of the difficulty of that task has a strong influence on her performance. There exists a rich literature on the impact of perceived task difficulty on performance and motivation. Yet, there is another topic that is closely related to the subject of the influence of perceived task difficulty that did not receive any attention in previous research - the influence of revealing the true difficulty of a task to the student. This paper investigates the impact of revealing the task difficulty on the student's performance, motivation, self-efficacy and subjective task value via an experiment in which workers are asked to solve matchstick riddles. Furthermore, we discuss how the experiment results might be relevant for AI-aided education. Specifically, we elaborate on the question of how a student's learning experience might be improved by supporting her with two types of AI systems: an AI system that predicts task difficulty and an AI system that determines when task difficulty should be revealed and when not.",,,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/2201.05646v1,ULTRA: A Data-driven Approach for Recommending Team Formation in Response to Proposal Calls,"['Biplav Srivastava', 'Tarmo Koppel', 'Ronak Shah', 'Owen Bond', 'Sai Teja Paladi', 'Rohit Sharma', 'Austin Hetherington']",2022-01-13 02:48:42+00:00,arxiv,...,345696c29f05c2c103d0a1ff6c9038ee,html,markdownify,2022-01-13 02:48:42+00:00,"We introduce an emerging AI-based approach and prototype system for assisting team formation when researchers respond to calls for proposals from funding agencies. This is an instance of the general problem of building teams when demand opportunities come periodically and potential members may vary over time. The novelties of our approach are that we: (a) extract technical skills needed about researchers and calls from multiple data sources and normalize them using Natural Language Processing (NLP) techniques, (b) build a prototype solution based on matching and teaming based on constraints, (c) describe initial feedback about system from researchers at a University to deploy, and (d) create and publish a dataset that others can use.",7 pages,,,cs.IR,"['cs.IR', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/2201.05647v1,Tools and Practices for Responsible AI Engineering,"['Ryan Soklaski', 'Justin Goodwin', 'Olivia Brown', 'Michael Yee', 'Jason Matterer']",2022-01-14 19:47:46+00:00,arxiv,...,16a182b2837e34ea717c39ba33509858,html,markdownify,2022-01-14 19:47:46+00:00,"Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']"
https://arxiv.org/abs/2201.12462v2,Explaining Reinforcement Learning Policies through Counterfactual Trajectories,"['Julius Frost', 'Olivia Watkins', 'Eric Weiner', 'Pieter Abbeel', 'Trevor Darrell', 'Bryan Plummer', 'Kate Saenko']",2022-01-29 00:52:37+00:00,arxiv,...,2d9eb565f63c92f8b1806b83369583b1,html,markdownify,2022-03-19 03:37:19+00:00,"In order for humans to confidently decide where to employ RL agents for real-world tasks, a human developer must validate that the agent will perform well at test-time. Some policy interpretability methods facilitate this by capturing the policy's decision making in a set of agent rollouts. However, even the most informative trajectories of training time behavior may give little insight into the agent's behavior out of distribution. In contrast, our method conveys how the agent performs under distribution shifts by showing the agent's behavior across a wider trajectory distribution. We generate these trajectories by guiding the agent to more diverse unseen states and showing the agent's behavior there. In a user study, we demonstrate that our method enables users to score better than baseline methods on one of two agent validation tasks.",Accepted at ICML HILL 2021 Workshop,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', 'cs.RO', 'I.2.6']"
https://arxiv.org/abs/2202.09039v1,Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness,"['Kanak Tekwani', 'Manojkumar Parmar']",2022-02-18 06:15:49+00:00,arxiv,...,90dfb07b0c2e591171af37405c1dc493,html,markdownify,2022-02-18 06:15:49+00:00,"From past couple of years there is a cycle of researchers proposing a defence model for adversaries in machine learning which is arguably defensible to most of the existing attacks in restricted condition (they evaluate on some bounded inputs or datasets). And then shortly another set of researcher finding the vulnerabilities in that defence model and breaking it by proposing a stronger attack model. Some common flaws are been noticed in the past defence models that were broken in very short time. Defence models being broken so easily is a point of concern as decision of many crucial activities are taken with the help of machine learning models. So there is an utter need of some defence checkpoints that any researcher should keep in mind while evaluating the soundness of technique and declaring it to be decent defence technique. In this paper, we have suggested few checkpoints that should be taken into consideration while building and evaluating the soundness of defence models. All these points are recommended after observing why some past defence models failed and how some model remained adamant and proved their soundness against some of the very strong attacks.","16 pages, 8 figures",,,cs.CR,"['cs.CR', 'cs.AI', 'cs.CV', 'cs.LG']"
https://arxiv.org/abs/2202.11960v1,All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL,"['Kai Arulkumaran', 'Dylan R. Ashley', 'JÃ¼rgen Schmidhuber', 'Rupesh K. Srivastava']",2022-02-24 08:44:11+00:00,arxiv,...,77441926fb32784fa761f4ee28d987a7,html,markdownify,2022-02-24 08:44:11+00:00,"Upside down reinforcement learning (UDRL) flips the conventional use of the return in the objective function in RL upside down, by taking returns as input and predicting actions. UDRL is based purely on supervised learning, and bypasses some prominent issues in RL: bootstrapping, off-policy corrections, and discount factors. While previous work with UDRL demonstrated it in a traditional online RL setting, here we show that this single algorithm can also work in the imitation learning and offline RL settings, be extended to the goal-conditioned RL setting, and even the meta-RL setting. With a general agent architecture, a single UDRL agent can learn across all paradigms.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2202.12985v1,OCR-IDL: OCR Annotations for Industry Document Library Dataset,"['Ali Furkan Biten', 'RubÃ¨n Tito', 'Lluis Gomez', 'Ernest Valveny', 'Dimosthenis Karatzas']",2022-02-25 21:30:48+00:00,arxiv,...,4bc0426b5a80096b05d28fd169acbbdb,html,markdownify,2022-02-25 21:30:48+00:00,"Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl_data.",,,,cs.CV,"['cs.CV', 'cs.AI']"
https://arxiv.org/abs/2203.02927v1,Enabling Automated Machine Learning for Model-Driven AI Engineering,"['Armin Moin', 'Ukrit Wattanavaekin', 'Alexandra Lungu', 'Moharram Challenger', 'Atta Badii', 'Stephan GÃ¼nnemann']",2022-03-06 10:12:56+00:00,arxiv,...,8bc6741cab11c8db2640505c4e8d7780,html,markdownify,2022-03-06 10:12:56+00:00,"Developing smart software services requires both Software Engineering and Artificial Intelligence (AI) skills. AI practitioners, such as data scientists often focus on the AI side, for example, creating and training Machine Learning (ML) models given a specific use case and data. They are typically not concerned with the entire software development life-cycle, architectural decisions for the system and performance issues beyond the predictive ML models (e.g., regarding the security, privacy, throughput, scalability, availability, as well as ethical, legal and regulatory compliance). In this manuscript, we propose a novel approach to enable Model-Driven Software Engineering and Model-Driven AI Engineering. In particular, we support Automated ML, thus assisting software engineers without deep AI knowledge in developing AI-intensive systems by choosing the most appropriate ML model, algorithm and techniques with suitable hyper-parameters for the task at hand. To validate our work, we carry out a case study in the smart energy domain.",Preliminary version,,,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2203.06760v1,CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification,"['Yuan Gong', 'Sameer Khurana', 'Andrew Rouditchenko', 'James Glass']",2022-03-13 21:14:04+00:00,arxiv,...,95664144b66ca9c181aa7d1d1c12ca34,html,markdownify,2022-03-13 21:14:04+00:00,"Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.",,,,cs.SD,"['cs.SD', 'cs.AI', 'eess.AS']"
https://arxiv.org/abs/2203.15414v1,Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice,"['Markus Borg', 'Johan Bengtsson', 'Harald Ãsterling', 'Alexander Hagelborn', 'Isabella Gagner', 'Piotr Tomaszewski']",2022-03-29 10:25:13+00:00,arxiv,...,472d950c1668257e613c1d15a1c9d1d5,html,markdownify,2022-03-29 10:25:13+00:00,"Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting.","Accepted for publication in the Proc. of the 1st International
  Conference on AI Engineering, 2022",,,cs.SE,"['cs.SE', 'cs.AI', 'cs.CL']"
https://arxiv.org/abs/2206.03378v1,Imitating Past Successes can be Very Suboptimal,"['Benjamin Eysenbach', 'Soumith Udatha', 'Sergey Levine', 'Ruslan Salakhutdinov']",2022-06-07 15:13:43+00:00,arxiv,...,d53fc5110dadfebd85113f0164f1a30c,html,markdownify,2022-06-07 15:13:43+00:00,"Prior work has proposed a simple strategy for reinforcement learning (RL): label experience with the outcomes achieved in that experience, and then imitate the relabeled experience. These outcome-conditioned imitation learning methods are appealing because of their simplicity, strong performance, and close ties with supervised learning. However, it remains unclear how these methods relate to the standard RL objective, reward maximization. In this paper, we prove that existing outcome-conditioned imitation learning methods do not necessarily improve the policy; rather, in some settings they can decrease the expected reward. Nonetheless, we show that a simple modification results in a method that does guarantee policy improvement, under some assumptions. Our aim is not to develop an entirely new method, but rather to explain how a variant of outcome-conditioned imitation learning can be used to maximize rewards.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2206.08325v2,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,"['Maribeth Rauh', 'John Mellor', 'Jonathan Uesato', 'Po-Sen Huang', 'Johannes Welbl', 'Laura Weidinger', 'Sumanth Dathathri', 'Amelia Glaese', 'Geoffrey Irving', 'Iason Gabriel', 'William Isaac', 'Lisa Anne Hendricks']",2022-06-16 17:28:01+00:00,arxiv,...,66dd853069156c5a88b0bd92f3d1a764,html,markdownify,2022-10-28 17:55:58+00:00,"Large language models produce human-like text that drive a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.","Accepted to NeurIPS 2022 Datasets and Benchmarks Track; 10 pages plus
  appendix",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/2208.07643v1,A Review of the Convergence of 5G/6G Architecture and Deep Learning,"['Olusola T. Odeyomi', 'Olubiyi O. Akintade', 'Temitayo O. Olowu', 'Gergely Zaruba']",2022-08-16 10:05:19+00:00,arxiv,...,b81a208bb71f477ff042cb5ae4e85d51,html,markdownify,2022-08-16 10:05:19+00:00,"The convergence of 5G architecture and deep learning has gained a lot of research interests in both the fields of wireless communication and artificial intelligence. This is because deep learning technologies have been identified to be the potential driver of the 5G technologies, that make up the 5G architecture. Hence, there have been extensive surveys on the convergence of 5G architecture and deep learning. However, most of the existing survey papers mainly focused on how deep learning can converge with a specific 5G technology, thus, not covering the full spectrum of the 5G architecture. Although there is a recent survey paper that appears to be robust, a review of that paper shows that it is not well structured to specifically cover the convergence of deep learning and the 5G technologies. Hence, this paper provides a robust overview of the convergence of the key 5G technologies and deep learning. The challenges faced by such convergence are discussed. In addition, a brief overview of the future 6G architecture, and how it can converge with deep learning is also discussed.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NI']"
https://arxiv.org/abs/1603.04068v5,A Signaling Game Approach to Databases Querying and Interaction,"['Ben McCamish', 'Vahid Ghadakchi', 'Arash Termehchy', 'Behrouz Touri']",2016-03-13 19:28:22+00:00,arxiv,...,a8b3977700a7b447b81357d231eef4d3,html,markdownify,2018-05-04 21:33:26+00:00,"As most users do not precisely know the structure and/or the content of databases, their queries do not exactly reflect their information needs. The database management systems (DBMS) may interact with users and use their feedback on the returned results to learn the information needs behind their queries. Current query interfaces assume that users do not learn and modify the way way they express their information needs in form of queries during their interaction with the DBMS. Using a real-world interaction workload, we show that users learn and modify how to express their information needs during their interactions with the DBMS and their learning is accurately modeled by a well-known reinforcement learning mechanism. As current data interaction systems assume that users do not modify their strategies, they cannot discover the information needs behind users' queries effectively. We model the interaction between users and DBMS as a game with identical interest between two rational agents whose goal is to establish a common language for representing information needs in form of queries. We propose a reinforcement learning method that learns and answers the information needs behind queries and adapts to the changes in users' strategies and prove that it improves the effectiveness of answering queries stochastically speaking. We propose two efficient implementation of this method over large relational databases. Our extensive empirical studies over real-world query workloads indicate that our algorithms are efficient and effective.",21 pages,,,cs.DB,"['cs.DB', 'cs.AI']"
https://arxiv.org/abs/1706.04972v2,Device Placement Optimization with Reinforcement Learning,"['Azalia Mirhoseini', 'Hieu Pham', 'Quoc V. Le', 'Benoit Steiner', 'Rasmus Larsen', 'Yuefeng Zhou', 'Naveen Kumar', 'Mohammad Norouzi', 'Samy Bengio', 'Jeff Dean']",2017-06-13 16:26:40+00:00,arxiv,...,8bd6c92d3015e0547c263ef3fe81eaea,html,markdownify,2017-06-25 23:55:21+00:00,"The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.",To appear at ICML 2017,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1707.06658v4,RAIL: Risk-Averse Imitation Learning,"['Anirban Santara', 'Abhishek Naik', 'Balaraman Ravindran', 'Dipankar Das', 'Dheevatsa Mudigere', 'Sasikanth Avancha', 'Bharat Kaul']",2017-07-20 18:01:45+00:00,arxiv,...,0e9b11c7acf3969f9d933f4538300904,html,markdownify,2017-11-29 12:44:19+00:00,"Imitation learning algorithms learn viable policies by imitating an expert's behavior when reward signals are not available. Generative Adversarial Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies when the expert's behavior is available as a fixed set of trajectories. We evaluate in terms of the expert's cost function and observe that the distribution of trajectory-costs is often more heavy-tailed for GAIL-agents than the expert at a number of benchmark continuous-control tasks. Thus, high-cost trajectories, corresponding to tail-end events of catastrophic failure, are more likely to be encountered by the GAIL-agents than the expert. This makes the reliability of GAIL-agents questionable when it comes to deployment in risk-sensitive applications like robotic surgery and autonomous driving. In this work, we aim to minimize the occurrence of tail-end events by minimizing tail risk within the GAIL framework. We quantify tail risk by the Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse Imitation Learning (RAIL) algorithm. We observe that the policies learned with RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed RAIL algorithm appears as a potent alternative to GAIL for improved reliability in risk-sensitive applications.","Accepted for presentation in Deep Reinforcement Learning Symposium at
  NIPS 2017",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1804.02477v3,Programmatically Interpretable Reinforcement Learning,"['Abhinav Verma', 'Vijayaraghavan Murali', 'Rishabh Singh', 'Pushmeet Kohli', 'Swarat Chaudhuri']",2018-04-06 22:17:18+00:00,arxiv,...,748baa46147e538a6b616ab4501210cf,html,markdownify,2019-04-10 09:09:46+00:00,"We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.","Published at The 35th International Conference on Machine Learning
  (ICML 2018)",PMLR 80:5045-5054,,cs.LG,"['cs.LG', 'cs.AI', 'cs.PL', 'stat.ML']"
https://arxiv.org/abs/1807.11113v1,Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images,"['Nanqing Dong', 'Michael Kampffmeyer', 'Xiaodan Liang', 'Zeya Wang', 'Wei Dai', 'Eric P. Xing']",2018-07-29 21:45:35+00:00,arxiv,...,9ea0a02540667fd8a643ea1254abe990,html,markdownify,2018-07-29 21:45:35+00:00,"Convolutional neural networks have led to significant breakthroughs in the domain of medical image analysis. However, the task of breast cancer segmentation in whole-slide images (WSIs) is still underexplored. WSIs are large histopathological images with extremely high resolution. Constrained by the hardware and field of view, using high-magnification patches can slow down the inference process and using low-magnification patches can cause the loss of information. In this paper, we aim to achieve two seemingly conflicting goals for breast cancer segmentation: accurate and fast prediction. We propose a simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this task. Motivated by the zoom-in operation of a pathologist using a digital microscope, RAZN learns a policy network to decide whether zooming is required in a given region of interest. Because the zoom-in action is selective, RAZN is robust to unbalanced and noisy ground truth labels and can efficiently reduce overfitting. We evaluate our method on a public breast cancer dataset. RAZN outperforms both single-scale and multi-scale baseline approaches, achieving better accuracy at low inference cost.","Accepted by MICCAI 2018 Workshop on Deep Learning in Medical Image
  Analysis",,,cs.CV,"['cs.CV', 'cs.AI']"
https://arxiv.org/abs/1812.05285v5,IRLAS: Inverse Reinforcement Learning for Architecture Search,"['Minghao Guo', 'Zhao Zhong', 'Wei Wu', 'Dahua Lin', 'Junjie Yan']",2018-12-13 06:53:36+00:00,arxiv,...,6a595a84d5c216742dad36c43a26aed4,html,markdownify,2019-11-06 02:30:08+00:00,"In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",,,,cs.CV,"['cs.CV', 'cs.AI']"
https://arxiv.org/abs/1903.12394v3,Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems,"['Laura von Rueden', 'Sebastian Mayer', 'Katharina Beckh', 'Bogdan Georgiev', 'Sven Giesselbach', 'Raoul Heese', 'Birgit Kirsch', 'Julius Pfrommer', 'Annika Pick', 'Rajkumar Ramamurthy', 'Michal Walczak', 'Jochen Garcke', 'Christian Bauckhage', 'Jannis Schuecker']",2019-03-29 08:37:40+00:00,arxiv,...,1293611d76949addfd1536daa84411ff,html,markdownify,2021-05-28 07:34:41+00:00,"Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.","Accepted at IEEE Transactions on Knowledge and Data Engineering:
  https://ieeexplore.ieee.org/document/9429985",,10.1109/TKDE.2021.3079836,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1905.04933v1,Lie on the Fly: Strategic Voting in an Iterative Preference Elicitation Process,"['Lihi Dery', 'Svetlana Obraztsova', 'Zinovi Rabinovich', 'Meir Kalech']",2019-05-13 09:32:17+00:00,arxiv,...,b756456082333287b5ac473dce603997,html,markdownify,2019-05-13 09:32:17+00:00,"A voting center is in charge of collecting and aggregating voter preferences. In an iterative process, the center sends comparison queries to voters, requesting them to submit their preference between two items. Voters might discuss the candidates among themselves, figuring out during the elicitation process which candidates stand a chance of winning and which do not. Consequently, strategic voters might attempt to manipulate by deviating from their true preferences and instead submit a different response in order to attempt to maximize their profit. We provide a practical algorithm for strategic voters which computes the best manipulative vote and maximizes the voter's selfish outcome when such a vote exists. We also provide a careful voting center which is aware of the possible manipulations and avoids manipulative queries when possible. In an empirical study on four real-world domains, we show that in practice manipulation occurs in a low percentage of settings and has a low impact on the final outcome. The careful voting center reduces manipulation even further, thus allowing for a non-distorted group decision process to take place. We thus provide a core technology study of a voting process that can be adopted in opinion or information aggregation systems and in crowdsourcing applications, e.g., peer grading in Massive Open Online Courses (MOOCs).",,,10.1007/s10726-019-09637-2,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1907.12392v5,A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment,"['Felix Leibfried', 'Sergio Pascual-Diaz', 'Jordi Grau-Moya']",2019-07-26 16:34:21+00:00,arxiv,...,b63738811b42f5112f9d8f477ae171ab,html,markdownify,2020-01-08 11:08:57+00:00,"Empowerment is an information-theoretic method that can be used to intrinsically motivate learning agents. It attempts to maximize an agent's control over the environment by encouraging visiting states with a large number of reachable next states. Empowered learning has been shown to lead to complex behaviors, without requiring an explicit reward signal. In this paper, we investigate the use of empowerment in the presence of an extrinsic reward signal. We hypothesize that empowerment can guide reinforcement learning (RL) agents to find good early behavioral solutions by encouraging highly empowered states. We propose a unified Bellman optimality principle for empowered reward maximization. Our empowered reward maximization approach generalizes both Bellman's optimality principle as well as recent information-theoretical extensions to it. We prove uniqueness of the empowered values and show convergence to the optimal solution. We then apply this idea to develop off-policy actor-critic RL algorithms which we validate in high-dimensional continuous robotics domains (MuJoCo). Our methods demonstrate improved initial and competitive final performance compared to model-free state-of-the-art techniques.","Proceedings of the 33rd Conference on Neural Information Processing
  Systems (NeurIPS), Vancouver, Canada, 2019",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1908.03566v2,That which we call private,"['Ãlfar Erlingsson', 'Ilya Mironov', 'Ananth Raghunathan', 'Shuang Song']",2019-08-08 22:09:52+00:00,arxiv,...,6e4cee2e8ba97c7df6750d274b8a3e17,html,markdownify,2020-04-20 18:39:55+00:00,"The guarantees of security and privacy defenses are often strengthened by relaxing the assumptions made about attackers or the context in which defenses are deployed. Such relaxations can be a highly worthwhile topic of exploration---even though they typically entail assuming a weaker, less powerful adversary---because there may indeed be great variability in both attackers' powers and their context.   However, no weakening or contextual discounting of attackers' power is assumed for what some have called ""relaxed definitions"" in the analysis of differential-privacy guarantees. Instead, the definitions so named are the basis of refinements and more advanced analyses of the worst-case implications of attackers---without any change assumed in attackers' powers.   Because they more precisely bound the worst-case privacy loss, these improved analyses can greatly strengthen the differential-privacy upper-bound guarantees---sometimes lowering the differential-privacy epsilon by orders-of-magnitude. As such, to the casual eye, these analyses may appear to imply a reduced privacy loss. This is a false perception: the privacy loss of any concrete mechanism cannot change with the choice of a worst-case-loss upper-bound analysis technique. Practitioners must be careful not to equate real-world privacy with differential-privacy epsilon values, at least not without full consideration of the context.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']"
https://arxiv.org/abs/1909.12892v2,Automated curricula through setter-solver interactions,"['Sebastien Racaniere', 'Andrew K. Lampinen', 'Adam Santoro', 'David P. Reichert', 'Vlad Firoiu', 'Timothy P. Lillicrap']",2019-09-27 20:11:12+00:00,arxiv,...,4996ff565d3153699e45673751b898b4,html,markdownify,2020-01-22 00:01:48+00:00,"Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula--the breakdown of tasks into simpler, static challenges with dense rewards--to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.",,"International Conference on Learning Representations, 2020",,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1910.03466v1,Can We Distinguish Machine Learning from Human Learning?,"['Vicki Bier', 'Paul B. Kantor', 'Gary Lupyan', 'Xiaojin Zhu']",2019-10-08 15:37:03+00:00,arxiv,...,58511f4e3d09f39a981d4a3a7d08689e,html,markdownify,2019-10-08 15:37:03+00:00,"What makes a task relatively more or less difficult for a machine compared to a human? Much AI/ML research has focused on expanding the range of tasks that machines can do, with a focus on whether machines can beat humans. Allowing for differences in scale, we can seek interesting (anomalous) pairs of tasks T, T'. We define interesting in this way: The ""harder to learn"" relation is reversed when comparing human intelligence (HI) to AI. While humans seems to be able to understand problems by formulating rules, ML using neural networks does not rely on constructing rules. We discuss a novel approach where the challenge is to ""perform well under rules that have been created by human beings."" We suggest that this provides a rigorous and precise pathway for understanding the difference between the two kinds of learning. Specifically, we suggest a large and extensible class of learning tasks, formulated as learning under rules. With these tasks, both the AI and HI will be studied with rigor and precision. The immediate goal is to find interesting groundtruth rule pairs. In the long term, the goal will be to understand, in a generalizable way, what distinguishes interesting pairs from ordinary pairs, and to define saliency behind interesting pairs. This may open new ways of thinking about AI, and provide unexpected insights into human learning.",14pp. 5 fig. Working Paper,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML', 'I.2']"
https://arxiv.org/abs/2001.00496v1,Uncertainty-Based Out-of-Distribution Classification in Deep Reinforcement Learning,"['Andreas Sedlmeier', 'Thomas Gabor', 'Thomy Phan', 'Lenz Belzner', 'Claudia Linnhoff-Popien']",2019-12-31 09:52:49+00:00,arxiv,...,7f138317d482f97c0583dfc4c7f19df6,html,markdownify,2019-12-31 09:52:49+00:00,"Robustness to out-of-distribution (OOD) data is an important goal in building reliable machine learning systems. Especially in autonomous systems, wrong predictions for OOD inputs can cause safety critical situations. As a first step towards a solution, we consider the problem of detecting such data in a value-based deep reinforcement learning (RL) setting. Modelling this problem as a one-class classification problem, we propose a framework for uncertainty-based OOD classification: UBOOD. It is based on the effect that an agent's epistemic uncertainty is reduced for situations encountered during training (in-distribution), and thus lower than for unencountered (OOD) situations. Being agnostic towards the approach used for estimating epistemic uncertainty, combinations with different uncertainty estimation methods, e.g. approximate Bayesian inference methods or ensembling techniques are possible. We further present a first viable solution for calculating a dynamic classification threshold, based on the uncertainty distribution of the training data. Evaluation shows that the framework produces reliable classification results when combined with ensemble-based estimators, while the combination with concrete dropout-based estimators fails to reliably detect OOD situations. In summary, UBOOD presents a viable approach for OOD classification in deep RL settings by leveraging the epistemic uncertainty of the agent's value function.",arXiv admin note: text overlap with arXiv:1901.02219,"Proceedings of the 12th International Conference on Agents and
  Artificial Intelligence - Volume 2: ICAART, 2020, ISBN 978-989-758-395-7,
  pages 522-529",10.5220/0008949905220529,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2001.00682v1,Auditing and Debugging Deep Learning Models via Decision Boundaries: Individual-level and Group-level Analysis,"['Roozbeh Yousefzadeh', ""Dianne P. O'Leary""]",2020-01-03 01:45:36+00:00,arxiv,...,8106cfe3e7c7ffbc3ca0694e95ccd9a1,html,markdownify,2020-01-03 01:45:36+00:00,"Deep learning models have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Nevertheless, they are consistently utilized in many applications, consequential to humans' lives, mostly because of their better performance. Therefore, there is a great need for computational methods that can explain, audit, and debug such models. Here, we use flip points to accomplish these goals for deep learning models with continuous output scores (e.g., computed by softmax), used in social applications. A flip point is any point that lies on the boundary between two output classes: e.g. for a model with a binary yes/no output, a flip point is any input that generates equal scores for ""yes"" and ""no"". The flip point closest to a given input is of particular importance because it reveals the least changes in the input that would change a model's classification, and we show that it is the solution to a well-posed optimization problem. Flip points also enable us to systematically study the decision boundaries of a deep learning classifier. The resulting insight into the decision boundaries of a deep model can clearly explain the model's output on the individual-level, via an explanation report that is understandable by non-experts. We also develop a procedure to understand and audit model behavior towards groups of people. Flip points can also be used to alter the decision boundaries in order to improve undesirable behaviors. We demonstrate our methods by investigating several models trained on standard datasets used in social applications of machine learning. We also identify the features that are most responsible for particular classifications and misclassifications.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2001.00818v1,A Framework for Democratizing AI,"['Shakkeel Ahmed', 'Ravi S. Mula', 'Soma S. Dhavala']",2020-01-01 17:30:14+00:00,arxiv,...,b4341d8a5d10cec984847b918c31fd8f,html,markdownify,2020-01-01 17:30:14+00:00,"Machine Learning and Artificial Intelligence are considered an integral part of the Fourth Industrial Revolution. Their impact, and far-reaching consequences, while acknowledged, are yet to be comprehended. These technologies are very specialized, and few organizations and select highly trained professionals have the wherewithal, in terms of money, manpower, and might, to chart the future. However, concentration of power can lead to marginalization, causing severe inequalities. Regulatory agencies and governments across the globe are creating national policies, and laws around these technologies to protect the rights of the digital citizens, as well as to empower them. Even private, not-for-profit organizations are also contributing to democratizing the technologies by making them \emph{accessible} and \emph{affordable}. However, accessibility and affordability are all but a few of the facets of democratizing the field. Others include, but not limited to, \emph{portability}, \emph{explainability}, \emph{credibility}, \emph{fairness}, among others. As one can imagine, democratizing AI is a multi-faceted problem, and it requires advancements in science, technology and policy. At \texttt{mlsquare}, we are developing scientific tools in this space. Specifically, we introduce an opinionated, extensible, \texttt{Python} framework that provides a single point of interface to a variety of solutions in each of the categories mentioned above. We present the design details, APIs of the framework, reference implementations, road map for development, and guidelines for contributions.","12 pages, 4 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2001.07417v5,Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach,"['Carlos FernÃ¡ndez-LorÃ­a', 'Foster Provost', 'Xintian Han']",2020-01-21 09:58:58+00:00,arxiv,...,f3c5249a50c5f502154fe9e9918acb8b,html,markdownify,2021-10-13 07:50:39+00:00,"We examine counterfactual explanations for explaining the decisions made by model-based AI systems. The counterfactual approach we consider defines an explanation as a set of the system's data inputs that causally drives the decision (i.e., changing the inputs in the set changes the decision) and is irreducible (i.e., changing any subset of the inputs does not change the decision). We (1) demonstrate how this framework may be used to provide explanations for decisions made by general, data-driven AI systems that may incorporate features with arbitrary data types and multiple predictive models, and (2) propose a heuristic procedure to find the most useful explanations depending on the context. We then contrast counterfactual explanations with methods that explain model predictions by weighting features according to their importance (e.g., SHAP, LIME) and present two fundamental reasons why we should carefully consider whether importance-weight explanations are well-suited to explain system decisions. Specifically, we show that (i) features that have a large importance weight for a model prediction may not affect the corresponding decision, and (ii) importance weights are insufficient to communicate whether and how features influence decisions. We demonstrate this with several concise examples and three detailed case studies that compare the counterfactual approach with SHAP to illustrate various conditions under which counterfactual explanations explain data-driven decisions better than importance weights.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2002.12500v4,Efficiently Guiding Imitation Learning Agents with Human Gaze,"['Akanksha Saran', 'Ruohan Zhang', 'Elaine Schaertl Short', 'Scott Niekum']",2020-02-28 00:55:30+00:00,arxiv,...,686c995db96aba996bc740e272a154a1,html,markdownify,2021-04-21 21:39:21+00:00,"Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.",AAMAS 2021,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2008.02790v4,Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices,"['Evan Zheran Liu', 'Aditi Raghunathan', 'Percy Liang', 'Chelsea Finn']",2020-08-06 17:57:36+00:00,arxiv,...,50620cb1e04cafa402cbeb6d39455b49,html,markdownify,2021-11-12 02:08:50+00:00,"The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: https://ezliu.github.io/dream/","International Conference on Machine Learning (ICML), 2021",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2008.07284v2,Forward and inverse reinforcement learning sharing network weights and hyperparameters,"['Eiji Uchibe', 'Kenji Doya']",2020-08-17 13:12:44+00:00,arxiv,...,93918429be53eb37a48389aff7ccc394,html,markdownify,2022-05-31 11:07:58+00:00,"This paper proposes model-free imitation learning named Entropy-Regularized Imitation Learning (ERIL) that minimizes the reverse Kullback-Leibler (KL) divergence. ERIL combines forward and inverse reinforcement learning (RL) under the framework of an entropy-regularized Markov decision process. An inverse RL step computes the log-ratio between two distributions by evaluating two binary discriminators. The first discriminator distinguishes the state generated by the forward RL step from the expert's state. The second discriminator, which is structured by the theory of entropy regularization, distinguishes the state-action-next-state tuples generated by the learner from the expert ones. One notable feature is that the second discriminator shares hyperparameters with the forward RL, which can be used to control the discriminator's ability. A forward RL step minimizes the reverse KL estimated by the inverse RL step. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy. Our experimental results on MuJoCo-simulated environments and vision-based reaching tasks with a robotic arm show that ERIL is more sample-efficient than the baseline methods. We apply the method to human behaviors that perform a pole-balancing task and describe how the estimated reward functions show how every subject achieves her goal.",Accepted for publication in the Neural Networks,"Neural Networks, December 2021, Pages 138-153",10.1016/j.neunet.2021.08.017,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']"
https://arxiv.org/abs/2008.07371v1,Artificial Intelligence is stupid and causal reasoning won't fix it,['John Mark Bishop'],2020-07-20 22:23:50+00:00,arxiv,...,2ffd041f487970906232bccbd9505c83,html,markdownify,2020-07-20 22:23:50+00:00,"Artificial Neural Networks have reached Grandmaster and even super-human performance across a variety of games: from those involving perfect-information (such as Go) to those involving imperfect-information (such as Starcraft). Such technological developments from AI-labs have ushered concomitant applications across the world of business - where an AI brand tag is fast becoming ubiquitous. A corollary of such widespread commercial deployment is that when AI gets things wrong - an autonomous vehicle crashes; a chatbot exhibits racist behaviour; automated credit scoring processes discriminate on gender etc. - there are often significant financial, legal and brand consequences and the incident becomes major news. As Judea Pearl sees it, the underlying reason for such mistakes is that, 'all the impressive achievements of deep learning amount to just curve fitting'. The key, Judea Pearl suggests, is to replace reasoning by association with causal-reasoning - the ability to infer causes from observed phenomena. It is a point that was echoed by Gary Marcus and Ernest Davis in a recent piece for the New York Times: 'we need to stop building computer systems that merely get better and better at detecting statistical patterns in data sets - often using an approach known as Deep Learning - and start building computer systems that from the moment of their assembly innately grasp three basic concepts: time, space and causality'. In this paper, foregrounding what in 1949 Gilbert Ryle termed a category mistake, I will offer an alternative explanation for AI errors: it is not so much that AI machinery cannot grasp causality, but that AI machinery - qua computation - cannot understand anything at all.",,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/2009.06114v1,Towards the Quantification of Safety Risks in Deep Neural Networks,"['Peipei Xu', 'Wenjie Ruan', 'Xiaowei Huang']",2020-09-13 23:30:09+00:00,arxiv,...,021bcf54627143906b4d7491f7ddd7ce,html,markdownify,2020-09-13 23:30:09+00:00,"Safety concerns on the deep neural networks (DNNs) have been raised when they are applied to critical sectors. In this paper, we define safety risks by requesting the alignment of the network's decision with human perception. To enable a general methodology for quantifying safety risks, we define a generic safety property and instantiate it to express various safety risks. For the quantification of risks, we take the maximum radius of safe norm balls, in which no safety risk exists. The computation of the maximum safe radius is reduced to the computation of their respective Lipschitz metrics - the quantities to be computed. In addition to the known adversarial example, reachability example, and invariant example, in this paper we identify a new class of risk - uncertainty example - on which humans can tell easily but the network is unsure. We develop an algorithm, inspired by derivative-free optimization techniques and accelerated by tensor-based parallelization on GPUs, to support efficient computation of the metrics. We perform evaluations on several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and ImageNet networks. The experiments show that, our method can achieve competitive performance on safety quantification in terms of the tightness and the efficiency of computation. Importantly, as a generic approach, our method can work with a broad class of safety risks and without restrictions on the structure of neural networks.","19 pages, 10 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
https://arxiv.org/abs/2009.12576v2,Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics,"['Minhae Kwon', 'Saurabh Daptardar', 'Paul Schrater', 'Xaq Pitkow']",2020-09-26 11:47:48+00:00,arxiv,...,e547bcac018f726f51c41fca129f2e23,html,markdownify,2020-10-30 07:09:41+00:00,"A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent's actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent.",NeurIPS2020,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'stat.ML']"
https://arxiv.org/abs/2010.02629v2,"A framework for predicting, interpreting, and improving Learning Outcomes","['Chintan Donda', 'Sayan Dasgupta', 'Soma S Dhavala', 'Keyur Faldu', 'Aditi Avasthi']",2020-10-06 11:22:27+00:00,arxiv,...,5373814a4c87176cca4f70961966764e,html,markdownify,2020-10-12 04:54:56+00:00,"It has long been recognized that academic success is a result of both cognitive and non-cognitive dimensions acting together. Consequently, any intelligent learning platform designed to improve learning outcomes (LOs) must provide actionable inputs to the learner in these dimensions. However, operationalizing such inputs in a production setting that is scalable is not trivial. We develop an Embibe Score Quotient model (ESQ) to predict test scores based on observed academic, behavioral and test-taking features of a student. ESQ can be used to predict the future scoring potential of a student as well as offer personalized learning nudges, both critical to improving LOs. Multiple machine learning models are evaluated for the prediction task. In order to provide meaningful feedback to the learner, individualized Shapley feature attributions for each feature are computed. Prediction intervals are obtained by applying non-parametric quantile regression, in an attempt to quantify the uncertainty in the predictions. We apply the above modelling strategy on a dataset consisting of more than a hundred million learner interactions on the Embibe learning platform. We observe that the Median Absolute Error between the observed and predicted scores is 4.58% across several user segments, and the correlation between predicted and observed responses is 0.93. Game-like what-if scenarios are played out to see the changes in LOs, on counterfactual examples. We briefly discuss how a rational agent can then apply an optimal policy to affect the learning outcomes by treating the above model like an Oracle.","9 pages, 10 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']"
https://arxiv.org/abs/2011.10753v2,Emergent Road Rules In Multi-Agent Driving Environments,"['Avik Pal', 'Jonah Philion', 'Yuan-Hong Liao', 'Sanja Fidler']",2020-11-21 09:43:50+00:00,arxiv,...,9c3f077bf3c275850997aa03a76eab33,html,markdownify,2021-03-17 07:29:41+00:00,"For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific ""road rules"" that human drivers have agreed to follow. ""Road rules"" include rules that drivers are required to follow by law -- such as the requirement that vehicles stop at red lights -- as well as more subtle social rules -- such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that -- instead of hard-coding road rules into self-driving algorithms -- a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow. We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents' spatial density. We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving.",International Conference on Learning Representations (2021),"International Conference on Learning Representations, 2021",,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA']"
https://arxiv.org/abs/2101.08001v3,UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers,"['Siyi Hu', 'Fengda Zhu', 'Xiaojun Chang', 'Xiaodan Liang']",2021-01-20 07:24:24+00:00,arxiv,...,70799364a5e8b47f11a05cff342922bd,html,markdownify,2021-02-07 10:28:41+00:00,"Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).","15 pages, 8 figures",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2102.06911v1,Modelling Cooperation in Network Games with Spatio-Temporal Complexity,"['Michiel A. Bakker', 'Richard Everett', 'Laura Weidinger', 'Iason Gabriel', 'William S. Isaac', 'Joel Z. Leibo', 'Edward Hughes']",2021-02-13 12:04:52+00:00,arxiv,...,dac0fe495575bf97d78875a14f701cc2,html,markdownify,2021-02-13 12:04:52+00:00,"The real world is awash with multi-agent problems that require collective action by self-interested agents, from the routing of packets across a computer network to the management of irrigation systems. Such systems have local incentives for individuals, whose behavior has an impact on the global outcome for the group. Given appropriate mechanisms describing agent interaction, groups may achieve socially beneficial outcomes, even in the face of short-term selfish incentives. In many cases, collective action problems possess an underlying graph structure, whose topology crucially determines the relationship between local decisions and emergent global effects. Such scenarios have received great attention through the lens of network games. However, this abstraction typically collapses important dimensions, such as geometry and time, relevant to the design of mechanisms promoting cooperation. In parallel work, multi-agent deep reinforcement learning has shown great promise in modelling the emergence of self-organized cooperation in complex gridworld domains. Here we apply this paradigm in graph-structured collective action problems. Using multi-agent deep reinforcement learning, we simulate an agent society for a variety of plausible mechanisms, finding clear transitions between different equilibria over time. We define analytic tools inspired by related literatures to measure the social outcomes, and use these to draw conclusions about the efficacy of different environmental interventions. Our methods have implications for mechanism design in both human and artificial agent systems.",AAMAS 2021,,,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/2102.13515v3,Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,"['VÃ­ctor Campos', 'Pablo Sprechmann', 'Steven Hansen', 'Andre Barreto', 'Steven Kapturowski', 'Alex Vitvitskyi', 'AdriÃ  PuigdomÃ¨nech Badia', 'Charles Blundell']",2021-02-24 16:51:02+00:00,arxiv,...,1fb62858e2614e132d772209a68c257d,html,markdownify,2021-06-08 09:36:51+00:00,"Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Knowledge acquired during an unsupervised pre-training phase is often transferred by fine-tuning neural network weights once rewards are exposed, as is common practice in supervised domains. Given the nature of the reinforcement learning problem, we argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. We introduce Behavior Transfer (BT), a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. Our experiments show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be leveraged by BT to discover better solutions than without pre-training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2103.08022v1,Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,"['Naoki Yokoyama', 'Sehoon Ha', 'Dhruv Batra']",2021-03-14 20:13:06+00:00,arxiv,...,b0f65180c10946ec582a96c18d29e0f0,html,markdownify,2021-03-14 20:13:06+00:00,"We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent's dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exemplifies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in an real robot to navigate an apartment, and show that they can generalize in a zero-shot manner.",,,,cs.RO,"['cs.RO', 'cs.AI']"
https://arxiv.org/abs/2103.14101v1,Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems,"['Grace A. Lewis', 'Stephany Bellomo', 'Ipek Ozkaya']",2021-03-25 19:40:29+00:00,arxiv,...,51afb3a2328722429781020617a58c39,html,markdownify,2021-03-25 19:40:29+00:00,"Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.","1st Workshop on AI Engineering: Software Engineering for AI (WAIN
  2021) held at the 2021 IEEE/ACM 43rd International Conference on Software
  Engineering",,,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2104.08440v3,Learning on a Budget via Teacher Imitation,"['Ercument Ilhan', 'Jeremy Gow', 'Diego Perez-Liebana']",2021-04-17 04:15:00+00:00,arxiv,...,7e52f964ea27132af4f95ff4eea30926,html,markdownify,2021-06-30 04:31:58+00:00,"Deep Reinforcement Learning (RL) techniques can benefit greatly from leveraging prior experience, which can be either self-generated or acquired from other entities. Action advising is a framework that provides a flexible way to transfer such knowledge in the form of actions between teacher-student peers. However, due to the realistic concerns, the number of these interactions is limited with a budget; therefore, it is crucial to perform these in the most appropriate moments. There have been several promising studies recently that address this problem setting especially from the student's perspective. Despite their success, they have some shortcomings when it comes to the practical applicability and integrity as an overall solution to the learning from advice challenge. In this paper, we extend the idea of advice reusing via teacher imitation to construct a unified approach that addresses both advice collection and advice utilisation problems. We also propose a method to automatically tune the relevant hyperparameters of these components on-the-fly to make it able to adapt to any task with minimal human intervention. The experiments we performed in 5 different Atari games verify that our algorithm either surpasses or performs on-par with its top competitors while being far simpler to be employed. Furthermore, its individual components are also found to be providing significant advantages alone.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2107.02692v4,ML-Quadrat & DriotData: A Model-Driven Engineering Tool and a Low-Code Platform for Smart IoT Services,"['Armin Moin', 'Andrei Mituca', 'Moharram Challenger', 'Atta Badii', 'Stephan GÃ¼nnemann']",2021-07-06 15:52:09+00:00,arxiv,...,bf75311b2811e526c771862e9fc9ef3c,html,markdownify,2022-02-16 13:21:36+00:00,"In this paper, we present ML-Quadrat, an open-source research prototype that is based on the Eclipse Modeling Framework (EMF) and the state of the art in the literature of Model-Driven Software Engineering (MDSE) for smart Cyber-Physical Systems (CPS) and the Internet of Things (IoT). Its envisioned users are mostly software developers who might not have deep knowledge and skills in the heterogeneous IoT platforms and the diverse Artificial Intelligence (AI) technologies, specifically regarding Machine Learning (ML). ML-Quadrat is released under the terms of the Apache 2.0 license on Github. Additionally, we demonstrate an early tool prototype of DriotData, a web-based Low-Code platform targeting citizen data scientists and citizen/end-user software developers. DriotData exploits and adopts ML-Quadrat in the industry by offering an extended version of it as a subscription-based service to companies, mainly Small- and Medium-Sized Enterprises (SME). The current preliminary version of DriotData has three web-based model editors: text-based, tree-/form-based and diagram-based. The latter is designed for domain experts in the problem or use case domains (namely the IoT vertical domains) who might not have knowledge and skills in the field of IT. Finally, a short video demonstrating the tools is available on YouTube: https://youtu.be/VAuz25w0a5k",ICSE'22 Tool Demo,,10.1109/ICSE-Companion55297.2022.9793752,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2110.13880v1,Understanding Interlocking Dynamics of Cooperative Rationalization,"['Mo Yu', 'Yang Zhang', 'Shiyu Chang', 'Tommi S. Jaakkola']",2021-10-26 17:39:18+00:00,arxiv,...,5860c59f01899ccc241ffe6cf2e86c10,html,markdownify,2021-10-26 17:39:18+00:00,"Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm -- model interlocking. Interlocking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator's selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments. We release our code at https://github.com/Gorov/Understanding_Interlocking.",Accepted at NeurIPS 2021,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']"
https://arxiv.org/abs/2111.04885v1,Lymph Node Detection in T2 MRI with Transformers,"['Tejas Sudharshan Mathai', 'Sungwon Lee', 'Daniel C. Elton', 'Thomas C. Shen', 'Yifan Peng', 'Zhiyong Lu', 'Ronald M. Summers']",2021-11-09 00:06:27+00:00,arxiv,...,7082de8e7c4dc5684fa6ac53386f65b7,html,markdownify,2021-11-09 00:06:27+00:00,"Identification of lymph nodes (LN) in T2 Magnetic Resonance Imaging (MRI) is an important step performed by radiologists during the assessment of lymphoproliferative diseases. The size of the nodes play a crucial role in their staging, and radiologists sometimes use an additional contrast sequence such as diffusion weighted imaging (DWI) for confirmation. However, lymph nodes have diverse appearances in T2 MRI scans, making it tough to stage for metastasis. Furthermore, radiologists often miss smaller metastatic lymph nodes over the course of a busy day. To deal with these issues, we propose to use the DEtection TRansformer (DETR) network to localize suspicious metastatic lymph nodes for staging in challenging T2 MRI scans acquired by different scanners and exam protocols. False positives (FP) were reduced through a bounding box fusion technique, and a precision of 65.41\% and sensitivity of 91.66\% at 4 FP per image was achieved. To the best of our knowledge, our results improve upon the current state-of-the-art for lymph node detection in T2 MRI scans.",Accepted at SPIE 2022,,,eess.IV,"['eess.IV', 'cs.AI', 'cs.CV', 'physics.med-ph']"
https://arxiv.org/abs/2112.09332v3,WebGPT: Browser-assisted question-answering with human feedback,"['Reiichiro Nakano', 'Jacob Hilton', 'Suchir Balaji', 'Jeff Wu', 'Long Ouyang', 'Christina Kim', 'Christopher Hesse', 'Shantanu Jain', 'Vineet Kosaraju', 'William Saunders', 'Xu Jiang', 'Karl Cobbe', 'Tyna Eloundou', 'Gretchen Krueger', 'Kevin Button', 'Matthew Knight', 'Benjamin Chess', 'John Schulman']",2021-12-17 05:43:43+00:00,arxiv,...,c8c154c7a7893c89ca7e311645ce4fe1,html,markdownify,2022-06-01 19:08:11+00:00,"We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",32 pages,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2201.02874v1,"Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture","['Tiago Gaspar Oliveira', 'Arlindo L. Oliveira']",2022-01-08 18:30:25+00:00,arxiv,...,b921c0188c607dd85e3bd733d278ae56,html,markdownify,2022-01-08 18:30:25+00:00,"The model-based reinforcement learning paradigm, which uses planning algorithms and neural network models, has recently achieved unprecedented results in diverse applications, leading to what is now known as deep reinforcement learning. These agents are quite complex and involve multiple components, factors that can create challenges for research. In this work, we propose a new modular software architecture suited for these types of agents, and a set of building blocks that can be easily reused and assembled to construct new model-based reinforcement learning agents. These building blocks include planning algorithms, policies, and loss functions.   We illustrate the use of this architecture by combining several of these building blocks to implement and test agents that are optimized to three different test environments: Cartpole, Minigrid, and Tictactoe. One particular planning algorithm, made available in our implementation and not previously used in reinforcement learning, which we called averaged minimax, achieved good results in the three tested environments.   Experiments performed with this architecture have shown that the best combination of planning algorithm, policy, and loss function is heavily problem dependent. This result provides evidence that the proposed architecture, which is modular and reusable, is useful for reinforcement learning researchers who want to study new environments and techniques.",,,,cs.LG,"['cs.LG', 'cs.AI', '49L20', 'I.2.6; I.2.8']"
https://arxiv.org/abs/2202.01691v2,Solving Dynamic Principal-Agent Problems with a Rationally Inattentive Principal,"['Tong Mu', 'Stephan Zheng', 'Alexander Trott']",2022-01-18 20:54:00+00:00,arxiv,...,7e6764cfaa5d9bed4b1a3d8221fbd748,html,markdownify,2022-02-17 20:46:03+00:00,"Principal-Agent (PA) problems describe a broad class of economic relationships characterized by misaligned incentives and asymmetric information. The Principal's problem is to find optimal incentives given the available information, e.g., a manager setting optimal wages for its employees. Whereas the Principal is often assumed rational, comparatively little is known about solutions when the Principal is boundedly rational, especially in the sequential setting, with multiple Agents, and with multiple information channels. Here, we develop RIRL, a deep reinforcement learning framework that solves such complex PA problems with a rationally inattentive Principal. Such a Principal incurs a cost for paying attention to information, which can model forms of bounded rationality. We use RIRL to analyze rich economic phenomena in manager-employee relationships. In the single-step setting, 1) RIRL yields wages that are consistent with theoretical predictions; and 2) non-zero attention costs lead to simpler but less profitable wage structures, and increased Agent welfare. In a sequential setting with multiple Agents, RIRL shows opposing consequences of the Principal's inattention to different information channels: 1) inattention to Agents' outputs closes wage gaps based on ability differences; and 2) inattention to Agents' efforts induces a social dilemma dynamic in which Agents work harder, but essentially for free. Moreover, RIRL reveals non-trivial relationships between the Principal's inattention and Agent types, e.g., if Agents are prone to sub-optimal effort choices, payment schedules are more sensitive to the Principal's attention cost. As such, RIRL can reveal novel economic relationships and enables progress towards understanding the effects of bounded rationality in dynamic settings.","22 pages, 8 figures, including appendix",,,cs.MA,"['cs.MA', 'cs.AI']"
https://arxiv.org/abs/2202.11812v1,Investigations of Performance and Bias in Human-AI Teamwork in Hiring,"['Andi Peng', 'Besmira Nushi', 'Emre Kiciman', 'Kori Inkpen', 'Ece Kamar']",2022-02-21 17:58:07+00:00,arxiv,...,8cac0c6c7539eae6ef5b31d2357a7ddf,html,markdownify,2022-02-21 17:58:07+00:00,"In AI-assisted decision-making, effective hybrid (human-AI) teamwork is not solely dependent on AI performance alone, but also on its impact on human decision-making. While prior work studies the effects of model accuracy on humans, we endeavour here to investigate the complex dynamics of how both a model's predictive performance and bias may transfer to humans in a recommendation-aided decision task. We consider the domain of ML-assisted hiring, where humans -- operating in a constrained selection setting -- can choose whether they wish to utilize a trained model's inferences to help select candidates from written biographies. We conduct a large-scale user study leveraging a re-created dataset of real bios from prior work, where humans predict the ground truth occupation of given candidates with and without the help of three different NLP classifiers (random, bag-of-words, and deep neural network). Our results demonstrate that while high-performance models significantly improve human performance in a hybrid setting, some models mitigate hybrid bias while others accentuate it. We examine these findings through the lens of decision conformity and observe that our model architecture choices have an impact on human-AI conformity and bias, motivating the explicit need to assess these complex dynamics prior to deployment.",Accepted at AAAI 2022,,,cs.HC,"['cs.HC', 'cs.AI']"
https://arxiv.org/abs/2203.01855v3,Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning,"['Michael S. Lee', 'Henny Admoni', 'Reid Simmons']",2022-03-03 17:06:37+00:00,arxiv,...,7fd025f5571b0f5d6d77e9599fd29825,html,markdownify,2022-08-03 21:17:33+00:00,"To collaborate well with robots, we must be able to understand their decision making. Humans naturally infer other agents' beliefs and desires by reasoning about their observable behavior in a way that resembles inverse reinforcement learning (IRL). Thus, robots can convey their beliefs and desires by providing demonstrations that are informative for a human learner's IRL. An informative demonstration is one that differs strongly from the learner's expectations of what the robot will do given their current understanding of the robot's decision making. However, standard IRL does not model the learner's existing expectations, and thus cannot do this counterfactual reasoning. We propose to incorporate the learner's current understanding of the robot's decision making into our model of human IRL, so that a robot can select demonstrations that maximize the human's understanding. We also propose a novel measure for estimating the difficulty for a human to predict instances of a robot's behavior in unseen environments. A user study finds that our test difficulty measure correlates well with human performance and confidence. Interestingly, considering human beliefs and counterfactuals when selecting demonstrations decreases human performance on easy tests, but increases performance on difficult tests, providing insight on how to best utilize such models.","8 pages, 5 figures, IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2022",,,cs.RO,"['cs.RO', 'cs.AI', 'cs.HC']"
https://arxiv.org/abs/2204.11464v2,Towards Evaluating Adaptivity of Model-Based Reinforcement Learning Methods,"['Yi Wan', 'Ali Rahimi-Kalahroudi', 'Janarthanan Rajendran', 'Ida Momennejad', 'Sarath Chandar', 'Harm van Seijen']",2022-04-25 06:45:16+00:00,arxiv,...,34546475d6092a2a8f690d7b724d96d7,html,markdownify,2022-06-25 11:28:42+00:00,"In recent years, a growing number of deep model-based reinforcement learning (RL) methods have been introduced. The interest in deep model-based RL is not surprising, given its many potential benefits, such as higher sample efficiency and the potential for fast adaption to changes in the environment. However, we demonstrate, using an improved version of the recently introduced Local Change Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and DreamerV2 perform poorly in their ability to adapt to local environmental changes. Combined with prior work that made a similar observation about the other popular model-based method, MuZero, a trend appears to emerge, suggesting that current deep model-based methods have serious limitations. We dive deeper into the causes of this poor performance, by identifying elements that hurt adaptive behavior and linking these to underlying techniques frequently used in deep model-based RL. We empirically validate these insights in the case of linear function approximation by demonstrating that a modified version of linear Dyna achieves effective adaptation to local changes. Furthermore, we provide detailed insights into the challenges of building an adaptive nonlinear model-based method, by experimenting with a nonlinear version of Dyna.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1604.06963,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.10800,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.08541,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.05671,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.10394,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1705.09990,Should Robots be Obedient?,"['Smitha Milli', 'Dylan Hadfield-Menell', 'Anca Dragan', 'Stuart Russell']",2017-05-28 20:51:19+00:00,arxiv,...,c69174ffa080b9f7c6bea067aac714b2,html,markdownify,2017-05-28 20:51:19+00:00,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",Accepted to IJCAI 2017,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1811.07834,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.13900,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.13872,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.14111,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.00452,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1909.06769,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.10646,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1902.09725,Conservative Agency via Attainable Utility Preservation,"['Alexander Matt Turner', 'Dylan Hadfield-Menell', 'Prasad Tadepalli']",2019-02-26 04:42:54+00:00,arxiv,...,a04e6386adba1b1579be6b97e3f2fb1e,html,markdownify,2020-06-10 15:10:04+00:00,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.","Published in AI, Ethics, and Society 2020",,10.1145/3375627.3375851,cs.AI,['cs.AI']
https://arxiv.org/abs/2102.01685,Agent Incentives: A Causal Perspective,"['Tom Everitt', 'Ryan Carey', 'Eric Langlois', 'Pedro A Ortega', 'Shane Legg']",2021-02-02 18:52:41+00:00,arxiv,...,e1510538208bd91d32bd354babdc1015,html,markdownify,2021-03-15 20:08:39+00:00,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.","In Proceedings of the AAAI 2021 Conference. Supersedes
  arXiv:1902.09980, arXiv:2001.07118",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1812.02795,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1705.08417,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1806.01186,Penalizing side effects using stepwise relative reachability,"['Victoria Krakovna', 'Laurent Orseau', 'Ramana Kumar', 'Miljan Martic', 'Shane Legg']",2018-06-04 16:30:17+00:00,arxiv,...,92bcc8e74c0a68fcf6fb7560e55dfbcb,html,markdownify,2019-03-08 09:17:21+00:00,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2006.08753,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.06275,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1912.01683,Optimal Policies Tend to Seek Power,"['Alexander Matt Turner', 'Logan Smith', 'Rohin Shah', 'Andrew Critch', 'Prasad Tadepalli']",2019-12-03 20:45:49+00:00,arxiv,...,cb222d5e429dea249f7c4ee98a656e2d,html,markdownify,2021-12-03 17:27:16+00:00,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.","Accepted to NeurIPS 2021 as spotlight paper. 12 pages, 44 pages with
  appendices",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2007.09540,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1906.09624,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","['Rohin Shah', 'Noah Gundotra', 'Pieter Abbeel', 'Anca D. Dragan']",2019-06-23 18:41:31+00:00,arxiv,...,e32efd2fba15b6b63005aedabb9c9355,html,markdownify,2019-06-23 18:41:31+00:00,"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",Published at ICML 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1802.01604,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1711.02827,Inverse Reward Design,"['Dylan Hadfield-Menell', 'Smitha Milli', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']",2017-11-08 04:44:32+00:00,arxiv,...,44c1c54d551686cc36d57842aeacf703,html,markdownify,2020-10-07 15:41:58+00:00,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.","Advances in Neural Information Processing Systems 30 (NIPS 2017)
  Revised Oct 2020 to fix a typo in Eq. 3",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1911.09005,Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']",2019-11-20 16:21:12+00:00,arxiv,...,6d3c0d9bf73617e1d05f6ee19efcaaf6,html,markdownify,2019-11-20 16:21:12+00:00,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",To be presented at the AI for Social Good workshop at NeurIPS 2019,,,cs.AI,"['cs.AI', 'cs.CY', 'cs.SY', 'eess.SY']"
https://arxiv.org/abs/1807.05037,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.01365,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.06547,Avoiding Side Effects in Complex Environments,"['Alexander Matt Turner', 'Neale Ratzlaff', 'Prasad Tadepalli']",2020-06-11 16:02:30+00:00,arxiv,...,d74b721fc0ea2227711579c66bd4c478,html,markdownify,2020-10-22 15:15:46+00:00,"Reward function specification can be difficult. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at https://avoiding-side-effects.github.io/.","Accepted as spotlight paper at NeurIPS 2020. 10 pages main paper; 19
  pages with appendices",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2006.04635,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.07877v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1605.03143,Avoiding Wireheading with Value Reinforcement Learning,"['Tom Everitt', 'Marcus Hutter']",2016-05-10 18:28:57+00:00,arxiv,...,4651c7a5aadbf88915e2cbabcaae6b97,html,markdownify,2016-05-10 18:28:57+00:00,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",Artificial General Intelligence (AGI) 2016,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1802.05666,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.03980,Building Ethically Bounded AI,"['Francesca Rossi', 'Nicholas Mattei']",2018-12-10 18:58:05+00:00,arxiv,...,0a09a5275e036577d6cd8d1910804e57,html,markdownify,2018-12-10 18:58:05+00:00,"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI's freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven example-based approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.","Published at AAAI Blue Sky Track, winner of Blue Sky Award",,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']"
https://arxiv.org/abs/1606.05374,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1712.04172,A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,"['Yueh-Hua Wu', 'Shou-De Lin']",2017-12-12 08:35:52+00:00,arxiv,...,8d38dd5f63123e80b210e46bae0fb5e1,html,markdownify,2018-09-10 04:59:19+00:00,"This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.",AAAI 2018 Oral Presentation,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1507.01986,Toward Idealized Decision Theory,"['Nate Soares', 'Benja Fallenstein']",2015-07-07 23:06:59+00:00,arxiv,...,675712158a5982defee9c8fc5ddb6200,html,markdownify,2015-07-07 23:06:59+00:00,"This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.",This is an extended version of a paper accepted to AGI-2015,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1506.07359,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1901.00064,Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),['Peter Eckersley'],2018-12-31 23:51:27+00:00,arxiv,...,251d546b08e57ade147353dcc7947f33,html,markdownify,2019-03-05 03:12:49+00:00,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense.   We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.","Published in SafeAI 2019: Proceedings of the AAAI Workshop on
  Artificial Intelligence Safety 2019",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2005.01831,Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,"['Peter Hase', 'Mohit Bansal']",2020-05-04 20:35:17+00:00,arxiv,...,ac545d878f5aae2fee71eafe6ff3e881,html,markdownify,2020-05-04 20:35:17+00:00,"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020",ACL 2020 (13 pages),,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/1807.08364,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.07532,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.13258,Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,"['Paul Barde', 'Julien Roy', 'Wonseok Jeon', 'Joelle Pineau', 'Christopher Pal', 'Derek Nowrouzezahrai']",2020-06-23 18:29:13+00:00,arxiv,...,f8e48ea358b02bd13942274f8d289e5c,html,markdownify,2021-04-16 10:09:13+00:00,"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.",,Advances in Neural Information Processing Systems 33 (2020),,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1808.04468v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.09882,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.03843,Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,"['Pedro Fernandes', 'Francisco C. Santos', 'Manuel Lopes']",2019-06-26 10:18:19+00:00,arxiv,...,75fb9002ede977c133d747507036cd7d,html,markdownify,2020-12-22 18:11:35+00:00,"The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.",,"AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020",10.3233/AIC-201502,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1908.04734,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2001.03246,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.03357,Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent,"['Michael K. Cohen', 'Elliot Catt', 'Marcus Hutter']",2020-06-05 10:42:29+00:00,arxiv,...,b75c7c62928c866b2952389d11617b38,html,markdownify,2021-05-26 15:55:28+00:00,"Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either ""destroyed"" or ""incapacitated"" with probability 1. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent, Mentee, with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration. Critically, Mentee's exploration probability depends on the expected information gain from exploring. In a simple non-ergodic environment with a weak mentor, we find Mentee outperforms existing asymptotically optimal agents and its mentor.","13 pages, with 5 page appendix; 3 figures",Journal of Selected Areas in Information Theory 2 (2021),,cs.LG,"['cs.LG', 'cs.AI', 'I.2.0; I.2.6']"
https://arxiv.org/abs/2002.05380,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1607.08289,Mammalian Value Systems,"['Gopal P. Sarma', 'Nick J. Hay']",2016-07-28 01:22:26+00:00,arxiv,...,fdec8f4ccedb8c9f1c8e41d95ffff130,html,markdownify,2019-01-21 19:29:30+00:00,"Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical ""intelligence explosion,"" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The ""value alignment problem"" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of ""mammalian value systems"" points to a potential avenue for fundamental research in AI safety and AI ethics.",12 pages,Informatica Vol. 41 No. 3 (2017),,cs.AI,"['cs.AI', 'cs.CY', 'cs.HC', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/2109.13916,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.13136,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.03386,Clusterability in Neural Networks,"['Daniel Filan', 'Stephen Casper', 'Shlomi Hod', 'Cody Wild', 'Andrew Critch', 'Stuart Russell']",2021-03-04 23:53:53+00:00,arxiv,...,cb97c3dddd707ff04f61b06976c041c5,html,markdownify,2021-03-04 23:53:53+00:00,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.","20 pages, 22 figures. arXiv admin note: text overlap with
  arXiv:2003.04881",,,cs.NE,['cs.NE']
https://arxiv.org/abs/2104.03946,Learning What To Do by Simulating the Past,"['David Lindner', 'Rohin Shah', 'Pieter Abbeel', 'Anca Dragan']",2021-04-08 17:43:29+00:00,arxiv,...,3fec6df69a0bcaff46af6f54f58e087d,html,markdownify,2021-05-03 10:51:40+00:00,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",Presented at ICLR 2021,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2012.01557,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.10268,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.04338,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.04948,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1712.05812,Occam's razor is insufficient to infer the preferences of irrational agents,"['Stuart Armstrong', 'SÃ¶ren Mindermann']",2017-12-15 19:05:01+00:00,arxiv,...,44642ae9abcc6de95ed2b2644b4a3c1e,html,markdownify,2019-01-11 14:36:40+00:00,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1806.00610,Between Progress and Potential Impact of AI: the Neglected Dimensions,"['Fernando MartÃ­nez-Plumed', 'Shahar Avin', 'Miles Brundage', 'Allan Dafoe', 'Sean Ã hÃigeartaigh', 'JosÃ© HernÃ¡ndez-Orallo']",2018-06-02 09:21:12+00:00,arxiv,...,366f61b6e0bb5dbf6ef82971c52640f6,html,markdownify,2022-07-02 09:54:55+00:00,"We reframe the analysis of progress in AI by incorporating into an overall framework both the task performance of a system, and the time and resource costs incurred in the development and deployment of the system. These costs include: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, and (what kind of) time. These costs are distributed over the life cycle of the system, and may place differing demands on different developers and users. The multidimensional performance and cost space we present can be collapsed to a single utility metric that measures the value of the system for different stakeholders. Even without a single utility function, AI advances can be generically assessed by whether they expand the Pareto surface. We label these types of costs as neglected dimensions of AI progress, and explore them using four case studies: Alpha* (Go, Chess, and other board games), ALE (Atari games), ImageNet (Image classification) and Virtual Personal Assistants (Siri, Alexa, Cortana, and Google Assistant). This broader model of progress in AI will lead to novel ways of estimating the potential societal use and impact of an AI system, and the establishment of milestones for future progress.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2105.02117,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.11513,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.06551,Axes for Sociotechnical Inquiry in AI Research,"['Sarah Dean', 'Thomas Krendl Gilbert', 'Nathan Lambert', 'Tom Zick']",2021-04-26 16:49:04+00:00,arxiv,...,c8b051da7ba803b2bdbb6509c30a3e9e,html,markdownify,2021-04-26 16:49:04+00:00,"The development of artificial intelligence (AI) technologies has far exceeded the investigation of their relationship with society. Sociotechnical inquiry is needed to mitigate the harms of new technologies whose potential impacts remain poorly understood. To date, subfields of AI research develop primarily individual views on their relationship with sociotechnics, while tools for external investigation, comparison, and cross-pollination are lacking. In this paper, we propose four directions for inquiry into new and evolving areas of technological development: value--what progress and direction does a field promote, optimization--how the defined system within a problem formulation relates to broader dynamics, consensus--how agreement is achieved and who is included in building it, and failure--what methods are pursued when the problem specification is found wanting. The paper provides a lexicon for sociotechnical inquiry and illustrates it through the example of consumer drone technology.","9 pages, 1 figure",,10.1109/TTS.2021.3074097,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1908.09203,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.10862,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.07445,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.14419,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.14076,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.11181,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.06613,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1206.5264,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1803.04585,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.01345,Decision Transformer: Reinforcement Learning via Sequence Modeling,"['Lili Chen', 'Kevin Lu', 'Aravind Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'Michael Laskin', 'Pieter Abbeel', 'Aravind Srinivas', 'Igor Mordatch']",2021-06-02 17:53:39+00:00,arxiv,...,63175412e0f927d9a603a25810902fe4,html,markdownify,2021-06-24 17:09:59+00:00,"We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.","First two authors contributed equally. Last two authors advised
  equally",,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1903.06758,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.14659,Alignment of Language Agents,"['Zachary Kenton', 'Tom Everitt', 'Laura Weidinger', 'Iason Gabriel', 'Vladimir Mikulik', 'Geoffrey Irving']",2021-03-26 18:01:48+00:00,arxiv,...,0ba484e76ec4cbd983ab23e17ea4ee24,html,markdownify,2021-03-26 18:01:48+00:00,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1803.05049,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.02546,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1807.10272,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1804.02485,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1802.01421,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1902.08265,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.11038,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1909.12673,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.04053,The 30-Year Cycle In The AI Debate,['Jean-Marie Chauvet'],2018-10-08 16:35:06+00:00,arxiv,...,f5acb6708a7ea2744bb207855860f6e1,html,markdownify,2018-10-08 16:35:06+00:00,"In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.","31 pages, 5 tables",,,cs.AI,"['cs.AI', 'I.2.0']"
https://arxiv.org/abs/1803.10664,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1803.08971,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.13676,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.06674,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1803.05859,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.06177,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.08691,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.10819,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.08176,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1901.08579,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2008.01848,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.00869,Training Machine Learning Models by Regularizing their Explanations,['Andrew Slavin Ross'],2018-09-29 17:43:21+00:00,arxiv,...,a785246d43796c9241d450b7c48e154b,html,markdownify,2018-09-29 17:43:21+00:00,"Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).","Harvard CSE master's thesis; includes portions of arxiv:1703.03717
  and arxiv:1711.09404",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1811.04350,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.01439,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1703.03717,Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"['Andrew Slavin Ross', 'Michael C. Hughes', 'Finale Doshi-Velez']",2017-03-10 15:35:32+00:00,arxiv,...,04637cfb42c4b2a5addd314aa3818290,html,markdownify,2017-05-25 05:38:45+00:00,"Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/1906.10667,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.08272,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.03877,Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,"['Smitha Milli', 'Anca D. Dragan']",2019-03-09 21:58:46+00:00,arxiv,...,b11098f5cbbe63bb3eea8142397ea7a1,html,markdownify,2019-06-29 03:26:48+00:00,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (""robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",Published at UAI 2019,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1907.03976,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2005.07648,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1902.10186,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.03938,Causal Analysis of Agent Behavior for AI Safety,"['GrÃ©goire DÃ©letang', 'Jordi Grau-Moya', 'Miljan Martic', 'Tim Genewein', 'Tom McGrath', 'Vladimir Mikulik', 'Markus Kunesch', 'Shane Legg', 'Pedro A. Ortega']",2021-03-05 20:51:12+00:00,arxiv,...,88fee0579164e3fcfd4ecf453c16e0da,html,markdownify,2021-03-05 20:51:12+00:00,"As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.","16 pages, 16 figures, 6 tables",,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1804.05917,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1805.08263,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.06118,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.03298,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.01652,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.04670,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.11545,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.01267,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2008.03525,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.13649,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.14715,Learning Rewards from Linguistic Feedback,"['Theodore R. Sumers', 'Mark K. Ho', 'Robert D. Hawkins', 'Karthik Narasimhan', 'Thomas L. Griffiths']",2020-09-30 14:51:00+00:00,arxiv,...,9e88997359f8a6471d6f505fb9c73082,html,markdownify,2021-07-03 19:03:12+00:00,"We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the ""pragmatic"" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.","9 pages, 4 figures. AAAI '21",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2106.12142,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.07002,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.04734,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.09266,Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,['Johannes Schneider'],2020-09-19 16:30:37+00:00,arxiv,...,306ba257a1afcbd352162f4adac3b8ce,html,markdownify,2020-09-19 16:30:37+00:00,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",,,,cs.HC,"['cs.HC', 'cs.AI', 'cs.CV']"
https://arxiv.org/abs/2101.08153,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.06857,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1805.06826,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.06060,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1912.00747,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.14796,AvE: Assistance via Empowerment,"['Yuqing Du', 'Stas Tiomkin', 'Emre Kiciman', 'Daniel Polani', 'Pieter Abbeel', 'Anca Dragan']",2020-06-26 04:40:11+00:00,arxiv,...,9eddfcc8dd5654fc14d84ee771541d2a,html,markdownify,2021-01-07 20:54:48+00:00,"One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s). Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.",Final version from NeurIPS 2020 Conference Proceedings,,,cs.AI,"['cs.AI', 'cs.LG', 'cs.RO']"
https://arxiv.org/abs/2002.08512,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.02818,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1912.06680,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.04953,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1807.11655,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.03653,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.02846,Safety Aware Reinforcement Learning (SARL),"['Santiago Miret', 'Somdeb Majumdar', 'Carroll Wainwright']",2020-10-06 16:08:28+00:00,arxiv,...,3618ea69395baf69bdf53a0d42f4b486,html,markdownify,2020-10-06 16:08:28+00:00,"As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2008.12146,Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,"['Sandhya Saisubramanian', 'Shlomo Zilberstein', 'Ece Kamar']",2020-08-24 16:48:46+00:00,arxiv,...,6a49c1fc5ae44394d8f122788c20e6a4,html,markdownify,2021-10-18 18:40:37+00:00,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model -- handcrafted or machine acquired -- is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of an agent's actions is critical to improve the safety and reliability of autonomous systems. Mitigating negative side effects is an emerging research topic that is attracting increased attention due to the rapid growth in the deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. The article concludes with a discussion of open questions and suggestions for future research directions.",9 pages,,,cs.CY,"['cs.CY', 'cs.AI']"
https://arxiv.org/abs/1807.06096,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.07871,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1606.06565,Concrete Problems in AI Safety,"['Dario Amodei', 'Chris Olah', 'Jacob Steinhardt', 'Paul Christiano', 'John Schulman', 'Dan ManÃ©']",2016-06-21 13:37:05+00:00,arxiv,...,89a7410428f59923ab5c2e80137cc64b,html,markdownify,2016-07-25 17:23:29+00:00,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",29 pages,,,cs.AI,"['cs.AI', 'cs.LG']"
https://arxiv.org/abs/1902.09980,Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,"['Tom Everitt', 'Pedro A. Ortega', 'Elizabeth Barnes', 'Shane Legg']",2019-02-26 14:54:09+00:00,arxiv,...,267567fa95206743d1a66719cc492469,html,markdownify,2022-01-20 17:39:06+00:00,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",Mostly superseded by arXiv:2102.01685,,,cs.AI,"['cs.AI', 'cs.LG', 'I.2.6; I.2.8']"
https://arxiv.org/abs/2010.07877,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1705.10720,Low Impact Artificial Intelligences,"['Stuart Armstrong', 'Benjamin Levinstein']",2017-05-30 16:15:16+00:00,arxiv,...,8ca6f14d7837e0c4b7fde9551c1d473b,html,markdownify,2017-05-30 16:15:16+00:00,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1611.08219,The Off-Switch Game,"['Dylan Hadfield-Menell', 'Anca Dragan', 'Pieter Abbeel', 'Stuart Russell']",2016-11-24 15:23:48+00:00,arxiv,...,4ce2bdaa70193147efd085ab496c85ff,html,markdownify,2017-06-16 01:41:59+00:00,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1707.06354,Pragmatic-Pedagogic Value Alignment,"['Jaime F. Fisac', 'Monica A. Gates', 'Jessica B. Hamrick', 'Chang Liu', 'Dylan Hadfield-Menell', 'Malayandi Palaniappan', 'Dhruv Malik', 'S. Shankar Sastry', 'Thomas L. Griffiths', 'Anca D. Dragan']",2017-07-20 03:07:19+00:00,arxiv,...,a9bea869c3ce8e9e6ef9d9303c3c602e,html,markdownify,2018-02-05 20:44:09+00:00,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.","Published at the International Symposium on Robotics Research (ISRR
  2017)","International Symposium on Robotics Research, 2017",,cs.AI,"['cs.AI', 'cs.HC', 'cs.LG', 'cs.RO', '68T05', 'I.2.0; I.2.6; I.2.8; I.2.9']"
https://arxiv.org/abs/1512.05832,"Learning the Preferences of Ignorant, Inconsistent Agents","['Owain Evans', 'Andreas Stuhlmueller', 'Noah D. Goodman']",2015-12-18 00:24:08+00:00,arxiv,...,097fd1be5a147511d95927ff6ff53b20,html,markdownify,2015-12-18 00:24:08+00:00,"An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.",AAAI 2016,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1707.08476,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2005.04305,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.07532,An overview of 11 proposals for building safe advanced AI,['Evan Hubinger'],2020-12-04 22:53:18+00:00,arxiv,...,b8705b05bebfca29acb4a4b1c5f5b314,html,markdownify,2020-12-04 22:53:18+00:00,"This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/2109.06160,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.09293,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.03544,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"['Alexander Pan', 'Kush Bhatia', 'Jacob Steinhardt']",2022-01-10 18:58:52+00:00,arxiv,...,94d5d1af9597150bc04b80f7566c83b3,html,markdownify,2022-02-14 09:05:38+00:00,"Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",ICLR 2022; 19 pages,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2110.07719,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.09506,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.08267,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.04480,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.12447,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.14341,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.08514,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.12427,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.07789,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.02155,Training language models to follow instructions with human feedback,"['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L. Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",2022-03-04 07:04:42+00:00,arxiv,...,21a59b4b3db83aa48df4924ab498c107,html,markdownify,2022-03-04 07:04:42+00:00,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']"
https://arxiv.org/abs/2202.05262,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.05834,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.01747,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.01441,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.01679,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.12440,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2112.00659,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.08555,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.13733,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1805.01109v2,AGI Safety Literature Review,"['Tom Everitt', 'Gary Lea', 'Marcus Hutter']",2018-05-03 04:26:48+00:00,arxiv,...,71765d736fb69fc66c8b33bef52445f0,html,markdownify,2018-05-21 16:30:20+00:00,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.","Published in International Joint Conference on Artificial
  Intelligence (IJCAI), 2018",,,cs.AI,['cs.AI']
https://arxiv.org/abs/2109.13916v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.04948v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.05008v1,Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,"['Lewis Hammond', 'James Fox', 'Tom Everitt', 'Alessandro Abate', 'Michael Wooldridge']",2021-02-09 18:20:50+00:00,arxiv,...,c6561d74c1546d1d9e21992ac67b0e1d,html,markdownify,2021-02-09 18:20:50+00:00,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.","Accepted to the 20th International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS-21)",,,cs.MA,"['cs.MA', 'cs.AI', 'cs.GT']"
https://arxiv.org/abs/2012.08630v1,Open Problems in Cooperative AI,"['Allan Dafoe', 'Edward Hughes', 'Yoram Bachrach', 'Tantum Collins', 'Kevin R. McKee', 'Joel Z. Leibo', 'Kate Larson', 'Thore Graepel']",2020-12-15 21:39:50+00:00,arxiv,...,aebd663b551c8a9ce2eab56700100f4e,html,markdownify,2020-12-15 21:39:50+00:00,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation.   We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",,,,cs.AI,"['cs.AI', 'cs.MA']"
https://arxiv.org/abs/2102.08686v1,Fully General Online Imitation Learning,"['Michael K. Cohen', 'Marcus Hutter', 'Neel Nanda']",2021-02-17 10:57:37+00:00,arxiv,...,09611c30781fbb84436e38659271bcee,html,markdownify,2021-02-17 10:57:37+00:00,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",13 pages with 8-page appendix,,,cs.LG,"['cs.LG', 'cs.AI', 'I.2.0; I.2.6']"
https://arxiv.org/abs/2102.03896v1,Consequences of Misaligned AI,"['Simon Zhuang', 'Dylan Hadfield-Menell']",2021-02-07 19:34:04+00:00,arxiv,...,dd8005c63777c23643ce8c44eed28d6b,html,markdownify,2021-02-07 19:34:04+00:00,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,,NeurIPS 2020,,cs.AI,['cs.AI']
https://arxiv.org/abs/2107.01969v1,The MineRL BASALT Competition on Learning from Human Feedback,"['Rohin Shah', 'Cody Wild', 'Steven H. Wang', 'Neel Alex', 'Brandon Houghton', 'William Guss', 'Sharada Mohanty', 'Anssi Kanervisto', 'Stephanie Milani', 'Nicholay Topin', 'Pieter Abbeel', 'Stuart Russell', 'Anca Dragan']",2021-07-05 12:18:17+00:00,arxiv,...,0731465f4e87f6922f0580c4bca85299,html,markdownify,2021-07-05 12:18:17+00:00,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve.   The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations.   Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",NeurIPS 2021 Competition Track,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1902.04198v2,Preferences Implicit in the State of the World,"['Rohin Shah', 'Dmitrii Krasheninnikov', 'Jordan Alexander', 'Pieter Abbeel', 'Anca Dragan']",2019-02-12 00:50:56+00:00,arxiv,...,cfffd6b954d12b508c6c4c18a1e213c6,html,markdownify,2019-04-18 22:15:37+00:00,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",Published at ICLR 2019,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']"
https://arxiv.org/abs/2109.11513v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.03374v2,Evaluating Large Language Models Trained on Code,"['Mark Chen', 'Jerry Tworek', 'Heewoo Jun', 'Qiming Yuan', 'Henrique Ponde de Oliveira Pinto', 'Jared Kaplan', 'Harri Edwards', 'Yuri Burda', 'Nicholas Joseph', 'Greg Brockman', 'Alex Ray', 'Raul Puri', 'Gretchen Krueger', 'Michael Petrov', 'Heidy Khlaaf', 'Girish Sastry', 'Pamela Mishkin', 'Brooke Chan', 'Scott Gray', 'Nick Ryder', 'Mikhail Pavlov', 'Alethea Power', 'Lukasz Kaiser', 'Mohammad Bavarian', 'Clemens Winter', 'Philippe Tillet', 'Felipe Petroski Such', 'Dave Cummings', 'Matthias Plappert', 'Fotios Chantzis', 'Elizabeth Barnes', 'Ariel Herbert-Voss', 'William Hebgen Guss', 'Alex Nichol', 'Alex Paino', 'Nikolas Tezak', 'Jie Tang', 'Igor Babuschkin', 'Suchir Balaji', 'Shantanu Jain', 'William Saunders', 'Christopher Hesse', 'Andrew N. Carr', 'Jan Leike', 'Josh Achiam', 'Vedant Misra', 'Evan Morikawa', 'Alec Radford', 'Matthew Knight', 'Miles Brundage', 'Mira Murati', 'Katie Mayer', 'Peter Welinder', 'Bob McGrew', 'Dario Amodei', 'Sam McCandlish', 'Ilya Sutskever', 'Wojciech Zaremba']",2021-07-07 17:41:24+00:00,arxiv,...,d6c35b597c3e2bd3af3a0eb938b123f7,html,markdownify,2021-07-14 17:16:02+00:00,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","corrected typos, added references, added authors, added
  acknowledgements",,,cs.LG,['cs.LG']
https://arxiv.org/abs/2109.07445v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.14419v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.07785v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.09071v1,Measurement in AI Policy: Opportunities and Challenges,"['Saurabh Mishra', 'Jack Clark', 'C. Raymond Perrault']",2020-09-10 05:37:40+00:00,arxiv,...,d5655d82a2c2b7d20f9b6ba507d87357,html,markdownify,2020-09-10 05:37:40+00:00,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",Workshop Paper,,,cs.CY,['cs.CY']
https://arxiv.org/abs/2103.06312v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.06674v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.14111v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.07574v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2008.02275v5,Aligning AI With Shared Human Values,"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'Jerry Li', 'Dawn Song', 'Jacob Steinhardt']",2020-08-05 17:59:16+00:00,arxiv,...,3c13721b70be9ef0165e300f9fac0ac7,html,markdownify,2021-07-24 04:40:33+00:00,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.","ICLR 2021; the ETHICS dataset is available at
  https://github.com/hendrycks/ethics/",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.CL', 'cs.LG']"
https://arxiv.org/abs/1303.1516v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1303.1494v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.3107v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1706.01303v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1906.10536v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2004.08599v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.02704v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.05363v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.08611v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/9508102v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/0305033v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/0909.0901v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1106.4871v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1206.4613v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1210.1785v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1211.4957v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1302.4970v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1302.6837v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1303.1488v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.1516v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.2357v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.2713v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.2751v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1408.1485v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1601.05977v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1604.04315v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1703.01908v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1707.08476v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1711.01927v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1804.02929v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1806.04915v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.02217v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.06510v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2003.03181v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.07703v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.00280v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.12278v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.05818v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.10436v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.04787v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.10018v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.00868v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.05058v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/0203013v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/0205078v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/0510079v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/0906.4321v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1006.1563v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1012.5705v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1106.0241v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1209.4838v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1301.6707v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1302.3568v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1302.4978v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1303.1458v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1303.5719v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1303.5720v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.1515v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.2376v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1304.2759v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1310.1328v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1310.1863v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1407.5380v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1407.7189v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1410.8233v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1505.00399v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1512.05849v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1512.07943v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1604.06963v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1607.06759v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1701.01487v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1701.08306v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1705.05254v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1706.06906v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.06166v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.06275v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.06692v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1710.07075v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1711.00363v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1711.08378v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1712.06440v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1802.07228v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1806.07552v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1807.00401v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1807.06142v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.11116v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.02216v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.04814v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.10144v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.03425v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.03606v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1905.03899v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1905.09130v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1911.04870v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2003.00260v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2003.05370v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2005.01908v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2005.13635v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2008.04096v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.12439v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.06005v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.07195v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.11705v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.06704v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.12047v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.00311v4,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.00834v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.07716v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.08029v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.06312v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.06602v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.15171v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.15746v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.03360v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.04235v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.12207v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.13668v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.03360v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.09586v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.05486v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.09672v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.01311v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.09240v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.15907v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.08156v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.09478v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2112.10190v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.00764v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.08115v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.11117v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.11441v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.12566v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.00905v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.03468v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.09201v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.03044v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.11812v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.11831v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.13477v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.08345v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.14426v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.00711v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.05170v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.13160v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.15111v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.00608v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.05125v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.08340v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.08906v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/0606010v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1307.7127v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1512.04021v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.03981v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.09516v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2004.11434v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.06133v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.07852v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.14502v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.03172v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.02790v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.10553v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.10256v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.12878v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.03427v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/9505103v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/cs/9709101v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1401.4600v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1602.04290v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1707.05858v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1807.07991v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.10187v6,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.01484v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.05282v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.09595v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.08753v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.00251v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.11740v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.05186v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.06410v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.00403v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.10804v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.03896v1,Consequences of Misaligned AI,"['Simon Zhuang', 'Dylan Hadfield-Menell']",2021-02-07 19:34:04+00:00,arxiv,...,dd8005c63777c23643ce8c44eed28d6b,html,markdownify,2021-02-07 19:34:04+00:00,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,,NeurIPS 2020,,cs.AI,['cs.AI']
https://arxiv.org/abs/2103.12558v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.12983v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.15551v7,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.03741v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.00691v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.06009v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.13249v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.06071v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.13734v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2112.07773v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.09694v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.04092v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.05983v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.01437v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.07612v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.12749v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.13728v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.08932v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.12503v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.06590v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.13578v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.13873v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.15767v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2211.01817v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2211.03157v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1401.3825v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1612.07896v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.04649v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2003.04080v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.00177v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.02776v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.03188v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.09360v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.07928v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.15906v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/0803.2981v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/0903.4513v7,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1003.1343v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1212.1625v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1307.2191v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1401.3426v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1411.2842v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1512.07942v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1708.08611v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1803.03146v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1808.04468v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.07871v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1903.03171v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.10386v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1906.00429v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.03848v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.10508v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1912.00747v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1912.08786v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2001.06528v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.04202v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.12156v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2003.01593v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2005.01539v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.08140v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.09181v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.10720v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.09297v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.14244v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.16089v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.08302v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.11190v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2009.13772v4,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.04112v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.09468v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.15578v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.04328v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.01365v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.09938v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.10033v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.13016v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.14536v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.07574v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.09343v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.09677v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.10248v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.12142v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.00163v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.04893v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.12547v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.00655v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.03927v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.04696v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.01915v4,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.05383v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.06692v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.13270v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.14052v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.14414v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.06217v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.07804v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.09404v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2108.11463v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.04504v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.08904v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.13916v5,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.01770v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.03175v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.08412v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.12588v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.04158v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.04838v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.04916v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.11212v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.13365v4,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.15366v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2112.08438v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.02759v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.00161v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.01351v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.02540v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.03597v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.05607v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.06264v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.07364v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.10153v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.01884v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.03668v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.08492v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.08594v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.11677v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.03361v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.04681v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.06407v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.07254v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.08324v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.04279v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.10785v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.13743v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.15948v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.00259v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.00364v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.02628v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.04436v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.06091v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.08364v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2206.13498v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.08651v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.09712v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.10050v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.13834v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2207.14378v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.04714v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.12645v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2208.13885v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.07636v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.10341v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.13046v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.14876v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2209.15157v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.03230v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.03729v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.04964v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.10999v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2210.17368v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1110.2765v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1206.5290v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1810.12644v4,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1811.08549v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.00190v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1901.08579v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.09024v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.12004v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.12134v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1905.06876v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.02140v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2001.08525v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.05671v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2006.12136v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.08911v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2007.12173v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.05769v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2010.07877v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.06057v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.02032v5,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2101.11832v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.02454v12,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.04527v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.06362v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.07152v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.09180v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2102.12962v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.00082v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2103.02354v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2104.04147v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.00884v3,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2105.13431v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.10394v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.11022v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2106.11039v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.08574v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.09045v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2107.14093v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.08273v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.08322v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2110.08731v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2111.13786v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2112.03575v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.04200v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2201.06619v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.00343v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.04943v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.05302v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2202.10122v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.02481v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.06555v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.06690v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.08465v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2203.16497v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.00323v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.02889v2,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.05151v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.09852v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2204.12822v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.02850v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.07722v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2205.10232v1,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1711.00363,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2012.02671,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1904.06866,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2002.01059,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1806.00610,Between Progress and Potential Impact of AI: the Neglected Dimensions,"['Fernando MartÃ­nez-Plumed', 'Shahar Avin', 'Miles Brundage', 'Allan Dafoe', 'Sean Ã hÃigeartaigh', 'JosÃ© HernÃ¡ndez-Orallo']",2018-06-02 09:21:12+00:00,arxiv,...,366f61b6e0bb5dbf6ef82971c52640f6,html,markdownify,2022-07-02 09:54:55+00:00,"We reframe the analysis of progress in AI by incorporating into an overall framework both the task performance of a system, and the time and resource costs incurred in the development and deployment of the system. These costs include: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, and (what kind of) time. These costs are distributed over the life cycle of the system, and may place differing demands on different developers and users. The multidimensional performance and cost space we present can be collapsed to a single utility metric that measures the value of the system for different stakeholders. Even without a single utility function, AI advances can be generically assessed by whether they expand the Pareto surface. We label these types of costs as neglected dimensions of AI progress, and explore them using four case studies: Alpha* (Go, Chess, and other board games), ALE (Atari games), ImageNet (Image classification) and Virtual Personal Assistants (Siri, Alexa, Cortana, and Google Assistant). This broader model of progress in AI will lead to novel ways of estimating the potential societal use and impact of an AI system, and the establishment of milestones for future progress.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/2011.06275,Performance of Bounded-Rational Agents With the Ability to Self-Modify,"['Jakub TÄtek', 'Marek Sklenka', 'TomÃ¡Å¡ GavenÄiak']",2020-11-12 09:25:08+00:00,arxiv,...,9efc199d383b3c914ab941b3e19cf2a9,html,markdownify,2021-01-18 09:55:26+00:00,"Self-modification of agents embedded in complex environments is hard to avoid, whether it happens via direct means (e.g. own code modification) or indirectly (e.g. influencing the operator, exploiting bugs or the environment). It has been argued that intelligent agents have an incentive to avoid modifying their utility function so that their future instances work towards the same goals.   Everitt et al. (2016) formally show that providing an option to self-modify is harmless for perfectly rational agents. We show that this result is no longer true for agents with bounded rationality. In such agents, self-modification may cause exponential deterioration in performance and gradual misalignment of a previously aligned agent. We investigate how the size of this effect depends on the type and magnitude of imperfections in the agent's rationality (1-4 below). We also discuss model assumptions and the wider problem and framing space.   We examine four ways in which an agent can be bounded-rational: it either (1) doesn't always choose the optimal action, (2) is not perfectly aligned with human values, (3) has an inaccurate model of the environment, or (4) uses the wrong temporal discounting factor. We show that while in the cases (2)-(4) the misalignment caused by the agent's imperfection does not increase over time, with (1) the misalignment may grow exponentially.",Fixed minor problems; To appear in SafeAI @ AAAI 2021,,,cs.AI,"['cs.AI', 'cs.CY']"
https://arxiv.org/abs/1711.05541,Good and safe uses of AI Oracles,"['Stuart Armstrong', ""Xavier O'Rorke""]",2017-11-15 12:47:17+00:00,arxiv,...,56f83ea9b9ffb5ec3c120c7eb9d8490c,html,markdownify,2018-06-05 11:13:48+00:00,"It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.","11 pages, 2 figures",,,cs.AI,['cs.AI']
https://arxiv.org/abs/1801.03737,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1812.05979,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2011.08820,REALab: An Embedded Perspective on Tampering,"['Ramana Kumar', 'Jonathan Uesato', 'Richard Ngo', 'Tom Everitt', 'Victoria Krakovna', 'Shane Legg']",2020-11-17 18:37:20+00:00,arxiv,...,dc213dca88c03e269d285eeb78c0d816,html,markdownify,2020-11-17 18:37:20+00:00,"This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",,,,cs.LG,"['cs.LG', 'cs.AI']"
https://arxiv.org/abs/1705.10720,Low Impact Artificial Intelligences,"['Stuart Armstrong', 'Benjamin Levinstein']",2017-05-30 16:15:16+00:00,arxiv,...,8ca6f14d7837e0c4b7fde9551c1d473b,html,markdownify,2017-05-30 16:15:16+00:00,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1712.06365,Indifference' methods for managing agent rewards,"['Stuart Armstrong', ""Xavier O'Rourke""]",2017-12-18 12:28:45+00:00,arxiv,...,d59ec4762a4aac48cb3a3caf3a66be34,html,markdownify,2018-06-05 11:10:23+00:00,"`Indifference' refers to a class of methods used to control reward based agents. Indifference techniques aim to achieve one or more of three distinct goals: rewards dependent on certain events (without the agent being motivated to manipulate the probability of those events), effective disbelief (where agents behave as if particular events could never happen), and seamless transition from one reward function to another (with the agent acting as if this change is unanticipated). This paper presents several methods for achieving these goals in the POMDP setting, establishing their uses, strengths, and requirements. These methods of control work even when the implications of the agent's reward are otherwise not fully understood.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1411.1373,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/2109.10996,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1610.07997,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1606.07092,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.04534,The Role of Cooperation in Responsible AI Development,"['Amanda Askell', 'Miles Brundage', 'Gillian Hadfield']",2019-07-10 06:51:04+00:00,arxiv,...,af45c5295eea0e8919156c7b309f1978,html,markdownify,2019-07-10 06:51:04+00:00,"In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.","23 pages, 1 table",,,cs.CY,"['cs.CY', 'cs.AI', 'K.4.1; K.1']"
https://arxiv.org/abs/1105.3821,Ontological Crises in Artificial Agents' Value Systems,['Peter de Blanc'],2011-05-19 09:32:46+00:00,arxiv,...,f4003ea83019b0a6362eb8eacef54f2f,html,markdownify,2011-05-19 09:32:46+00:00,"Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals.   We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",,,,cs.AI,['cs.AI']
https://arxiv.org/abs/1908.07613,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1709.06166,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,
https://arxiv.org/abs/1907.09273,Why Build an Assistant in Minecraft?,"['Arthur Szlam', 'Jonathan Gray', 'Kavya Srinet', 'Yacine Jernite', 'Armand Joulin', 'Gabriel Synnaeve', 'Douwe Kiela', 'Haonan Yu', 'Zhuoyuan Chen', 'Siddharth Goyal', 'Demi Guo', 'Danielle Rothermel', 'C. Lawrence Zitnick', 'Jason Weston']",2019-07-22 12:32:15+00:00,arxiv,...,d31afb79cc3303878666bb2913fa3058,html,markdownify,2019-07-25 21:52:08+00:00,"In this document we describe a rationale for a research program aimed at building an open ""assistant"" in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.",,,,cs.AI,"['cs.AI', 'cs.CL']"
https://arxiv.org/abs/1602.04019,n/a,n/a,n/a,arxiv,...,274b68192b056e268f128ff63bfcd4a4,,,,,,,,,